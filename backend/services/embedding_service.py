import hashlib
import json
from pathlib import Path
from typing import Dict, List, Optional

from backend.core.config import settings
from backend.core.exceptions import GeminiAPIError
from backend.core.logging import get_logger
from backend import models

logger = get_logger(__name__)

# Gemini embedding model - text-embedding-004 produces 768-dim vectors
EMBEDDING_MODEL = "text-embedding-004"
EMBEDDING_DIM = 768  # text-embedding-004 produces 768 dimensions

# Cost optimization settings
MAX_EMBEDDING_BATCH_SIZE = 100  # Max texts per API call
MAX_TEXT_LENGTH = 4000  # Truncate to reduce tokens (saves ~50% vs 8000)
EMBEDDING_CACHE_DIR = Path("/tmp/vragent_embedding_cache")

# Smart filtering - skip code that's unlikely to have security issues
SKIP_PATTERNS = {
    # Test files
    "test_", "_test.py", ".spec.ts", ".test.ts", ".spec.js", ".test.js",
    # Config/data files  
    "__init__.py", "conftest.py", "setup.py",
    # Generated code patterns
    "# auto-generated", "# generated by", "// auto-generated",
}

# Prioritize security-relevant code
SECURITY_KEYWORDS = {
    "password", "secret", "token", "key", "auth", "credential",
    "eval", "exec", "shell", "subprocess", "command", "sql",
    "inject", "sanitize", "escape", "encode", "decode", "encrypt",
    "decrypt", "hash", "sign", "verify", "certificate", "ssl", "tls",
    "cookie", "session", "jwt", "oauth", "permission", "role", "admin",
    "input", "request", "query", "param", "header", "upload", "download",
    "file", "path", "url", "redirect", "cors", "csrf", "xss",
}


def _compute_hash(text: str) -> str:
    """Compute a hash for cache lookup."""
    return hashlib.sha256(text.encode()).hexdigest()[:16]


def _get_cache_path(text_hash: str) -> Path:
    """Get the cache file path for a given hash."""
    EMBEDDING_CACHE_DIR.mkdir(parents=True, exist_ok=True)
    return EMBEDDING_CACHE_DIR / f"{text_hash}.json"


def _load_from_cache(text_hash: str) -> Optional[List[float]]:
    """Load embedding from disk cache if exists."""
    cache_path = _get_cache_path(text_hash)
    if cache_path.exists():
        try:
            with open(cache_path, "r") as f:
                return json.load(f)
        except Exception:
            pass
    return None


def _save_to_cache(text_hash: str, embedding: List[float]) -> None:
    """Save embedding to disk cache."""
    try:
        cache_path = _get_cache_path(text_hash)
        with open(cache_path, "w") as f:
            json.dump(embedding, f)
    except Exception as e:
        logger.debug(f"Failed to cache embedding: {e}")


def _is_security_relevant(code: str) -> bool:
    """Check if code contains security-relevant patterns."""
    code_lower = code.lower()
    return any(keyword in code_lower for keyword in SECURITY_KEYWORDS)


def _should_skip_chunk(code: str, file_path: str = "") -> bool:
    """Determine if a code chunk should be skipped for embedding."""
    # Skip very short chunks (likely just imports or blank)
    if len(code.strip()) < 50:
        return True
    
    # Skip test files
    file_lower = file_path.lower()
    for pattern in SKIP_PATTERNS:
        if pattern in file_lower or pattern in code.lower()[:200]:
            return True
    
    return False


def prioritize_chunks(chunks: List[models.CodeChunk], max_chunks: int = 500) -> List[models.CodeChunk]:
    """
    Prioritize chunks for embedding based on security relevance.
    
    This reduces costs by only embedding the most security-relevant code.
    
    Args:
        chunks: All code chunks from the project
        max_chunks: Maximum number of chunks to embed
        
    Returns:
        Prioritized list of chunks (most security-relevant first)
    """
    if len(chunks) <= max_chunks:
        return chunks
    
    # Score chunks by security relevance
    scored = []
    for chunk in chunks:
        if _should_skip_chunk(chunk.code, chunk.file_path):
            continue
        
        # Count security keywords
        code_lower = chunk.code.lower()
        score = sum(1 for kw in SECURITY_KEYWORDS if kw in code_lower)
        
        # Boost certain file types
        if any(ext in chunk.file_path for ext in [".py", ".js", ".ts", ".java", ".go"]):
            score += 1
        
        # Boost files with security-related names
        path_lower = chunk.file_path.lower()
        if any(term in path_lower for term in ["auth", "security", "crypto", "api", "route", "handler"]):
            score += 3
        
        scored.append((score, chunk))
    
    # Sort by score (descending) and take top chunks
    scored.sort(key=lambda x: x[0], reverse=True)
    selected = [chunk for _, chunk in scored[:max_chunks]]
    
    logger.info(f"Prioritized {len(selected)} of {len(chunks)} chunks for embedding (saved {len(chunks) - len(selected)} API calls)")
    return selected


async def embed_texts(texts: List[str], use_cache: bool = True) -> List[List[float]]:
    """
    Generate embeddings for text snippets using Gemini API with caching.
    
    Cost optimization features:
    - Disk caching to avoid re-embedding identical code
    - Text truncation to reduce token usage
    - Batch processing to minimize API calls
    
    Falls back to zero vectors if API key not configured or on failure.
    
    Args:
        texts: List of text strings to embed
        use_cache: Whether to use disk caching (default True)
        
    Returns:
        List of embedding vectors (768 dimensions each for text-embedding-004)
    """
    if not texts:
        return []
    
    if not settings.gemini_api_key:
        logger.debug("Gemini API key not configured, returning zero vectors")
        return [[0.0] * EMBEDDING_DIM for _ in texts]
    
    # Check cache first
    results: Dict[int, List[float]] = {}
    texts_to_embed: List[tuple[int, str, str]] = []  # (index, hash, text)
    
    for i, text in enumerate(texts):
        truncated = text[:MAX_TEXT_LENGTH]  # Truncate to save tokens
        text_hash = _compute_hash(truncated)
        
        if use_cache:
            cached = _load_from_cache(text_hash)
            if cached:
                results[i] = cached
                continue
        
        texts_to_embed.append((i, text_hash, truncated))
    
    cache_hits = len(texts) - len(texts_to_embed)
    if cache_hits > 0:
        logger.info(f"Embedding cache: {cache_hits} hits, {len(texts_to_embed)} misses")
    
    if not texts_to_embed:
        return [results[i] for i in range(len(texts))]
    
    # Use Google GenAI SDK
    try:
        from google import genai
        
        client = genai.Client(api_key=settings.gemini_api_key)
        
        # Process in batches to avoid API limits
        for batch_start in range(0, len(texts_to_embed), MAX_EMBEDDING_BATCH_SIZE):
            batch = texts_to_embed[batch_start:batch_start + MAX_EMBEDDING_BATCH_SIZE]
            
            for idx, text_hash, text in batch:
                try:
                    response = client.models.embed_content(
                        model=EMBEDDING_MODEL,
                        contents=text
                    )
                    # Handle response - embeddings is a list of ContentEmbedding objects
                    if response.embeddings and len(response.embeddings) > 0:
                        embedding = list(response.embeddings[0].values)
                    else:
                        embedding = [0.0] * EMBEDDING_DIM
                    results[idx] = embedding
                    
                    # Cache the result
                    if use_cache and embedding:
                        _save_to_cache(text_hash, embedding)
                        
                except Exception as e:
                    logger.warning(f"Failed to embed text {idx}: {e}")
                    results[idx] = [0.0] * EMBEDDING_DIM
                    
    except ImportError:
        logger.error("google-genai package not installed. Run: pip install google-genai")
        for idx, _, _ in texts_to_embed:
            results[idx] = [0.0] * EMBEDDING_DIM
    except Exception as e:
        logger.error(f"Gemini embedding error: {e}")
        for idx, _, _ in texts_to_embed:
            if idx not in results:
                results[idx] = [0.0] * EMBEDDING_DIM
    
    # Fill any missing results
    for i in range(len(texts)):
        if i not in results:
            results[i] = [0.0] * EMBEDDING_DIM
    
    api_calls = len(texts_to_embed)
    logger.info(f"Generated {len(texts_to_embed)} embeddings via {api_calls} API call(s)")
    
    return [results[i] for i in range(len(texts))]


async def enrich_code_chunks(
    chunks: List[models.CodeChunk], 
    prioritize: bool = True,
    max_chunks: int = 500
) -> List[models.CodeChunk]:
    """
    Add embeddings to code chunks with cost optimization.
    
    Args:
        chunks: List of CodeChunk models to enrich
        prioritize: Whether to prioritize security-relevant chunks (saves money)
        max_chunks: Maximum chunks to embed when prioritizing
        
    Returns:
        Same list of chunks with embeddings populated
    """
    if not chunks:
        return chunks
    
    # Prioritize chunks to reduce API costs
    if prioritize and len(chunks) > max_chunks:
        priority_chunks = prioritize_chunks(chunks, max_chunks)
        priority_set = set(id(c) for c in priority_chunks)
        
        # Get embeddings only for priority chunks
        texts = [c.code for c in priority_chunks]
        embeddings = await embed_texts(texts)
        
        # Map embeddings back and set zero vectors for non-priority
        emb_map = {id(c): emb for c, emb in zip(priority_chunks, embeddings)}
        for chunk in chunks:
            if id(chunk) in emb_map:
                chunk.embedding = emb_map[id(chunk)]
            else:
                chunk.embedding = [0.0] * EMBEDDING_DIM
    else:
        embeddings = await embed_texts([c.code for c in chunks])
        for chunk, emb in zip(chunks, embeddings):
            chunk.embedding = emb
    
    logger.debug(f"Enriched {len(chunks)} code chunks with embeddings")
    return chunks
