import asyncio
import hashlib
import json
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor

from backend.core.config import settings
from backend.core.exceptions import GeminiAPIError
from backend.core.logging import get_logger
from backend import models

logger = get_logger(__name__)

# Gemini embedding model - gemini-embedding-001 supports 256-3072 dimensions via MRL
# Using 768 dims for good balance of quality and storage (default is 3072)
EMBEDDING_MODEL = "gemini-embedding-001"
EMBEDDING_DIM = 768  # Can be 256, 512, 768, 1536, or 3072

# Cost optimization settings
MAX_EMBEDDING_BATCH_SIZE = 100  # Max texts per API call
MAX_TEXT_LENGTH = 4000  # Truncate to reduce tokens (saves ~50% vs 8000)
EMBEDDING_CACHE_DIR = Path("/tmp/vragent_embedding_cache")

# Performance settings
PARALLEL_EMBEDDING_WORKERS = 4  # Concurrent API calls for speed
EMBEDDING_TIMEOUT = 30  # Seconds per embedding request

# Large codebase handling
ADAPTIVE_EMBEDDING_THRESHOLD = 1000  # Start adaptive mode above this many chunks
HIGH_PRIORITY_PERCENTAGE = 0.3  # Embed top 30% when above threshold

# Smart filtering - skip code that's unlikely to have security issues
SKIP_PATTERNS = {
    # Test files
    "test_", "_test.py", ".spec.ts", ".test.ts", ".spec.js", ".test.js",
    # Config/data files  
    "__init__.py", "conftest.py", "setup.py",
    # Generated code patterns
    "# auto-generated", "# generated by", "// auto-generated",
}

# Prioritize security-relevant code
SECURITY_KEYWORDS = {
    "password", "secret", "token", "key", "auth", "credential",
    "eval", "exec", "shell", "subprocess", "command", "sql",
    "inject", "sanitize", "escape", "encode", "decode", "encrypt",
    "decrypt", "hash", "sign", "verify", "certificate", "ssl", "tls",
    "cookie", "session", "jwt", "oauth", "permission", "role", "admin",
    "input", "request", "query", "param", "header", "upload", "download",
    "file", "path", "url", "redirect", "cors", "csrf", "xss",
}


def _compute_hash(text: str) -> str:
    """Compute a hash for cache lookup."""
    return hashlib.sha256(text.encode()).hexdigest()[:16]


def _get_cache_path(text_hash: str) -> Path:
    """Get the cache file path for a given hash."""
    EMBEDDING_CACHE_DIR.mkdir(parents=True, exist_ok=True)
    return EMBEDDING_CACHE_DIR / f"{text_hash}.json"


def _load_from_cache(text_hash: str) -> Optional[List[float]]:
    """Load embedding from disk cache if exists."""
    cache_path = _get_cache_path(text_hash)
    if cache_path.exists():
        try:
            with open(cache_path, "r") as f:
                return json.load(f)
        except Exception:
            pass
    return None


def _save_to_cache(text_hash: str, embedding: List[float]) -> None:
    """Save embedding to disk cache."""
    try:
        cache_path = _get_cache_path(text_hash)
        with open(cache_path, "w") as f:
            json.dump(embedding, f)
    except Exception as e:
        logger.debug(f"Failed to cache embedding: {e}")


def _is_security_relevant(code: str) -> bool:
    """Check if code contains security-relevant patterns."""
    code_lower = code.lower()
    return any(keyword in code_lower for keyword in SECURITY_KEYWORDS)


def _should_skip_chunk(code: str, file_path: str = "") -> bool:
    """Determine if a code chunk should be skipped for embedding."""
    # Skip very short chunks (likely just imports or blank)
    if len(code.strip()) < 50:
        return True
    
    # Skip test files
    file_lower = file_path.lower()
    for pattern in SKIP_PATTERNS:
        if pattern in file_lower or pattern in code.lower()[:200]:
            return True
    
    return False


def prioritize_chunks(chunks: List[models.CodeChunk], max_chunks: int = 500) -> List[models.CodeChunk]:
    """
    Prioritize chunks for embedding based on security relevance.
    
    This reduces costs by only embedding the most security-relevant code.
    Uses a sophisticated scoring system that considers:
    - Security keyword density
    - File path indicators (auth, api, etc.)
    - Code complexity signals
    - Entry point indicators
    
    Args:
        chunks: All code chunks from the project
        max_chunks: Maximum number of chunks to embed
        
    Returns:
        Prioritized list of chunks (most security-relevant first)
    """
    if len(chunks) <= max_chunks:
        return chunks
    
    # Adaptive scaling for very large codebases
    if len(chunks) > ADAPTIVE_EMBEDDING_THRESHOLD:
        # Scale down max_chunks proportionally but keep minimum coverage
        scale_factor = ADAPTIVE_EMBEDDING_THRESHOLD / len(chunks)
        adaptive_max = max(
            int(max_chunks * scale_factor * 1.5),  # Scale with boost
            int(len(chunks) * HIGH_PRIORITY_PERCENTAGE),  # At least top 30%
            200  # Minimum baseline
        )
        max_chunks = min(max_chunks, adaptive_max)
        logger.info(f"Adaptive embedding: {len(chunks)} chunks -> limiting to {max_chunks} (scale factor: {scale_factor:.2f})")
    
    # Score chunks by security relevance with enhanced algorithm
    scored = []
    for chunk in chunks:
        score, skip_reason = _score_chunk_for_embedding(chunk)
        if skip_reason:
            continue
        scored.append((score, chunk))
    
    # Sort by score (descending) and take top chunks
    scored.sort(key=lambda x: x[0], reverse=True)
    selected = [chunk for _, chunk in scored[:max_chunks]]
    
    # Log distribution for debugging
    if scored:
        top_score = scored[0][0] if scored else 0
        threshold_score = scored[max_chunks - 1][0] if len(scored) >= max_chunks else 0
        logger.info(
            f"Prioritized {len(selected)} of {len(chunks)} chunks for embedding "
            f"(score range: {threshold_score:.1f}-{top_score:.1f}, saved {len(chunks) - len(selected)} API calls)"
        )
    
    return selected


def _score_chunk_for_embedding(chunk: models.CodeChunk) -> Tuple[float, Optional[str]]:
    """
    Score a code chunk for embedding priority.
    
    Returns:
        Tuple of (score, skip_reason) where skip_reason is None if chunk should be processed
    """
    code = chunk.code
    file_path = chunk.file_path
    
    # Skip conditions
    if _should_skip_chunk(code, file_path):
        return 0.0, "Skipped by filter"
    
    # Base score
    score = 0.0
    code_lower = code.lower()
    path_lower = file_path.lower()
    
    # 1. Security keyword scoring (primary factor)
    keyword_hits = 0
    for kw in SECURITY_KEYWORDS:
        count = code_lower.count(kw)
        if count > 0:
            keyword_hits += min(count, 3)  # Cap per-keyword contribution
    score += keyword_hits * 2
    
    # 2. High-value security patterns (boost significantly)
    high_value_patterns = [
        (r'(?:password|passwd|pwd)\s*=', 5, "password assignment"),
        (r'(?:api[_-]?key|secret[_-]?key|auth[_-]?token)\s*=', 5, "credential assignment"),
        (r'execute\s*\([^)]*\+', 4, "dynamic query execution"),
        (r'eval\s*\(|exec\s*\(', 4, "code execution"),
        (r'subprocess|os\.system|shell\s*=\s*true', 4, "command execution"),
        (r'sql.*?["\'].*?\+|f["\'].*?select|f["\'].*?insert', 4, "SQL construction"),
        (r'\.innerHTML\s*=|document\.write', 3, "DOM manipulation"),
        (r'pickle\.load|yaml\.load|deserialize', 3, "deserialization"),
        (r'verify\s*=\s*false|cert\s*=\s*false', 3, "cert verification disabled"),
        (r'@app\.route|@router\.|@api\.|\.get\(|\.post\(', 2, "API endpoint"),
        (r'def\s+(?:login|auth|verify|validate)', 2, "auth function"),
        (r'crypto|cipher|hash|hmac|bcrypt|argon', 2, "cryptography"),
    ]
    
    for pattern, boost, _ in high_value_patterns:
        if re.search(pattern, code_lower):
            score += boost
    
    # 3. File path scoring
    high_value_paths = [
        ("auth", 4), ("login", 4), ("security", 4), ("crypto", 4),
        ("api", 3), ("route", 3), ("handler", 3), ("controller", 3),
        ("middleware", 3), ("validator", 2), ("sanitize", 2),
        ("admin", 2), ("user", 2), ("permission", 2), ("role", 2),
        ("upload", 2), ("download", 2), ("file", 1), ("config", 1),
    ]
    
    for term, boost in high_value_paths:
        if term in path_lower:
            score += boost
    
    # 4. Language boost (some languages need more attention)
    if any(ext in file_path for ext in [".py", ".php", ".rb", ".js"]):
        score += 1  # Interpreted languages often have more vulns
    
    # 5. Code complexity indicators
    if len(code) > 500:
        score += 1  # Longer functions more likely to have issues
    
    # Penalize likely low-value code
    if re.search(r'^(?:import|from|require|use)\s', code):
        if code.count('\n') < 5:  # Pure import block
            score -= 2
    
    if re.search(r'(?:test_|_test|spec_|_spec|mock|stub)', path_lower):
        score -= 3  # Test files less priority
    
    return max(score, 0.1), None  # Ensure minimum positive score


async def embed_texts(texts: List[str], use_cache: bool = True) -> List[List[float]]:
    """
    Generate embeddings for text snippets using Gemini API with caching.
    
    Cost optimization features:
    - Disk caching to avoid re-embedding identical code
    - Text truncation to reduce token usage
    - Batch processing to minimize API calls
    
    Falls back to zero vectors if API key not configured or on failure.
    
    Args:
        texts: List of text strings to embed
        use_cache: Whether to use disk caching (default True)
        
    Returns:
        List of embedding vectors (768 dimensions each for text-embedding-004)
    """
    if not texts:
        return []
    
    if not settings.gemini_api_key:
        logger.debug("Gemini API key not configured, returning zero vectors")
        return [[0.0] * EMBEDDING_DIM for _ in texts]
    
    # Check cache first
    results: Dict[int, List[float]] = {}
    texts_to_embed: List[tuple[int, str, str]] = []  # (index, hash, text)
    
    for i, text in enumerate(texts):
        truncated = text[:MAX_TEXT_LENGTH]  # Truncate to save tokens
        text_hash = _compute_hash(truncated)
        
        if use_cache:
            cached = _load_from_cache(text_hash)
            if cached:
                results[i] = cached
                continue
        
        texts_to_embed.append((i, text_hash, truncated))
    
    cache_hits = len(texts) - len(texts_to_embed)
    if cache_hits > 0:
        logger.info(f"Embedding cache: {cache_hits} hits, {len(texts_to_embed)} misses")
    
    if not texts_to_embed:
        return [results[i] for i in range(len(texts))]
    
    # Use Google GenAI SDK (new unified SDK) with PARALLEL batch processing
    try:
        from google import genai
        from google.genai import types
        
        client = genai.Client(api_key=settings.gemini_api_key)
        
        def embed_single(item: Tuple[int, str, str]) -> Tuple[int, str, List[float]]:
            """Embed a single text item - designed for parallel execution."""
            idx, text_hash, text = item
            try:
                response = client.models.embed_content(
                    model=EMBEDDING_MODEL,
                    contents=text,
                    config=types.EmbedContentConfig(
                        output_dimensionality=EMBEDDING_DIM
                    )
                )
                if response and response.embeddings:
                    embedding = list(response.embeddings[0].values)
                else:
                    embedding = [0.0] * EMBEDDING_DIM
                return idx, text_hash, embedding
            except Exception as e:
                logger.debug(f"Failed to embed text {idx}: {e}")
                return idx, text_hash, [0.0] * EMBEDDING_DIM
        
        # Process in parallel batches for MUCH faster embedding
        import time
        start_time = time.time()
        
        # Use ThreadPoolExecutor for parallel API calls
        with ThreadPoolExecutor(max_workers=PARALLEL_EMBEDDING_WORKERS) as executor:
            # Submit all embedding tasks
            futures = list(executor.map(embed_single, texts_to_embed, timeout=EMBEDDING_TIMEOUT * len(texts_to_embed)))
            
            for idx, text_hash, embedding in futures:
                results[idx] = embedding
                # Cache the result
                if use_cache and embedding and any(v != 0.0 for v in embedding[:5]):
                    _save_to_cache(text_hash, embedding)
        
        elapsed = time.time() - start_time
        rate = len(texts_to_embed) / elapsed if elapsed > 0 else 0
        logger.info(f"Parallel embedding: {len(texts_to_embed)} texts in {elapsed:.1f}s ({rate:.1f}/s)")
                    
    except ImportError:
        logger.error("google-genai package not installed. Run: pip install google-genai")
        for idx, _, _ in texts_to_embed:
            results[idx] = [0.0] * EMBEDDING_DIM
    except Exception as e:
        logger.error(f"Gemini embedding error: {e}")
        for idx, _, _ in texts_to_embed:
            if idx not in results:
                results[idx] = [0.0] * EMBEDDING_DIM
    
    # Fill any missing results
    for i in range(len(texts)):
        if i not in results:
            results[i] = [0.0] * EMBEDDING_DIM
    
    api_calls = len(texts_to_embed)
    logger.info(f"Generated {len(texts_to_embed)} embeddings via parallel API calls")
    
    return [results[i] for i in range(len(texts))]


async def enrich_code_chunks(
    chunks: List[models.CodeChunk], 
    prioritize: bool = True,
    max_chunks: int = 500
) -> List[models.CodeChunk]:
    """
    Add embeddings to code chunks with cost optimization.
    
    Args:
        chunks: List of CodeChunk models to enrich
        prioritize: Whether to prioritize security-relevant chunks (saves money)
        max_chunks: Maximum chunks to embed when prioritizing
        
    Returns:
        Same list of chunks with embeddings populated
    """
    if not chunks:
        return chunks
    
    # Prioritize chunks to reduce API costs
    if prioritize and len(chunks) > max_chunks:
        priority_chunks = prioritize_chunks(chunks, max_chunks)
        priority_set = set(id(c) for c in priority_chunks)
        
        # Get embeddings only for priority chunks
        texts = [c.code for c in priority_chunks]
        embeddings = await embed_texts(texts)
        
        # Map embeddings back and set zero vectors for non-priority
        emb_map = {id(c): emb for c, emb in zip(priority_chunks, embeddings)}
        for chunk in chunks:
            if id(chunk) in emb_map:
                chunk.embedding = emb_map[id(chunk)]
            else:
                chunk.embedding = [0.0] * EMBEDDING_DIM
    else:
        embeddings = await embed_texts([c.code for c in chunks])
        for chunk, emb in zip(chunks, embeddings):
            chunk.embedding = emb
    
    logger.debug(f"Enriched {len(chunks)} code chunks with embeddings")
    return chunks
