from typing import List, Optional

from sqlalchemy.orm import Session

from backend import models
from backend.core.config import settings
from backend.core.logging import get_logger

logger = get_logger(__name__)

# Cost optimization settings
MAX_FINDINGS_FOR_LLM = 20  # Limit LLM calls for large reports
MAX_CODE_SNIPPET_LENGTH = 2000  # Truncate code snippets
MAX_PROMPT_LENGTH = 4000  # Total prompt size limit

# Pre-built exploit templates for common vulnerabilities (no LLM needed)
EXPLOIT_TEMPLATES = {
    "eval": {
        "title": "Code Injection via eval()",
        "narrative": "The use of eval() with untrusted input allows arbitrary code execution. An attacker can inject malicious Python/JavaScript code through user-controlled input that reaches this eval() call.",
        "preconditions": "Attacker must be able to control input that flows into eval()",
        "impact": "Full code execution in the application context, potential RCE",
        "poc_outline": "1. Identify user input reaching eval()\n2. Inject payload: __import__('os').system('id')\n3. Verify command execution",
        "mitigation": "Remove eval() entirely. Use ast.literal_eval() for safe parsing, or implement a whitelist-based parser.",
    },
    "exec": {
        "title": "Code Injection via exec()",
        "narrative": "Similar to eval(), exec() executes arbitrary code. Any user-controlled data reaching exec() enables full code injection.",
        "preconditions": "User input must flow to exec() parameter",
        "impact": "Arbitrary code execution, potential server compromise",
        "poc_outline": "1. Trace data flow to exec()\n2. Inject: exec('import os; os.system(\"whoami\")')\n3. Confirm execution",
        "mitigation": "Eliminate exec() usage. Use safe alternatives like predefined functions or sandboxed execution.",
    },
    "shell=true": {
        "title": "Command Injection via shell=True",
        "narrative": "Using shell=True in subprocess calls enables shell metacharacter injection. Attackers can append commands using ; | & or other shell operators.",
        "preconditions": "User input concatenated into shell command string",
        "impact": "OS command execution, lateral movement, data exfiltration",
        "poc_outline": "1. Find user input in command string\n2. Inject: '; cat /etc/passwd'\n3. Verify command chaining works",
        "mitigation": "Use shell=False with argument list. Validate and sanitize all inputs. Consider shlex.quote().",
    },
    "sql": {
        "title": "SQL Injection Vulnerability",
        "narrative": "Direct string concatenation in SQL queries allows attackers to manipulate query logic, extract data, or modify the database.",
        "preconditions": "User input directly embedded in SQL string",
        "impact": "Data breach, authentication bypass, data manipulation, potential RCE via xp_cmdshell",
        "poc_outline": "1. Test input with single quote: '\n2. Try UNION SELECT: ' UNION SELECT username,password FROM users--\n3. Extract sensitive data",
        "mitigation": "Use parameterized queries or ORM. Never concatenate user input into SQL strings.",
    },
    "xss": {
        "title": "Cross-Site Scripting (XSS)",
        "narrative": "Unsanitized user input rendered in HTML enables script injection. Attackers can steal session cookies, perform actions as the user, or redirect to phishing sites.",
        "preconditions": "User input reflected in page without encoding",
        "impact": "Session hijacking, credential theft, malware distribution",
        "poc_outline": "1. Inject: <script>alert(document.cookie)</script>\n2. Or: <img src=x onerror=alert(1)>\n3. Verify script execution",
        "mitigation": "HTML-encode all output. Use Content-Security-Policy headers. Implement input validation.",
    },
    "secret": {
        "title": "Hardcoded Secret Exposure",
        "narrative": "Credentials or API keys committed to source code can be extracted by anyone with repository access, including via leaked git history.",
        "preconditions": "Access to source code or git history",
        "impact": "Unauthorized access to external services, data breach, financial loss",
        "poc_outline": "1. Search git history: git log -p | grep -i password\n2. Use extracted credentials\n3. Access protected resources",
        "mitigation": "Remove secrets from code. Use environment variables or secret management (Vault, AWS Secrets Manager). Rotate compromised credentials immediately.",
    },
    "path_traversal": {
        "title": "Path Traversal / Directory Traversal",
        "narrative": "User-controlled file paths without validation allow reading or writing arbitrary files using ../ sequences.",
        "preconditions": "User input used in file path operations",
        "impact": "Read sensitive files (/etc/passwd, config files), potential RCE via file upload",
        "poc_outline": "1. Request: ?file=../../../etc/passwd\n2. Check for file contents in response\n3. Escalate to write if possible",
        "mitigation": "Validate paths against whitelist. Use os.path.basename() and resolve canonical paths. Implement chroot or sandboxing.",
    },
}


def _get_template_for_finding(finding: models.Finding) -> Optional[dict]:
    """Check if a finding matches a pre-built template."""
    summary_lower = finding.summary.lower()
    details_str = str(finding.details).lower() if finding.details else ""
    combined = summary_lower + " " + details_str
    
    for key, template in EXPLOIT_TEMPLATES.items():
        if key in combined:
            return template
    
    # Check finding type
    if finding.type == "secret":
        return EXPLOIT_TEMPLATES["secret"]
    
    return None


def _build_prompt(finding: models.Finding, code_snippet: str) -> str:
    """Build the prompt for exploitability analysis with length limits."""
    # Truncate code snippet to save tokens
    truncated_code = code_snippet[:MAX_CODE_SNIPPET_LENGTH]
    if len(code_snippet) > MAX_CODE_SNIPPET_LENGTH:
        truncated_code += "\n... (truncated)"
    
    prompt = (
        "You are a security analyst. Given the finding details and code snippet, provide a concise analysis:\n"
        "1. How an attacker could exploit this (2-3 sentences)\n"
        "2. Key preconditions required\n"
        "3. Potential impact\n"
        "4. One mitigation recommendation\n\n"
        "Do not provide runnable exploit code. Be concise.\n\n"
        f"Finding: {finding.summary}\n"
        f"Severity: {finding.severity}\n"
        f"File: {finding.file_path}\n"
        f"Code:\n{truncated_code}"
    )
    
    # Enforce total prompt length
    return prompt[:MAX_PROMPT_LENGTH]


async def call_gemini(prompt: str) -> str:
    """
    Call Gemini API for exploitability analysis using the new google-genai SDK.
    
    Args:
        prompt: The analysis prompt to send
        
    Returns:
        Generated narrative or fallback message
    """
    if not settings.gemini_api_key:
        logger.debug("Gemini API key not configured, returning stub narrative")
        return "Gemini API key not configured. This is a stubbed exploitability narrative."
    
    try:
        from google import genai
        
        client = genai.Client(api_key=settings.gemini_api_key)
        
        response = client.models.generate_content(
            model=settings.gemini_model_id,
            contents=prompt
        )
        
        if response and response.text:
            logger.debug("Successfully generated exploitability narrative")
            return response.text
        
        logger.warning("Gemini returned no content")
        return "No exploitability narrative generated - no content returned."
        
    except ImportError:
        logger.error("google-genai package not installed. Run: pip install google-genai")
        return "google-genai package not installed. Please install it."
    except Exception as e:
        logger.error(f"Gemini API error: {e}")
        return f"Failed to contact Gemini: {str(e)}. Provide manual analysis based on finding details."


async def generate_exploit_scenarios(db: Session, report: models.Report) -> List[models.ExploitScenario]:
    """
    Generate exploitability scenarios for high/critical findings in a report.
    
    Cost optimization:
    - Uses pre-built templates for common vulnerabilities (no LLM call)
    - Limits LLM calls to MAX_FINDINGS_FOR_LLM
    - Prioritizes highest severity findings
    
    Args:
        db: Database session
        report: Report model to generate scenarios for
        
    Returns:
        List of created ExploitScenario models
    """
    findings = (
        db.query(models.Finding)
        .filter(models.Finding.scan_run_id == report.scan_run_id)
        .filter(models.Finding.severity.in_(["high", "critical"]))
        .all()
    )
    
    # Sort by severity (critical first)
    findings.sort(key=lambda f: 0 if f.severity == "critical" else 1)
    
    logger.info(f"Processing {len(findings)} high/critical findings for exploit scenarios")
    
    scenarios: List[models.ExploitScenario] = []
    llm_calls_made = 0
    template_uses = 0
    
    for finding in findings:
        # Check for pre-built template first (FREE - no LLM call!)
        template = _get_template_for_finding(finding)
        
        if template:
            # Use template directly
            scenario = models.ExploitScenario(
                report_id=report.id,
                finding_id=finding.id,
                severity=finding.severity,
                title=template["title"],
                narrative=template["narrative"],
                preconditions=template["preconditions"],
                impact=template["impact"],
                poc_outline=template["poc_outline"],
                mitigation_notes=template["mitigation"],
            )
            db.add(scenario)
            scenarios.append(scenario)
            template_uses += 1
            continue
        
        # Fall back to LLM for unique findings (with limit)
        if llm_calls_made >= MAX_FINDINGS_FOR_LLM:
            # Create basic scenario without LLM
            scenario = models.ExploitScenario(
                report_id=report.id,
                finding_id=finding.id,
                severity=finding.severity,
                title=f"Analysis of {finding.summary}",
                narrative="LLM analysis limit reached. Review this finding manually based on its severity and context.",
                preconditions="Review access requirements and attack surface.",
                impact=f"Severity: {finding.severity}. Impact depends on deployment context.",
                poc_outline="Manual security testing recommended.",
                mitigation_notes="Apply secure coding practices relevant to this finding type.",
            )
            db.add(scenario)
            scenarios.append(scenario)
            continue
        
        # Get code snippet
        chunk = (
            db.query(models.CodeChunk)
            .filter(models.CodeChunk.project_id == report.project_id)
            .filter(models.CodeChunk.file_path == finding.file_path)
            .first()
        )
        code_snippet = chunk.code if chunk else "Code snippet unavailable"
        
        # Call LLM
        narrative = await call_gemini(_build_prompt(finding, code_snippet))
        llm_calls_made += 1
        
        scenario = models.ExploitScenario(
            report_id=report.id,
            finding_id=finding.id,
            severity=finding.severity,
            title=f"Exploitability of {finding.summary}",
            narrative=narrative,
            preconditions="Review access requirements, pre-auth vs post-auth.",
            impact="Potential compromise depending on deployment context.",
            poc_outline="Outline manual testing steps; avoid providing exploit code.",
            mitigation_notes="Apply secure coding practices and patches.",
        )
        db.add(scenario)
        scenarios.append(scenario)
    
    db.commit()
    logger.info(
        f"Generated {len(scenarios)} exploit scenarios for report {report.id} "
        f"(templates: {template_uses}, LLM calls: {llm_calls_made})"
    )
    return scenarios
